{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf57.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 02:25:08 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '3Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000021DAE528550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000021DABFA6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0802, Accuracy:0.3729, Validation Loss:1.0769, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0758, Accuracy:0.3713, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0730, Accuracy:0.3943, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0725, Accuracy:0.3943, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0714, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0705, Accuracy:0.3943, Validation Loss:1.0702, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0688, Accuracy:0.3943, Validation Loss:1.0683, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0665, Accuracy:0.3943, Validation Loss:1.0657, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0628, Accuracy:0.3943, Validation Loss:1.0615, Validation Accuracy:0.3957\n",
    "Epoch #26: Loss:1.0577, Accuracy:0.3971, Validation Loss:1.0559, Validation Accuracy:0.4039\n",
    "Epoch #27: Loss:1.0505, Accuracy:0.3947, Validation Loss:1.0489, Validation Accuracy:0.4089\n",
    "Epoch #28: Loss:1.0421, Accuracy:0.4057, Validation Loss:1.0419, Validation Accuracy:0.4007\n",
    "Epoch #29: Loss:1.0339, Accuracy:0.4090, Validation Loss:1.0376, Validation Accuracy:0.4269\n",
    "Epoch #30: Loss:1.0315, Accuracy:0.4185, Validation Loss:1.0391, Validation Accuracy:0.4154\n",
    "Epoch #31: Loss:1.0275, Accuracy:0.4214, Validation Loss:1.0401, Validation Accuracy:0.4154\n",
    "Epoch #32: Loss:1.0280, Accuracy:0.4181, Validation Loss:1.0424, Validation Accuracy:0.4105\n",
    "Epoch #33: Loss:1.0297, Accuracy:0.4234, Validation Loss:1.0355, Validation Accuracy:0.4171\n",
    "Epoch #34: Loss:1.0263, Accuracy:0.4214, Validation Loss:1.0287, Validation Accuracy:0.4154\n",
    "Epoch #35: Loss:1.0238, Accuracy:0.4292, Validation Loss:1.0277, Validation Accuracy:0.4171\n",
    "Epoch #36: Loss:1.0214, Accuracy:0.4402, Validation Loss:1.0285, Validation Accuracy:0.4122\n",
    "Epoch #37: Loss:1.0206, Accuracy:0.4230, Validation Loss:1.0272, Validation Accuracy:0.4105\n",
    "Epoch #38: Loss:1.0199, Accuracy:0.4316, Validation Loss:1.0251, Validation Accuracy:0.4302\n",
    "Epoch #39: Loss:1.0194, Accuracy:0.4283, Validation Loss:1.0239, Validation Accuracy:0.4204\n",
    "Epoch #40: Loss:1.0175, Accuracy:0.4316, Validation Loss:1.0238, Validation Accuracy:0.4138\n",
    "Epoch #41: Loss:1.0173, Accuracy:0.4398, Validation Loss:1.0226, Validation Accuracy:0.4171\n",
    "Epoch #42: Loss:1.0153, Accuracy:0.4485, Validation Loss:1.0212, Validation Accuracy:0.4138\n",
    "Epoch #43: Loss:1.0148, Accuracy:0.4402, Validation Loss:1.0203, Validation Accuracy:0.4220\n",
    "Epoch #44: Loss:1.0144, Accuracy:0.4353, Validation Loss:1.0180, Validation Accuracy:0.4154\n",
    "Epoch #45: Loss:1.0138, Accuracy:0.4497, Validation Loss:1.0166, Validation Accuracy:0.4220\n",
    "Epoch #46: Loss:1.0139, Accuracy:0.4402, Validation Loss:1.0155, Validation Accuracy:0.4154\n",
    "Epoch #47: Loss:1.0112, Accuracy:0.4538, Validation Loss:1.0147, Validation Accuracy:0.4171\n",
    "Epoch #48: Loss:1.0113, Accuracy:0.4419, Validation Loss:1.0132, Validation Accuracy:0.4204\n",
    "Epoch #49: Loss:1.0110, Accuracy:0.4448, Validation Loss:1.0113, Validation Accuracy:0.4204\n",
    "Epoch #50: Loss:1.0120, Accuracy:0.4337, Validation Loss:1.0101, Validation Accuracy:0.4286\n",
    "Epoch #51: Loss:1.0107, Accuracy:0.4444, Validation Loss:1.0090, Validation Accuracy:0.4302\n",
    "Epoch #52: Loss:1.0064, Accuracy:0.4427, Validation Loss:1.0076, Validation Accuracy:0.4204\n",
    "Epoch #53: Loss:1.0063, Accuracy:0.4497, Validation Loss:1.0059, Validation Accuracy:0.4253\n",
    "Epoch #54: Loss:1.0031, Accuracy:0.4546, Validation Loss:1.0041, Validation Accuracy:0.4368\n",
    "Epoch #55: Loss:1.0010, Accuracy:0.4628, Validation Loss:1.0021, Validation Accuracy:0.4269\n",
    "Epoch #56: Loss:1.0001, Accuracy:0.4513, Validation Loss:1.0003, Validation Accuracy:0.4384\n",
    "Epoch #57: Loss:0.9994, Accuracy:0.4641, Validation Loss:0.9986, Validation Accuracy:0.4286\n",
    "Epoch #58: Loss:0.9950, Accuracy:0.4657, Validation Loss:0.9953, Validation Accuracy:0.4532\n",
    "Epoch #59: Loss:0.9931, Accuracy:0.4686, Validation Loss:0.9927, Validation Accuracy:0.4351\n",
    "Epoch #60: Loss:0.9941, Accuracy:0.4624, Validation Loss:0.9948, Validation Accuracy:0.4138\n",
    "Epoch #61: Loss:0.9944, Accuracy:0.4608, Validation Loss:0.9945, Validation Accuracy:0.4631\n",
    "Epoch #62: Loss:0.9978, Accuracy:0.4476, Validation Loss:0.9921, Validation Accuracy:0.4581\n",
    "Epoch #63: Loss:0.9962, Accuracy:0.4686, Validation Loss:1.0064, Validation Accuracy:0.4335\n",
    "Epoch #64: Loss:0.9935, Accuracy:0.4645, Validation Loss:1.0009, Validation Accuracy:0.4466\n",
    "Epoch #65: Loss:0.9942, Accuracy:0.4665, Validation Loss:0.9844, Validation Accuracy:0.4319\n",
    "Epoch #66: Loss:0.9841, Accuracy:0.4768, Validation Loss:0.9784, Validation Accuracy:0.4483\n",
    "Epoch #67: Loss:0.9822, Accuracy:0.4682, Validation Loss:0.9794, Validation Accuracy:0.4647\n",
    "Epoch #68: Loss:0.9796, Accuracy:0.4784, Validation Loss:0.9752, Validation Accuracy:0.4401\n",
    "Epoch #69: Loss:0.9774, Accuracy:0.4760, Validation Loss:0.9714, Validation Accuracy:0.4516\n",
    "Epoch #70: Loss:0.9751, Accuracy:0.4776, Validation Loss:0.9704, Validation Accuracy:0.4680\n",
    "Epoch #71: Loss:0.9704, Accuracy:0.4961, Validation Loss:0.9654, Validation Accuracy:0.4729\n",
    "Epoch #72: Loss:0.9702, Accuracy:0.4973, Validation Loss:0.9626, Validation Accuracy:0.4745\n",
    "Epoch #73: Loss:0.9668, Accuracy:0.5035, Validation Loss:0.9600, Validation Accuracy:0.4795\n",
    "Epoch #74: Loss:0.9627, Accuracy:0.5031, Validation Loss:0.9592, Validation Accuracy:0.4713\n",
    "Epoch #75: Loss:0.9654, Accuracy:0.5064, Validation Loss:0.9571, Validation Accuracy:0.4811\n",
    "Epoch #76: Loss:0.9584, Accuracy:0.5138, Validation Loss:0.9574, Validation Accuracy:0.4795\n",
    "Epoch #77: Loss:0.9575, Accuracy:0.4973, Validation Loss:0.9563, Validation Accuracy:0.4992\n",
    "Epoch #78: Loss:0.9540, Accuracy:0.5109, Validation Loss:0.9622, Validation Accuracy:0.4729\n",
    "Epoch #79: Loss:0.9604, Accuracy:0.5068, Validation Loss:0.9752, Validation Accuracy:0.4631\n",
    "Epoch #80: Loss:0.9662, Accuracy:0.5043, Validation Loss:0.9891, Validation Accuracy:0.4532\n",
    "Epoch #81: Loss:0.9688, Accuracy:0.4953, Validation Loss:0.9555, Validation Accuracy:0.4893\n",
    "Epoch #82: Loss:0.9540, Accuracy:0.5113, Validation Loss:0.9554, Validation Accuracy:0.4992\n",
    "Epoch #83: Loss:0.9563, Accuracy:0.5023, Validation Loss:0.9671, Validation Accuracy:0.4893\n",
    "Epoch #84: Loss:0.9605, Accuracy:0.4957, Validation Loss:0.9560, Validation Accuracy:0.4959\n",
    "Epoch #85: Loss:0.9600, Accuracy:0.5035, Validation Loss:0.9694, Validation Accuracy:0.4844\n",
    "Epoch #86: Loss:0.9650, Accuracy:0.5006, Validation Loss:0.9593, Validation Accuracy:0.4893\n",
    "Epoch #87: Loss:0.9573, Accuracy:0.5125, Validation Loss:0.9612, Validation Accuracy:0.4877\n",
    "Epoch #88: Loss:0.9577, Accuracy:0.5092, Validation Loss:0.9528, Validation Accuracy:0.4975\n",
    "Epoch #89: Loss:0.9534, Accuracy:0.5084, Validation Loss:0.9654, Validation Accuracy:0.4926\n",
    "Epoch #90: Loss:0.9630, Accuracy:0.5105, Validation Loss:0.9523, Validation Accuracy:0.5008\n",
    "Epoch #91: Loss:0.9510, Accuracy:0.5150, Validation Loss:0.9708, Validation Accuracy:0.4713\n",
    "Epoch #92: Loss:0.9547, Accuracy:0.5138, Validation Loss:0.9572, Validation Accuracy:0.4844\n",
    "Epoch #93: Loss:0.9582, Accuracy:0.5068, Validation Loss:0.9570, Validation Accuracy:0.4844\n",
    "Epoch #94: Loss:0.9533, Accuracy:0.5133, Validation Loss:0.9505, Validation Accuracy:0.4975\n",
    "Epoch #95: Loss:0.9497, Accuracy:0.5138, Validation Loss:0.9539, Validation Accuracy:0.4877\n",
    "Epoch #96: Loss:0.9522, Accuracy:0.5117, Validation Loss:0.9505, Validation Accuracy:0.4992\n",
    "Epoch #97: Loss:0.9531, Accuracy:0.5129, Validation Loss:0.9693, Validation Accuracy:0.4828\n",
    "Epoch #98: Loss:0.9632, Accuracy:0.5150, Validation Loss:0.9508, Validation Accuracy:0.4959\n",
    "Epoch #99: Loss:0.9574, Accuracy:0.5060, Validation Loss:0.9765, Validation Accuracy:0.4647\n",
    "Epoch #100: Loss:0.9610, Accuracy:0.5051, Validation Loss:0.9532, Validation Accuracy:0.4811\n",
    "Epoch #101: Loss:0.9532, Accuracy:0.5129, Validation Loss:0.9554, Validation Accuracy:0.4828\n",
    "Epoch #102: Loss:0.9562, Accuracy:0.5088, Validation Loss:0.9675, Validation Accuracy:0.4762\n",
    "Epoch #103: Loss:0.9560, Accuracy:0.5236, Validation Loss:0.9507, Validation Accuracy:0.4943\n",
    "Epoch #104: Loss:0.9491, Accuracy:0.5154, Validation Loss:0.9493, Validation Accuracy:0.5025\n",
    "Epoch #105: Loss:0.9467, Accuracy:0.5158, Validation Loss:0.9496, Validation Accuracy:0.4959\n",
    "Epoch #106: Loss:0.9480, Accuracy:0.5138, Validation Loss:0.9488, Validation Accuracy:0.4975\n",
    "Epoch #107: Loss:0.9466, Accuracy:0.5142, Validation Loss:0.9506, Validation Accuracy:0.4860\n",
    "Epoch #108: Loss:0.9469, Accuracy:0.5154, Validation Loss:0.9496, Validation Accuracy:0.4992\n",
    "Epoch #109: Loss:0.9498, Accuracy:0.5195, Validation Loss:0.9515, Validation Accuracy:0.4959\n",
    "Epoch #110: Loss:0.9490, Accuracy:0.5150, Validation Loss:0.9489, Validation Accuracy:0.4992\n",
    "Epoch #111: Loss:0.9489, Accuracy:0.5175, Validation Loss:0.9573, Validation Accuracy:0.4992\n",
    "Epoch #112: Loss:0.9542, Accuracy:0.5027, Validation Loss:0.9489, Validation Accuracy:0.4943\n",
    "Epoch #113: Loss:0.9495, Accuracy:0.5175, Validation Loss:0.9584, Validation Accuracy:0.4943\n",
    "Epoch #114: Loss:0.9521, Accuracy:0.5027, Validation Loss:0.9505, Validation Accuracy:0.4943\n",
    "Epoch #115: Loss:0.9505, Accuracy:0.5105, Validation Loss:0.9498, Validation Accuracy:0.4943\n",
    "Epoch #116: Loss:0.9453, Accuracy:0.5162, Validation Loss:0.9543, Validation Accuracy:0.4844\n",
    "Epoch #117: Loss:0.9491, Accuracy:0.5133, Validation Loss:0.9476, Validation Accuracy:0.4975\n",
    "Epoch #118: Loss:0.9454, Accuracy:0.5146, Validation Loss:0.9529, Validation Accuracy:0.4992\n",
    "Epoch #119: Loss:0.9514, Accuracy:0.5138, Validation Loss:0.9566, Validation Accuracy:0.4893\n",
    "Epoch #120: Loss:0.9521, Accuracy:0.5117, Validation Loss:0.9475, Validation Accuracy:0.5025\n",
    "Epoch #121: Loss:0.9433, Accuracy:0.5125, Validation Loss:0.9472, Validation Accuracy:0.5025\n",
    "Epoch #122: Loss:0.9425, Accuracy:0.5166, Validation Loss:0.9479, Validation Accuracy:0.4959\n",
    "Epoch #123: Loss:0.9435, Accuracy:0.5187, Validation Loss:0.9495, Validation Accuracy:0.4910\n",
    "Epoch #124: Loss:0.9442, Accuracy:0.5162, Validation Loss:0.9476, Validation Accuracy:0.5008\n",
    "Epoch #125: Loss:0.9417, Accuracy:0.5240, Validation Loss:0.9497, Validation Accuracy:0.4893\n",
    "Epoch #126: Loss:0.9427, Accuracy:0.5199, Validation Loss:0.9556, Validation Accuracy:0.4959\n",
    "Epoch #127: Loss:0.9459, Accuracy:0.5101, Validation Loss:0.9470, Validation Accuracy:0.4926\n",
    "Epoch #128: Loss:0.9422, Accuracy:0.5203, Validation Loss:0.9469, Validation Accuracy:0.4910\n",
    "Epoch #129: Loss:0.9414, Accuracy:0.5162, Validation Loss:0.9467, Validation Accuracy:0.5025\n",
    "Epoch #130: Loss:0.9413, Accuracy:0.5203, Validation Loss:0.9469, Validation Accuracy:0.4910\n",
    "Epoch #131: Loss:0.9417, Accuracy:0.5220, Validation Loss:0.9485, Validation Accuracy:0.4926\n",
    "Epoch #132: Loss:0.9438, Accuracy:0.5183, Validation Loss:0.9472, Validation Accuracy:0.4959\n",
    "Epoch #133: Loss:0.9400, Accuracy:0.5175, Validation Loss:0.9486, Validation Accuracy:0.4959\n",
    "Epoch #134: Loss:0.9429, Accuracy:0.5162, Validation Loss:0.9465, Validation Accuracy:0.5041\n",
    "Epoch #135: Loss:0.9420, Accuracy:0.5179, Validation Loss:0.9481, Validation Accuracy:0.4910\n",
    "Epoch #136: Loss:0.9421, Accuracy:0.5199, Validation Loss:0.9467, Validation Accuracy:0.4844\n",
    "Epoch #137: Loss:0.9424, Accuracy:0.5216, Validation Loss:0.9457, Validation Accuracy:0.4943\n",
    "Epoch #138: Loss:0.9397, Accuracy:0.5211, Validation Loss:0.9444, Validation Accuracy:0.5057\n",
    "Epoch #139: Loss:0.9398, Accuracy:0.5220, Validation Loss:0.9481, Validation Accuracy:0.4910\n",
    "Epoch #140: Loss:0.9411, Accuracy:0.5183, Validation Loss:0.9460, Validation Accuracy:0.4926\n",
    "Epoch #141: Loss:0.9439, Accuracy:0.5105, Validation Loss:0.9503, Validation Accuracy:0.5008\n",
    "Epoch #142: Loss:0.9427, Accuracy:0.5216, Validation Loss:0.9461, Validation Accuracy:0.4910\n",
    "Epoch #143: Loss:0.9431, Accuracy:0.5261, Validation Loss:0.9521, Validation Accuracy:0.4943\n",
    "Epoch #144: Loss:0.9424, Accuracy:0.5187, Validation Loss:0.9457, Validation Accuracy:0.4959\n",
    "Epoch #145: Loss:0.9407, Accuracy:0.5179, Validation Loss:0.9501, Validation Accuracy:0.5057\n",
    "Epoch #146: Loss:0.9450, Accuracy:0.5211, Validation Loss:0.9449, Validation Accuracy:0.5008\n",
    "Epoch #147: Loss:0.9446, Accuracy:0.5216, Validation Loss:0.9595, Validation Accuracy:0.5025\n",
    "Epoch #148: Loss:0.9498, Accuracy:0.5121, Validation Loss:0.9594, Validation Accuracy:0.4943\n",
    "Epoch #149: Loss:0.9453, Accuracy:0.5240, Validation Loss:0.9542, Validation Accuracy:0.5008\n",
    "Epoch #150: Loss:0.9499, Accuracy:0.5207, Validation Loss:0.9474, Validation Accuracy:0.4877\n",
    "Epoch #151: Loss:0.9446, Accuracy:0.5154, Validation Loss:0.9595, Validation Accuracy:0.4926\n",
    "Epoch #152: Loss:0.9481, Accuracy:0.5203, Validation Loss:0.9538, Validation Accuracy:0.4975\n",
    "Epoch #153: Loss:0.9445, Accuracy:0.5195, Validation Loss:0.9441, Validation Accuracy:0.4959\n",
    "Epoch #154: Loss:0.9416, Accuracy:0.5162, Validation Loss:0.9499, Validation Accuracy:0.5025\n",
    "Epoch #155: Loss:0.9420, Accuracy:0.5236, Validation Loss:0.9550, Validation Accuracy:0.5025\n",
    "Epoch #156: Loss:0.9416, Accuracy:0.5175, Validation Loss:0.9578, Validation Accuracy:0.4959\n",
    "Epoch #157: Loss:0.9451, Accuracy:0.5195, Validation Loss:0.9458, Validation Accuracy:0.4943\n",
    "Epoch #158: Loss:0.9395, Accuracy:0.5187, Validation Loss:0.9441, Validation Accuracy:0.4992\n",
    "Epoch #159: Loss:0.9368, Accuracy:0.5183, Validation Loss:0.9440, Validation Accuracy:0.4975\n",
    "Epoch #160: Loss:0.9379, Accuracy:0.5216, Validation Loss:0.9446, Validation Accuracy:0.5074\n",
    "Epoch #161: Loss:0.9369, Accuracy:0.5232, Validation Loss:0.9434, Validation Accuracy:0.4943\n",
    "Epoch #162: Loss:0.9359, Accuracy:0.5175, Validation Loss:0.9432, Validation Accuracy:0.5008\n",
    "Epoch #163: Loss:0.9353, Accuracy:0.5195, Validation Loss:0.9438, Validation Accuracy:0.4992\n",
    "Epoch #164: Loss:0.9355, Accuracy:0.5162, Validation Loss:0.9430, Validation Accuracy:0.4943\n",
    "Epoch #165: Loss:0.9353, Accuracy:0.5220, Validation Loss:0.9430, Validation Accuracy:0.5008\n",
    "Epoch #166: Loss:0.9364, Accuracy:0.5179, Validation Loss:0.9457, Validation Accuracy:0.4975\n",
    "Epoch #167: Loss:0.9385, Accuracy:0.5216, Validation Loss:0.9450, Validation Accuracy:0.5041\n",
    "Epoch #168: Loss:0.9352, Accuracy:0.5187, Validation Loss:0.9442, Validation Accuracy:0.4992\n",
    "Epoch #169: Loss:0.9347, Accuracy:0.5211, Validation Loss:0.9453, Validation Accuracy:0.4975\n",
    "Epoch #170: Loss:0.9365, Accuracy:0.5265, Validation Loss:0.9423, Validation Accuracy:0.4959\n",
    "Epoch #171: Loss:0.9345, Accuracy:0.5191, Validation Loss:0.9423, Validation Accuracy:0.4959\n",
    "Epoch #172: Loss:0.9352, Accuracy:0.5166, Validation Loss:0.9423, Validation Accuracy:0.5008\n",
    "Epoch #173: Loss:0.9358, Accuracy:0.5257, Validation Loss:0.9473, Validation Accuracy:0.4910\n",
    "Epoch #174: Loss:0.9395, Accuracy:0.5195, Validation Loss:0.9579, Validation Accuracy:0.4959\n",
    "Epoch #175: Loss:0.9420, Accuracy:0.5261, Validation Loss:0.9427, Validation Accuracy:0.4975\n",
    "Epoch #176: Loss:0.9341, Accuracy:0.5203, Validation Loss:0.9427, Validation Accuracy:0.4943\n",
    "Epoch #177: Loss:0.9334, Accuracy:0.5220, Validation Loss:0.9498, Validation Accuracy:0.5041\n",
    "Epoch #178: Loss:0.9351, Accuracy:0.5261, Validation Loss:0.9448, Validation Accuracy:0.4992\n",
    "Epoch #179: Loss:0.9345, Accuracy:0.5253, Validation Loss:0.9417, Validation Accuracy:0.4943\n",
    "Epoch #180: Loss:0.9337, Accuracy:0.5253, Validation Loss:0.9415, Validation Accuracy:0.4943\n",
    "Epoch #181: Loss:0.9329, Accuracy:0.5203, Validation Loss:0.9417, Validation Accuracy:0.4992\n",
    "Epoch #182: Loss:0.9372, Accuracy:0.5162, Validation Loss:0.9480, Validation Accuracy:0.4959\n",
    "Epoch #183: Loss:0.9393, Accuracy:0.5207, Validation Loss:0.9437, Validation Accuracy:0.4910\n",
    "Epoch #184: Loss:0.9344, Accuracy:0.5232, Validation Loss:0.9440, Validation Accuracy:0.5008\n",
    "Epoch #185: Loss:0.9378, Accuracy:0.5195, Validation Loss:0.9418, Validation Accuracy:0.5008\n",
    "Epoch #186: Loss:0.9346, Accuracy:0.5216, Validation Loss:0.9450, Validation Accuracy:0.5008\n",
    "Epoch #187: Loss:0.9352, Accuracy:0.5183, Validation Loss:0.9528, Validation Accuracy:0.5025\n",
    "Epoch #188: Loss:0.9380, Accuracy:0.5228, Validation Loss:0.9413, Validation Accuracy:0.4959\n",
    "Epoch #189: Loss:0.9305, Accuracy:0.5203, Validation Loss:0.9476, Validation Accuracy:0.5025\n",
    "Epoch #190: Loss:0.9397, Accuracy:0.5191, Validation Loss:0.9450, Validation Accuracy:0.4943\n",
    "Epoch #191: Loss:0.9434, Accuracy:0.5158, Validation Loss:0.9537, Validation Accuracy:0.5074\n",
    "Epoch #192: Loss:0.9433, Accuracy:0.5290, Validation Loss:0.9600, Validation Accuracy:0.4844\n",
    "Epoch #193: Loss:0.9428, Accuracy:0.5273, Validation Loss:0.9499, Validation Accuracy:0.4975\n",
    "Epoch #194: Loss:0.9403, Accuracy:0.5207, Validation Loss:0.9421, Validation Accuracy:0.4959\n",
    "Epoch #195: Loss:0.9383, Accuracy:0.5183, Validation Loss:0.9539, Validation Accuracy:0.5025\n",
    "Epoch #196: Loss:0.9383, Accuracy:0.5277, Validation Loss:0.9420, Validation Accuracy:0.4959\n",
    "Epoch #197: Loss:0.9330, Accuracy:0.5228, Validation Loss:0.9403, Validation Accuracy:0.4910\n",
    "Epoch #198: Loss:0.9327, Accuracy:0.5179, Validation Loss:0.9445, Validation Accuracy:0.5090\n",
    "Epoch #199: Loss:0.9333, Accuracy:0.5285, Validation Loss:0.9460, Validation Accuracy:0.4959\n",
    "Epoch #200: Loss:0.9347, Accuracy:0.5302, Validation Loss:0.9405, Validation Accuracy:0.4910\n",
    "Epoch #201: Loss:0.9299, Accuracy:0.5224, Validation Loss:0.9402, Validation Accuracy:0.4910\n",
    "Epoch #202: Loss:0.9296, Accuracy:0.5257, Validation Loss:0.9401, Validation Accuracy:0.4992\n",
    "Epoch #203: Loss:0.9296, Accuracy:0.5290, Validation Loss:0.9417, Validation Accuracy:0.4926\n",
    "Epoch #204: Loss:0.9290, Accuracy:0.5236, Validation Loss:0.9406, Validation Accuracy:0.4910\n",
    "Epoch #205: Loss:0.9290, Accuracy:0.5232, Validation Loss:0.9412, Validation Accuracy:0.4910\n",
    "Epoch #206: Loss:0.9302, Accuracy:0.5211, Validation Loss:0.9435, Validation Accuracy:0.5025\n",
    "Epoch #207: Loss:0.9315, Accuracy:0.5294, Validation Loss:0.9411, Validation Accuracy:0.5008\n",
    "Epoch #208: Loss:0.9345, Accuracy:0.5265, Validation Loss:0.9526, Validation Accuracy:0.5107\n",
    "Epoch #209: Loss:0.9351, Accuracy:0.5179, Validation Loss:0.9491, Validation Accuracy:0.5041\n",
    "Epoch #210: Loss:0.9354, Accuracy:0.5195, Validation Loss:0.9406, Validation Accuracy:0.4893\n",
    "Epoch #211: Loss:0.9291, Accuracy:0.5290, Validation Loss:0.9402, Validation Accuracy:0.4910\n",
    "Epoch #212: Loss:0.9282, Accuracy:0.5253, Validation Loss:0.9423, Validation Accuracy:0.5008\n",
    "Epoch #213: Loss:0.9302, Accuracy:0.5257, Validation Loss:0.9433, Validation Accuracy:0.5057\n",
    "Epoch #214: Loss:0.9284, Accuracy:0.5343, Validation Loss:0.9404, Validation Accuracy:0.5008\n",
    "Epoch #215: Loss:0.9297, Accuracy:0.5310, Validation Loss:0.9416, Validation Accuracy:0.4926\n",
    "Epoch #216: Loss:0.9285, Accuracy:0.5228, Validation Loss:0.9406, Validation Accuracy:0.4943\n",
    "Epoch #217: Loss:0.9310, Accuracy:0.5347, Validation Loss:0.9510, Validation Accuracy:0.5025\n",
    "Epoch #218: Loss:0.9353, Accuracy:0.5133, Validation Loss:0.9449, Validation Accuracy:0.5057\n",
    "Epoch #219: Loss:0.9303, Accuracy:0.5257, Validation Loss:0.9393, Validation Accuracy:0.4926\n",
    "Epoch #220: Loss:0.9278, Accuracy:0.5248, Validation Loss:0.9390, Validation Accuracy:0.4943\n",
    "Epoch #221: Loss:0.9261, Accuracy:0.5302, Validation Loss:0.9407, Validation Accuracy:0.5008\n",
    "Epoch #222: Loss:0.9272, Accuracy:0.5302, Validation Loss:0.9435, Validation Accuracy:0.5025\n",
    "Epoch #223: Loss:0.9312, Accuracy:0.5261, Validation Loss:0.9402, Validation Accuracy:0.4943\n",
    "Epoch #224: Loss:0.9273, Accuracy:0.5228, Validation Loss:0.9398, Validation Accuracy:0.4943\n",
    "Epoch #225: Loss:0.9264, Accuracy:0.5331, Validation Loss:0.9405, Validation Accuracy:0.5074\n",
    "Epoch #226: Loss:0.9267, Accuracy:0.5277, Validation Loss:0.9422, Validation Accuracy:0.4975\n",
    "Epoch #227: Loss:0.9267, Accuracy:0.5298, Validation Loss:0.9381, Validation Accuracy:0.4943\n",
    "Epoch #228: Loss:0.9271, Accuracy:0.5277, Validation Loss:0.9409, Validation Accuracy:0.5008\n",
    "Epoch #229: Loss:0.9288, Accuracy:0.5257, Validation Loss:0.9387, Validation Accuracy:0.4959\n",
    "Epoch #230: Loss:0.9250, Accuracy:0.5302, Validation Loss:0.9399, Validation Accuracy:0.4926\n",
    "Epoch #231: Loss:0.9256, Accuracy:0.5302, Validation Loss:0.9394, Validation Accuracy:0.4943\n",
    "Epoch #232: Loss:0.9261, Accuracy:0.5306, Validation Loss:0.9422, Validation Accuracy:0.5057\n",
    "Epoch #233: Loss:0.9270, Accuracy:0.5269, Validation Loss:0.9388, Validation Accuracy:0.4877\n",
    "Epoch #234: Loss:0.9287, Accuracy:0.5220, Validation Loss:0.9406, Validation Accuracy:0.4959\n",
    "Epoch #235: Loss:0.9258, Accuracy:0.5244, Validation Loss:0.9384, Validation Accuracy:0.4959\n",
    "Epoch #236: Loss:0.9267, Accuracy:0.5331, Validation Loss:0.9409, Validation Accuracy:0.5041\n",
    "Epoch #237: Loss:0.9256, Accuracy:0.5347, Validation Loss:0.9395, Validation Accuracy:0.4926\n",
    "Epoch #238: Loss:0.9245, Accuracy:0.5281, Validation Loss:0.9403, Validation Accuracy:0.5008\n",
    "Epoch #239: Loss:0.9235, Accuracy:0.5281, Validation Loss:0.9454, Validation Accuracy:0.5025\n",
    "Epoch #240: Loss:0.9251, Accuracy:0.5277, Validation Loss:0.9493, Validation Accuracy:0.5041\n",
    "Epoch #241: Loss:0.9361, Accuracy:0.5195, Validation Loss:0.9429, Validation Accuracy:0.5041\n",
    "Epoch #242: Loss:0.9309, Accuracy:0.5363, Validation Loss:0.9473, Validation Accuracy:0.5057\n",
    "Epoch #243: Loss:0.9389, Accuracy:0.5142, Validation Loss:0.9494, Validation Accuracy:0.4992\n",
    "Epoch #244: Loss:0.9353, Accuracy:0.5228, Validation Loss:0.9397, Validation Accuracy:0.4943\n",
    "Epoch #245: Loss:0.9277, Accuracy:0.5265, Validation Loss:0.9396, Validation Accuracy:0.5025\n",
    "Epoch #246: Loss:0.9235, Accuracy:0.5273, Validation Loss:0.9475, Validation Accuracy:0.5008\n",
    "Epoch #247: Loss:0.9269, Accuracy:0.5253, Validation Loss:0.9388, Validation Accuracy:0.5008\n",
    "Epoch #248: Loss:0.9233, Accuracy:0.5298, Validation Loss:0.9384, Validation Accuracy:0.4975\n",
    "Epoch #249: Loss:0.9240, Accuracy:0.5257, Validation Loss:0.9382, Validation Accuracy:0.4926\n",
    "Epoch #250: Loss:0.9245, Accuracy:0.5326, Validation Loss:0.9400, Validation Accuracy:0.5025\n",
    "Epoch #251: Loss:0.9227, Accuracy:0.5306, Validation Loss:0.9407, Validation Accuracy:0.4975\n",
    "Epoch #252: Loss:0.9227, Accuracy:0.5314, Validation Loss:0.9383, Validation Accuracy:0.5025\n",
    "Epoch #253: Loss:0.9235, Accuracy:0.5335, Validation Loss:0.9379, Validation Accuracy:0.4959\n",
    "Epoch #254: Loss:0.9223, Accuracy:0.5273, Validation Loss:0.9378, Validation Accuracy:0.5025\n",
    "Epoch #255: Loss:0.9234, Accuracy:0.5257, Validation Loss:0.9447, Validation Accuracy:0.5025\n",
    "Epoch #256: Loss:0.9254, Accuracy:0.5363, Validation Loss:0.9458, Validation Accuracy:0.5025\n",
    "Epoch #257: Loss:0.9301, Accuracy:0.5294, Validation Loss:0.9408, Validation Accuracy:0.4975\n",
    "Epoch #258: Loss:0.9270, Accuracy:0.5265, Validation Loss:0.9370, Validation Accuracy:0.4926\n",
    "Epoch #259: Loss:0.9225, Accuracy:0.5343, Validation Loss:0.9390, Validation Accuracy:0.5008\n",
    "Epoch #260: Loss:0.9229, Accuracy:0.5310, Validation Loss:0.9388, Validation Accuracy:0.4910\n",
    "Epoch #261: Loss:0.9253, Accuracy:0.5257, Validation Loss:0.9425, Validation Accuracy:0.4975\n",
    "Epoch #262: Loss:0.9240, Accuracy:0.5359, Validation Loss:0.9370, Validation Accuracy:0.4959\n",
    "Epoch #263: Loss:0.9225, Accuracy:0.5331, Validation Loss:0.9374, Validation Accuracy:0.4975\n",
    "Epoch #264: Loss:0.9205, Accuracy:0.5302, Validation Loss:0.9395, Validation Accuracy:0.4959\n",
    "Epoch #265: Loss:0.9221, Accuracy:0.5322, Validation Loss:0.9423, Validation Accuracy:0.4975\n",
    "Epoch #266: Loss:0.9241, Accuracy:0.5298, Validation Loss:0.9377, Validation Accuracy:0.4992\n",
    "Epoch #267: Loss:0.9262, Accuracy:0.5236, Validation Loss:0.9445, Validation Accuracy:0.4975\n",
    "Epoch #268: Loss:0.9274, Accuracy:0.5248, Validation Loss:0.9428, Validation Accuracy:0.4959\n",
    "Epoch #269: Loss:0.9283, Accuracy:0.5306, Validation Loss:0.9527, Validation Accuracy:0.4975\n",
    "Epoch #270: Loss:0.9336, Accuracy:0.5294, Validation Loss:0.9376, Validation Accuracy:0.4959\n",
    "Epoch #271: Loss:0.9290, Accuracy:0.5273, Validation Loss:0.9589, Validation Accuracy:0.4877\n",
    "Epoch #272: Loss:0.9356, Accuracy:0.5236, Validation Loss:0.9417, Validation Accuracy:0.5025\n",
    "Epoch #273: Loss:0.9185, Accuracy:0.5384, Validation Loss:0.9441, Validation Accuracy:0.5008\n",
    "Epoch #274: Loss:0.9250, Accuracy:0.5294, Validation Loss:0.9356, Validation Accuracy:0.4992\n",
    "Epoch #275: Loss:0.9204, Accuracy:0.5372, Validation Loss:0.9359, Validation Accuracy:0.4992\n",
    "Epoch #276: Loss:0.9231, Accuracy:0.5290, Validation Loss:0.9424, Validation Accuracy:0.4926\n",
    "Epoch #277: Loss:0.9223, Accuracy:0.5294, Validation Loss:0.9365, Validation Accuracy:0.4910\n",
    "Epoch #278: Loss:0.9228, Accuracy:0.5302, Validation Loss:0.9412, Validation Accuracy:0.4975\n",
    "Epoch #279: Loss:0.9235, Accuracy:0.5343, Validation Loss:0.9363, Validation Accuracy:0.4943\n",
    "Epoch #280: Loss:0.9200, Accuracy:0.5347, Validation Loss:0.9475, Validation Accuracy:0.4992\n",
    "Epoch #281: Loss:0.9282, Accuracy:0.5216, Validation Loss:0.9369, Validation Accuracy:0.4975\n",
    "Epoch #282: Loss:0.9330, Accuracy:0.5187, Validation Loss:0.9589, Validation Accuracy:0.5041\n",
    "Epoch #283: Loss:0.9417, Accuracy:0.5055, Validation Loss:0.9536, Validation Accuracy:0.4910\n",
    "Epoch #284: Loss:0.9360, Accuracy:0.5232, Validation Loss:0.9366, Validation Accuracy:0.4926\n",
    "Epoch #285: Loss:0.9258, Accuracy:0.5294, Validation Loss:0.9409, Validation Accuracy:0.5025\n",
    "Epoch #286: Loss:0.9242, Accuracy:0.5298, Validation Loss:0.9361, Validation Accuracy:0.4943\n",
    "Epoch #287: Loss:0.9187, Accuracy:0.5335, Validation Loss:0.9345, Validation Accuracy:0.4959\n",
    "Epoch #288: Loss:0.9201, Accuracy:0.5351, Validation Loss:0.9343, Validation Accuracy:0.5008\n",
    "Epoch #289: Loss:0.9194, Accuracy:0.5339, Validation Loss:0.9388, Validation Accuracy:0.5041\n",
    "Epoch #290: Loss:0.9208, Accuracy:0.5269, Validation Loss:0.9457, Validation Accuracy:0.4959\n",
    "Epoch #291: Loss:0.9227, Accuracy:0.5310, Validation Loss:0.9358, Validation Accuracy:0.4943\n",
    "Epoch #292: Loss:0.9213, Accuracy:0.5339, Validation Loss:0.9395, Validation Accuracy:0.4992\n",
    "Epoch #293: Loss:0.9202, Accuracy:0.5211, Validation Loss:0.9420, Validation Accuracy:0.4943\n",
    "Epoch #294: Loss:0.9189, Accuracy:0.5351, Validation Loss:0.9355, Validation Accuracy:0.4910\n",
    "Epoch #295: Loss:0.9176, Accuracy:0.5355, Validation Loss:0.9392, Validation Accuracy:0.4926\n",
    "Epoch #296: Loss:0.9219, Accuracy:0.5347, Validation Loss:0.9348, Validation Accuracy:0.4943\n",
    "Epoch #297: Loss:0.9204, Accuracy:0.5359, Validation Loss:0.9447, Validation Accuracy:0.4975\n",
    "Epoch #298: Loss:0.9241, Accuracy:0.5441, Validation Loss:0.9386, Validation Accuracy:0.4959\n",
    "Epoch #299: Loss:0.9226, Accuracy:0.5306, Validation Loss:0.9408, Validation Accuracy:0.4975\n",
    "Epoch #300: Loss:0.9205, Accuracy:0.5269, Validation Loss:0.9362, Validation Accuracy:0.5008\n",
    "\n",
    "Test:\n",
    "Test Loss:0.93616563, Accuracy:0.5008\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  134   74  32\n",
    "t:02  112  100  15\n",
    "t:03   64    7  71\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.43      0.56      0.49       240\n",
    "          02       0.55      0.44      0.49       227\n",
    "          03       0.60      0.50      0.55       142\n",
    "\n",
    "    accuracy                           0.50       609\n",
    "   macro avg       0.53      0.50      0.51       609\n",
    "weighted avg       0.52      0.50      0.50       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 03:05:40 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 31 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0769073857462466, 1.0745074909504606, 1.0745297507895233, 1.0748442106059033, 1.0746989753250222, 1.0743414018737467, 1.0744763307383496, 1.0744305130687646, 1.0743981370784965, 1.0744300487593477, 1.0742527931586079, 1.0742076930937117, 1.074236435060235, 1.0740882119130226, 1.0739724855313355, 1.0737949435542566, 1.0736175196119913, 1.0732865548877686, 1.0728557299706345, 1.0722987099821344, 1.07141617405395, 1.0701510833793477, 1.0682958014101427, 1.0657356274734773, 1.061481436876632, 1.055921928440213, 1.0488571737004422, 1.0419260218421422, 1.0376126472585894, 1.039144111580058, 1.0400814976793988, 1.0424139346982457, 1.0354904715455029, 1.0286656042625164, 1.027709926877703, 1.0285084615591515, 1.0271524008858968, 1.0250579363411088, 1.0238973475833637, 1.0237628824409397, 1.022594505342944, 1.0212207004941742, 1.020251799099551, 1.0180262260640587, 1.016644479605952, 1.0155032260468833, 1.0147354005788543, 1.0132214578697443, 1.0113269906913118, 1.0100612624721184, 1.0089809465878115, 1.0075602623433706, 1.0059105016915082, 1.0041244300128205, 1.0020911401911519, 1.0002636026670584, 0.9985997708168719, 0.9953166295350675, 0.9926628199312683, 0.9948280419426403, 0.9945012612883093, 0.9920852708894826, 1.0063638593175728, 1.0008543825697624, 0.9843886579786029, 0.9784495752237504, 0.979448983332598, 0.9751725368116094, 0.9713666710164551, 0.9704003546429777, 0.9653789625183506, 0.9626259975049688, 0.9599703303698836, 0.9591911690575736, 0.9571242777780554, 0.9574377038600214, 0.9563252527529774, 0.9622008946337332, 0.9751857606256732, 0.9891254224604965, 0.955463429781408, 0.9553613314291918, 0.9670592453483682, 0.9560082857244707, 0.9694113641341136, 0.9592788366261374, 0.9612494327360381, 0.9528006152762176, 0.9653889986094583, 0.9523191016491607, 0.9707746221905663, 0.957214969523826, 0.9570211139805799, 0.9505113631437956, 0.9538906415303549, 0.9505405045299499, 0.9693172461489347, 0.9507534016529328, 0.9765173629391174, 0.9532049550798726, 0.9554491083218742, 0.9674876548582305, 0.9506641046949991, 0.9492820822350889, 0.9496110457504912, 0.9488119486908999, 0.9505728281581735, 0.9496347208328435, 0.9514798820508134, 0.948943867080513, 0.9573495527010637, 0.948920053997259, 0.9584316671971226, 0.9505332218993865, 0.9498086457182033, 0.9542744534551999, 0.9476106404670941, 0.9528918990556439, 0.9565941076756307, 0.9475063822539569, 0.9471551031118934, 0.9478606797987212, 0.9495368912106469, 0.9476078590148775, 0.9497323745772952, 0.9555636983982644, 0.947038300910411, 0.946948410940092, 0.9466759914835098, 0.9469497512126791, 0.948537782495245, 0.9472250448071898, 0.9486143596849613, 0.9464906341532376, 0.9481187062506213, 0.946661967949327, 0.9457032791107942, 0.9443642485317926, 0.9480912275306501, 0.9460455009894222, 0.9502576656137977, 0.9461463618356801, 0.9520815544331994, 0.9456651022868791, 0.9500854564417759, 0.9448568613462651, 0.9595356436785806, 0.9593641368430628, 0.9542382269461558, 0.9473713872272197, 0.9594876184839333, 0.9538141340262001, 0.9440557200137422, 0.9499186164835599, 0.9550171633463579, 0.9577833443439653, 0.9457596561004376, 0.9441026336649564, 0.9439611051274442, 0.9446264184362978, 0.9434149268225496, 0.9431633273956224, 0.9438432834810029, 0.9429979895919023, 0.9429501352834779, 0.9457195522554207, 0.9449508612966303, 0.9441893157504854, 0.945262898658884, 0.9423328264201999, 0.9422881788220899, 0.9423024105321011, 0.9473328465115652, 0.9578653849991672, 0.94272977877133, 0.9427314367396099, 0.9497542822693761, 0.9448096370462127, 0.9416632362578694, 0.9415384849108303, 0.9416796059052541, 0.947994067751128, 0.9436940012502748, 0.943986046001046, 0.9417613350307609, 0.944962057476169, 0.9528487970676328, 0.9413170015870644, 0.9475909446064866, 0.9450101798586853, 0.9536948363573484, 0.9599888876741156, 0.9499080611762938, 0.9421135368214061, 0.9539065310129, 0.9419792054713457, 0.9402606596891907, 0.9445033629343819, 0.9459781732856738, 0.9404898215397238, 0.9402253183433771, 0.9401051335538354, 0.9416645627303664, 0.9405756317727476, 0.9412420755145193, 0.9435202442170756, 0.9410807637158286, 0.9525683760055768, 0.949090477672508, 0.9405534324191865, 0.9401556321944313, 0.9422798742018701, 0.9432624987585009, 0.9404426376612119, 0.9416452559148542, 0.9405802931691626, 0.9509812154010403, 0.944879456791776, 0.9393288483369135, 0.9389698523997477, 0.9407026673772653, 0.943522055748061, 0.9402346163743431, 0.9398425098123222, 0.9405406188886546, 0.9422307766129818, 0.9381409902095011, 0.9409292655429621, 0.9386753307774737, 0.9399234006557559, 0.9393722370927557, 0.942191447530474, 0.9388202219369572, 0.9405681714048526, 0.9384083030454826, 0.9408773830763029, 0.9394986221355758, 0.9403297144595429, 0.9454159523270205, 0.9493025121430458, 0.942861732395216, 0.9472704091291319, 0.9494393422881566, 0.9397019146894195, 0.9395673382654175, 0.9474955462469843, 0.938820651599339, 0.9383647128670478, 0.9382357693266594, 0.9400142951943409, 0.940668864986188, 0.9382601690605552, 0.9378643114187056, 0.9377912542307122, 0.944746093699106, 0.9458425203567655, 0.940797716819594, 0.9370353346191995, 0.9389922085458227, 0.9388216305248843, 0.9425083293116151, 0.9370315082750492, 0.9374363838353964, 0.939543239295189, 0.9422611435645907, 0.9377106520146964, 0.9445347562799313, 0.9428288387547573, 0.9527483279090405, 0.9376440932010782, 0.9588587226343077, 0.9416741595675401, 0.9440639471185619, 0.9356270921837129, 0.9358783208677921, 0.942434978504682, 0.9364861785288906, 0.9411693232008584, 0.9362563058269043, 0.9475053648643306, 0.9368553123450631, 0.9588989237650666, 0.9535665895747042, 0.9365598639635421, 0.9408580236051275, 0.9360876210608897, 0.9344888204228506, 0.9342716922509455, 0.93883878550506, 0.9457080478738682, 0.9357829390488235, 0.9394891508694353, 0.9419858121911098, 0.9354954607576768, 0.9391597789300878, 0.9348020611333925, 0.9446560192382198, 0.9386334447633653, 0.9407520855001628, 0.9361655729744822], 'val_acc': [0.37274219930074093, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3957307048521214, 0.40394088528034916, 0.40886699404622534, 0.40065681332437864, 0.4269293916147135, 0.41543513825178535, 0.4154351391326422, 0.4105090296816552, 0.4170771744744531, 0.41543513825178535, 0.41707717437658015, 0.4121510665894338, 0.410509030562512, 0.43021346376643, 0.42036124672404257, 0.4137931019312447, 0.41707717437658015, 0.4137931019312447, 0.4220032835339482, 0.4154351381539124, 0.4220032835339482, 0.4154351391326422, 0.4170771740829612, 0.4203612476048994, 0.42036124682191556, 0.4285714274458893, 0.43021346406004896, 0.4203612476048994, 0.42528735558779174, 0.4367816087549739, 0.4269293921040784, 0.43842364478189566, 0.4285714284246191, 0.45320197000292134, 0.43513957292379807, 0.4137931032035934, 0.4630541869474358, 0.4581280781815596, 0.43349753670113034, 0.4466338250143775, 0.4318555004784626, 0.44827586192215607, 0.4646962230722305, 0.44006567674708874, 0.4515599339759996, 0.46798029126009133, 0.4729064038919502, 0.47454843991887197, 0.4794745483911292, 0.4712643676692825, 0.48111658451592393, 0.4794745482932562, 0.49917897733757255, 0.4729064038919502, 0.46305418704530876, 0.4532019701986673, 0.4893267604909311, 0.49917897743544554, 0.48932676504202466, 0.49589490518585605, 0.4844006564718945, 0.48932676513989765, 0.4876847290151029, 0.49753694150639677, 0.4926108372916142, 0.5008210137559863, 0.4712643631181889, 0.48440065211654687, 0.48440065211654687, 0.4975369413106508, 0.487684728819357, 0.49917897743544554, 0.48275862034709977, 0.495894905381602, 0.4646962231701035, 0.4811165798669574, 0.4827586160896251, 0.47619047149257315, 0.4942528691589343, 0.502463049880781, 0.49589490518585605, 0.4975369413106508, 0.4860426883392146, 0.49917897733757255, 0.49589490518585605, 0.49917897743544554, 0.49917897733757255, 0.4942528690610613, 0.49425286886531533, 0.4942528690610613, 0.4942528690610613, 0.48440065211654687, 0.4975369413106508, 0.49917898188866616, 0.48932676513989765, 0.5024630496850351, 0.502463049587162, 0.49589490508798306, 0.49096879671359883, 0.5008210136581133, 0.4893267605888041, 0.4958949049901101, 0.49261083303413955, 0.4909687969093448, 0.502463049880781, 0.49096879671359883, 0.4926108328383936, 0.49589490518585605, 0.49589490518585605, 0.5041050859077028, 0.4909687968114718, 0.48440065221441986, 0.4942528691589343, 0.5057471219346246, 0.49096879671359883, 0.4926108328383936, 0.5008210134623673, 0.49096879671359883, 0.4942528689631883, 0.49589490518585605, 0.5057471218367515, 0.5008210135602403, 0.502463049587162, 0.4942528735142819, 0.5008210134623673, 0.48768472456188233, 0.49261083738948713, 0.4975369412127778, 0.49589490508798306, 0.502463049587162, 0.5024630496850351, 0.49589490508798306, 0.4942528692568073, 0.49917897743544554, 0.4975369412127778, 0.5073891580594193, 0.4942528690610613, 0.5008210135602403, 0.49917897733757255, 0.4942528689631883, 0.5008210135602403, 0.4975369411149048, 0.5041050858098298, 0.4991789772396996, 0.4975369413106508, 0.49589490518585605, 0.49589490518585605, 0.5008210135602403, 0.4909687968114718, 0.49589490508798306, 0.4975369413106508, 0.4942528689631883, 0.5041050858098298, 0.49917897733757255, 0.4942528689631883, 0.4942528689631883, 0.4991789772396996, 0.49589490518585605, 0.49096879661572584, 0.5008210133644944, 0.5008210133644944, 0.5008210135602403, 0.502463049587162, 0.4958949049901101, 0.5024630496850351, 0.4942528690610613, 0.5073891580594193, 0.4844006567655134, 0.4975369414085238, 0.49589490518585605, 0.5024630494892891, 0.49589490518585605, 0.49096879661572584, 0.5090311939884681, 0.49589490518585605, 0.49096879661572584, 0.4909688012646924, 0.4991789772396996, 0.49261083738948713, 0.49096879661572584, 0.49096879661572584, 0.5024630494892891, 0.5008210133644944, 0.5106732305047548, 0.5041050856140838, 0.48932676513989765, 0.4909688012646924, 0.5008210133644944, 0.5057471218367515, 0.5008210133644944, 0.49261083738948713, 0.4942528735142819, 0.502463049782908, 0.5057471218367515, 0.4926108372916142, 0.4942528735142819, 0.5008210180134609, 0.502463049587162, 0.49425287341640894, 0.4942528735142819, 0.5073891579615463, 0.4975369411149048, 0.49425287341640894, 0.5008210134623673, 0.4958949095412037, 0.4926108372916142, 0.49425287341640894, 0.5057471219346246, 0.4876847289172299, 0.4958949049901101, 0.4958949095412037, 0.5041050856140838, 0.4926108372916142, 0.5008210180134609, 0.502463049782908, 0.5041050857119568, 0.5041050858098298, 0.5057471220324975, 0.49917897733757255, 0.49425287341640894, 0.5024630541382556, 0.5008210134623673, 0.5008210180134609, 0.4975369457638714, 0.4926108372916142, 0.5024630494892891, 0.4975369457638714, 0.5024630540403826, 0.4958949094433307, 0.5024630540403826, 0.5024630541382556, 0.502463049587162, 0.4975369411149048, 0.4926108371937412, 0.5008210180134609, 0.49096880116681946, 0.4975369457638714, 0.4958949095412037, 0.49753694566599843, 0.4958949095412037, 0.4975369457638714, 0.4991789816929202, 0.49753694556812544, 0.4958949094433307, 0.4975369411149048, 0.4958949094433307, 0.48768472436613636, 0.5024630541382556, 0.5008210178177149, 0.4991789817907932, 0.4991789817907932, 0.4926108371937412, 0.4909688009710735, 0.4975369457638714, 0.49425287341640894, 0.4991789772396996, 0.49753694566599843, 0.5041050902630504, 0.49096879671359883, 0.4926108370958682, 0.5024630541382556, 0.49425287322066297, 0.4958949094433307, 0.5008210178177149, 0.5041050901651773, 0.4958949095412037, 0.49425287331853596, 0.4991789817907932, 0.49425287331853596, 0.49096880116681946, 0.4926108372916142, 0.49425287322066297, 0.4975369411149048, 0.4958949095412037, 0.49753694566599843, 0.5008210178177149], 'loss': [1.0802247926195054, 1.0758477535091142, 1.0744046387975956, 1.0747368556518084, 1.074900776698604, 1.074207674649217, 1.0742608048343072, 1.0743750520310607, 1.0742320552254114, 1.0743506435496117, 1.074314168493361, 1.0740741211286071, 1.0741658271950127, 1.0740458643656736, 1.0740107694445695, 1.0737670404955102, 1.073621940612793, 1.073369434628888, 1.0729979267355354, 1.0725235601714993, 1.071586960639797, 1.0704721065517324, 1.068822835749914, 1.0664776563154843, 1.062771073797645, 1.0576690155377868, 1.0504913253216284, 1.0420941060083848, 1.0338615327889915, 1.0314582509916177, 1.0274809779572536, 1.0280085656677183, 1.0296524919278813, 1.0262847233601908, 1.0237955307324074, 1.0214252633970131, 1.0206409536592769, 1.0198744118825611, 1.0193946154455384, 1.0174642023854188, 1.0173497770846012, 1.0153085015882457, 1.0148003862623807, 1.0144419427035525, 1.0138135184497559, 1.0139331274208836, 1.0111738006681876, 1.0113378514009825, 1.01096376063153, 1.0120397735670117, 1.0107001934452957, 1.0064178282720107, 1.0062501275074311, 1.003102617709299, 1.0010443143531282, 1.0000584560987642, 0.9993883991143542, 0.9949574103100833, 0.9930566593606859, 0.9941423542690473, 0.9944220863083795, 0.9978027683998281, 0.9962358258588113, 0.9935158258101289, 0.9942386118538326, 0.9841058711251683, 0.9821926109599871, 0.9796055316680266, 0.9773788309684769, 0.9751422118846885, 0.9703551159747082, 0.9702051078269613, 0.9667666416638196, 0.962650683944475, 0.9653614295091962, 0.9584303296811771, 0.9574658788449956, 0.9540426218534153, 0.9604314742881415, 0.9662216108682464, 0.9687674700112313, 0.9539773510711639, 0.9562840114873538, 0.9605073102935384, 0.9600243778444169, 0.9649502569155527, 0.9573147302045959, 0.9576982548349448, 0.9533706977137305, 0.9630341158020912, 0.950977122294095, 0.9547026999432449, 0.9581714570155134, 0.9532615353439378, 0.9497004879818315, 0.9522293262657933, 0.9530907616723, 0.9632136818564648, 0.9574448355659078, 0.9610292294676543, 0.9532004985965987, 0.9561964855791362, 0.9559691140783886, 0.9490828726570709, 0.9467223413426283, 0.947986526631232, 0.9466343699294684, 0.9469106690594793, 0.9498346926984845, 0.9489945598206726, 0.9488688313006376, 0.9542423865633579, 0.9495395819754081, 0.9521065019239391, 0.9505154230511409, 0.9453099670106625, 0.9490677311924693, 0.9454352595478113, 0.9513670530896902, 0.9521411889877163, 0.9432803036985457, 0.942452822769447, 0.9435102584425673, 0.944157919697693, 0.9417323201833564, 0.9427210601448278, 0.9459403932706532, 0.9421964467428549, 0.9413785528108569, 0.9413221543329697, 0.9416661280381361, 0.9438454767761779, 0.9400013749849135, 0.942923217338703, 0.9419957455423579, 0.9421475866492034, 0.9424435627778698, 0.9397003472708089, 0.9397922312944087, 0.9410712749316708, 0.943908181229656, 0.9427075906455884, 0.9431240860196844, 0.9423649003128741, 0.9406832626712885, 0.9450433274314144, 0.9446458224153617, 0.949826844468009, 0.9452998714525352, 0.9499355924447704, 0.9446436582894295, 0.948146377574247, 0.944524384230314, 0.9415769088929195, 0.941972227091662, 0.9416268922709832, 0.9451173867043529, 0.9394803102501119, 0.9368227253949128, 0.9379461422103631, 0.9369216719936786, 0.9359205451344562, 0.9353422384732069, 0.935505055083876, 0.9352916138862437, 0.9363955042200657, 0.9385354883127389, 0.935175014276524, 0.9347404122597383, 0.9365306925969447, 0.9345385950938387, 0.9352068449437496, 0.9358026706462523, 0.93949733935832, 0.9419752221577466, 0.9340809276950922, 0.9333657379757452, 0.9350532516806522, 0.9344993029531757, 0.9337161803637197, 0.9329129680960575, 0.9372002463565959, 0.9392880066709107, 0.9343913655261484, 0.937829163725616, 0.9346405614572874, 0.9351625163941902, 0.9379872506893636, 0.9305441127420696, 0.9397269159371848, 0.9434340313229962, 0.9433186318350524, 0.9427685360888926, 0.940324012944341, 0.9382635125144552, 0.9382569207058306, 0.9329848253017089, 0.9326938243127701, 0.9332873489332885, 0.93468684009458, 0.9299226295287114, 0.9295667236590532, 0.9295967965155412, 0.928973729067025, 0.9289917133672037, 0.9301929870670091, 0.931547069671952, 0.9345473238819678, 0.9351118124485995, 0.9354367761151746, 0.929149401457158, 0.9282007882482463, 0.9302187732602537, 0.9284354781223274, 0.9297458141491398, 0.9285184827183796, 0.930988460938299, 0.9353429243305136, 0.9302971614459702, 0.9278309379514972, 0.9261374196722277, 0.9272462977520984, 0.931192173164728, 0.9273312073713456, 0.9264179875228928, 0.9267268662335202, 0.9267137069476948, 0.9271210542694499, 0.9287974507167354, 0.9249670723135711, 0.925564202660163, 0.9261171917406196, 0.926987155604901, 0.9287124914310306, 0.9257621353167038, 0.9267003501465189, 0.9255872044719955, 0.9245010917925982, 0.9234664914054792, 0.9251371018695635, 0.9360970322845897, 0.9308572354992313, 0.9389258535490878, 0.9353054460558803, 0.9276763858246853, 0.9235426171114802, 0.926891667226012, 0.9233404956559136, 0.9240130932424103, 0.9244529026244945, 0.9227260207983013, 0.9227093047674676, 0.9235071067937346, 0.9222508602073795, 0.9233829910505479, 0.9253579853741295, 0.9300878595767325, 0.9270296863217129, 0.9225122338692511, 0.9228865818566121, 0.9252920914479594, 0.9240338135793713, 0.9225252773972262, 0.9204969370634404, 0.9221046783596094, 0.924139516211633, 0.9261670510382133, 0.9274359710897019, 0.9283315400323339, 0.9336299066181301, 0.9290420473968224, 0.9355859854872467, 0.9185134315637592, 0.9250190987968836, 0.9203574534559152, 0.9230789703999701, 0.9222869388376663, 0.9228172994492235, 0.9234717292462531, 0.9199933346292076, 0.9282239690943176, 0.9330390188238704, 0.941715363090288, 0.9359651586847874, 0.9258441120202536, 0.9242130327763254, 0.9186696254741974, 0.9200980487055848, 0.9194440307803222, 0.9208165013814609, 0.9227398084664002, 0.9212888228819845, 0.9201762883080594, 0.9188845747794948, 0.9175588582085877, 0.921851697178592, 0.9203832699532871, 0.92414886356624, 0.9225814003963979, 0.9205056922881265], 'acc': [0.37289527855369836, 0.37125256631898196, 0.3942505152808078, 0.3942505152808078, 0.3942505125392389, 0.39425051449750237, 0.39425051449750237, 0.3942505126983478, 0.394250514301676, 0.3942505113275634, 0.39425051391002336, 0.3942505152808078, 0.39425051508498143, 0.3942505113275634, 0.3942505125392389, 0.394250514301676, 0.39425051508498143, 0.3942505129308916, 0.39425051156010715, 0.39425051547663414, 0.3942505156724605, 0.3942505146933287, 0.39425051449750237, 0.3942505119150424, 0.3942505151216989, 0.3971252544582257, 0.3946611927030512, 0.4057494879625661, 0.40903490874801574, 0.4184804937310777, 0.4213552339243448, 0.41806981513387614, 0.4234086257353945, 0.421355234940194, 0.42915810886349764, 0.4402464080762569, 0.4229979463548876, 0.43162217653018004, 0.42833675441066343, 0.43162217793768193, 0.43983572732496556, 0.4484599590668688, 0.4402464055305144, 0.43531827646359283, 0.4496919895344446, 0.4402464063138198, 0.45379876868925545, 0.4418891148278356, 0.4447638618750249, 0.4336755640330501, 0.4443531812828424, 0.4427104739805022, 0.44969199290021, 0.45462012271371954, 0.4628336772659231, 0.451334703764142, 0.46406570992430624, 0.46570841722664647, 0.4685831615322669, 0.46242299667374065, 0.4607802858098087, 0.44763860523823107, 0.46858316235228975, 0.464476383784958, 0.4665297734419178, 0.47679671291453146, 0.4681724839141971, 0.4784394273400551, 0.4759753586575236, 0.47761806775901844, 0.4960985607679865, 0.4973305955804594, 0.5034907588723748, 0.5030800814501314, 0.5063655061153906, 0.5137577032406472, 0.4973305975754403, 0.5108829587392004, 0.5067761811877178, 0.5043121145001672, 0.49527720768593664, 0.511293637336402, 0.502258724647381, 0.4956878851081801, 0.5034907590682012, 0.5006160178958023, 0.5125256701906115, 0.5092402453295259, 0.5084188912683444, 0.5104722777920827, 0.5149897372698148, 0.5137576999115993, 0.5067761804044124, 0.513347023505205, 0.5137577016740364, 0.5117043104504658, 0.5129363438921543, 0.5149897368781621, 0.5059548296722788, 0.5051334683655224, 0.5129363452629387, 0.5088295702571987, 0.5236139654868437, 0.5154004101880522, 0.5158110864353376, 0.5137577026531681, 0.5141683783129745, 0.5154004109713576, 0.5195071825746148, 0.5149897321783297, 0.5174538016074492, 0.5026694038320616, 0.5174537984742277, 0.5026694042237143, 0.510472282491915, 0.5162217669908026, 0.513347024839272, 0.5145790598475712, 0.5137577024573419, 0.51170431397534, 0.5125256699947851, 0.5166324465671359, 0.5186858323075688, 0.5162217671866289, 0.52402464330074, 0.5199178680991735, 0.5100616038947134, 0.5203285447381115, 0.5162217664033236, 0.5203285429756744, 0.5219712507063848, 0.5182751568435888, 0.5174537994533593, 0.5162217681657607, 0.5178644776589082, 0.5199178657492572, 0.5215605772006683, 0.5211498987992931, 0.5219712552103908, 0.5182751552769781, 0.5104722817086096, 0.5215605756340576, 0.5260780306077836, 0.5186858336783533, 0.5178644790296927, 0.5211499003659039, 0.5215605708607903, 0.5121149919850626, 0.5240246413424764, 0.5207392198104388, 0.5154004143004055, 0.5203285429756744, 0.5195071893061456, 0.5162217671866289, 0.5236139629411012, 0.5174537971034432, 0.519507188167905, 0.518685833482527, 0.5182751539061936, 0.5215605772006683, 0.523203284539726, 0.5174537967117905, 0.5195071865645767, 0.5162217691448925, 0.5219712571686543, 0.5178644800088245, 0.5215605764173629, 0.5186858328950479, 0.5211498958618979, 0.5264887088133324, 0.5190965095339859, 0.5166324457838305, 0.5256673518147557, 0.5195071887186665, 0.5260780327618734, 0.5203285431715008, 0.5219712516855165, 0.5260780254428636, 0.5252566743925122, 0.5252566741966859, 0.5203285410174109, 0.5162217682024781, 0.5207392225520077, 0.5232032870854685, 0.5195071871520558, 0.5215605762215365, 0.5182751552769781, 0.5227926073133089, 0.5203285430123918, 0.5190965109047704, 0.5158110878061221, 0.5289527751092303, 0.527310063461993, 0.520739221572876, 0.5182751560602834, 0.5277207426466736, 0.5227926104465305, 0.5178644774630818, 0.528542094945418, 0.530184805221871, 0.5223819304785444, 0.525667351423103, 0.5289527725634878, 0.523613964507712, 0.5232032855188578, 0.5211498995825985, 0.5293634535106055, 0.5264887082258534, 0.5178644796171717, 0.5195071889144929, 0.5289527751092303, 0.525256676546602, 0.5256673527938874, 0.5342915843399643, 0.5310061614371423, 0.522792607900788, 0.5347022609789024, 0.5133470250350983, 0.5256673488773604, 0.5248459983410532, 0.5301848067884817, 0.5301848075717871, 0.5260780315869154, 0.5227926122089676, 0.5330595461984434, 0.5277207432341526, 0.5297741276038012, 0.5277207420591946, 0.5256673541646718, 0.5301848081592662, 0.5301848071801344, 0.5305954832315934, 0.5268993864314022, 0.5219712526646483, 0.5244353168064564, 0.533059549331665, 0.5347022633288185, 0.5281314186981326, 0.5281314204605698, 0.5277207386567118, 0.5195071859770977, 0.5363449724303134, 0.514168381446196, 0.5227926084882669, 0.5264887084216797, 0.5273100646369511, 0.5252566749799913, 0.5297741295620646, 0.5256673494648395, 0.5326488752384695, 0.5305954834274198, 0.5314168392510384, 0.5334702291038247, 0.5273100662035619, 0.5256673539688455, 0.5363449649154772, 0.5293634523356475, 0.5264887103799433, 0.534291581402569, 0.5310061645703639, 0.5256673518147557, 0.5359342952038962, 0.5330595507024495, 0.5301848002527774, 0.53223819331222, 0.5297741276038012, 0.5236139611786641, 0.5248459985368795, 0.5305954871481204, 0.5293634535106055, 0.5273100616995559, 0.5236139662701491, 0.5383983595415307, 0.5293634515523421, 0.5371663286455848, 0.5289527741300987, 0.5293634535106055, 0.5301848085509189, 0.5342915829691799, 0.5347022633288185, 0.5215605750465785, 0.5186858344616586, 0.5055441485293347, 0.523203289043732, 0.5293634454082904, 0.5297741283871065, 0.5334702281246929, 0.5351129403594094, 0.5338809067218945, 0.5268993881938394, 0.5310061643745375, 0.5338809069177208, 0.5211498980159877, 0.5351129391844512, 0.5355236177816528, 0.5347022613705551, 0.5359342950080699, 0.5441478471736398, 0.5305954782624999, 0.5268993876063603]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
