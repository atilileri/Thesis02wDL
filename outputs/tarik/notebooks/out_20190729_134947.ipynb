{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf5.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 13:49:47 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '1', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '02', '01', '05', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000230CB1FBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000230C59F7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6074, Accuracy:0.2123, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6055, Accuracy:0.2345, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6023, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6020, Accuracy:0.2345, Validation Loss:1.6031, Validation Accuracy:0.2381\n",
    "Epoch #26: Loss:1.6015, Accuracy:0.2423, Validation Loss:1.6033, Validation Accuracy:0.2414\n",
    "Epoch #27: Loss:1.6029, Accuracy:0.2353, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #28: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6022, Validation Accuracy:0.2299\n",
    "Epoch #29: Loss:1.6025, Accuracy:0.2361, Validation Loss:1.6028, Validation Accuracy:0.2397\n",
    "Epoch #30: Loss:1.6027, Accuracy:0.2324, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #31: Loss:1.6018, Accuracy:0.2329, Validation Loss:1.6020, Validation Accuracy:0.2430\n",
    "Epoch #32: Loss:1.6018, Accuracy:0.2407, Validation Loss:1.6022, Validation Accuracy:0.2447\n",
    "Epoch #33: Loss:1.6013, Accuracy:0.2423, Validation Loss:1.6023, Validation Accuracy:0.2545\n",
    "Epoch #34: Loss:1.6028, Accuracy:0.2411, Validation Loss:1.6027, Validation Accuracy:0.2430\n",
    "Epoch #35: Loss:1.6022, Accuracy:0.2435, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #36: Loss:1.6019, Accuracy:0.2390, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #37: Loss:1.6032, Accuracy:0.2345, Validation Loss:1.6032, Validation Accuracy:0.2430\n",
    "Epoch #38: Loss:1.6016, Accuracy:0.2398, Validation Loss:1.6033, Validation Accuracy:0.2479\n",
    "Epoch #39: Loss:1.6017, Accuracy:0.2382, Validation Loss:1.6039, Validation Accuracy:0.2299\n",
    "Epoch #40: Loss:1.6022, Accuracy:0.2398, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #41: Loss:1.6012, Accuracy:0.2402, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #42: Loss:1.6017, Accuracy:0.2452, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #43: Loss:1.6018, Accuracy:0.2398, Validation Loss:1.6020, Validation Accuracy:0.2463\n",
    "Epoch #44: Loss:1.6016, Accuracy:0.2419, Validation Loss:1.6026, Validation Accuracy:0.2414\n",
    "Epoch #45: Loss:1.6014, Accuracy:0.2423, Validation Loss:1.6023, Validation Accuracy:0.2414\n",
    "Epoch #46: Loss:1.6010, Accuracy:0.2411, Validation Loss:1.6024, Validation Accuracy:0.2447\n",
    "Epoch #47: Loss:1.6009, Accuracy:0.2398, Validation Loss:1.6019, Validation Accuracy:0.2463\n",
    "Epoch #48: Loss:1.6004, Accuracy:0.2452, Validation Loss:1.6020, Validation Accuracy:0.2430\n",
    "Epoch #49: Loss:1.6001, Accuracy:0.2427, Validation Loss:1.6027, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.5999, Accuracy:0.2423, Validation Loss:1.6029, Validation Accuracy:0.2365\n",
    "Epoch #51: Loss:1.5996, Accuracy:0.2444, Validation Loss:1.6025, Validation Accuracy:0.2315\n",
    "Epoch #52: Loss:1.5999, Accuracy:0.2431, Validation Loss:1.6024, Validation Accuracy:0.2397\n",
    "Epoch #53: Loss:1.5996, Accuracy:0.2411, Validation Loss:1.6017, Validation Accuracy:0.2315\n",
    "Epoch #54: Loss:1.6006, Accuracy:0.2394, Validation Loss:1.6034, Validation Accuracy:0.2365\n",
    "Epoch #55: Loss:1.5995, Accuracy:0.2398, Validation Loss:1.6040, Validation Accuracy:0.2282\n",
    "Epoch #56: Loss:1.6000, Accuracy:0.2444, Validation Loss:1.6048, Validation Accuracy:0.2250\n",
    "Epoch #57: Loss:1.5990, Accuracy:0.2427, Validation Loss:1.6042, Validation Accuracy:0.2299\n",
    "Epoch #58: Loss:1.5994, Accuracy:0.2431, Validation Loss:1.6048, Validation Accuracy:0.2414\n",
    "Epoch #59: Loss:1.6004, Accuracy:0.2419, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #60: Loss:1.6000, Accuracy:0.2423, Validation Loss:1.6023, Validation Accuracy:0.2299\n",
    "Epoch #61: Loss:1.5999, Accuracy:0.2439, Validation Loss:1.6035, Validation Accuracy:0.2348\n",
    "Epoch #62: Loss:1.6006, Accuracy:0.2398, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #63: Loss:1.6005, Accuracy:0.2411, Validation Loss:1.6032, Validation Accuracy:0.2414\n",
    "Epoch #64: Loss:1.5991, Accuracy:0.2431, Validation Loss:1.6036, Validation Accuracy:0.2250\n",
    "Epoch #65: Loss:1.5991, Accuracy:0.2423, Validation Loss:1.6043, Validation Accuracy:0.2217\n",
    "Epoch #66: Loss:1.5993, Accuracy:0.2431, Validation Loss:1.6040, Validation Accuracy:0.2184\n",
    "Epoch #67: Loss:1.5989, Accuracy:0.2419, Validation Loss:1.6041, Validation Accuracy:0.2184\n",
    "Epoch #68: Loss:1.5984, Accuracy:0.2411, Validation Loss:1.6043, Validation Accuracy:0.2414\n",
    "Epoch #69: Loss:1.5985, Accuracy:0.2415, Validation Loss:1.6031, Validation Accuracy:0.2250\n",
    "Epoch #70: Loss:1.5980, Accuracy:0.2415, Validation Loss:1.6031, Validation Accuracy:0.2200\n",
    "Epoch #71: Loss:1.5978, Accuracy:0.2431, Validation Loss:1.6030, Validation Accuracy:0.2200\n",
    "Epoch #72: Loss:1.5973, Accuracy:0.2448, Validation Loss:1.6039, Validation Accuracy:0.2167\n",
    "Epoch #73: Loss:1.5968, Accuracy:0.2448, Validation Loss:1.6038, Validation Accuracy:0.2184\n",
    "Epoch #74: Loss:1.5971, Accuracy:0.2448, Validation Loss:1.6041, Validation Accuracy:0.2167\n",
    "Epoch #75: Loss:1.5965, Accuracy:0.2419, Validation Loss:1.6047, Validation Accuracy:0.2233\n",
    "Epoch #76: Loss:1.5965, Accuracy:0.2419, Validation Loss:1.6046, Validation Accuracy:0.2217\n",
    "Epoch #77: Loss:1.5961, Accuracy:0.2423, Validation Loss:1.6044, Validation Accuracy:0.2167\n",
    "Epoch #78: Loss:1.5966, Accuracy:0.2431, Validation Loss:1.6055, Validation Accuracy:0.2233\n",
    "Epoch #79: Loss:1.5962, Accuracy:0.2435, Validation Loss:1.6051, Validation Accuracy:0.2167\n",
    "Epoch #80: Loss:1.5959, Accuracy:0.2431, Validation Loss:1.6051, Validation Accuracy:0.2184\n",
    "Epoch #81: Loss:1.5956, Accuracy:0.2435, Validation Loss:1.6053, Validation Accuracy:0.2167\n",
    "Epoch #82: Loss:1.5955, Accuracy:0.2439, Validation Loss:1.6055, Validation Accuracy:0.2167\n",
    "Epoch #83: Loss:1.5953, Accuracy:0.2435, Validation Loss:1.6065, Validation Accuracy:0.2184\n",
    "Epoch #84: Loss:1.5952, Accuracy:0.2444, Validation Loss:1.6055, Validation Accuracy:0.2233\n",
    "Epoch #85: Loss:1.5949, Accuracy:0.2472, Validation Loss:1.6047, Validation Accuracy:0.2233\n",
    "Epoch #86: Loss:1.5950, Accuracy:0.2444, Validation Loss:1.6047, Validation Accuracy:0.2266\n",
    "Epoch #87: Loss:1.5953, Accuracy:0.2444, Validation Loss:1.6055, Validation Accuracy:0.2167\n",
    "Epoch #88: Loss:1.5947, Accuracy:0.2489, Validation Loss:1.6056, Validation Accuracy:0.2299\n",
    "Epoch #89: Loss:1.5942, Accuracy:0.2476, Validation Loss:1.6068, Validation Accuracy:0.2266\n",
    "Epoch #90: Loss:1.5941, Accuracy:0.2448, Validation Loss:1.6071, Validation Accuracy:0.2200\n",
    "Epoch #91: Loss:1.5940, Accuracy:0.2456, Validation Loss:1.6070, Validation Accuracy:0.2217\n",
    "Epoch #92: Loss:1.5943, Accuracy:0.2464, Validation Loss:1.6071, Validation Accuracy:0.2184\n",
    "Epoch #93: Loss:1.5944, Accuracy:0.2468, Validation Loss:1.6076, Validation Accuracy:0.2233\n",
    "Epoch #94: Loss:1.5941, Accuracy:0.2452, Validation Loss:1.6071, Validation Accuracy:0.2200\n",
    "Epoch #95: Loss:1.5942, Accuracy:0.2452, Validation Loss:1.6091, Validation Accuracy:0.2151\n",
    "Epoch #96: Loss:1.5922, Accuracy:0.2493, Validation Loss:1.6090, Validation Accuracy:0.2266\n",
    "Epoch #97: Loss:1.5930, Accuracy:0.2468, Validation Loss:1.6110, Validation Accuracy:0.2085\n",
    "Epoch #98: Loss:1.5931, Accuracy:0.2452, Validation Loss:1.6091, Validation Accuracy:0.2184\n",
    "Epoch #99: Loss:1.5921, Accuracy:0.2439, Validation Loss:1.6090, Validation Accuracy:0.2167\n",
    "Epoch #100: Loss:1.5919, Accuracy:0.2448, Validation Loss:1.6123, Validation Accuracy:0.2069\n",
    "Epoch #101: Loss:1.5928, Accuracy:0.2464, Validation Loss:1.6099, Validation Accuracy:0.2233\n",
    "Epoch #102: Loss:1.5927, Accuracy:0.2485, Validation Loss:1.6065, Validation Accuracy:0.2315\n",
    "Epoch #103: Loss:1.5928, Accuracy:0.2513, Validation Loss:1.6073, Validation Accuracy:0.2282\n",
    "Epoch #104: Loss:1.5927, Accuracy:0.2468, Validation Loss:1.6056, Validation Accuracy:0.2365\n",
    "Epoch #105: Loss:1.5930, Accuracy:0.2452, Validation Loss:1.6049, Validation Accuracy:0.2217\n",
    "Epoch #106: Loss:1.5925, Accuracy:0.2427, Validation Loss:1.6078, Validation Accuracy:0.2085\n",
    "Epoch #107: Loss:1.5939, Accuracy:0.2378, Validation Loss:1.6096, Validation Accuracy:0.2233\n",
    "Epoch #108: Loss:1.5943, Accuracy:0.2489, Validation Loss:1.6080, Validation Accuracy:0.2200\n",
    "Epoch #109: Loss:1.5940, Accuracy:0.2489, Validation Loss:1.6112, Validation Accuracy:0.1987\n",
    "Epoch #110: Loss:1.5961, Accuracy:0.2485, Validation Loss:1.6097, Validation Accuracy:0.2118\n",
    "Epoch #111: Loss:1.5944, Accuracy:0.2497, Validation Loss:1.6075, Validation Accuracy:0.2233\n",
    "Epoch #112: Loss:1.5926, Accuracy:0.2460, Validation Loss:1.6085, Validation Accuracy:0.2250\n",
    "Epoch #113: Loss:1.5931, Accuracy:0.2485, Validation Loss:1.6106, Validation Accuracy:0.2118\n",
    "Epoch #114: Loss:1.5928, Accuracy:0.2480, Validation Loss:1.6102, Validation Accuracy:0.2118\n",
    "Epoch #115: Loss:1.5915, Accuracy:0.2435, Validation Loss:1.6102, Validation Accuracy:0.2151\n",
    "Epoch #116: Loss:1.5914, Accuracy:0.2497, Validation Loss:1.6084, Validation Accuracy:0.2250\n",
    "Epoch #117: Loss:1.5912, Accuracy:0.2444, Validation Loss:1.6106, Validation Accuracy:0.2200\n",
    "Epoch #118: Loss:1.5913, Accuracy:0.2419, Validation Loss:1.6084, Validation Accuracy:0.2167\n",
    "Epoch #119: Loss:1.5907, Accuracy:0.2497, Validation Loss:1.6093, Validation Accuracy:0.2266\n",
    "Epoch #120: Loss:1.5899, Accuracy:0.2460, Validation Loss:1.6102, Validation Accuracy:0.2233\n",
    "Epoch #121: Loss:1.5898, Accuracy:0.2517, Validation Loss:1.6125, Validation Accuracy:0.2167\n",
    "Epoch #122: Loss:1.5915, Accuracy:0.2452, Validation Loss:1.6109, Validation Accuracy:0.2200\n",
    "Epoch #123: Loss:1.5921, Accuracy:0.2456, Validation Loss:1.6112, Validation Accuracy:0.2184\n",
    "Epoch #124: Loss:1.5914, Accuracy:0.2550, Validation Loss:1.6117, Validation Accuracy:0.2167\n",
    "Epoch #125: Loss:1.5912, Accuracy:0.2571, Validation Loss:1.6094, Validation Accuracy:0.2217\n",
    "Epoch #126: Loss:1.5887, Accuracy:0.2517, Validation Loss:1.6077, Validation Accuracy:0.2217\n",
    "Epoch #127: Loss:1.5891, Accuracy:0.2509, Validation Loss:1.6084, Validation Accuracy:0.2217\n",
    "Epoch #128: Loss:1.5891, Accuracy:0.2522, Validation Loss:1.6112, Validation Accuracy:0.2250\n",
    "Epoch #129: Loss:1.5889, Accuracy:0.2476, Validation Loss:1.6090, Validation Accuracy:0.2250\n",
    "Epoch #130: Loss:1.5892, Accuracy:0.2534, Validation Loss:1.6076, Validation Accuracy:0.2053\n",
    "Epoch #131: Loss:1.5891, Accuracy:0.2608, Validation Loss:1.6075, Validation Accuracy:0.2184\n",
    "Epoch #132: Loss:1.5878, Accuracy:0.2505, Validation Loss:1.6105, Validation Accuracy:0.2299\n",
    "Epoch #133: Loss:1.5877, Accuracy:0.2468, Validation Loss:1.6080, Validation Accuracy:0.2315\n",
    "Epoch #134: Loss:1.5886, Accuracy:0.2460, Validation Loss:1.6088, Validation Accuracy:0.2200\n",
    "Epoch #135: Loss:1.5874, Accuracy:0.2493, Validation Loss:1.6104, Validation Accuracy:0.2118\n",
    "Epoch #136: Loss:1.5872, Accuracy:0.2554, Validation Loss:1.6126, Validation Accuracy:0.2053\n",
    "Epoch #137: Loss:1.5861, Accuracy:0.2604, Validation Loss:1.6107, Validation Accuracy:0.2151\n",
    "Epoch #138: Loss:1.5866, Accuracy:0.2505, Validation Loss:1.6121, Validation Accuracy:0.2118\n",
    "Epoch #139: Loss:1.5899, Accuracy:0.2513, Validation Loss:1.6115, Validation Accuracy:0.2151\n",
    "Epoch #140: Loss:1.5881, Accuracy:0.2522, Validation Loss:1.6094, Validation Accuracy:0.2151\n",
    "Epoch #141: Loss:1.5862, Accuracy:0.2559, Validation Loss:1.6092, Validation Accuracy:0.2118\n",
    "Epoch #142: Loss:1.5884, Accuracy:0.2497, Validation Loss:1.6114, Validation Accuracy:0.2266\n",
    "Epoch #143: Loss:1.5870, Accuracy:0.2517, Validation Loss:1.6128, Validation Accuracy:0.2184\n",
    "Epoch #144: Loss:1.5879, Accuracy:0.2554, Validation Loss:1.6107, Validation Accuracy:0.2020\n",
    "Epoch #145: Loss:1.5888, Accuracy:0.2559, Validation Loss:1.6107, Validation Accuracy:0.2217\n",
    "Epoch #146: Loss:1.5891, Accuracy:0.2559, Validation Loss:1.6092, Validation Accuracy:0.2036\n",
    "Epoch #147: Loss:1.5887, Accuracy:0.2517, Validation Loss:1.6113, Validation Accuracy:0.2184\n",
    "Epoch #148: Loss:1.5870, Accuracy:0.2538, Validation Loss:1.6089, Validation Accuracy:0.2135\n",
    "Epoch #149: Loss:1.5873, Accuracy:0.2522, Validation Loss:1.6106, Validation Accuracy:0.2151\n",
    "Epoch #150: Loss:1.5880, Accuracy:0.2522, Validation Loss:1.6099, Validation Accuracy:0.2282\n",
    "Epoch #151: Loss:1.5878, Accuracy:0.2468, Validation Loss:1.6079, Validation Accuracy:0.2282\n",
    "Epoch #152: Loss:1.5877, Accuracy:0.2604, Validation Loss:1.6102, Validation Accuracy:0.2184\n",
    "Epoch #153: Loss:1.5885, Accuracy:0.2641, Validation Loss:1.6109, Validation Accuracy:0.2167\n",
    "Epoch #154: Loss:1.5869, Accuracy:0.2530, Validation Loss:1.6118, Validation Accuracy:0.2217\n",
    "Epoch #155: Loss:1.5869, Accuracy:0.2567, Validation Loss:1.6080, Validation Accuracy:0.2118\n",
    "Epoch #156: Loss:1.5873, Accuracy:0.2559, Validation Loss:1.6091, Validation Accuracy:0.2135\n",
    "Epoch #157: Loss:1.5879, Accuracy:0.2505, Validation Loss:1.6091, Validation Accuracy:0.2282\n",
    "Epoch #158: Loss:1.5873, Accuracy:0.2509, Validation Loss:1.6082, Validation Accuracy:0.2151\n",
    "Epoch #159: Loss:1.5868, Accuracy:0.2587, Validation Loss:1.6074, Validation Accuracy:0.2315\n",
    "Epoch #160: Loss:1.5859, Accuracy:0.2579, Validation Loss:1.6064, Validation Accuracy:0.2200\n",
    "Epoch #161: Loss:1.5851, Accuracy:0.2616, Validation Loss:1.6092, Validation Accuracy:0.2151\n",
    "Epoch #162: Loss:1.5839, Accuracy:0.2595, Validation Loss:1.6105, Validation Accuracy:0.2135\n",
    "Epoch #163: Loss:1.5841, Accuracy:0.2612, Validation Loss:1.6083, Validation Accuracy:0.2102\n",
    "Epoch #164: Loss:1.5833, Accuracy:0.2579, Validation Loss:1.6076, Validation Accuracy:0.2069\n",
    "Epoch #165: Loss:1.5832, Accuracy:0.2517, Validation Loss:1.6125, Validation Accuracy:0.2200\n",
    "Epoch #166: Loss:1.5829, Accuracy:0.2542, Validation Loss:1.6126, Validation Accuracy:0.2118\n",
    "Epoch #167: Loss:1.5819, Accuracy:0.2620, Validation Loss:1.6125, Validation Accuracy:0.1987\n",
    "Epoch #168: Loss:1.5834, Accuracy:0.2645, Validation Loss:1.6125, Validation Accuracy:0.2167\n",
    "Epoch #169: Loss:1.5826, Accuracy:0.2632, Validation Loss:1.6116, Validation Accuracy:0.2020\n",
    "Epoch #170: Loss:1.5832, Accuracy:0.2583, Validation Loss:1.6132, Validation Accuracy:0.1987\n",
    "Epoch #171: Loss:1.5828, Accuracy:0.2591, Validation Loss:1.6138, Validation Accuracy:0.2069\n",
    "Epoch #172: Loss:1.5831, Accuracy:0.2608, Validation Loss:1.6153, Validation Accuracy:0.2151\n",
    "Epoch #173: Loss:1.5811, Accuracy:0.2706, Validation Loss:1.6129, Validation Accuracy:0.2151\n",
    "Epoch #174: Loss:1.5812, Accuracy:0.2645, Validation Loss:1.6104, Validation Accuracy:0.2151\n",
    "Epoch #175: Loss:1.5807, Accuracy:0.2682, Validation Loss:1.6126, Validation Accuracy:0.2085\n",
    "Epoch #176: Loss:1.5831, Accuracy:0.2608, Validation Loss:1.6128, Validation Accuracy:0.2020\n",
    "Epoch #177: Loss:1.5811, Accuracy:0.2604, Validation Loss:1.6119, Validation Accuracy:0.2184\n",
    "Epoch #178: Loss:1.5833, Accuracy:0.2505, Validation Loss:1.6124, Validation Accuracy:0.2020\n",
    "Epoch #179: Loss:1.5826, Accuracy:0.2579, Validation Loss:1.6150, Validation Accuracy:0.1954\n",
    "Epoch #180: Loss:1.5835, Accuracy:0.2522, Validation Loss:1.6132, Validation Accuracy:0.2085\n",
    "Epoch #181: Loss:1.5846, Accuracy:0.2608, Validation Loss:1.6124, Validation Accuracy:0.2003\n",
    "Epoch #182: Loss:1.5840, Accuracy:0.2501, Validation Loss:1.6162, Validation Accuracy:0.1970\n",
    "Epoch #183: Loss:1.5839, Accuracy:0.2563, Validation Loss:1.6142, Validation Accuracy:0.2003\n",
    "Epoch #184: Loss:1.5829, Accuracy:0.2595, Validation Loss:1.6133, Validation Accuracy:0.2020\n",
    "Epoch #185: Loss:1.5812, Accuracy:0.2653, Validation Loss:1.6111, Validation Accuracy:0.2233\n",
    "Epoch #186: Loss:1.5819, Accuracy:0.2637, Validation Loss:1.6103, Validation Accuracy:0.2348\n",
    "Epoch #187: Loss:1.5822, Accuracy:0.2608, Validation Loss:1.6151, Validation Accuracy:0.2085\n",
    "Epoch #188: Loss:1.5821, Accuracy:0.2595, Validation Loss:1.6155, Validation Accuracy:0.1954\n",
    "Epoch #189: Loss:1.5821, Accuracy:0.2620, Validation Loss:1.6136, Validation Accuracy:0.2233\n",
    "Epoch #190: Loss:1.5792, Accuracy:0.2674, Validation Loss:1.6150, Validation Accuracy:0.2069\n",
    "Epoch #191: Loss:1.5786, Accuracy:0.2669, Validation Loss:1.6179, Validation Accuracy:0.2020\n",
    "Epoch #192: Loss:1.5812, Accuracy:0.2669, Validation Loss:1.6155, Validation Accuracy:0.2036\n",
    "Epoch #193: Loss:1.5824, Accuracy:0.2645, Validation Loss:1.6114, Validation Accuracy:0.2217\n",
    "Epoch #194: Loss:1.5802, Accuracy:0.2665, Validation Loss:1.6097, Validation Accuracy:0.2020\n",
    "Epoch #195: Loss:1.5792, Accuracy:0.2665, Validation Loss:1.6125, Validation Accuracy:0.2036\n",
    "Epoch #196: Loss:1.5803, Accuracy:0.2600, Validation Loss:1.6133, Validation Accuracy:0.1954\n",
    "Epoch #197: Loss:1.5836, Accuracy:0.2575, Validation Loss:1.6200, Validation Accuracy:0.2020\n",
    "Epoch #198: Loss:1.5872, Accuracy:0.2632, Validation Loss:1.6174, Validation Accuracy:0.2151\n",
    "Epoch #199: Loss:1.5858, Accuracy:0.2550, Validation Loss:1.6145, Validation Accuracy:0.2036\n",
    "Epoch #200: Loss:1.5840, Accuracy:0.2616, Validation Loss:1.6110, Validation Accuracy:0.2151\n",
    "Epoch #201: Loss:1.5829, Accuracy:0.2550, Validation Loss:1.6163, Validation Accuracy:0.1938\n",
    "Epoch #202: Loss:1.5798, Accuracy:0.2583, Validation Loss:1.6142, Validation Accuracy:0.2151\n",
    "Epoch #203: Loss:1.5804, Accuracy:0.2595, Validation Loss:1.6137, Validation Accuracy:0.2200\n",
    "Epoch #204: Loss:1.5807, Accuracy:0.2604, Validation Loss:1.6165, Validation Accuracy:0.2167\n",
    "Epoch #205: Loss:1.5789, Accuracy:0.2604, Validation Loss:1.6171, Validation Accuracy:0.2167\n",
    "Epoch #206: Loss:1.5774, Accuracy:0.2628, Validation Loss:1.6154, Validation Accuracy:0.2233\n",
    "Epoch #207: Loss:1.5777, Accuracy:0.2682, Validation Loss:1.6142, Validation Accuracy:0.1954\n",
    "Epoch #208: Loss:1.5781, Accuracy:0.2616, Validation Loss:1.6146, Validation Accuracy:0.2233\n",
    "Epoch #209: Loss:1.5770, Accuracy:0.2678, Validation Loss:1.6150, Validation Accuracy:0.2053\n",
    "Epoch #210: Loss:1.5763, Accuracy:0.2686, Validation Loss:1.6169, Validation Accuracy:0.2085\n",
    "Epoch #211: Loss:1.5759, Accuracy:0.2628, Validation Loss:1.6149, Validation Accuracy:0.2003\n",
    "Epoch #212: Loss:1.5779, Accuracy:0.2669, Validation Loss:1.6162, Validation Accuracy:0.2069\n",
    "Epoch #213: Loss:1.5781, Accuracy:0.2587, Validation Loss:1.6168, Validation Accuracy:0.2250\n",
    "Epoch #214: Loss:1.5788, Accuracy:0.2559, Validation Loss:1.6181, Validation Accuracy:0.1938\n",
    "Epoch #215: Loss:1.5772, Accuracy:0.2604, Validation Loss:1.6154, Validation Accuracy:0.2233\n",
    "Epoch #216: Loss:1.5761, Accuracy:0.2661, Validation Loss:1.6160, Validation Accuracy:0.2069\n",
    "Epoch #217: Loss:1.5735, Accuracy:0.2653, Validation Loss:1.6193, Validation Accuracy:0.2085\n",
    "Epoch #218: Loss:1.5723, Accuracy:0.2674, Validation Loss:1.6170, Validation Accuracy:0.2003\n",
    "Epoch #219: Loss:1.5737, Accuracy:0.2637, Validation Loss:1.6186, Validation Accuracy:0.2020\n",
    "Epoch #220: Loss:1.5742, Accuracy:0.2653, Validation Loss:1.6171, Validation Accuracy:0.1987\n",
    "Epoch #221: Loss:1.5764, Accuracy:0.2657, Validation Loss:1.6182, Validation Accuracy:0.1921\n",
    "Epoch #222: Loss:1.5784, Accuracy:0.2612, Validation Loss:1.6176, Validation Accuracy:0.2003\n",
    "Epoch #223: Loss:1.5755, Accuracy:0.2698, Validation Loss:1.6153, Validation Accuracy:0.2184\n",
    "Epoch #224: Loss:1.5756, Accuracy:0.2645, Validation Loss:1.6158, Validation Accuracy:0.2167\n",
    "Epoch #225: Loss:1.5772, Accuracy:0.2620, Validation Loss:1.6148, Validation Accuracy:0.2332\n",
    "Epoch #226: Loss:1.5760, Accuracy:0.2632, Validation Loss:1.6155, Validation Accuracy:0.2365\n",
    "Epoch #227: Loss:1.5743, Accuracy:0.2682, Validation Loss:1.6141, Validation Accuracy:0.2200\n",
    "Epoch #228: Loss:1.5742, Accuracy:0.2661, Validation Loss:1.6163, Validation Accuracy:0.2299\n",
    "Epoch #229: Loss:1.5751, Accuracy:0.2665, Validation Loss:1.6191, Validation Accuracy:0.2003\n",
    "Epoch #230: Loss:1.5773, Accuracy:0.2645, Validation Loss:1.6158, Validation Accuracy:0.2299\n",
    "Epoch #231: Loss:1.5767, Accuracy:0.2645, Validation Loss:1.6174, Validation Accuracy:0.2069\n",
    "Epoch #232: Loss:1.5771, Accuracy:0.2616, Validation Loss:1.6215, Validation Accuracy:0.1987\n",
    "Epoch #233: Loss:1.5762, Accuracy:0.2706, Validation Loss:1.6219, Validation Accuracy:0.2365\n",
    "Epoch #234: Loss:1.5770, Accuracy:0.2624, Validation Loss:1.6196, Validation Accuracy:0.2020\n",
    "Epoch #235: Loss:1.5761, Accuracy:0.2706, Validation Loss:1.6193, Validation Accuracy:0.2167\n",
    "Epoch #236: Loss:1.5729, Accuracy:0.2690, Validation Loss:1.6169, Validation Accuracy:0.2053\n",
    "Epoch #237: Loss:1.5746, Accuracy:0.2694, Validation Loss:1.6148, Validation Accuracy:0.2365\n",
    "Epoch #238: Loss:1.5756, Accuracy:0.2661, Validation Loss:1.6177, Validation Accuracy:0.2020\n",
    "Epoch #239: Loss:1.5788, Accuracy:0.2579, Validation Loss:1.6180, Validation Accuracy:0.2167\n",
    "Epoch #240: Loss:1.5775, Accuracy:0.2649, Validation Loss:1.6096, Validation Accuracy:0.2282\n",
    "Epoch #241: Loss:1.5765, Accuracy:0.2702, Validation Loss:1.6102, Validation Accuracy:0.2299\n",
    "Epoch #242: Loss:1.5771, Accuracy:0.2587, Validation Loss:1.6133, Validation Accuracy:0.2381\n",
    "Epoch #243: Loss:1.5759, Accuracy:0.2682, Validation Loss:1.6141, Validation Accuracy:0.2069\n",
    "Epoch #244: Loss:1.5733, Accuracy:0.2653, Validation Loss:1.6130, Validation Accuracy:0.2200\n",
    "Epoch #245: Loss:1.5739, Accuracy:0.2739, Validation Loss:1.6136, Validation Accuracy:0.2200\n",
    "Epoch #246: Loss:1.5727, Accuracy:0.2567, Validation Loss:1.6175, Validation Accuracy:0.2315\n",
    "Epoch #247: Loss:1.5716, Accuracy:0.2710, Validation Loss:1.6163, Validation Accuracy:0.2200\n",
    "Epoch #248: Loss:1.5713, Accuracy:0.2706, Validation Loss:1.6177, Validation Accuracy:0.2250\n",
    "Epoch #249: Loss:1.5739, Accuracy:0.2682, Validation Loss:1.6236, Validation Accuracy:0.2053\n",
    "Epoch #250: Loss:1.5717, Accuracy:0.2669, Validation Loss:1.6174, Validation Accuracy:0.2332\n",
    "Epoch #251: Loss:1.5731, Accuracy:0.2719, Validation Loss:1.6165, Validation Accuracy:0.2233\n",
    "Epoch #252: Loss:1.5793, Accuracy:0.2628, Validation Loss:1.6231, Validation Accuracy:0.2332\n",
    "Epoch #253: Loss:1.5836, Accuracy:0.2554, Validation Loss:1.6196, Validation Accuracy:0.2036\n",
    "Epoch #254: Loss:1.5804, Accuracy:0.2612, Validation Loss:1.6179, Validation Accuracy:0.1872\n",
    "Epoch #255: Loss:1.5779, Accuracy:0.2542, Validation Loss:1.6159, Validation Accuracy:0.2184\n",
    "Epoch #256: Loss:1.5727, Accuracy:0.2624, Validation Loss:1.6140, Validation Accuracy:0.2036\n",
    "Epoch #257: Loss:1.5764, Accuracy:0.2534, Validation Loss:1.6134, Validation Accuracy:0.2217\n",
    "Epoch #258: Loss:1.5763, Accuracy:0.2661, Validation Loss:1.6135, Validation Accuracy:0.2151\n",
    "Epoch #259: Loss:1.5735, Accuracy:0.2665, Validation Loss:1.6161, Validation Accuracy:0.2315\n",
    "Epoch #260: Loss:1.5745, Accuracy:0.2653, Validation Loss:1.6168, Validation Accuracy:0.1938\n",
    "Epoch #261: Loss:1.5720, Accuracy:0.2587, Validation Loss:1.6130, Validation Accuracy:0.2414\n",
    "Epoch #262: Loss:1.5740, Accuracy:0.2641, Validation Loss:1.6185, Validation Accuracy:0.1970\n",
    "Epoch #263: Loss:1.5728, Accuracy:0.2575, Validation Loss:1.6231, Validation Accuracy:0.2102\n",
    "Epoch #264: Loss:1.5711, Accuracy:0.2637, Validation Loss:1.6256, Validation Accuracy:0.2069\n",
    "Epoch #265: Loss:1.5704, Accuracy:0.2669, Validation Loss:1.6252, Validation Accuracy:0.2102\n",
    "Epoch #266: Loss:1.5699, Accuracy:0.2674, Validation Loss:1.6220, Validation Accuracy:0.2102\n",
    "Epoch #267: Loss:1.5683, Accuracy:0.2637, Validation Loss:1.6213, Validation Accuracy:0.2036\n",
    "Epoch #268: Loss:1.5680, Accuracy:0.2637, Validation Loss:1.6211, Validation Accuracy:0.2299\n",
    "Epoch #269: Loss:1.5692, Accuracy:0.2645, Validation Loss:1.6224, Validation Accuracy:0.2085\n",
    "Epoch #270: Loss:1.5695, Accuracy:0.2665, Validation Loss:1.6231, Validation Accuracy:0.2167\n",
    "Epoch #271: Loss:1.5715, Accuracy:0.2706, Validation Loss:1.6214, Validation Accuracy:0.2315\n",
    "Epoch #272: Loss:1.5701, Accuracy:0.2686, Validation Loss:1.6235, Validation Accuracy:0.2200\n",
    "Epoch #273: Loss:1.5693, Accuracy:0.2686, Validation Loss:1.6181, Validation Accuracy:0.2447\n",
    "Epoch #274: Loss:1.5689, Accuracy:0.2723, Validation Loss:1.6190, Validation Accuracy:0.2167\n",
    "Epoch #275: Loss:1.5687, Accuracy:0.2694, Validation Loss:1.6276, Validation Accuracy:0.1938\n",
    "Epoch #276: Loss:1.5687, Accuracy:0.2637, Validation Loss:1.6265, Validation Accuracy:0.2250\n",
    "Epoch #277: Loss:1.5671, Accuracy:0.2731, Validation Loss:1.6251, Validation Accuracy:0.2069\n",
    "Epoch #278: Loss:1.5698, Accuracy:0.2694, Validation Loss:1.6232, Validation Accuracy:0.2282\n",
    "Epoch #279: Loss:1.5719, Accuracy:0.2743, Validation Loss:1.6203, Validation Accuracy:0.2167\n",
    "Epoch #280: Loss:1.5709, Accuracy:0.2723, Validation Loss:1.6254, Validation Accuracy:0.2151\n",
    "Epoch #281: Loss:1.5686, Accuracy:0.2801, Validation Loss:1.6240, Validation Accuracy:0.2003\n",
    "Epoch #282: Loss:1.5704, Accuracy:0.2710, Validation Loss:1.6263, Validation Accuracy:0.2069\n",
    "Epoch #283: Loss:1.5715, Accuracy:0.2661, Validation Loss:1.6304, Validation Accuracy:0.1954\n",
    "Epoch #284: Loss:1.5732, Accuracy:0.2608, Validation Loss:1.6318, Validation Accuracy:0.1987\n",
    "Epoch #285: Loss:1.5704, Accuracy:0.2686, Validation Loss:1.6305, Validation Accuracy:0.2184\n",
    "Epoch #286: Loss:1.5706, Accuracy:0.2710, Validation Loss:1.6218, Validation Accuracy:0.2299\n",
    "Epoch #287: Loss:1.5701, Accuracy:0.2772, Validation Loss:1.6227, Validation Accuracy:0.2332\n",
    "Epoch #288: Loss:1.5755, Accuracy:0.2612, Validation Loss:1.6234, Validation Accuracy:0.2250\n",
    "Epoch #289: Loss:1.5763, Accuracy:0.2719, Validation Loss:1.6221, Validation Accuracy:0.2282\n",
    "Epoch #290: Loss:1.5809, Accuracy:0.2665, Validation Loss:1.6172, Validation Accuracy:0.2365\n",
    "Epoch #291: Loss:1.5773, Accuracy:0.2678, Validation Loss:1.6220, Validation Accuracy:0.2282\n",
    "Epoch #292: Loss:1.5758, Accuracy:0.2723, Validation Loss:1.6224, Validation Accuracy:0.2250\n",
    "Epoch #293: Loss:1.5785, Accuracy:0.2669, Validation Loss:1.6179, Validation Accuracy:0.2233\n",
    "Epoch #294: Loss:1.5752, Accuracy:0.2694, Validation Loss:1.6171, Validation Accuracy:0.2200\n",
    "Epoch #295: Loss:1.5751, Accuracy:0.2686, Validation Loss:1.6177, Validation Accuracy:0.2250\n",
    "Epoch #296: Loss:1.5730, Accuracy:0.2789, Validation Loss:1.6160, Validation Accuracy:0.2200\n",
    "Epoch #297: Loss:1.5727, Accuracy:0.2731, Validation Loss:1.6170, Validation Accuracy:0.2250\n",
    "Epoch #298: Loss:1.5722, Accuracy:0.2752, Validation Loss:1.6210, Validation Accuracy:0.2217\n",
    "Epoch #299: Loss:1.5703, Accuracy:0.2793, Validation Loss:1.6252, Validation Accuracy:0.2217\n",
    "Epoch #300: Loss:1.5699, Accuracy:0.2772, Validation Loss:1.6243, Validation Accuracy:0.2200\n",
    "\n",
    "Test:\n",
    "Test Loss:1.62434292, Accuracy:0.2200\n",
    "Labels: ['03', '02', '01', '05', '04']\n",
    "Confusion Matrix:\n",
    "      03  02  01  05  04\n",
    "t:03  31   4  32  34  14\n",
    "t:02  25   9  25  38  17\n",
    "t:01  27  10  30  49  10\n",
    "t:05  34   9  30  50  19\n",
    "t:04  32  11  16  39  14\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.21      0.27      0.23       115\n",
    "          02       0.21      0.08      0.11       114\n",
    "          01       0.23      0.24      0.23       126\n",
    "          05       0.24      0.35      0.28       142\n",
    "          04       0.19      0.12      0.15       112\n",
    "\n",
    "    accuracy                           0.22       609\n",
    "   macro avg       0.21      0.21      0.20       609\n",
    "weighted avg       0.22      0.22      0.21       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 14:05:21 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6051635035544585, 1.6054794060185624, 1.6055192105680067, 1.6057203638142552, 1.6057950191701378, 1.6058526342529773, 1.6059381685820706, 1.6061492224632226, 1.6062600489320427, 1.606072230683563, 1.6057381054450726, 1.6059654252282505, 1.6060844056907742, 1.6056302545105883, 1.605509594547729, 1.605470856030782, 1.6050920341598185, 1.604630717503026, 1.6047092727056669, 1.6038022186172811, 1.6032249285474003, 1.603622356267594, 1.6029606695441385, 1.6033061408057, 1.60309345988413, 1.6033428934798843, 1.6038740607122286, 1.602246505677798, 1.6028456343414357, 1.602451245381523, 1.6019749841079336, 1.6022142698416373, 1.6022890397089065, 1.602746503693717, 1.6024415896248152, 1.6054071900488316, 1.603205969376713, 1.6032620181003814, 1.6038795331820284, 1.6043028743396253, 1.6025980992857458, 1.6028462656221563, 1.6020451110767808, 1.6026130855964322, 1.602349311260167, 1.6024058497402272, 1.60186855370188, 1.602037694458108, 1.6026932701884429, 1.602902070250613, 1.6025128544649272, 1.6023545617540482, 1.601692479623754, 1.603390141269452, 1.6039600601337227, 1.6047520837173086, 1.6041590531275582, 1.6048239911914068, 1.6018854700677305, 1.6023246443330361, 1.6034647328121516, 1.604103421147038, 1.6032460747876973, 1.603578839591767, 1.6043181695374362, 1.6040430646420307, 1.6040561766851515, 1.6043251386808448, 1.6030694408761261, 1.6030943742135084, 1.6030462432181698, 1.6038604608701759, 1.6037887479675619, 1.604107077877314, 1.6047152909152027, 1.604647232198167, 1.6044285138839571, 1.6054645449852905, 1.6050897560683377, 1.6051323036059175, 1.605284264326487, 1.6055287537707876, 1.6064972859885305, 1.6054928040465306, 1.6047317701803248, 1.6047218260898184, 1.6055367194568777, 1.6055539842505369, 1.6067544161196805, 1.6070756745847379, 1.6070224395134962, 1.607065858707835, 1.6076374277105472, 1.6071193531424737, 1.6090699974539244, 1.6089590764398058, 1.6110269872621559, 1.6091041459238589, 1.6089696715813748, 1.6122755479734323, 1.6098780835594841, 1.6064955990498484, 1.6072640747859561, 1.6056424561392497, 1.604893298767666, 1.607823737344914, 1.6096458671911205, 1.6080403376878385, 1.6112299094646436, 1.609717490050593, 1.6074527936615968, 1.6084584426410093, 1.6106150383236764, 1.6101959205809093, 1.6102453566145623, 1.6083633568878049, 1.6105697523001183, 1.6083812420004107, 1.6092740438254596, 1.6102436718290858, 1.612482058982348, 1.6109180632483195, 1.6111578972664569, 1.6116658288661287, 1.6094166938894487, 1.6077346102944736, 1.6084373672607497, 1.6111636138314684, 1.6089512650015318, 1.6075695954715872, 1.607482880011372, 1.6105055480167783, 1.6080274983188398, 1.6087987272218727, 1.610355990273612, 1.6126419826485645, 1.61066669431226, 1.6120809250081505, 1.6114759276848905, 1.6094338052182753, 1.6091629769806008, 1.6113618451777743, 1.612815091175399, 1.6106811738366564, 1.6106593847666273, 1.6091622805164756, 1.6113146936952187, 1.6088611674426225, 1.6106189724259776, 1.6099131729802474, 1.6079171739384066, 1.610246806896379, 1.6108524740623136, 1.6118413575960107, 1.6079923354933414, 1.6090760140974925, 1.609125140069545, 1.608188470596163, 1.6073700630018863, 1.6063810614333756, 1.6091826318324298, 1.6104822256686457, 1.608315304778088, 1.6075811002446316, 1.6124927747034283, 1.6126220711737822, 1.6125198087864516, 1.6124570271847478, 1.6115758305504209, 1.6132297290761286, 1.6137821515792696, 1.6153143945781665, 1.6129397619729755, 1.61039528881975, 1.612595956705278, 1.6127898307465176, 1.6118817168895052, 1.612366302064291, 1.6150061949133285, 1.6131874340508372, 1.612352391769146, 1.6161966997218642, 1.614198464478178, 1.6132637035279047, 1.611104326099402, 1.6103340814070553, 1.6150618092767124, 1.6154750278234873, 1.6135709636121351, 1.61503307357406, 1.6179034138352217, 1.6155001819623123, 1.6114117582443312, 1.6097477662739494, 1.6124575326008161, 1.6132622520716124, 1.6200326726158656, 1.6173682778535414, 1.6145373843182091, 1.6110406967219462, 1.6163257086414031, 1.6142228332842121, 1.6137198673680497, 1.6164619961787132, 1.6170883294201053, 1.6153580919275143, 1.6142019293774132, 1.6145736407763853, 1.6149664582877323, 1.616878417716629, 1.6148838221733206, 1.6162481415643677, 1.6167914665782785, 1.6180780010270368, 1.6154492500380342, 1.615973720605346, 1.619285918613177, 1.6169805704862221, 1.618632074255857, 1.6171267841054104, 1.6181879070983536, 1.6175565120621855, 1.615332563913906, 1.6157950681614366, 1.6148294846608329, 1.6155408705005114, 1.6140876313539954, 1.6162975588064084, 1.619057414566942, 1.6157924165866646, 1.617375613041895, 1.6215208490885342, 1.6219230408738987, 1.6195654350352797, 1.6192849459515024, 1.616946387564999, 1.6148000855751226, 1.6177397809788119, 1.6180051950789829, 1.609639043682706, 1.6101839652006653, 1.6133454178745914, 1.6141274458864836, 1.612994918878051, 1.6136486154471712, 1.617452430803396, 1.616324021702721, 1.6176520053584784, 1.6236095530254695, 1.617421338319387, 1.6165048921441014, 1.6231062112770644, 1.61959092370395, 1.617854523149813, 1.6159136277701467, 1.6139640156271422, 1.6133668455975787, 1.6135061452737192, 1.616130482191327, 1.6168018526631622, 1.6130068924626693, 1.6185373686413067, 1.6231297757629495, 1.625570731014258, 1.6251725269655877, 1.6220236771995407, 1.6213411306121275, 1.621111380447112, 1.6224086615448123, 1.6231109906104202, 1.6213612004453912, 1.623490509337001, 1.618057775771481, 1.619041646642638, 1.6276407053905169, 1.6264826405811779, 1.625089914732183, 1.6231507963343403, 1.620275845864332, 1.6253688965720692, 1.6239974336279632, 1.6263208042811879, 1.6304461020358483, 1.6317976848245255, 1.63046784980348, 1.6217640896736107, 1.6226892782549553, 1.6234385188185718, 1.6221183440563909, 1.6172070383829829, 1.6220094086892891, 1.6223859207579263, 1.6178562218332526, 1.6170821358221896, 1.6176913121259466, 1.615993672795288, 1.6170298732168764, 1.620994186949456, 1.6252026217324393, 1.624342845969991], 'val_acc': [0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23809523807076985, 0.24137930814268554, 0.23316912950063964, 0.22988505725105016, 0.2397372719200178, 0.23316912950063964, 0.24302134644515408, 0.24466338058802098, 0.2545155972389165, 0.24302134456109922, 0.2331691275187118, 0.23316912950063964, 0.24302134644515408, 0.24794745264186452, 0.2298850552691223, 0.23316912771445777, 0.24466338068589397, 0.24302134644515408, 0.2463054168106887, 0.2413793083384315, 0.2413793083384315, 0.24466338068589397, 0.2463054168106887, 0.24302134456109922, 0.23645319996404726, 0.23645319996404726, 0.23152709149179004, 0.23973727240938272, 0.23152709158966303, 0.23645319986617427, 0.22824301924220056, 0.22495894689473808, 0.22988505517124935, 0.24137930804481256, 0.2331691276165848, 0.2298850554648683, 0.23481116383925252, 0.23152709149179004, 0.24137930853417747, 0.22495894699261107, 0.22167487474302158, 0.2183908023955591, 0.21839080249343207, 0.2413793083384315, 0.22495894699261107, 0.22003283852035385, 0.22003283852035385, 0.21674876627076436, 0.2183908023955591, 0.21674876627076436, 0.22331691086781633, 0.22167487474302158, 0.21674876627076436, 0.22331691067207035, 0.21674876617289138, 0.21839080249343207, 0.21674876636863732, 0.21674876627076436, 0.2183908023955591, 0.22331691086781633, 0.22331691086781633, 0.2266009832152788, 0.21674876617289138, 0.2298850554648683, 0.2266009831174058, 0.2200328387160998, 0.22167487484089454, 0.21839080249343207, 0.22331691096568929, 0.2200328387160998, 0.21510673043958856, 0.22660098341102475, 0.20853858564679062, 0.21839080259130506, 0.2167487666622563, 0.20689654971774185, 0.22331691116143526, 0.23152709168753602, 0.22824301914432757, 0.2364532001597932, 0.22167487493876753, 0.2085385857446636, 0.22331691096568929, 0.2200328388139728, 0.198686370977823, 0.2118226578963801, 0.22331691106356227, 0.22495894709048403, 0.2118226578963801, 0.21182265997618094, 0.21510673024384258, 0.22495894709048403, 0.22003283852035385, 0.21674876627076436, 0.2266009832152788, 0.22331691076994334, 0.21674876627076436, 0.2200328388139728, 0.2183908023955591, 0.21674876636863732, 0.2216748751345135, 0.22167487474302158, 0.22167487484089454, 0.22495894699261107, 0.22495894709048403, 0.2052545135929471, 0.21839080229768612, 0.2298850554648683, 0.23152709178540898, 0.22003283891184577, 0.21182265779850715, 0.20525451329932815, 0.21510673024384258, 0.2118226579942531, 0.21510673034171557, 0.21510673024384258, 0.21182265818999907, 0.2266009832152788, 0.2183908045732329, 0.20197044332528544, 0.22167487493876753, 0.20361247925433423, 0.21839080229768612, 0.21346469402117488, 0.21510673024384258, 0.22824301914432757, 0.22824301914432757, 0.21839080249343207, 0.21674876636863732, 0.22167487474302158, 0.2118226578963801, 0.21346469421692083, 0.2282430195358195, 0.21510673004809663, 0.23152709149179004, 0.22003283852035385, 0.21510673004809663, 0.21346469619884867, 0.21018062394925918, 0.2068965516996697, 0.22003283861822684, 0.21182266007405393, 0.19868637107569595, 0.21674876636863732, 0.20197044114761165, 0.19868636880014917, 0.20689654971774185, 0.21510673004809663, 0.21510673024384258, 0.21510673043958856, 0.20853858554891766, 0.20197044332528544, 0.21839080278705103, 0.20197044104973866, 0.19540229863036052, 0.20853858554891766, 0.20032840492494391, 0.19704433267535443, 0.2003284050228169, 0.2019704413433576, 0.22331691106356227, 0.2348111639371255, 0.20853858782446444, 0.19540229872823348, 0.22331691086781633, 0.2068965516996697, 0.20197044322741248, 0.2036124772724064, 0.2216748746451486, 0.20197044104973866, 0.20361247935220722, 0.19540229882610646, 0.20197044124548463, 0.21510673014596962, 0.2036124771745334, 0.21510673043958856, 0.19376026240769278, 0.21510673024384258, 0.22003283861822684, 0.21674876627076436, 0.21674876627076436, 0.22331691294761713, 0.19540229882610646, 0.22331691067207035, 0.20525451349507412, 0.20853858554891766, 0.2003284050228169, 0.20689654942412292, 0.22495894689473808, 0.19376026240769278, 0.22331691106356227, 0.20689654942412292, 0.2085385875308455, 0.20032840680899877, 0.20197044332528544, 0.19868636889802216, 0.19211822657651698, 0.2003284050228169, 0.21839080278705103, 0.21674876636863732, 0.2331691275187118, 0.23645319976830131, 0.22003283842248086, 0.22988505725105016, 0.20032840700474475, 0.22988505517124935, 0.20689655140605073, 0.19868637078207702, 0.23645319976830131, 0.20197044104973866, 0.21674876844843816, 0.20525451349507412, 0.23645319996404726, 0.20197044104973866, 0.21674876636863732, 0.22824301934007354, 0.2298850553669953, 0.23809523599096902, 0.2068965516017967, 0.22003283842248086, 0.22003284069802764, 0.2315270912960441, 0.22003284069802764, 0.22495894699261107, 0.20525451329932815, 0.2331691275187118, 0.22331691096568929, 0.23316912771445777, 0.20361247746815236, 0.18719211602445893, 0.21839080259130506, 0.20361247737027938, 0.22167487503664052, 0.2151067307332075, 0.23152709149179004, 0.19376026042576494, 0.24137930814268554, 0.19704433475515526, 0.2101806216737124, 0.20689654942412292, 0.21018062177158536, 0.2101806216737124, 0.2036124794500802, 0.22988505517124935, 0.20853858782446444, 0.21674876617289138, 0.23152709149179004, 0.22003283852035385, 0.24466338049014802, 0.21674876627076436, 0.19376026240769278, 0.2249589489745389, 0.20689654952199588, 0.22824301914432757, 0.21674876627076436, 0.21510673004809663, 0.20032840690687176, 0.2068965516996697, 0.19540229872823348, 0.198686370977823, 0.21839080268917804, 0.22988505566061423, 0.23316912771445777, 0.22495894699261107, 0.2282430194379465, 0.2364532001597932, 0.22824301924220056, 0.22495894718835702, 0.22331691106356227, 0.2200328387160998, 0.22495894709048403, 0.2200328387160998, 0.22495894718835702, 0.2216748746451486, 0.22167487503664052, 0.22003283861822684], 'loss': [1.607418015066848, 1.6054956724511524, 1.6052899649011036, 1.6049695844278198, 1.6049117165669278, 1.6048564365022726, 1.6052170899369633, 1.6054576298049832, 1.6058441127350198, 1.605498549433949, 1.605437268368762, 1.604856686621476, 1.604883594777305, 1.6048349917057358, 1.6045356339742516, 1.6043621536153052, 1.6045091835135552, 1.6042840238469338, 1.604124929527972, 1.6037932205004368, 1.6032634681744742, 1.6029742244332723, 1.6027492158956351, 1.6022792400031118, 1.6019567273235908, 1.6014981778495365, 1.6029221471575006, 1.603464521177006, 1.6025174501250656, 1.6027404376368748, 1.6018028119261505, 1.6017892020928541, 1.601334702601423, 1.602839834978938, 1.602192646277269, 1.6019201324705716, 1.6032348399779146, 1.601633787204108, 1.6017046261617045, 1.602235825350642, 1.6012139822668119, 1.6017269818934572, 1.601823407962337, 1.6015514717944104, 1.6014002939514067, 1.6010208775620196, 1.6009072262158874, 1.60037962778393, 1.6001497912945444, 1.5999131256549999, 1.5995651151610106, 1.5999062318821462, 1.5996166394231746, 1.6005513287178055, 1.59948612772219, 1.5999588400431481, 1.599034976665489, 1.5994017515828722, 1.6004457697976051, 1.599994784165212, 1.599863245403987, 1.6006199296739803, 1.600462265034231, 1.5991433763895682, 1.5990963056591747, 1.5992843031638457, 1.5988623855539905, 1.5983987542882836, 1.5984996802018654, 1.5979960919405645, 1.5977559288913954, 1.5972548377587321, 1.5968137442453687, 1.597092617806468, 1.5964835156160704, 1.59645211951199, 1.5960901877228975, 1.5965856919787993, 1.596183935230028, 1.5958702833990297, 1.5955759821486424, 1.5954998469695418, 1.5953088134710793, 1.5952305113265646, 1.594884220971219, 1.5950089588791927, 1.5953376704417706, 1.594665734576983, 1.5941598711562108, 1.5940729730183094, 1.5939913406998714, 1.5943394972803167, 1.594426049344104, 1.5940508836103906, 1.5941818893322954, 1.5922131104146187, 1.5930221209535853, 1.593102219560063, 1.5920918428922335, 1.5919217478812842, 1.592762076536488, 1.5926772053971183, 1.5927973335038954, 1.5926690702320858, 1.5929506959366848, 1.592511174370376, 1.5939003704999262, 1.5943158644180768, 1.5939697260729342, 1.5961420546811709, 1.594427483478366, 1.5925800789553037, 1.593064253971562, 1.5928120414823967, 1.591481294475297, 1.5913720666505473, 1.5912370216185552, 1.5912776159309998, 1.5906789615658519, 1.5898731540605517, 1.5898323214274412, 1.5915005048442425, 1.5920724575034892, 1.5914099536147694, 1.5912249033945542, 1.5887027298400533, 1.5890974964694076, 1.5890891361530313, 1.5889078868732804, 1.589176491153803, 1.589059048266871, 1.5877510053665975, 1.5877282509323998, 1.5886023858734224, 1.5873615433792803, 1.5871671677125307, 1.5861054836112616, 1.5865784024311043, 1.5898658828813683, 1.5880771367701663, 1.5862110764094202, 1.5883845994604686, 1.5869796929662967, 1.5878955584042371, 1.5887522894743777, 1.5890915193107338, 1.5886861370329495, 1.586984178515675, 1.587283139454021, 1.5880144277882038, 1.5878328821252747, 1.5876736600785775, 1.5884944292064564, 1.5869448001869406, 1.5869360863550488, 1.587291460360345, 1.5879129743184397, 1.5873192797940858, 1.5868430235057887, 1.5858747540558142, 1.5850516269339183, 1.5839107482585084, 1.5841183051436345, 1.5833029277515607, 1.5831607374567271, 1.5829372108839377, 1.5819194087747186, 1.58339813411603, 1.5825714475565134, 1.583229250291045, 1.582800017783774, 1.58313491285704, 1.5811232626070966, 1.5812454593744611, 1.5807086732598057, 1.5831198054907014, 1.5810960927783098, 1.583345654221286, 1.5826283367752294, 1.5834774230784705, 1.5846179679678696, 1.5839934686370944, 1.583886919129311, 1.5829389374848508, 1.5811943585378188, 1.5819472982653358, 1.5822185571678364, 1.582110839115276, 1.5821146469096627, 1.5792174839141189, 1.57857374420401, 1.5811950760945157, 1.582437865738996, 1.5802328557204417, 1.5791913010011709, 1.58025754550644, 1.5835747526901214, 1.5871738609102473, 1.5858179655897544, 1.5839516937365523, 1.5828647639961948, 1.5797521110922403, 1.580421797303938, 1.5807407622954195, 1.5789433263410533, 1.5774072743049636, 1.5776758781938338, 1.5781233294054224, 1.5770214814180221, 1.5763445599123194, 1.5759266271238699, 1.5779025736775487, 1.5781173487218743, 1.5787531026334978, 1.5772240167280978, 1.5760700751623824, 1.5734674605500771, 1.5722971020782752, 1.573676033529168, 1.5742111458181112, 1.5763833243744085, 1.5784335938812037, 1.57549652044778, 1.5755740476584776, 1.5771886521051552, 1.5759584707890693, 1.574279958609438, 1.5741975609037175, 1.575076357442006, 1.57734464810859, 1.5767231209811734, 1.5770983355735606, 1.576228492852354, 1.576988039026515, 1.5760880520701164, 1.5728501660133534, 1.574640352476304, 1.5755980122015951, 1.5788251599736771, 1.577491871874925, 1.576453468442208, 1.5771213651927345, 1.575902193723518, 1.573293525384437, 1.5739090943483356, 1.5727127749817083, 1.5715521709140565, 1.5713056542790156, 1.5738980341740947, 1.5716778667555698, 1.5730704682074044, 1.5793048115481587, 1.5835787069136602, 1.580401701555115, 1.5778797732731156, 1.5727002602093518, 1.576406491806375, 1.5763456354885375, 1.5734896301488857, 1.5744978624202877, 1.5719804067631278, 1.5739600141924754, 1.5728127335620856, 1.5710909878693566, 1.5704221286812847, 1.5698564516200666, 1.5683458477075094, 1.5680203387379892, 1.5692158396483937, 1.5694730449261363, 1.5715138284087915, 1.5700964272634204, 1.5692663767499355, 1.5688614186809782, 1.5686894852033142, 1.568741898468143, 1.5671270135981346, 1.5697631552968427, 1.571851111633332, 1.5708624199920123, 1.5685803712026294, 1.5704184062671858, 1.5715099702870332, 1.5731986155500157, 1.5703940865929855, 1.57055194480708, 1.570095112093665, 1.5754762708773604, 1.57628110884641, 1.5809381854118019, 1.5772644896282062, 1.5757941334154573, 1.5785224847480257, 1.5751719481156836, 1.5751223881386633, 1.5730237198071804, 1.5726505192887856, 1.5722474069810746, 1.570316478848702, 1.5699023969853927], 'acc': [0.2123203291968888, 0.2344969221209109, 0.2328542102778472, 0.23285420990455322, 0.23285420953125924, 0.23285421049203225, 0.23285421029620593, 0.23285420931707418, 0.23285420892542147, 0.23285421029620593, 0.2328542087112364, 0.23285420890706277, 0.23285421049203225, 0.23285420851541005, 0.23285421010037957, 0.2328542094945418, 0.23285420953125924, 0.23285420912124782, 0.23285420970872686, 0.2328542091028891, 0.2328542094945418, 0.23285421011873828, 0.23285420912124782, 0.23285420951290053, 0.23449691937934203, 0.24229979570763802, 0.23531827600462482, 0.23285421008202084, 0.23613963004744762, 0.23244353189483072, 0.23285420851541005, 0.2406570830629102, 0.24229979588510564, 0.24106776187429682, 0.24353182856184746, 0.23901437317810997, 0.23449691879186296, 0.23983572802259692, 0.2381930185294494, 0.23983572802259692, 0.24024640564066674, 0.245174539015768, 0.23983572802259692, 0.24188911924616757, 0.24229979374937452, 0.24106776185593812, 0.2398357284142496, 0.24517453686167817, 0.24271047213239103, 0.2422997945326799, 0.24435318201719122, 0.24312114935880813, 0.24106776087680637, 0.23942505179367027, 0.2398357284142496, 0.24435318456293376, 0.24271047291569642, 0.2431211503562986, 0.24188911748373043, 0.24229979510180025, 0.2439425063573849, 0.23983572921591373, 0.24106776126845905, 0.24312115055212494, 0.24229979551181166, 0.2431211477554799, 0.24188911509709682, 0.24106776087680637, 0.24147844025731333, 0.24147844007984567, 0.24312114975046084, 0.24476386237682993, 0.24476386122023056, 0.2447638602227401, 0.24188911826703582, 0.24188911789374185, 0.2422997945326799, 0.2431211507479513, 0.2435318263893989, 0.2431211499462872, 0.24353182776018334, 0.24394250420329508, 0.24353182797436843, 0.2443531837979871, 0.24722792495202725, 0.24435318319214933, 0.2443531837979871, 0.24887063757839634, 0.24763860296174975, 0.2447638598310874, 0.24558521626054383, 0.24640657226163015, 0.24681724911475328, 0.24517453803663625, 0.24517453784080992, 0.24928131501899853, 0.2468172496838736, 0.24517453725333085, 0.24394250299161954, 0.2447638610244042, 0.24640657090920443, 0.2484599578062367, 0.25133470328681523, 0.24681724852727424, 0.24517453881994164, 0.24271047291569642, 0.2377823417130437, 0.24887063718674365, 0.248870637009276, 0.2484599607436319, 0.24969199242288326, 0.24599589483938666, 0.24845995839371574, 0.24804928018816688, 0.24353182817019475, 0.24969199085627247, 0.2443531818213649, 0.2418891176979155, 0.24969199144375154, 0.24599589483938666, 0.2517453799257533, 0.24517453785916862, 0.24558521545887974, 0.25503079867950457, 0.2570841897439663, 0.25174538168819044, 0.2509240228904591, 0.25215605854131357, 0.24763860415506656, 0.2533880909855116, 0.26078028850242096, 0.25051334628823846, 0.24681724890056822, 0.24599589270365557, 0.2492813142173344, 0.2554414794675134, 0.2603696075185858, 0.2505133468757175, 0.25133470172020445, 0.25215605852295486, 0.25585215806471495, 0.24969199263706834, 0.25174538207984315, 0.2554414790758607, 0.25585215532314604, 0.25585215767306224, 0.2517453809232438, 0.253798767232797, 0.25215605715217043, 0.2521560569563441, 0.24681724831308918, 0.26036960771441214, 0.26406570846784777, 0.2529774129757891, 0.25667351212589645, 0.25585215411147055, 0.2505133480506756, 0.2509240239063083, 0.2587269011953773, 0.2579055440009742, 0.2616016425636025, 0.2595482554523852, 0.2611909669037962, 0.25790554278929867, 0.2517453807274174, 0.25420944442249666, 0.2620123185783441, 0.2644763849109595, 0.2632443530358818, 0.2583162231856548, 0.25913757861762077, 0.260780285919961, 0.2706365511402702, 0.26447638608591756, 0.26817248265356497, 0.260780285332482, 0.26036961108017753, 0.2505133484423283, 0.25790554537175864, 0.25215605852295486, 0.26078028791494196, 0.25010266867016867, 0.256262834330359, 0.2595482548832649, 0.26529774112623083, 0.2636550330038678, 0.2607802888940737, 0.2595482558440379, 0.2620123225315885, 0.26735113137066974, 0.26694045120685744, 0.2669404496035292, 0.2644763872608757, 0.26652977633035646, 0.26652977300130853, 0.2599589328746286, 0.25749486638290436, 0.2632443534275345, 0.2550308014577909, 0.2616016431694403, 0.25503080067448547, 0.2583162226165345, 0.2595482548832649, 0.26036961108017753, 0.26036961010104576, 0.2628336736186574, 0.26817248464854593, 0.2616016431510816, 0.26776180918456594, 0.2685831628540948, 0.26283367342283104, 0.26694045140268374, 0.25872689925547254, 0.25585215649810417, 0.2603696083018912, 0.2661190965581968, 0.2652977417137099, 0.2673511280049044, 0.26365503202473606, 0.26529774034292547, 0.2657084165534934, 0.26119096514135903, 0.2698151956715868, 0.26447638765252834, 0.26201232194410945, 0.2632443526442291, 0.26817248562767765, 0.26611909636237047, 0.26652977417626667, 0.2644763878483547, 0.264476387456702, 0.26160164295525523, 0.2706365525110546, 0.262422999953832, 0.27063655016113847, 0.26899383867301, 0.26940451946101884, 0.2661190967540232, 0.2579055446068119, 0.2648870642914664, 0.27022587489298483, 0.2587269000204192, 0.268172485823504, 0.26529774094876324, 0.27392197028567417, 0.2566735109509384, 0.27104722719172925, 0.2706365515319229, 0.2681724838285231, 0.26694045336094724, 0.2718685831744568, 0.26283367541781194, 0.2554414779009026, 0.26119096396640096, 0.25420944778826204, 0.2624229976039158, 0.25338809139552304, 0.2661190955790651, 0.2665297735887876, 0.26529774230118897, 0.25872689884546113, 0.2640657078803687, 0.2574948677536888, 0.2636550284631443, 0.26694045081520473, 0.26735112960823265, 0.26365502967481985, 0.26365503202473606, 0.264476386869223, 0.2665297745862781, 0.27063654797033115, 0.2685831632641062, 0.2685831598799821, 0.2722792618083758, 0.2694045188735398, 0.26365503124143064, 0.27310061626121, 0.26940451946101884, 0.2743326495070722, 0.27227926161254945, 0.2800821377266604, 0.2710472291132753, 0.2661190987122867, 0.2607802888940737, 0.26858316189332176, 0.27104722637170636, 0.2772073938126926, 0.2611909653371854, 0.2718685830153479, 0.26652977535122474, 0.26776180820543416, 0.2722792584426104, 0.2669404535567736, 0.26940451671945, 0.2685831622666157, 0.27885010091920653, 0.2731006142662291, 0.2751540045473855, 0.27926078029971346, 0.2772073906427536]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
