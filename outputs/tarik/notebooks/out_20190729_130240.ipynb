{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf2.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 13:02:40 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '0', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '04', '02', '05', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002652A1BBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000026526957EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6072, Accuracy:0.2324, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6056, Accuracy:0.2349, Validation Loss:1.6061, Validation Accuracy:0.2315\n",
    "Epoch #4: Loss:1.6048, Accuracy:0.2333, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6041, Accuracy:0.2349, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6038, Accuracy:0.2324, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6034, Accuracy:0.2345, Validation Loss:1.6052, Validation Accuracy:0.2315\n",
    "Epoch #8: Loss:1.6029, Accuracy:0.2279, Validation Loss:1.6054, Validation Accuracy:0.2250\n",
    "Epoch #9: Loss:1.6029, Accuracy:0.2316, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #10: Loss:1.6022, Accuracy:0.2296, Validation Loss:1.6038, Validation Accuracy:0.2381\n",
    "Epoch #11: Loss:1.6019, Accuracy:0.2349, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #12: Loss:1.6014, Accuracy:0.2390, Validation Loss:1.6040, Validation Accuracy:0.2430\n",
    "Epoch #13: Loss:1.6009, Accuracy:0.2394, Validation Loss:1.6032, Validation Accuracy:0.2430\n",
    "Epoch #14: Loss:1.6006, Accuracy:0.2419, Validation Loss:1.6033, Validation Accuracy:0.2414\n",
    "Epoch #15: Loss:1.6000, Accuracy:0.2444, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #16: Loss:1.5999, Accuracy:0.2460, Validation Loss:1.6029, Validation Accuracy:0.2397\n",
    "Epoch #17: Loss:1.6000, Accuracy:0.2452, Validation Loss:1.6034, Validation Accuracy:0.2315\n",
    "Epoch #18: Loss:1.6001, Accuracy:0.2427, Validation Loss:1.6040, Validation Accuracy:0.2315\n",
    "Epoch #19: Loss:1.5997, Accuracy:0.2444, Validation Loss:1.6049, Validation Accuracy:0.2365\n",
    "Epoch #20: Loss:1.6004, Accuracy:0.2423, Validation Loss:1.6047, Validation Accuracy:0.2397\n",
    "Epoch #21: Loss:1.6008, Accuracy:0.2407, Validation Loss:1.6048, Validation Accuracy:0.2397\n",
    "Epoch #22: Loss:1.6015, Accuracy:0.2444, Validation Loss:1.6057, Validation Accuracy:0.2447\n",
    "Epoch #23: Loss:1.6011, Accuracy:0.2427, Validation Loss:1.6043, Validation Accuracy:0.2414\n",
    "Epoch #24: Loss:1.6007, Accuracy:0.2435, Validation Loss:1.6033, Validation Accuracy:0.2496\n",
    "Epoch #25: Loss:1.6001, Accuracy:0.2448, Validation Loss:1.6031, Validation Accuracy:0.2479\n",
    "Epoch #26: Loss:1.6004, Accuracy:0.2448, Validation Loss:1.6025, Validation Accuracy:0.2397\n",
    "Epoch #27: Loss:1.5998, Accuracy:0.2464, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #28: Loss:1.5994, Accuracy:0.2464, Validation Loss:1.6030, Validation Accuracy:0.2463\n",
    "Epoch #29: Loss:1.5989, Accuracy:0.2468, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #30: Loss:1.5980, Accuracy:0.2501, Validation Loss:1.6028, Validation Accuracy:0.2447\n",
    "Epoch #31: Loss:1.5977, Accuracy:0.2546, Validation Loss:1.6026, Validation Accuracy:0.2381\n",
    "Epoch #32: Loss:1.5978, Accuracy:0.2534, Validation Loss:1.6029, Validation Accuracy:0.2414\n",
    "Epoch #33: Loss:1.5977, Accuracy:0.2522, Validation Loss:1.6025, Validation Accuracy:0.2397\n",
    "Epoch #34: Loss:1.5978, Accuracy:0.2517, Validation Loss:1.6042, Validation Accuracy:0.2381\n",
    "Epoch #35: Loss:1.5980, Accuracy:0.2476, Validation Loss:1.6043, Validation Accuracy:0.2381\n",
    "Epoch #36: Loss:1.5977, Accuracy:0.2513, Validation Loss:1.6040, Validation Accuracy:0.2397\n",
    "Epoch #37: Loss:1.5971, Accuracy:0.2509, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #38: Loss:1.5965, Accuracy:0.2505, Validation Loss:1.6063, Validation Accuracy:0.2348\n",
    "Epoch #39: Loss:1.5967, Accuracy:0.2509, Validation Loss:1.6035, Validation Accuracy:0.2430\n",
    "Epoch #40: Loss:1.5969, Accuracy:0.2526, Validation Loss:1.6036, Validation Accuracy:0.2397\n",
    "Epoch #41: Loss:1.5989, Accuracy:0.2509, Validation Loss:1.6025, Validation Accuracy:0.2414\n",
    "Epoch #42: Loss:1.5994, Accuracy:0.2480, Validation Loss:1.6034, Validation Accuracy:0.2282\n",
    "Epoch #43: Loss:1.6007, Accuracy:0.2382, Validation Loss:1.6039, Validation Accuracy:0.2315\n",
    "Epoch #44: Loss:1.6008, Accuracy:0.2308, Validation Loss:1.6058, Validation Accuracy:0.2250\n",
    "Epoch #45: Loss:1.6017, Accuracy:0.2345, Validation Loss:1.6049, Validation Accuracy:0.2217\n",
    "Epoch #46: Loss:1.6012, Accuracy:0.2398, Validation Loss:1.6034, Validation Accuracy:0.2397\n",
    "Epoch #47: Loss:1.6019, Accuracy:0.2485, Validation Loss:1.6032, Validation Accuracy:0.2397\n",
    "Epoch #48: Loss:1.6004, Accuracy:0.2415, Validation Loss:1.6040, Validation Accuracy:0.2447\n",
    "Epoch #49: Loss:1.6007, Accuracy:0.2394, Validation Loss:1.6019, Validation Accuracy:0.2200\n",
    "Epoch #50: Loss:1.5995, Accuracy:0.2493, Validation Loss:1.6020, Validation Accuracy:0.2430\n",
    "Epoch #51: Loss:1.5991, Accuracy:0.2485, Validation Loss:1.6021, Validation Accuracy:0.2430\n",
    "Epoch #52: Loss:1.5995, Accuracy:0.2411, Validation Loss:1.6031, Validation Accuracy:0.2282\n",
    "Epoch #53: Loss:1.6005, Accuracy:0.2439, Validation Loss:1.6025, Validation Accuracy:0.2233\n",
    "Epoch #54: Loss:1.5996, Accuracy:0.2460, Validation Loss:1.6023, Validation Accuracy:0.2365\n",
    "Epoch #55: Loss:1.5985, Accuracy:0.2501, Validation Loss:1.6023, Validation Accuracy:0.2266\n",
    "Epoch #56: Loss:1.5985, Accuracy:0.2468, Validation Loss:1.6027, Validation Accuracy:0.2200\n",
    "Epoch #57: Loss:1.5991, Accuracy:0.2435, Validation Loss:1.6029, Validation Accuracy:0.2397\n",
    "Epoch #58: Loss:1.5978, Accuracy:0.2489, Validation Loss:1.6022, Validation Accuracy:0.2233\n",
    "Epoch #59: Loss:1.5972, Accuracy:0.2522, Validation Loss:1.6012, Validation Accuracy:0.2250\n",
    "Epoch #60: Loss:1.5971, Accuracy:0.2489, Validation Loss:1.6016, Validation Accuracy:0.2250\n",
    "Epoch #61: Loss:1.5970, Accuracy:0.2456, Validation Loss:1.6019, Validation Accuracy:0.2365\n",
    "Epoch #62: Loss:1.5969, Accuracy:0.2493, Validation Loss:1.6010, Validation Accuracy:0.2250\n",
    "Epoch #63: Loss:1.5969, Accuracy:0.2493, Validation Loss:1.6008, Validation Accuracy:0.2266\n",
    "Epoch #64: Loss:1.5971, Accuracy:0.2501, Validation Loss:1.5997, Validation Accuracy:0.2233\n",
    "Epoch #65: Loss:1.5962, Accuracy:0.2505, Validation Loss:1.6013, Validation Accuracy:0.2282\n",
    "Epoch #66: Loss:1.5959, Accuracy:0.2509, Validation Loss:1.6016, Validation Accuracy:0.2397\n",
    "Epoch #67: Loss:1.5961, Accuracy:0.2480, Validation Loss:1.6019, Validation Accuracy:0.2266\n",
    "Epoch #68: Loss:1.5957, Accuracy:0.2509, Validation Loss:1.6029, Validation Accuracy:0.2266\n",
    "Epoch #69: Loss:1.5959, Accuracy:0.2517, Validation Loss:1.6034, Validation Accuracy:0.2250\n",
    "Epoch #70: Loss:1.5969, Accuracy:0.2472, Validation Loss:1.6043, Validation Accuracy:0.2266\n",
    "Epoch #71: Loss:1.5960, Accuracy:0.2497, Validation Loss:1.6035, Validation Accuracy:0.2266\n",
    "Epoch #72: Loss:1.5965, Accuracy:0.2497, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #73: Loss:1.5974, Accuracy:0.2452, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #74: Loss:1.5965, Accuracy:0.2468, Validation Loss:1.6012, Validation Accuracy:0.2299\n",
    "Epoch #75: Loss:1.5961, Accuracy:0.2480, Validation Loss:1.6099, Validation Accuracy:0.2365\n",
    "Epoch #76: Loss:1.5951, Accuracy:0.2489, Validation Loss:1.6064, Validation Accuracy:0.2151\n",
    "Epoch #77: Loss:1.5994, Accuracy:0.2415, Validation Loss:1.6104, Validation Accuracy:0.2250\n",
    "Epoch #78: Loss:1.5976, Accuracy:0.2448, Validation Loss:1.6093, Validation Accuracy:0.2085\n",
    "Epoch #79: Loss:1.5986, Accuracy:0.2390, Validation Loss:1.6074, Validation Accuracy:0.2184\n",
    "Epoch #80: Loss:1.5958, Accuracy:0.2501, Validation Loss:1.6055, Validation Accuracy:0.2348\n",
    "Epoch #81: Loss:1.5968, Accuracy:0.2460, Validation Loss:1.6040, Validation Accuracy:0.2397\n",
    "Epoch #82: Loss:1.5953, Accuracy:0.2444, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #83: Loss:1.5954, Accuracy:0.2501, Validation Loss:1.6036, Validation Accuracy:0.2233\n",
    "Epoch #84: Loss:1.5953, Accuracy:0.2497, Validation Loss:1.6032, Validation Accuracy:0.2397\n",
    "Epoch #85: Loss:1.5952, Accuracy:0.2464, Validation Loss:1.6020, Validation Accuracy:0.2266\n",
    "Epoch #86: Loss:1.5935, Accuracy:0.2497, Validation Loss:1.6012, Validation Accuracy:0.2282\n",
    "Epoch #87: Loss:1.5928, Accuracy:0.2538, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #88: Loss:1.5929, Accuracy:0.2522, Validation Loss:1.6043, Validation Accuracy:0.2365\n",
    "Epoch #89: Loss:1.5915, Accuracy:0.2522, Validation Loss:1.6035, Validation Accuracy:0.2414\n",
    "Epoch #90: Loss:1.5934, Accuracy:0.2444, Validation Loss:1.6030, Validation Accuracy:0.2348\n",
    "Epoch #91: Loss:1.5949, Accuracy:0.2517, Validation Loss:1.6055, Validation Accuracy:0.2200\n",
    "Epoch #92: Loss:1.5960, Accuracy:0.2497, Validation Loss:1.6029, Validation Accuracy:0.2397\n",
    "Epoch #93: Loss:1.5949, Accuracy:0.2505, Validation Loss:1.6016, Validation Accuracy:0.2479\n",
    "Epoch #94: Loss:1.5922, Accuracy:0.2538, Validation Loss:1.6034, Validation Accuracy:0.2266\n",
    "Epoch #95: Loss:1.5933, Accuracy:0.2444, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #96: Loss:1.5938, Accuracy:0.2493, Validation Loss:1.6054, Validation Accuracy:0.2282\n",
    "Epoch #97: Loss:1.5972, Accuracy:0.2464, Validation Loss:1.6043, Validation Accuracy:0.2282\n",
    "Epoch #98: Loss:1.5920, Accuracy:0.2534, Validation Loss:1.6025, Validation Accuracy:0.2266\n",
    "Epoch #99: Loss:1.5921, Accuracy:0.2485, Validation Loss:1.6059, Validation Accuracy:0.2102\n",
    "Epoch #100: Loss:1.5918, Accuracy:0.2534, Validation Loss:1.6058, Validation Accuracy:0.2381\n",
    "Epoch #101: Loss:1.5924, Accuracy:0.2571, Validation Loss:1.6062, Validation Accuracy:0.2069\n",
    "Epoch #102: Loss:1.5932, Accuracy:0.2513, Validation Loss:1.6073, Validation Accuracy:0.2315\n",
    "Epoch #103: Loss:1.5915, Accuracy:0.2571, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #104: Loss:1.5918, Accuracy:0.2612, Validation Loss:1.6009, Validation Accuracy:0.2250\n",
    "Epoch #105: Loss:1.5906, Accuracy:0.2505, Validation Loss:1.6017, Validation Accuracy:0.2282\n",
    "Epoch #106: Loss:1.5923, Accuracy:0.2550, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #107: Loss:1.5916, Accuracy:0.2501, Validation Loss:1.6023, Validation Accuracy:0.2184\n",
    "Epoch #108: Loss:1.5892, Accuracy:0.2550, Validation Loss:1.6013, Validation Accuracy:0.2315\n",
    "Epoch #109: Loss:1.5893, Accuracy:0.2542, Validation Loss:1.6036, Validation Accuracy:0.2118\n",
    "Epoch #110: Loss:1.5890, Accuracy:0.2468, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #111: Loss:1.5905, Accuracy:0.2534, Validation Loss:1.5998, Validation Accuracy:0.2414\n",
    "Epoch #112: Loss:1.5918, Accuracy:0.2595, Validation Loss:1.6043, Validation Accuracy:0.2315\n",
    "Epoch #113: Loss:1.5989, Accuracy:0.2452, Validation Loss:1.6034, Validation Accuracy:0.2430\n",
    "Epoch #114: Loss:1.5926, Accuracy:0.2497, Validation Loss:1.6030, Validation Accuracy:0.2397\n",
    "Epoch #115: Loss:1.5972, Accuracy:0.2431, Validation Loss:1.6087, Validation Accuracy:0.2282\n",
    "Epoch #116: Loss:1.6058, Accuracy:0.2337, Validation Loss:1.6206, Validation Accuracy:0.2053\n",
    "Epoch #117: Loss:1.6146, Accuracy:0.2230, Validation Loss:1.6093, Validation Accuracy:0.2217\n",
    "Epoch #118: Loss:1.6022, Accuracy:0.2423, Validation Loss:1.6128, Validation Accuracy:0.2562\n",
    "Epoch #119: Loss:1.6092, Accuracy:0.2398, Validation Loss:1.6093, Validation Accuracy:0.2496\n",
    "Epoch #120: Loss:1.6033, Accuracy:0.2423, Validation Loss:1.6047, Validation Accuracy:0.2430\n",
    "Epoch #121: Loss:1.6001, Accuracy:0.2439, Validation Loss:1.6051, Validation Accuracy:0.2282\n",
    "Epoch #122: Loss:1.6001, Accuracy:0.2378, Validation Loss:1.6054, Validation Accuracy:0.2282\n",
    "Epoch #123: Loss:1.6000, Accuracy:0.2345, Validation Loss:1.6045, Validation Accuracy:0.2250\n",
    "Epoch #124: Loss:1.5996, Accuracy:0.2390, Validation Loss:1.6035, Validation Accuracy:0.2463\n",
    "Epoch #125: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6032, Validation Accuracy:0.2479\n",
    "Epoch #126: Loss:1.5979, Accuracy:0.2489, Validation Loss:1.6034, Validation Accuracy:0.2479\n",
    "Epoch #127: Loss:1.5981, Accuracy:0.2485, Validation Loss:1.6038, Validation Accuracy:0.2414\n",
    "Epoch #128: Loss:1.5976, Accuracy:0.2456, Validation Loss:1.6037, Validation Accuracy:0.2299\n",
    "Epoch #129: Loss:1.5973, Accuracy:0.2460, Validation Loss:1.6040, Validation Accuracy:0.2315\n",
    "Epoch #130: Loss:1.5974, Accuracy:0.2452, Validation Loss:1.6039, Validation Accuracy:0.2315\n",
    "Epoch #131: Loss:1.5977, Accuracy:0.2439, Validation Loss:1.6071, Validation Accuracy:0.2217\n",
    "Epoch #132: Loss:1.5990, Accuracy:0.2423, Validation Loss:1.6051, Validation Accuracy:0.2266\n",
    "Epoch #133: Loss:1.5982, Accuracy:0.2415, Validation Loss:1.6056, Validation Accuracy:0.2414\n",
    "Epoch #134: Loss:1.5986, Accuracy:0.2439, Validation Loss:1.6058, Validation Accuracy:0.2512\n",
    "Epoch #135: Loss:1.5976, Accuracy:0.2480, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #136: Loss:1.5962, Accuracy:0.2456, Validation Loss:1.6032, Validation Accuracy:0.2217\n",
    "Epoch #137: Loss:1.5963, Accuracy:0.2493, Validation Loss:1.6030, Validation Accuracy:0.2282\n",
    "Epoch #138: Loss:1.5957, Accuracy:0.2497, Validation Loss:1.6021, Validation Accuracy:0.2315\n",
    "Epoch #139: Loss:1.5948, Accuracy:0.2472, Validation Loss:1.6026, Validation Accuracy:0.2348\n",
    "Epoch #140: Loss:1.5944, Accuracy:0.2485, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #141: Loss:1.5940, Accuracy:0.2550, Validation Loss:1.6021, Validation Accuracy:0.2299\n",
    "Epoch #142: Loss:1.5948, Accuracy:0.2497, Validation Loss:1.6011, Validation Accuracy:0.2463\n",
    "Epoch #143: Loss:1.5956, Accuracy:0.2480, Validation Loss:1.6022, Validation Accuracy:0.2348\n",
    "Epoch #144: Loss:1.5956, Accuracy:0.2538, Validation Loss:1.6018, Validation Accuracy:0.2282\n",
    "Epoch #145: Loss:1.5950, Accuracy:0.2460, Validation Loss:1.6019, Validation Accuracy:0.2315\n",
    "Epoch #146: Loss:1.5943, Accuracy:0.2472, Validation Loss:1.6033, Validation Accuracy:0.2200\n",
    "Epoch #147: Loss:1.5937, Accuracy:0.2526, Validation Loss:1.6031, Validation Accuracy:0.2299\n",
    "Epoch #148: Loss:1.5929, Accuracy:0.2550, Validation Loss:1.6030, Validation Accuracy:0.2447\n",
    "Epoch #149: Loss:1.5925, Accuracy:0.2559, Validation Loss:1.6021, Validation Accuracy:0.2315\n",
    "Epoch #150: Loss:1.5929, Accuracy:0.2587, Validation Loss:1.6040, Validation Accuracy:0.2299\n",
    "Epoch #151: Loss:1.5931, Accuracy:0.2579, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #152: Loss:1.5959, Accuracy:0.2538, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #153: Loss:1.5935, Accuracy:0.2608, Validation Loss:1.6029, Validation Accuracy:0.2266\n",
    "Epoch #154: Loss:1.5929, Accuracy:0.2517, Validation Loss:1.6040, Validation Accuracy:0.2315\n",
    "Epoch #155: Loss:1.5913, Accuracy:0.2595, Validation Loss:1.6050, Validation Accuracy:0.2250\n",
    "Epoch #156: Loss:1.5912, Accuracy:0.2600, Validation Loss:1.6049, Validation Accuracy:0.2135\n",
    "Epoch #157: Loss:1.5917, Accuracy:0.2628, Validation Loss:1.6080, Validation Accuracy:0.2217\n",
    "Epoch #158: Loss:1.5922, Accuracy:0.2559, Validation Loss:1.6067, Validation Accuracy:0.2266\n",
    "Epoch #159: Loss:1.5937, Accuracy:0.2542, Validation Loss:1.6060, Validation Accuracy:0.2282\n",
    "Epoch #160: Loss:1.5935, Accuracy:0.2669, Validation Loss:1.6048, Validation Accuracy:0.2266\n",
    "Epoch #161: Loss:1.5902, Accuracy:0.2632, Validation Loss:1.6093, Validation Accuracy:0.2282\n",
    "Epoch #162: Loss:1.5925, Accuracy:0.2460, Validation Loss:1.6091, Validation Accuracy:0.1987\n",
    "Epoch #163: Loss:1.5927, Accuracy:0.2604, Validation Loss:1.6111, Validation Accuracy:0.2348\n",
    "Epoch #164: Loss:1.5927, Accuracy:0.2575, Validation Loss:1.6119, Validation Accuracy:0.2003\n",
    "Epoch #165: Loss:1.5941, Accuracy:0.2472, Validation Loss:1.6088, Validation Accuracy:0.2069\n",
    "Epoch #166: Loss:1.5932, Accuracy:0.2489, Validation Loss:1.6066, Validation Accuracy:0.2529\n",
    "Epoch #167: Loss:1.5938, Accuracy:0.2517, Validation Loss:1.6059, Validation Accuracy:0.2365\n",
    "Epoch #168: Loss:1.5923, Accuracy:0.2550, Validation Loss:1.6074, Validation Accuracy:0.2447\n",
    "Epoch #169: Loss:1.5928, Accuracy:0.2604, Validation Loss:1.6089, Validation Accuracy:0.2118\n",
    "Epoch #170: Loss:1.5948, Accuracy:0.2497, Validation Loss:1.6050, Validation Accuracy:0.2348\n",
    "Epoch #171: Loss:1.5920, Accuracy:0.2489, Validation Loss:1.6068, Validation Accuracy:0.2250\n",
    "Epoch #172: Loss:1.5929, Accuracy:0.2452, Validation Loss:1.6087, Validation Accuracy:0.2266\n",
    "Epoch #173: Loss:1.5926, Accuracy:0.2460, Validation Loss:1.6071, Validation Accuracy:0.2397\n",
    "Epoch #174: Loss:1.5934, Accuracy:0.2480, Validation Loss:1.6065, Validation Accuracy:0.2151\n",
    "Epoch #175: Loss:1.5911, Accuracy:0.2554, Validation Loss:1.6054, Validation Accuracy:0.2414\n",
    "Epoch #176: Loss:1.5908, Accuracy:0.2583, Validation Loss:1.6061, Validation Accuracy:0.2118\n",
    "Epoch #177: Loss:1.5899, Accuracy:0.2579, Validation Loss:1.6072, Validation Accuracy:0.2217\n",
    "Epoch #178: Loss:1.5895, Accuracy:0.2559, Validation Loss:1.6074, Validation Accuracy:0.2233\n",
    "Epoch #179: Loss:1.5898, Accuracy:0.2538, Validation Loss:1.6076, Validation Accuracy:0.2151\n",
    "Epoch #180: Loss:1.5892, Accuracy:0.2546, Validation Loss:1.6087, Validation Accuracy:0.2135\n",
    "Epoch #181: Loss:1.5888, Accuracy:0.2546, Validation Loss:1.6077, Validation Accuracy:0.2151\n",
    "Epoch #182: Loss:1.5881, Accuracy:0.2595, Validation Loss:1.6078, Validation Accuracy:0.2102\n",
    "Epoch #183: Loss:1.5884, Accuracy:0.2534, Validation Loss:1.6113, Validation Accuracy:0.2069\n",
    "Epoch #184: Loss:1.5887, Accuracy:0.2554, Validation Loss:1.6129, Validation Accuracy:0.2200\n",
    "Epoch #185: Loss:1.5899, Accuracy:0.2509, Validation Loss:1.6084, Validation Accuracy:0.1954\n",
    "Epoch #186: Loss:1.5875, Accuracy:0.2563, Validation Loss:1.6096, Validation Accuracy:0.2102\n",
    "Epoch #187: Loss:1.5875, Accuracy:0.2530, Validation Loss:1.6106, Validation Accuracy:0.2233\n",
    "Epoch #188: Loss:1.5882, Accuracy:0.2575, Validation Loss:1.6094, Validation Accuracy:0.2299\n",
    "Epoch #189: Loss:1.5918, Accuracy:0.2427, Validation Loss:1.6118, Validation Accuracy:0.2003\n",
    "Epoch #190: Loss:1.5914, Accuracy:0.2571, Validation Loss:1.6114, Validation Accuracy:0.2233\n",
    "Epoch #191: Loss:1.5909, Accuracy:0.2517, Validation Loss:1.6107, Validation Accuracy:0.2332\n",
    "Epoch #192: Loss:1.5894, Accuracy:0.2595, Validation Loss:1.6090, Validation Accuracy:0.2282\n",
    "Epoch #193: Loss:1.5900, Accuracy:0.2620, Validation Loss:1.6101, Validation Accuracy:0.2282\n",
    "Epoch #194: Loss:1.5915, Accuracy:0.2489, Validation Loss:1.6079, Validation Accuracy:0.2529\n",
    "Epoch #195: Loss:1.5987, Accuracy:0.2456, Validation Loss:1.6052, Validation Accuracy:0.2118\n",
    "Epoch #196: Loss:1.5978, Accuracy:0.2333, Validation Loss:1.6100, Validation Accuracy:0.2184\n",
    "Epoch #197: Loss:1.5952, Accuracy:0.2353, Validation Loss:1.6059, Validation Accuracy:0.2660\n",
    "Epoch #198: Loss:1.6052, Accuracy:0.2390, Validation Loss:1.6043, Validation Accuracy:0.2660\n",
    "Epoch #199: Loss:1.5962, Accuracy:0.2468, Validation Loss:1.6079, Validation Accuracy:0.2365\n",
    "Epoch #200: Loss:1.5986, Accuracy:0.2296, Validation Loss:1.6047, Validation Accuracy:0.2397\n",
    "Epoch #201: Loss:1.5952, Accuracy:0.2390, Validation Loss:1.6032, Validation Accuracy:0.2562\n",
    "Epoch #202: Loss:1.5946, Accuracy:0.2472, Validation Loss:1.6040, Validation Accuracy:0.2578\n",
    "Epoch #203: Loss:1.5940, Accuracy:0.2489, Validation Loss:1.6044, Validation Accuracy:0.2282\n",
    "Epoch #204: Loss:1.5933, Accuracy:0.2431, Validation Loss:1.6022, Validation Accuracy:0.2397\n",
    "Epoch #205: Loss:1.5931, Accuracy:0.2460, Validation Loss:1.6022, Validation Accuracy:0.2545\n",
    "Epoch #206: Loss:1.5923, Accuracy:0.2501, Validation Loss:1.6020, Validation Accuracy:0.2414\n",
    "Epoch #207: Loss:1.5921, Accuracy:0.2489, Validation Loss:1.6014, Validation Accuracy:0.2348\n",
    "Epoch #208: Loss:1.5915, Accuracy:0.2460, Validation Loss:1.6018, Validation Accuracy:0.2512\n",
    "Epoch #209: Loss:1.5906, Accuracy:0.2509, Validation Loss:1.6008, Validation Accuracy:0.2282\n",
    "Epoch #210: Loss:1.5904, Accuracy:0.2546, Validation Loss:1.5999, Validation Accuracy:0.2430\n",
    "Epoch #211: Loss:1.5899, Accuracy:0.2505, Validation Loss:1.6021, Validation Accuracy:0.2381\n",
    "Epoch #212: Loss:1.5900, Accuracy:0.2485, Validation Loss:1.6021, Validation Accuracy:0.2381\n",
    "Epoch #213: Loss:1.5903, Accuracy:0.2587, Validation Loss:1.6013, Validation Accuracy:0.2545\n",
    "Epoch #214: Loss:1.5903, Accuracy:0.2554, Validation Loss:1.6053, Validation Accuracy:0.2479\n",
    "Epoch #215: Loss:1.5897, Accuracy:0.2546, Validation Loss:1.6034, Validation Accuracy:0.2365\n",
    "Epoch #216: Loss:1.5902, Accuracy:0.2538, Validation Loss:1.6019, Validation Accuracy:0.2496\n",
    "Epoch #217: Loss:1.5898, Accuracy:0.2501, Validation Loss:1.6048, Validation Accuracy:0.2381\n",
    "Epoch #218: Loss:1.5897, Accuracy:0.2530, Validation Loss:1.6031, Validation Accuracy:0.2463\n",
    "Epoch #219: Loss:1.5897, Accuracy:0.2669, Validation Loss:1.6034, Validation Accuracy:0.2266\n",
    "Epoch #220: Loss:1.5884, Accuracy:0.2554, Validation Loss:1.6042, Validation Accuracy:0.2529\n",
    "Epoch #221: Loss:1.5891, Accuracy:0.2493, Validation Loss:1.5994, Validation Accuracy:0.2512\n",
    "Epoch #222: Loss:1.5894, Accuracy:0.2497, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #223: Loss:1.5891, Accuracy:0.2583, Validation Loss:1.6006, Validation Accuracy:0.2282\n",
    "Epoch #224: Loss:1.5894, Accuracy:0.2567, Validation Loss:1.6018, Validation Accuracy:0.2463\n",
    "Epoch #225: Loss:1.5877, Accuracy:0.2624, Validation Loss:1.6031, Validation Accuracy:0.2414\n",
    "Epoch #226: Loss:1.5872, Accuracy:0.2534, Validation Loss:1.6034, Validation Accuracy:0.2414\n",
    "Epoch #227: Loss:1.5879, Accuracy:0.2649, Validation Loss:1.6028, Validation Accuracy:0.2315\n",
    "Epoch #228: Loss:1.5875, Accuracy:0.2550, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #229: Loss:1.5885, Accuracy:0.2669, Validation Loss:1.6032, Validation Accuracy:0.2496\n",
    "Epoch #230: Loss:1.5889, Accuracy:0.2567, Validation Loss:1.6036, Validation Accuracy:0.2397\n",
    "Epoch #231: Loss:1.5875, Accuracy:0.2608, Validation Loss:1.6024, Validation Accuracy:0.2545\n",
    "Epoch #232: Loss:1.5882, Accuracy:0.2559, Validation Loss:1.6032, Validation Accuracy:0.2529\n",
    "Epoch #233: Loss:1.5857, Accuracy:0.2678, Validation Loss:1.6042, Validation Accuracy:0.2365\n",
    "Epoch #234: Loss:1.5857, Accuracy:0.2522, Validation Loss:1.6057, Validation Accuracy:0.2414\n",
    "Epoch #235: Loss:1.5873, Accuracy:0.2657, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #236: Loss:1.5862, Accuracy:0.2600, Validation Loss:1.6059, Validation Accuracy:0.2414\n",
    "Epoch #237: Loss:1.5854, Accuracy:0.2587, Validation Loss:1.6036, Validation Accuracy:0.2611\n",
    "Epoch #238: Loss:1.5866, Accuracy:0.2575, Validation Loss:1.6045, Validation Accuracy:0.2496\n",
    "Epoch #239: Loss:1.5855, Accuracy:0.2620, Validation Loss:1.6060, Validation Accuracy:0.2562\n",
    "Epoch #240: Loss:1.5864, Accuracy:0.2591, Validation Loss:1.6062, Validation Accuracy:0.2529\n",
    "Epoch #241: Loss:1.5854, Accuracy:0.2571, Validation Loss:1.6047, Validation Accuracy:0.2512\n",
    "Epoch #242: Loss:1.5881, Accuracy:0.2604, Validation Loss:1.6068, Validation Accuracy:0.2611\n",
    "Epoch #243: Loss:1.5877, Accuracy:0.2550, Validation Loss:1.6028, Validation Accuracy:0.2512\n",
    "Epoch #244: Loss:1.5877, Accuracy:0.2538, Validation Loss:1.6023, Validation Accuracy:0.2627\n",
    "Epoch #245: Loss:1.5838, Accuracy:0.2595, Validation Loss:1.6048, Validation Accuracy:0.2447\n",
    "Epoch #246: Loss:1.5846, Accuracy:0.2641, Validation Loss:1.6039, Validation Accuracy:0.2611\n",
    "Epoch #247: Loss:1.5840, Accuracy:0.2583, Validation Loss:1.6042, Validation Accuracy:0.2594\n",
    "Epoch #248: Loss:1.5866, Accuracy:0.2591, Validation Loss:1.6035, Validation Accuracy:0.2496\n",
    "Epoch #249: Loss:1.5855, Accuracy:0.2637, Validation Loss:1.6014, Validation Accuracy:0.2693\n",
    "Epoch #250: Loss:1.5856, Accuracy:0.2600, Validation Loss:1.6038, Validation Accuracy:0.2496\n",
    "Epoch #251: Loss:1.5846, Accuracy:0.2595, Validation Loss:1.6033, Validation Accuracy:0.2660\n",
    "Epoch #252: Loss:1.5849, Accuracy:0.2604, Validation Loss:1.6038, Validation Accuracy:0.2644\n",
    "Epoch #253: Loss:1.5841, Accuracy:0.2608, Validation Loss:1.6067, Validation Accuracy:0.2479\n",
    "Epoch #254: Loss:1.5844, Accuracy:0.2620, Validation Loss:1.6096, Validation Accuracy:0.2627\n",
    "Epoch #255: Loss:1.5853, Accuracy:0.2608, Validation Loss:1.6076, Validation Accuracy:0.2562\n",
    "Epoch #256: Loss:1.5840, Accuracy:0.2534, Validation Loss:1.6046, Validation Accuracy:0.2611\n",
    "Epoch #257: Loss:1.5849, Accuracy:0.2563, Validation Loss:1.6023, Validation Accuracy:0.2594\n",
    "Epoch #258: Loss:1.5814, Accuracy:0.2628, Validation Loss:1.6042, Validation Accuracy:0.2578\n",
    "Epoch #259: Loss:1.5819, Accuracy:0.2674, Validation Loss:1.6052, Validation Accuracy:0.2529\n",
    "Epoch #260: Loss:1.5835, Accuracy:0.2550, Validation Loss:1.6086, Validation Accuracy:0.2660\n",
    "Epoch #261: Loss:1.5826, Accuracy:0.2624, Validation Loss:1.6116, Validation Accuracy:0.2315\n",
    "Epoch #262: Loss:1.5880, Accuracy:0.2591, Validation Loss:1.6049, Validation Accuracy:0.2529\n",
    "Epoch #263: Loss:1.5871, Accuracy:0.2591, Validation Loss:1.6052, Validation Accuracy:0.2660\n",
    "Epoch #264: Loss:1.5844, Accuracy:0.2517, Validation Loss:1.6070, Validation Accuracy:0.2627\n",
    "Epoch #265: Loss:1.5797, Accuracy:0.2657, Validation Loss:1.6021, Validation Accuracy:0.2677\n",
    "Epoch #266: Loss:1.5819, Accuracy:0.2591, Validation Loss:1.6019, Validation Accuracy:0.2594\n",
    "Epoch #267: Loss:1.5800, Accuracy:0.2595, Validation Loss:1.6055, Validation Accuracy:0.2496\n",
    "Epoch #268: Loss:1.5800, Accuracy:0.2637, Validation Loss:1.6033, Validation Accuracy:0.2578\n",
    "Epoch #269: Loss:1.5803, Accuracy:0.2591, Validation Loss:1.6046, Validation Accuracy:0.2529\n",
    "Epoch #270: Loss:1.5889, Accuracy:0.2559, Validation Loss:1.6083, Validation Accuracy:0.2479\n",
    "Epoch #271: Loss:1.5870, Accuracy:0.2534, Validation Loss:1.6066, Validation Accuracy:0.2447\n",
    "Epoch #272: Loss:1.5834, Accuracy:0.2571, Validation Loss:1.6038, Validation Accuracy:0.2545\n",
    "Epoch #273: Loss:1.5810, Accuracy:0.2628, Validation Loss:1.6049, Validation Accuracy:0.2529\n",
    "Epoch #274: Loss:1.5789, Accuracy:0.2624, Validation Loss:1.6120, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5775, Accuracy:0.2616, Validation Loss:1.6108, Validation Accuracy:0.2512\n",
    "Epoch #276: Loss:1.5777, Accuracy:0.2657, Validation Loss:1.6072, Validation Accuracy:0.2479\n",
    "Epoch #277: Loss:1.5778, Accuracy:0.2686, Validation Loss:1.6034, Validation Accuracy:0.2594\n",
    "Epoch #278: Loss:1.5771, Accuracy:0.2731, Validation Loss:1.6047, Validation Accuracy:0.2594\n",
    "Epoch #279: Loss:1.5769, Accuracy:0.2649, Validation Loss:1.6045, Validation Accuracy:0.2611\n",
    "Epoch #280: Loss:1.5768, Accuracy:0.2727, Validation Loss:1.6081, Validation Accuracy:0.2479\n",
    "Epoch #281: Loss:1.5778, Accuracy:0.2747, Validation Loss:1.6107, Validation Accuracy:0.2578\n",
    "Epoch #282: Loss:1.5749, Accuracy:0.2739, Validation Loss:1.6108, Validation Accuracy:0.2414\n",
    "Epoch #283: Loss:1.5768, Accuracy:0.2710, Validation Loss:1.6105, Validation Accuracy:0.2496\n",
    "Epoch #284: Loss:1.5778, Accuracy:0.2632, Validation Loss:1.6067, Validation Accuracy:0.2512\n",
    "Epoch #285: Loss:1.5778, Accuracy:0.2772, Validation Loss:1.6122, Validation Accuracy:0.2332\n",
    "Epoch #286: Loss:1.5756, Accuracy:0.2632, Validation Loss:1.6106, Validation Accuracy:0.2447\n",
    "Epoch #287: Loss:1.5770, Accuracy:0.2669, Validation Loss:1.6094, Validation Accuracy:0.2365\n",
    "Epoch #288: Loss:1.5769, Accuracy:0.2620, Validation Loss:1.6092, Validation Accuracy:0.2496\n",
    "Epoch #289: Loss:1.5775, Accuracy:0.2628, Validation Loss:1.6141, Validation Accuracy:0.2562\n",
    "Epoch #290: Loss:1.5811, Accuracy:0.2665, Validation Loss:1.6082, Validation Accuracy:0.2496\n",
    "Epoch #291: Loss:1.5800, Accuracy:0.2608, Validation Loss:1.6149, Validation Accuracy:0.2496\n",
    "Epoch #292: Loss:1.5775, Accuracy:0.2702, Validation Loss:1.6156, Validation Accuracy:0.2315\n",
    "Epoch #293: Loss:1.5817, Accuracy:0.2628, Validation Loss:1.6151, Validation Accuracy:0.2496\n",
    "Epoch #294: Loss:1.5796, Accuracy:0.2608, Validation Loss:1.6142, Validation Accuracy:0.2512\n",
    "Epoch #295: Loss:1.5762, Accuracy:0.2702, Validation Loss:1.6142, Validation Accuracy:0.2463\n",
    "Epoch #296: Loss:1.5763, Accuracy:0.2715, Validation Loss:1.6165, Validation Accuracy:0.2512\n",
    "Epoch #297: Loss:1.5782, Accuracy:0.2645, Validation Loss:1.6078, Validation Accuracy:0.2529\n",
    "Epoch #298: Loss:1.5799, Accuracy:0.2719, Validation Loss:1.6089, Validation Accuracy:0.2479\n",
    "Epoch #299: Loss:1.5809, Accuracy:0.2637, Validation Loss:1.6097, Validation Accuracy:0.2562\n",
    "Epoch #300: Loss:1.5783, Accuracy:0.2632, Validation Loss:1.6074, Validation Accuracy:0.2430\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60736811, Accuracy:0.2430\n",
    "Labels: ['01', '04', '02', '05', '03']\n",
    "Confusion Matrix:\n",
    "      01  04  02  05  03\n",
    "t:01  54  14  14  41   3\n",
    "t:04  31  24  15  38   4\n",
    "t:02  41  24  18  29   2\n",
    "t:05  54  17  24  45   2\n",
    "t:03  45  14  16  33   7\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.24      0.43      0.31       126\n",
    "          04       0.26      0.21      0.23       112\n",
    "          02       0.21      0.16      0.18       114\n",
    "          05       0.24      0.32      0.27       142\n",
    "          03       0.39      0.06      0.11       115\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.27      0.24      0.22       609\n",
    "weighted avg       0.27      0.24      0.22       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 13:18:25 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6061977426015293, 1.605692329273631, 1.606054005168733, 1.6057699885470136, 1.6059921874201357, 1.6057347121888585, 1.605195788718601, 1.605382053918635, 1.6040993973930873, 1.6038323985336254, 1.6041369234595588, 1.6040182971014765, 1.6031872037987795, 1.603287793536883, 1.602982038151846, 1.6029197136169584, 1.6033512900028322, 1.6039784087727613, 1.6048811255221689, 1.604733193644945, 1.6048351299195063, 1.6057176098643462, 1.6043225675576622, 1.603278993581512, 1.6031127420356512, 1.6024832978037191, 1.6029856212816411, 1.602975109723597, 1.6026120236746, 1.6028178585769703, 1.6026272012290892, 1.60293062743295, 1.602537083116854, 1.6041769484189539, 1.6043002168924743, 1.6040278383467976, 1.6055114731217057, 1.606330960450697, 1.6035269001630335, 1.60363869968502, 1.602513722598259, 1.603379970309378, 1.6038735942496063, 1.6057524248492738, 1.6048674401391316, 1.603387899195228, 1.6032183099850057, 1.6040044778282028, 1.6018909519333362, 1.6019903646509832, 1.6021263577863696, 1.6030545953067847, 1.6025203174753926, 1.6022855301795922, 1.602300401978892, 1.6026975184630095, 1.6029156926034511, 1.6021698957985062, 1.6011550945209947, 1.6015895873259245, 1.6018891675131661, 1.6009557247161865, 1.6008481981327576, 1.5997229070694772, 1.6013253962465108, 1.6015579451872601, 1.601922416726161, 1.6028764083467681, 1.6033732509378142, 1.6043099807009518, 1.6035163692261394, 1.605484157947484, 1.6053465579335129, 1.6012270661997678, 1.6098942335799018, 1.606429496030698, 1.610381319800817, 1.609281624479247, 1.607378763714056, 1.6054818297450375, 1.6040279757604614, 1.6024500673823365, 1.6035623710926725, 1.6032493641028067, 1.6020053807150554, 1.601223569785433, 1.602566278626766, 1.6043127418934613, 1.6034956573461272, 1.603006221977948, 1.605466623415892, 1.6028745274238398, 1.6015742525874297, 1.6034373256373289, 1.6045244828429324, 1.6053854622472878, 1.6042593284976503, 1.6024969641993982, 1.6058952632208763, 1.6057998315845607, 1.6062360021281126, 1.6072779815576739, 1.6041032780567412, 1.6008889835652067, 1.601681182145681, 1.603579825172675, 1.6023300555343503, 1.6013061379759965, 1.603554310274046, 1.6004850365258203, 1.5997792584163997, 1.6043064696056697, 1.6033923524158147, 1.602999741612201, 1.6086919642434332, 1.6205943490092587, 1.609262925063448, 1.612788580517072, 1.6092986091604373, 1.6046675320329338, 1.6050934349179071, 1.6053816115327657, 1.604503371641162, 1.6035031409099185, 1.603212096812494, 1.6033907283115856, 1.60375294916344, 1.6037380707087776, 1.6040039759355618, 1.6039022874753854, 1.6070817699181819, 1.6050638851078076, 1.605608432359492, 1.6058417004708978, 1.6044060939246994, 1.6031711958899286, 1.602986270766736, 1.60206879261875, 1.602609195341226, 1.6011361044224455, 1.6021300493594266, 1.601109422486404, 1.6022009442396743, 1.6017929824506512, 1.601908525026882, 1.603269202172854, 1.6031380848735814, 1.6029783894471543, 1.6020791732227468, 1.6039975658426144, 1.6062437996684233, 1.6040663539090962, 1.6029322141497007, 1.603993642897833, 1.6049842474300091, 1.6048763328780877, 1.6079621301300224, 1.6066748132846627, 1.6059591114423153, 1.6048290161859422, 1.609346413455769, 1.609097816282501, 1.6111093107702696, 1.6118513337888545, 1.6087789874162972, 1.6065914904934235, 1.6058770052122169, 1.6073579907613043, 1.6088824346539226, 1.6049845099253413, 1.6067882190979956, 1.6087270098171016, 1.6071188813947104, 1.606515022529953, 1.6054233480948337, 1.606054250242675, 1.6072033215038881, 1.607429720498071, 1.6075728282161144, 1.6087296878175783, 1.6076595168591328, 1.607825693042799, 1.6113247728504374, 1.6128965015286099, 1.6083684408018741, 1.609555180436872, 1.6106146744319372, 1.6094471128116101, 1.6117889863516897, 1.6113528849064618, 1.610696748363952, 1.609017380548424, 1.6101096441788822, 1.6079394271221068, 1.6051659797408506, 1.6100415570786826, 1.6058755054066725, 1.604286799877148, 1.6078772585967491, 1.60470065616426, 1.6032151760921884, 1.6039712757899844, 1.6044188771146075, 1.6022244687933835, 1.6022173939471567, 1.6019937629965921, 1.6014483564201443, 1.601843540695892, 1.6007642391671493, 1.599851165499006, 1.6021030027486616, 1.6021490510069873, 1.6013248781069551, 1.605314499443192, 1.6033715516671367, 1.6019088926377947, 1.6047803713574589, 1.6031498368737733, 1.6034136548613875, 1.6041826810351343, 1.5993707544110678, 1.6002735273395656, 1.6006126697427534, 1.6017645200093586, 1.6031427565466594, 1.603365650513685, 1.6028405365294032, 1.6035571603352212, 1.6031755017138076, 1.603607806470398, 1.6023786040558212, 1.6031839647903818, 1.6042300282636495, 1.6056790984127125, 1.607408729680066, 1.6058830369282238, 1.603593349652533, 1.604502651491776, 1.6059818332418432, 1.606209090190568, 1.6047229672887642, 1.6068064076168391, 1.602814920821605, 1.6022737685878485, 1.6047683040105258, 1.6038841594420434, 1.604233210309973, 1.6035430881581674, 1.6014103942316742, 1.603803720380285, 1.6032626096446723, 1.6038016805116375, 1.606736630641768, 1.609639906335151, 1.6076355064639514, 1.6045827470193748, 1.602264151197349, 1.6041701938131172, 1.6052255605046188, 1.6086361725342098, 1.6115717449407467, 1.6048822471465187, 1.6051502959873094, 1.6070145924494577, 1.6021190558748293, 1.6019078706481382, 1.6055301833035323, 1.6033032100971891, 1.604591765035745, 1.6083088169739947, 1.6066182237149067, 1.6038006279856114, 1.604927780984462, 1.6119732271470069, 1.6107950664701915, 1.6072225151782358, 1.603356523075323, 1.6047242674334297, 1.6045295714549048, 1.6081371800652866, 1.6106600448220039, 1.6108216847887964, 1.6104994241044244, 1.6067155417550374, 1.612150715293947, 1.6105550462976466, 1.6094026044867504, 1.6092177277128097, 1.614107423815234, 1.6082013015480856, 1.6148820733789153, 1.6156086755307828, 1.6151213307294547, 1.6142349656188038, 1.614234116864322, 1.6165011019150808, 1.6078183474799095, 1.6089073568337853, 1.6097091452045784, 1.6073681891258127], 'val_acc': [0.23316912950063964, 0.23316912950063964, 0.2315270933758449, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.2315270933758449, 0.2249589489745389, 0.2315270934737179, 0.2380952378750239, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.2413793100267404, 0.24302134615153514, 0.23973727390194566, 0.23152709298435298, 0.23152709318009895, 0.23645320165235617, 0.23973727390194566, 0.23973727390194566, 0.2446633822763299, 0.2413793100267404, 0.2495894908464601, 0.24794745462379236, 0.23973727390194566, 0.2430213459557892, 0.24630541840112463, 0.2446633821784569, 0.2446633822763299, 0.23809523767927793, 0.2413793099288674, 0.23973727380407267, 0.23809523758140494, 0.23809523748353198, 0.23973727380407267, 0.2364532014566102, 0.2348111651360695, 0.2430213459557892, 0.23973727380407267, 0.2413793100267404, 0.22824302093050947, 0.23152709318009895, 0.22495894838730102, 0.22167487623558452, 0.2397372741955646, 0.23973727380407267, 0.24466338208058394, 0.22003284001291679, 0.24302134605366216, 0.24302134605366216, 0.22824302053901754, 0.22331691226250627, 0.23645320155448318, 0.22660098460996875, 0.22003284001291679, 0.23973727390194566, 0.22331691236037926, 0.22495894838730102, 0.224958948583047, 0.23645320155448318, 0.22495894838730102, 0.22660098460996875, 0.2233169121646333, 0.2282430207347635, 0.23973727390194566, 0.22660098451209576, 0.22660098460996875, 0.224958948485174, 0.22660098470784173, 0.22660098460996875, 0.2282430207347635, 0.2331691288155288, 0.2298850566638123, 0.2364532014566102, 0.21510673154065957, 0.22495894838730102, 0.2085385869436076, 0.2183908036923761, 0.23481116533181545, 0.23973727390194566, 0.23316912910914772, 0.22331691236037926, 0.23973727380407267, 0.22660098460996875, 0.22824302063689053, 0.23316912901127476, 0.23645320135873724, 0.24137930963524848, 0.23481116533181545, 0.22003283981717084, 0.2397372733147078, 0.24794745452591938, 0.22660098421847683, 0.2331691288155288, 0.22824302063689053, 0.2282430207347635, 0.2266009844142228, 0.21018062306840235, 0.238095237385659, 0.20689655101455884, 0.23152709288648002, 0.23152709269073404, 0.224958948485174, 0.2282430207347635, 0.23316912910914772, 0.21839080418174098, 0.23152709308222597, 0.21182265938894307, 0.23481116533181545, 0.2413793099288674, 0.23152709308222597, 0.24302134615153514, 0.23973727360832672, 0.2282430207347635, 0.20525451518338303, 0.22167487643133046, 0.25615763534563907, 0.24958949055284116, 0.24302134605366216, 0.2282430207347635, 0.2282430207347635, 0.224958948583047, 0.24630541830325164, 0.24794745433017343, 0.24794745433017343, 0.24137930983099445, 0.22988505685955823, 0.23152709298435298, 0.23152709308222597, 0.22167487643133046, 0.22660098460996875, 0.24137930983099445, 0.25123152687338185, 0.23645320126086425, 0.22167487603983857, 0.22824302083263648, 0.23152709308222597, 0.23481116533181545, 0.2413793099288674, 0.22988505695743122, 0.24630541840112463, 0.23481116533181545, 0.2282430207347635, 0.23152709288648002, 0.2200328399150438, 0.22988505685955823, 0.24466338188483797, 0.23152709298435298, 0.22988505685955823, 0.23316912910914772, 0.23316912901127476, 0.22660098460996875, 0.23152709308222597, 0.224958948485174, 0.2134646955137378, 0.2216748763334575, 0.22660098490358768, 0.22824302102838243, 0.22660098470784173, 0.2282430213220014, 0.1986863703905851, 0.23481116533181545, 0.20032840641750688, 0.20689655101455884, 0.2528735629003036, 0.23645320165235617, 0.2446633821784569, 0.21182265938894307, 0.23481116503819652, 0.22495894868091998, 0.22660098480571472, 0.23973727380407267, 0.21510673163853256, 0.2413793099288674, 0.21182265948681606, 0.2216748763334575, 0.2233169126539982, 0.21510673212789747, 0.21346469590522973, 0.21510673212789747, 0.21018062365564025, 0.20689655130817777, 0.22003284020866276, 0.1954022982388686, 0.21018062345989427, 0.22331691226250627, 0.22988505744679613, 0.2003284067111258, 0.2233169126539982, 0.23316912940276668, 0.2282430207347635, 0.22824302093050947, 0.25287356319392257, 0.21182265929107008, 0.21839080418174098, 0.2660098518007886, 0.26600985199653454, 0.23645320165235617, 0.23973727351045374, 0.2561576351498931, 0.2577996713725608, 0.22824302063689053, 0.23973727351045374, 0.25451559912297134, 0.24137930973312147, 0.23481116503819652, 0.25123152677550886, 0.22824302044114456, 0.24302134605366216, 0.23809523758140494, 0.23809523748353198, 0.2545155992208443, 0.24794745423230044, 0.23645320135873724, 0.24958949045496817, 0.23809523748353198, 0.24630541820537868, 0.22660098451209576, 0.2528735628024306, 0.2512315265797629, 0.24302134634728112, 0.22824302093050947, 0.24630541849899762, 0.24137931022248635, 0.24137931022248635, 0.23152709327797194, 0.2331691293048937, 0.24958949045496817, 0.23973727390194566, 0.2545155992208443, 0.2528735629003036, 0.23645320165235617, 0.2413793101246134, 0.23316912950063964, 0.2413793100267404, 0.2610837438178963, 0.24958949094433308, 0.2561576352477661, 0.25287356319392257, 0.25123152677550886, 0.2610837415423495, 0.25123152677550886, 0.26272577974694505, 0.2446633822763299, 0.2610837437200233, 0.25944170769310154, 0.2495894907485871, 0.26929392236206917, 0.24958949094433308, 0.2660098501124797, 0.26436781398768494, 0.2479474548195383, 0.2627257776671442, 0.25615763544351206, 0.26108374183596844, 0.25944170749735557, 0.2577996715683068, 0.2528735629981766, 0.26600984991673376, 0.23152709298435298, 0.2528735629981766, 0.26600985021035267, 0.2627257776671442, 0.26765188623727443, 0.25944170730160965, 0.24958949065071412, 0.25779967127468784, 0.2528735628024306, 0.24794745423230044, 0.24466338208058394, 0.25451559912297134, 0.2528735629981766, 0.23481116533181545, 0.2512315265797629, 0.24794745462379236, 0.2594417055154277, 0.2594417055154277, 0.2610837415423495, 0.24794745462379236, 0.25779966929276, 0.24137930983099445, 0.2495894907485871, 0.2512315270691278, 0.2331691293048937, 0.24466338247207586, 0.23645320165235617, 0.24958949045496817, 0.25615763544351206, 0.2495894908464601, 0.2495894908464601, 0.23152709298435298, 0.2495894908464601, 0.25123152697125484, 0.2463054185968706, 0.25123152697125484, 0.25287356319392257, 0.24794745462379236, 0.2561576351498931, 0.24302134605366216], 'loss': [1.6071698862424377, 1.605893560699369, 1.6056214530854744, 1.6047523653727538, 1.60411786044158, 1.6037798977975237, 1.6034081867343346, 1.6029488067607371, 1.6029472092583439, 1.60223068172682, 1.6018909563518893, 1.6013970671737954, 1.6008993770552367, 1.600620984443649, 1.6000146946623095, 1.59990422216529, 1.6000222366203762, 1.6000823654922862, 1.5997404502402586, 1.6003506009828383, 1.6007890221029826, 1.6014784371828397, 1.6010633715858695, 1.6007447969742135, 1.6001389670420967, 1.6003557530271935, 1.5997504131994698, 1.5993516198418714, 1.5989282315271836, 1.5980235137978618, 1.597748620798945, 1.5977752433910017, 1.5976839668207345, 1.5977924388047362, 1.5979547799735099, 1.5976993615622392, 1.5970562664145562, 1.5965333507780666, 1.5967109639052248, 1.596942025582159, 1.5988590885726333, 1.599427747432701, 1.6006637379374102, 1.6008048834986754, 1.601668999865804, 1.6011549984405173, 1.6018994893381484, 1.6004342868832346, 1.6006655262236233, 1.5994605593612796, 1.5990648749427874, 1.599477752767794, 1.6004666987385838, 1.5995803551507437, 1.5984617068292668, 1.59851750289635, 1.5991339346711395, 1.5978490014830165, 1.5972375063925552, 1.5970617450483036, 1.5969751454476704, 1.5968557075798144, 1.5969247318635975, 1.5971052302961721, 1.5962379489836018, 1.5959327826020164, 1.5960767598122787, 1.5956954016577782, 1.5958627656744735, 1.5969460583320634, 1.5959766704199005, 1.5964765101242848, 1.5973966198535425, 1.5964568356958504, 1.5960500293688609, 1.59513711488712, 1.5993746624345406, 1.597577038144184, 1.5986335605566506, 1.5957858752421041, 1.5968433168634497, 1.5953261777850392, 1.5954136684934705, 1.5952983432726693, 1.5951884236913443, 1.5935262381908095, 1.592835644285292, 1.5929059645968051, 1.5915263181839145, 1.5934307941910668, 1.5948842711027642, 1.5960356382863476, 1.594936483890369, 1.592173037538783, 1.5933158039312343, 1.5937982710479957, 1.5972009887930305, 1.5919646561757739, 1.5921095967537569, 1.5918419257326537, 1.5923774987030812, 1.5932489795116918, 1.5914975821849502, 1.591846621991183, 1.5906431407164745, 1.592253216140324, 1.5915924902079777, 1.5892275312842774, 1.5893317263718747, 1.5890447596994515, 1.590517488544237, 1.5917542091874861, 1.5988524901548695, 1.5925773064948205, 1.5972329173979083, 1.6058052617176848, 1.6146116586681265, 1.6021810953132425, 1.6092140006823217, 1.6032538971127426, 1.60010079353497, 1.6000950066705504, 1.600048252738232, 1.5996269001363483, 1.598372722749103, 1.5978939401540424, 1.5981221926040963, 1.597587969317818, 1.5973316222980036, 1.5974286497006427, 1.5977491634337564, 1.5990000185780457, 1.5981556860083672, 1.598572752412095, 1.5975553277581624, 1.5962010505018294, 1.596299081318677, 1.5956856290907342, 1.5948091757126168, 1.594424315350746, 1.5940096034651174, 1.5947962654689498, 1.5956354881948513, 1.5956240920805098, 1.5950389520343569, 1.5943024221142215, 1.593683625344623, 1.5929200534213495, 1.5924560814667532, 1.5928814885063094, 1.593109342498701, 1.595921464965084, 1.59348252402194, 1.5929036956058635, 1.5912891206310515, 1.5912349518319664, 1.5917210963717232, 1.5921542341949024, 1.5936971337888275, 1.5935388958674437, 1.5902471311772872, 1.5925495431163716, 1.5927311262800463, 1.5927381410735832, 1.5940594376969386, 1.593155589387647, 1.5937512852083242, 1.5922666985396243, 1.5928294514728523, 1.5948026816702967, 1.5919925899721024, 1.5929235398646988, 1.5925768679417134, 1.5933866968389898, 1.591120283118998, 1.590842971958419, 1.58990461503211, 1.5894904307516204, 1.5897740994145984, 1.5892099897474723, 1.588771838720819, 1.5881230340601238, 1.5883574080418268, 1.5886871081847675, 1.5898966664895875, 1.5875128334797384, 1.5874862211196086, 1.5881607425286295, 1.5918397811893565, 1.5913802139568134, 1.590913124006142, 1.5893829241425594, 1.5900326980457657, 1.591537825527622, 1.598653382148586, 1.5978466976839414, 1.5951675534493135, 1.60523684108037, 1.59617654777895, 1.5986384520540493, 1.5952340997954413, 1.594565130259222, 1.5940375135664577, 1.5933218030714156, 1.593068795869972, 1.5923147429186215, 1.5921441180994869, 1.5914549102039064, 1.5906274136086997, 1.5904019366544375, 1.5898523258722292, 1.5900172305547726, 1.5902768088072476, 1.5902952257367864, 1.5896920228640892, 1.5902239423512923, 1.5897711431710873, 1.5896563525562168, 1.5896726280756799, 1.5884104632254255, 1.5891471008010958, 1.5894224129173544, 1.5891206856380988, 1.5894374280494832, 1.5877212850954499, 1.5871525956375154, 1.5879467265561866, 1.5875246866038204, 1.5885226058274569, 1.5888625730479278, 1.5874516140508945, 1.588202045879325, 1.5857308241865717, 1.5857336653821033, 1.587339921506768, 1.5861943066242539, 1.5853908332711129, 1.5866065049318319, 1.5855294569317075, 1.5864067062949743, 1.5854017228806043, 1.5881494199960384, 1.587687675125545, 1.587714065271726, 1.5838096485979993, 1.5845980782773215, 1.5840222161898134, 1.586648561528576, 1.5854503009353575, 1.5855534643607951, 1.584649666280962, 1.5848768444766255, 1.584065968544821, 1.5844032492725757, 1.5853457212937685, 1.5839593539737333, 1.5849265192568425, 1.581373502194759, 1.5819090358040906, 1.583510634933409, 1.5826156501162958, 1.588046912539911, 1.5870815020077527, 1.584368757056015, 1.5797365193983857, 1.5819478404105811, 1.579986609276805, 1.5800197381503283, 1.5803393263102068, 1.5888737089090523, 1.5869795109946625, 1.5833929766374937, 1.5809546810400805, 1.578922638511266, 1.5775430954946874, 1.5777372590325451, 1.577825622098402, 1.5771052547548832, 1.5769067331995563, 1.5767842533162486, 1.5777597888539214, 1.5748675743412432, 1.5767593757817389, 1.5777766610562678, 1.5777901152565739, 1.5756363683167913, 1.577042210420299, 1.5768838413442186, 1.5775219941775658, 1.581101678480113, 1.5799785666886785, 1.5775483711054683, 1.581712664471025, 1.5795971109637, 1.5762198791366828, 1.5763000682638901, 1.5782407791462767, 1.579853711529679, 1.580938503727531, 1.5782920564714153], 'acc': [0.23244353070151388, 0.23285421049203225, 0.23490759858238133, 0.23326488752262303, 0.234907598386555, 0.232443531503178, 0.23449692153343185, 0.22792607770441004, 0.23162217664033236, 0.22956878876416836, 0.23490759797654356, 0.23901437339229506, 0.2394250504045271, 0.24188911748373043, 0.24435318280049662, 0.24599589307694955, 0.24517453803663625, 0.24271047293405512, 0.24435318281885535, 0.2422997941226685, 0.24065708365038924, 0.24435318201719122, 0.2427104725240437, 0.24353182697687795, 0.24476385924360836, 0.24476386002691375, 0.24640657147832476, 0.24640657226163015, 0.24681724811726283, 0.2501026700593118, 0.2546201228605893, 0.2533880913771643, 0.2521560563321476, 0.2517453779307724, 0.24763860454671927, 0.2513347019160308, 0.25092402488544, 0.2505133472673702, 0.2509240250812664, 0.2525667347702402, 0.25092402230298005, 0.24804928195060402, 0.2381930185478081, 0.23080082142255146, 0.23449692018100612, 0.23983572999921912, 0.24845995996032652, 0.24147843849487619, 0.23942505138365885, 0.24928131441316076, 0.2484599585895421, 0.24106776166011176, 0.24394250480913285, 0.24599589407444, 0.25010266829687466, 0.246817250467179, 0.24353182756435698, 0.24887063679509094, 0.2521560583271285, 0.2488706374009287, 0.24558521326807245, 0.24928131202652715, 0.24928131382568172, 0.2501026686885274, 0.25051334728572894, 0.25092402488544, 0.24804928038399324, 0.2509240235146556, 0.25174538013993836, 0.24722792495202725, 0.24969199320618865, 0.24969199242288326, 0.2451745386241153, 0.24681724831308918, 0.24804928136312496, 0.2488706354243065, 0.24147844025731333, 0.24476385924360836, 0.23901437417560045, 0.2501026706284321, 0.24599589407444, 0.24435318280049662, 0.2501026704509645, 0.2496919926187096, 0.24640657030336666, 0.24969199301036232, 0.2537987689952341, 0.2521560569747028, 0.25215605773964944, 0.24435318221301758, 0.25174538129653773, 0.24969199085627247, 0.2505133476590229, 0.25379876742862334, 0.24435318242720266, 0.24928131462734582, 0.24640656971588762, 0.2533880892230745, 0.24845995841207447, 0.2533880900247386, 0.25708418915648723, 0.25133470426594695, 0.25708418915648723, 0.26119096709962253, 0.25051334667989117, 0.25503080126196453, 0.2501026684743423, 0.2550307998911801, 0.2542094460258249, 0.24681724891892692, 0.2533880882072253, 0.25954825466907977, 0.245174535845829, 0.24969199107045756, 0.2431211487713291, 0.23367556355572333, 0.22299794626921354, 0.2422997945143212, 0.23983572782677057, 0.24229979492433262, 0.24394250619827598, 0.2377823409297383, 0.23449691920187438, 0.23901437300064235, 0.2443531837979871, 0.24887063816587537, 0.24845995880372715, 0.24558521581381498, 0.24599589307694955, 0.2451745386241153, 0.24394250576990587, 0.24229979353518946, 0.24147844027567203, 0.24394250420329508, 0.24804927897649134, 0.2455852160647175, 0.2492813150006398, 0.2496919928328947, 0.24722792413200442, 0.2484599578062367, 0.25503080126196453, 0.24969199265542705, 0.2480492815589513, 0.25379876703697063, 0.24599589348696096, 0.2472279271244758, 0.25256673398693485, 0.2550308008703118, 0.2558521566939305, 0.2587268996287665, 0.2579055445884532, 0.2537987662169478, 0.2607802867399838, 0.2517453807274174, 0.2595482524782725, 0.2599589326788023, 0.2628336746345066, 0.2558521559106251, 0.2542094470049566, 0.2669404499951819, 0.26324435381918715, 0.24599589268529684, 0.26036961166765654, 0.2574948671845685, 0.2472279267144644, 0.24887063642179696, 0.251745380317406, 0.25503080167197595, 0.2603696107068835, 0.24969199105209883, 0.24887063816587537, 0.24517453883830037, 0.2459958920978178, 0.24804928195060402, 0.255441476101748, 0.2583162196240631, 0.2579055432176688, 0.2558521566939305, 0.25379876997436585, 0.2546201220772839, 0.25462012423137376, 0.2595482538857744, 0.2533880892230745, 0.2554414768850534, 0.25092402470797237, 0.2562628329412159, 0.2529774133858005, 0.2574948681453415, 0.2427104727382288, 0.25708418915648723, 0.2517453811007114, 0.2595482542957858, 0.262012319985846, 0.24887063601178555, 0.24558521545887974, 0.23326488654349128, 0.23531827716122417, 0.23901437417560045, 0.24681724831308918, 0.2295687889599947, 0.23901437513637347, 0.24722792749776978, 0.2488706369909173, 0.24312115014211352, 0.24599589268529684, 0.2501026706284321, 0.24887063718674365, 0.24599589348696096, 0.2509240254729191, 0.2546201250146791, 0.2505133470899026, 0.2484599578245954, 0.2587268976337856, 0.25544147927168703, 0.2546201230564157, 0.2537987691910605, 0.2501026692576477, 0.2529774125841364, 0.26694045077848727, 0.25544147868420797, 0.24928131362985537, 0.24969199183540422, 0.2583162220106967, 0.2566735117342438, 0.26242299879723263, 0.25338809100387033, 0.2648870646831191, 0.2550308016536172, 0.26694045218598916, 0.25667351310502823, 0.26078028811076825, 0.25585215532314604, 0.26776180918456594, 0.252156056368865, 0.26570842109421683, 0.2599589315038442, 0.25872689923711384, 0.257494865599599, 0.2620123189699968, 0.25913757822596806, 0.25708418915648723, 0.26036960849771756, 0.25503080263274897, 0.2537987682119287, 0.25954825564821155, 0.2640657068645195, 0.25831622181487035, 0.25913757665935727, 0.26365503222056236, 0.2599589326788023, 0.2595482568231696, 0.2603696083018912, 0.2607802877191156, 0.2620123207875101, 0.26078028948155274, 0.25338809157299064, 0.25626283372452124, 0.2628336746345066, 0.26735112921657994, 0.25503080067448547, 0.26242299819139486, 0.25913757804850046, 0.25913757583933444, 0.25174538129653773, 0.26570842031091146, 0.25913757724683634, 0.25954825662734327, 0.26365503222056236, 0.25913757724683634, 0.25585215610645146, 0.25338809078968527, 0.2570841887281171, 0.2628336771802491, 0.2624229995621793, 0.26160164037279526, 0.2657084197234324, 0.2685831642248792, 0.27310061367875005, 0.2648870628839646, 0.2726899394264456, 0.2747433284959264, 0.2739219724764814, 0.2710472299332981, 0.26324435381918715, 0.27720739440017167, 0.2632443524484027, 0.2669404523818155, 0.26201232135663044, 0.26283367620111736, 0.26652977633035646, 0.26078028967737904, 0.2702258735405591, 0.26283367776772815, 0.26078028791494196, 0.27022587093974043, 0.27145790340229714, 0.26447638369928395, 0.27186858281952153, 0.26365503006647256, 0.26324435143255354]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
