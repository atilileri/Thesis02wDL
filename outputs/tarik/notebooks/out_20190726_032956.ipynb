{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf17.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 03:29:56 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '1', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '05', '03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000289071DBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002897D776EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6069, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6061, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6023, Accuracy:0.2329, Validation Loss:1.6008, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6007, Accuracy:0.2349, Validation Loss:1.5983, Validation Accuracy:0.2348\n",
    "Epoch #20: Loss:1.5975, Accuracy:0.2378, Validation Loss:1.5937, Validation Accuracy:0.2381\n",
    "Epoch #21: Loss:1.5924, Accuracy:0.2563, Validation Loss:1.5857, Validation Accuracy:0.2693\n",
    "Epoch #22: Loss:1.5824, Accuracy:0.2710, Validation Loss:1.5719, Validation Accuracy:0.2824\n",
    "Epoch #23: Loss:1.5672, Accuracy:0.2842, Validation Loss:1.5503, Validation Accuracy:0.2890\n",
    "Epoch #24: Loss:1.5452, Accuracy:0.2924, Validation Loss:1.5222, Validation Accuracy:0.3038\n",
    "Epoch #25: Loss:1.5226, Accuracy:0.3014, Validation Loss:1.5090, Validation Accuracy:0.3284\n",
    "Epoch #26: Loss:1.5002, Accuracy:0.3129, Validation Loss:1.4940, Validation Accuracy:0.3300\n",
    "Epoch #27: Loss:1.4921, Accuracy:0.3121, Validation Loss:1.4798, Validation Accuracy:0.3202\n",
    "Epoch #28: Loss:1.4759, Accuracy:0.3203, Validation Loss:1.4775, Validation Accuracy:0.3284\n",
    "Epoch #29: Loss:1.4706, Accuracy:0.3285, Validation Loss:1.4777, Validation Accuracy:0.3333\n",
    "Epoch #30: Loss:1.4661, Accuracy:0.3261, Validation Loss:1.4753, Validation Accuracy:0.3300\n",
    "Epoch #31: Loss:1.4629, Accuracy:0.3363, Validation Loss:1.4747, Validation Accuracy:0.3284\n",
    "Epoch #32: Loss:1.4614, Accuracy:0.3347, Validation Loss:1.4807, Validation Accuracy:0.3465\n",
    "Epoch #33: Loss:1.4639, Accuracy:0.3392, Validation Loss:1.4869, Validation Accuracy:0.3498\n",
    "Epoch #34: Loss:1.4656, Accuracy:0.3405, Validation Loss:1.4733, Validation Accuracy:0.3366\n",
    "Epoch #35: Loss:1.4569, Accuracy:0.3413, Validation Loss:1.4735, Validation Accuracy:0.3333\n",
    "Epoch #36: Loss:1.4580, Accuracy:0.3421, Validation Loss:1.4721, Validation Accuracy:0.3366\n",
    "Epoch #37: Loss:1.4547, Accuracy:0.3421, Validation Loss:1.4730, Validation Accuracy:0.3333\n",
    "Epoch #38: Loss:1.4553, Accuracy:0.3437, Validation Loss:1.4726, Validation Accuracy:0.3350\n",
    "Epoch #39: Loss:1.4542, Accuracy:0.3454, Validation Loss:1.4776, Validation Accuracy:0.3317\n",
    "Epoch #40: Loss:1.4529, Accuracy:0.3487, Validation Loss:1.4762, Validation Accuracy:0.3333\n",
    "Epoch #41: Loss:1.4550, Accuracy:0.3437, Validation Loss:1.4726, Validation Accuracy:0.3268\n",
    "Epoch #42: Loss:1.4513, Accuracy:0.3450, Validation Loss:1.4732, Validation Accuracy:0.3153\n",
    "Epoch #43: Loss:1.4508, Accuracy:0.3446, Validation Loss:1.4753, Validation Accuracy:0.3317\n",
    "Epoch #44: Loss:1.4527, Accuracy:0.3478, Validation Loss:1.4822, Validation Accuracy:0.3317\n",
    "Epoch #45: Loss:1.4504, Accuracy:0.3425, Validation Loss:1.4729, Validation Accuracy:0.3284\n",
    "Epoch #46: Loss:1.4489, Accuracy:0.3491, Validation Loss:1.4735, Validation Accuracy:0.3218\n",
    "Epoch #47: Loss:1.4485, Accuracy:0.3425, Validation Loss:1.4759, Validation Accuracy:0.3186\n",
    "Epoch #48: Loss:1.4499, Accuracy:0.3515, Validation Loss:1.4749, Validation Accuracy:0.3186\n",
    "Epoch #49: Loss:1.4496, Accuracy:0.3474, Validation Loss:1.4736, Validation Accuracy:0.3186\n",
    "Epoch #50: Loss:1.4481, Accuracy:0.3524, Validation Loss:1.4760, Validation Accuracy:0.3284\n",
    "Epoch #51: Loss:1.4495, Accuracy:0.3507, Validation Loss:1.4731, Validation Accuracy:0.3186\n",
    "Epoch #52: Loss:1.4454, Accuracy:0.3544, Validation Loss:1.4752, Validation Accuracy:0.3317\n",
    "Epoch #53: Loss:1.4459, Accuracy:0.3598, Validation Loss:1.4755, Validation Accuracy:0.3268\n",
    "Epoch #54: Loss:1.4477, Accuracy:0.3507, Validation Loss:1.4777, Validation Accuracy:0.3284\n",
    "Epoch #55: Loss:1.4467, Accuracy:0.3429, Validation Loss:1.4861, Validation Accuracy:0.3251\n",
    "Epoch #56: Loss:1.4563, Accuracy:0.3536, Validation Loss:1.4728, Validation Accuracy:0.3218\n",
    "Epoch #57: Loss:1.4529, Accuracy:0.3614, Validation Loss:1.4738, Validation Accuracy:0.3317\n",
    "Epoch #58: Loss:1.4457, Accuracy:0.3569, Validation Loss:1.4772, Validation Accuracy:0.3218\n",
    "Epoch #59: Loss:1.4442, Accuracy:0.3593, Validation Loss:1.4769, Validation Accuracy:0.3300\n",
    "Epoch #60: Loss:1.4452, Accuracy:0.3589, Validation Loss:1.4739, Validation Accuracy:0.3251\n",
    "Epoch #61: Loss:1.4416, Accuracy:0.3606, Validation Loss:1.4742, Validation Accuracy:0.3366\n",
    "Epoch #62: Loss:1.4429, Accuracy:0.3634, Validation Loss:1.4731, Validation Accuracy:0.3300\n",
    "Epoch #63: Loss:1.4416, Accuracy:0.3569, Validation Loss:1.4746, Validation Accuracy:0.3300\n",
    "Epoch #64: Loss:1.4415, Accuracy:0.3593, Validation Loss:1.4728, Validation Accuracy:0.3300\n",
    "Epoch #65: Loss:1.4429, Accuracy:0.3565, Validation Loss:1.4755, Validation Accuracy:0.3235\n",
    "Epoch #66: Loss:1.4396, Accuracy:0.3647, Validation Loss:1.4784, Validation Accuracy:0.3333\n",
    "Epoch #67: Loss:1.4455, Accuracy:0.3634, Validation Loss:1.4750, Validation Accuracy:0.3284\n",
    "Epoch #68: Loss:1.4423, Accuracy:0.3618, Validation Loss:1.4787, Validation Accuracy:0.3350\n",
    "Epoch #69: Loss:1.4439, Accuracy:0.3573, Validation Loss:1.4733, Validation Accuracy:0.3268\n",
    "Epoch #70: Loss:1.4398, Accuracy:0.3561, Validation Loss:1.4743, Validation Accuracy:0.3235\n",
    "Epoch #71: Loss:1.4397, Accuracy:0.3569, Validation Loss:1.4775, Validation Accuracy:0.3333\n",
    "Epoch #72: Loss:1.4414, Accuracy:0.3532, Validation Loss:1.4737, Validation Accuracy:0.3300\n",
    "Epoch #73: Loss:1.4392, Accuracy:0.3630, Validation Loss:1.4743, Validation Accuracy:0.3284\n",
    "Epoch #74: Loss:1.4385, Accuracy:0.3602, Validation Loss:1.4764, Validation Accuracy:0.3350\n",
    "Epoch #75: Loss:1.4377, Accuracy:0.3647, Validation Loss:1.4777, Validation Accuracy:0.3350\n",
    "Epoch #76: Loss:1.4372, Accuracy:0.3667, Validation Loss:1.4723, Validation Accuracy:0.3350\n",
    "Epoch #77: Loss:1.4339, Accuracy:0.3639, Validation Loss:1.4734, Validation Accuracy:0.3317\n",
    "Epoch #78: Loss:1.4339, Accuracy:0.3639, Validation Loss:1.4730, Validation Accuracy:0.3383\n",
    "Epoch #79: Loss:1.4339, Accuracy:0.3577, Validation Loss:1.4742, Validation Accuracy:0.3268\n",
    "Epoch #80: Loss:1.4354, Accuracy:0.3630, Validation Loss:1.4731, Validation Accuracy:0.3317\n",
    "Epoch #81: Loss:1.4335, Accuracy:0.3618, Validation Loss:1.4715, Validation Accuracy:0.3383\n",
    "Epoch #82: Loss:1.4326, Accuracy:0.3655, Validation Loss:1.4787, Validation Accuracy:0.3317\n",
    "Epoch #83: Loss:1.4348, Accuracy:0.3643, Validation Loss:1.4776, Validation Accuracy:0.3432\n",
    "Epoch #84: Loss:1.4350, Accuracy:0.3647, Validation Loss:1.4746, Validation Accuracy:0.3218\n",
    "Epoch #85: Loss:1.4329, Accuracy:0.3634, Validation Loss:1.4714, Validation Accuracy:0.3317\n",
    "Epoch #86: Loss:1.4353, Accuracy:0.3667, Validation Loss:1.4781, Validation Accuracy:0.3366\n",
    "Epoch #87: Loss:1.4396, Accuracy:0.3630, Validation Loss:1.4745, Validation Accuracy:0.3333\n",
    "Epoch #88: Loss:1.4291, Accuracy:0.3692, Validation Loss:1.4733, Validation Accuracy:0.3383\n",
    "Epoch #89: Loss:1.4308, Accuracy:0.3688, Validation Loss:1.4720, Validation Accuracy:0.3333\n",
    "Epoch #90: Loss:1.4297, Accuracy:0.3692, Validation Loss:1.4726, Validation Accuracy:0.3300\n",
    "Epoch #91: Loss:1.4282, Accuracy:0.3684, Validation Loss:1.4732, Validation Accuracy:0.3383\n",
    "Epoch #92: Loss:1.4290, Accuracy:0.3684, Validation Loss:1.4716, Validation Accuracy:0.3300\n",
    "Epoch #93: Loss:1.4274, Accuracy:0.3680, Validation Loss:1.4744, Validation Accuracy:0.3350\n",
    "Epoch #94: Loss:1.4284, Accuracy:0.3741, Validation Loss:1.4798, Validation Accuracy:0.3350\n",
    "Epoch #95: Loss:1.4360, Accuracy:0.3626, Validation Loss:1.4845, Validation Accuracy:0.3366\n",
    "Epoch #96: Loss:1.4306, Accuracy:0.3667, Validation Loss:1.4770, Validation Accuracy:0.3366\n",
    "Epoch #97: Loss:1.4301, Accuracy:0.3704, Validation Loss:1.4729, Validation Accuracy:0.3383\n",
    "Epoch #98: Loss:1.4300, Accuracy:0.3614, Validation Loss:1.4713, Validation Accuracy:0.3366\n",
    "Epoch #99: Loss:1.4295, Accuracy:0.3737, Validation Loss:1.4748, Validation Accuracy:0.3350\n",
    "Epoch #100: Loss:1.4337, Accuracy:0.3704, Validation Loss:1.4788, Validation Accuracy:0.3448\n",
    "Epoch #101: Loss:1.4301, Accuracy:0.3663, Validation Loss:1.4731, Validation Accuracy:0.3333\n",
    "Epoch #102: Loss:1.4289, Accuracy:0.3684, Validation Loss:1.4698, Validation Accuracy:0.3300\n",
    "Epoch #103: Loss:1.4295, Accuracy:0.3700, Validation Loss:1.4732, Validation Accuracy:0.3333\n",
    "Epoch #104: Loss:1.4284, Accuracy:0.3733, Validation Loss:1.4720, Validation Accuracy:0.3350\n",
    "Epoch #105: Loss:1.4257, Accuracy:0.3700, Validation Loss:1.4746, Validation Accuracy:0.3350\n",
    "Epoch #106: Loss:1.4259, Accuracy:0.3741, Validation Loss:1.4714, Validation Accuracy:0.3333\n",
    "Epoch #107: Loss:1.4260, Accuracy:0.3704, Validation Loss:1.4698, Validation Accuracy:0.3317\n",
    "Epoch #108: Loss:1.4243, Accuracy:0.3704, Validation Loss:1.4700, Validation Accuracy:0.3317\n",
    "Epoch #109: Loss:1.4215, Accuracy:0.3680, Validation Loss:1.4707, Validation Accuracy:0.3268\n",
    "Epoch #110: Loss:1.4236, Accuracy:0.3729, Validation Loss:1.4712, Validation Accuracy:0.3333\n",
    "Epoch #111: Loss:1.4232, Accuracy:0.3717, Validation Loss:1.4694, Validation Accuracy:0.3268\n",
    "Epoch #112: Loss:1.4225, Accuracy:0.3688, Validation Loss:1.4703, Validation Accuracy:0.3350\n",
    "Epoch #113: Loss:1.4207, Accuracy:0.3733, Validation Loss:1.4687, Validation Accuracy:0.3284\n",
    "Epoch #114: Loss:1.4204, Accuracy:0.3733, Validation Loss:1.4704, Validation Accuracy:0.3333\n",
    "Epoch #115: Loss:1.4197, Accuracy:0.3704, Validation Loss:1.4696, Validation Accuracy:0.3284\n",
    "Epoch #116: Loss:1.4195, Accuracy:0.3688, Validation Loss:1.4698, Validation Accuracy:0.3300\n",
    "Epoch #117: Loss:1.4185, Accuracy:0.3741, Validation Loss:1.4710, Validation Accuracy:0.3350\n",
    "Epoch #118: Loss:1.4258, Accuracy:0.3717, Validation Loss:1.4690, Validation Accuracy:0.3350\n",
    "Epoch #119: Loss:1.4201, Accuracy:0.3696, Validation Loss:1.4688, Validation Accuracy:0.3350\n",
    "Epoch #120: Loss:1.4194, Accuracy:0.3758, Validation Loss:1.4685, Validation Accuracy:0.3317\n",
    "Epoch #121: Loss:1.4208, Accuracy:0.3721, Validation Loss:1.4682, Validation Accuracy:0.3333\n",
    "Epoch #122: Loss:1.4187, Accuracy:0.3729, Validation Loss:1.4689, Validation Accuracy:0.3333\n",
    "Epoch #123: Loss:1.4169, Accuracy:0.3749, Validation Loss:1.4691, Validation Accuracy:0.3317\n",
    "Epoch #124: Loss:1.4230, Accuracy:0.3749, Validation Loss:1.4677, Validation Accuracy:0.3333\n",
    "Epoch #125: Loss:1.4193, Accuracy:0.3688, Validation Loss:1.4681, Validation Accuracy:0.3333\n",
    "Epoch #126: Loss:1.4168, Accuracy:0.3737, Validation Loss:1.4713, Validation Accuracy:0.3317\n",
    "Epoch #127: Loss:1.4223, Accuracy:0.3696, Validation Loss:1.4733, Validation Accuracy:0.3366\n",
    "Epoch #128: Loss:1.4170, Accuracy:0.3766, Validation Loss:1.4666, Validation Accuracy:0.3300\n",
    "Epoch #129: Loss:1.4156, Accuracy:0.3721, Validation Loss:1.4679, Validation Accuracy:0.3350\n",
    "Epoch #130: Loss:1.4178, Accuracy:0.3708, Validation Loss:1.4690, Validation Accuracy:0.3317\n",
    "Epoch #131: Loss:1.4183, Accuracy:0.3721, Validation Loss:1.4664, Validation Accuracy:0.3333\n",
    "Epoch #132: Loss:1.4176, Accuracy:0.3737, Validation Loss:1.4663, Validation Accuracy:0.3350\n",
    "Epoch #133: Loss:1.4150, Accuracy:0.3741, Validation Loss:1.4671, Validation Accuracy:0.3300\n",
    "Epoch #134: Loss:1.4135, Accuracy:0.3708, Validation Loss:1.4653, Validation Accuracy:0.3317\n",
    "Epoch #135: Loss:1.4132, Accuracy:0.3717, Validation Loss:1.4654, Validation Accuracy:0.3284\n",
    "Epoch #136: Loss:1.4130, Accuracy:0.3733, Validation Loss:1.4657, Validation Accuracy:0.3317\n",
    "Epoch #137: Loss:1.4131, Accuracy:0.3737, Validation Loss:1.4665, Validation Accuracy:0.3268\n",
    "Epoch #138: Loss:1.4124, Accuracy:0.3733, Validation Loss:1.4651, Validation Accuracy:0.3284\n",
    "Epoch #139: Loss:1.4122, Accuracy:0.3754, Validation Loss:1.4641, Validation Accuracy:0.3284\n",
    "Epoch #140: Loss:1.4126, Accuracy:0.3754, Validation Loss:1.4666, Validation Accuracy:0.3317\n",
    "Epoch #141: Loss:1.4123, Accuracy:0.3745, Validation Loss:1.4660, Validation Accuracy:0.3284\n",
    "Epoch #142: Loss:1.4133, Accuracy:0.3741, Validation Loss:1.4652, Validation Accuracy:0.3317\n",
    "Epoch #143: Loss:1.4113, Accuracy:0.3733, Validation Loss:1.4694, Validation Accuracy:0.3333\n",
    "Epoch #144: Loss:1.4112, Accuracy:0.3791, Validation Loss:1.4652, Validation Accuracy:0.3317\n",
    "Epoch #145: Loss:1.4108, Accuracy:0.3774, Validation Loss:1.4650, Validation Accuracy:0.3284\n",
    "Epoch #146: Loss:1.4099, Accuracy:0.3754, Validation Loss:1.4635, Validation Accuracy:0.3284\n",
    "Epoch #147: Loss:1.4111, Accuracy:0.3729, Validation Loss:1.4715, Validation Accuracy:0.3415\n",
    "Epoch #148: Loss:1.4159, Accuracy:0.3708, Validation Loss:1.4641, Validation Accuracy:0.3366\n",
    "Epoch #149: Loss:1.4101, Accuracy:0.3745, Validation Loss:1.4669, Validation Accuracy:0.3317\n",
    "Epoch #150: Loss:1.4136, Accuracy:0.3725, Validation Loss:1.4666, Validation Accuracy:0.3366\n",
    "Epoch #151: Loss:1.4100, Accuracy:0.3770, Validation Loss:1.4635, Validation Accuracy:0.3317\n",
    "Epoch #152: Loss:1.4095, Accuracy:0.3770, Validation Loss:1.4649, Validation Accuracy:0.3268\n",
    "Epoch #153: Loss:1.4115, Accuracy:0.3766, Validation Loss:1.4663, Validation Accuracy:0.3350\n",
    "Epoch #154: Loss:1.4103, Accuracy:0.3766, Validation Loss:1.4667, Validation Accuracy:0.3333\n",
    "Epoch #155: Loss:1.4146, Accuracy:0.3795, Validation Loss:1.4624, Validation Accuracy:0.3300\n",
    "Epoch #156: Loss:1.4083, Accuracy:0.3791, Validation Loss:1.4626, Validation Accuracy:0.3300\n",
    "Epoch #157: Loss:1.4072, Accuracy:0.3799, Validation Loss:1.4681, Validation Accuracy:0.3350\n",
    "Epoch #158: Loss:1.4095, Accuracy:0.3762, Validation Loss:1.4640, Validation Accuracy:0.3317\n",
    "Epoch #159: Loss:1.4100, Accuracy:0.3828, Validation Loss:1.4654, Validation Accuracy:0.3300\n",
    "Epoch #160: Loss:1.4123, Accuracy:0.3811, Validation Loss:1.4627, Validation Accuracy:0.3383\n",
    "Epoch #161: Loss:1.4104, Accuracy:0.3774, Validation Loss:1.4649, Validation Accuracy:0.3383\n",
    "Epoch #162: Loss:1.4088, Accuracy:0.3766, Validation Loss:1.4615, Validation Accuracy:0.3350\n",
    "Epoch #163: Loss:1.4066, Accuracy:0.3807, Validation Loss:1.4609, Validation Accuracy:0.3317\n",
    "Epoch #164: Loss:1.4083, Accuracy:0.3786, Validation Loss:1.4651, Validation Accuracy:0.3415\n",
    "Epoch #165: Loss:1.4071, Accuracy:0.3754, Validation Loss:1.4603, Validation Accuracy:0.3317\n",
    "Epoch #166: Loss:1.4083, Accuracy:0.3819, Validation Loss:1.4640, Validation Accuracy:0.3432\n",
    "Epoch #167: Loss:1.4063, Accuracy:0.3737, Validation Loss:1.4681, Validation Accuracy:0.3432\n",
    "Epoch #168: Loss:1.4072, Accuracy:0.3778, Validation Loss:1.4590, Validation Accuracy:0.3366\n",
    "Epoch #169: Loss:1.4038, Accuracy:0.3786, Validation Loss:1.4624, Validation Accuracy:0.3432\n",
    "Epoch #170: Loss:1.4055, Accuracy:0.3807, Validation Loss:1.4638, Validation Accuracy:0.3465\n",
    "Epoch #171: Loss:1.4049, Accuracy:0.3766, Validation Loss:1.4690, Validation Accuracy:0.3481\n",
    "Epoch #172: Loss:1.4036, Accuracy:0.3770, Validation Loss:1.4590, Validation Accuracy:0.3300\n",
    "Epoch #173: Loss:1.4044, Accuracy:0.3733, Validation Loss:1.4589, Validation Accuracy:0.3350\n",
    "Epoch #174: Loss:1.4058, Accuracy:0.3819, Validation Loss:1.4617, Validation Accuracy:0.3333\n",
    "Epoch #175: Loss:1.4016, Accuracy:0.3803, Validation Loss:1.4604, Validation Accuracy:0.3333\n",
    "Epoch #176: Loss:1.4019, Accuracy:0.3807, Validation Loss:1.4627, Validation Accuracy:0.3333\n",
    "Epoch #177: Loss:1.4035, Accuracy:0.3869, Validation Loss:1.4588, Validation Accuracy:0.3350\n",
    "Epoch #178: Loss:1.4020, Accuracy:0.3819, Validation Loss:1.4581, Validation Accuracy:0.3383\n",
    "Epoch #179: Loss:1.4040, Accuracy:0.3774, Validation Loss:1.4730, Validation Accuracy:0.3448\n",
    "Epoch #180: Loss:1.4076, Accuracy:0.3774, Validation Loss:1.4592, Validation Accuracy:0.3333\n",
    "Epoch #181: Loss:1.4075, Accuracy:0.3815, Validation Loss:1.4609, Validation Accuracy:0.3284\n",
    "Epoch #182: Loss:1.4003, Accuracy:0.3828, Validation Loss:1.4790, Validation Accuracy:0.3432\n",
    "Epoch #183: Loss:1.4145, Accuracy:0.3832, Validation Loss:1.4730, Validation Accuracy:0.3366\n",
    "Epoch #184: Loss:1.4099, Accuracy:0.3762, Validation Loss:1.4628, Validation Accuracy:0.3432\n",
    "Epoch #185: Loss:1.4066, Accuracy:0.3762, Validation Loss:1.4577, Validation Accuracy:0.3448\n",
    "Epoch #186: Loss:1.4056, Accuracy:0.3840, Validation Loss:1.4614, Validation Accuracy:0.3383\n",
    "Epoch #187: Loss:1.4044, Accuracy:0.3766, Validation Loss:1.4649, Validation Accuracy:0.3465\n",
    "Epoch #188: Loss:1.4029, Accuracy:0.3815, Validation Loss:1.4595, Validation Accuracy:0.3317\n",
    "Epoch #189: Loss:1.4014, Accuracy:0.3807, Validation Loss:1.4602, Validation Accuracy:0.3350\n",
    "Epoch #190: Loss:1.4033, Accuracy:0.3819, Validation Loss:1.4656, Validation Accuracy:0.3415\n",
    "Epoch #191: Loss:1.4042, Accuracy:0.3877, Validation Loss:1.4620, Validation Accuracy:0.3350\n",
    "Epoch #192: Loss:1.4009, Accuracy:0.3807, Validation Loss:1.4638, Validation Accuracy:0.3366\n",
    "Epoch #193: Loss:1.3975, Accuracy:0.3856, Validation Loss:1.4582, Validation Accuracy:0.3432\n",
    "Epoch #194: Loss:1.3980, Accuracy:0.3897, Validation Loss:1.4605, Validation Accuracy:0.3333\n",
    "Epoch #195: Loss:1.3978, Accuracy:0.3823, Validation Loss:1.4585, Validation Accuracy:0.3383\n",
    "Epoch #196: Loss:1.3966, Accuracy:0.3811, Validation Loss:1.4612, Validation Accuracy:0.3366\n",
    "Epoch #197: Loss:1.3976, Accuracy:0.3873, Validation Loss:1.4586, Validation Accuracy:0.3333\n",
    "Epoch #198: Loss:1.3961, Accuracy:0.3860, Validation Loss:1.4572, Validation Accuracy:0.3448\n",
    "Epoch #199: Loss:1.3972, Accuracy:0.3836, Validation Loss:1.4651, Validation Accuracy:0.3498\n",
    "Epoch #200: Loss:1.3973, Accuracy:0.3881, Validation Loss:1.4693, Validation Accuracy:0.3350\n",
    "Epoch #201: Loss:1.4046, Accuracy:0.3823, Validation Loss:1.4661, Validation Accuracy:0.3432\n",
    "Epoch #202: Loss:1.4022, Accuracy:0.3856, Validation Loss:1.4585, Validation Accuracy:0.3383\n",
    "Epoch #203: Loss:1.3952, Accuracy:0.3885, Validation Loss:1.4604, Validation Accuracy:0.3415\n",
    "Epoch #204: Loss:1.3955, Accuracy:0.3848, Validation Loss:1.4602, Validation Accuracy:0.3448\n",
    "Epoch #205: Loss:1.3956, Accuracy:0.3832, Validation Loss:1.4617, Validation Accuracy:0.3415\n",
    "Epoch #206: Loss:1.3963, Accuracy:0.3934, Validation Loss:1.4591, Validation Accuracy:0.3383\n",
    "Epoch #207: Loss:1.3941, Accuracy:0.3840, Validation Loss:1.4622, Validation Accuracy:0.3448\n",
    "Epoch #208: Loss:1.3947, Accuracy:0.3836, Validation Loss:1.4570, Validation Accuracy:0.3383\n",
    "Epoch #209: Loss:1.3940, Accuracy:0.3848, Validation Loss:1.4589, Validation Accuracy:0.3448\n",
    "Epoch #210: Loss:1.3951, Accuracy:0.3893, Validation Loss:1.4609, Validation Accuracy:0.3498\n",
    "Epoch #211: Loss:1.3931, Accuracy:0.3893, Validation Loss:1.4594, Validation Accuracy:0.3481\n",
    "Epoch #212: Loss:1.3930, Accuracy:0.3860, Validation Loss:1.4568, Validation Accuracy:0.3514\n",
    "Epoch #213: Loss:1.3947, Accuracy:0.3815, Validation Loss:1.4667, Validation Accuracy:0.3498\n",
    "Epoch #214: Loss:1.3939, Accuracy:0.3893, Validation Loss:1.4575, Validation Accuracy:0.3448\n",
    "Epoch #215: Loss:1.3914, Accuracy:0.3893, Validation Loss:1.4564, Validation Accuracy:0.3448\n",
    "Epoch #216: Loss:1.3919, Accuracy:0.3918, Validation Loss:1.4611, Validation Accuracy:0.3547\n",
    "Epoch #217: Loss:1.3928, Accuracy:0.3893, Validation Loss:1.4592, Validation Accuracy:0.3514\n",
    "Epoch #218: Loss:1.3946, Accuracy:0.3914, Validation Loss:1.4592, Validation Accuracy:0.3514\n",
    "Epoch #219: Loss:1.3917, Accuracy:0.3897, Validation Loss:1.4597, Validation Accuracy:0.3399\n",
    "Epoch #220: Loss:1.3886, Accuracy:0.3889, Validation Loss:1.4574, Validation Accuracy:0.3498\n",
    "Epoch #221: Loss:1.3891, Accuracy:0.3901, Validation Loss:1.4569, Validation Accuracy:0.3514\n",
    "Epoch #222: Loss:1.3882, Accuracy:0.3918, Validation Loss:1.4577, Validation Accuracy:0.3432\n",
    "Epoch #223: Loss:1.3891, Accuracy:0.3918, Validation Loss:1.4572, Validation Accuracy:0.3514\n",
    "Epoch #224: Loss:1.3877, Accuracy:0.3918, Validation Loss:1.4596, Validation Accuracy:0.3481\n",
    "Epoch #225: Loss:1.3884, Accuracy:0.3869, Validation Loss:1.4598, Validation Accuracy:0.3514\n",
    "Epoch #226: Loss:1.3884, Accuracy:0.3910, Validation Loss:1.4553, Validation Accuracy:0.3563\n",
    "Epoch #227: Loss:1.3899, Accuracy:0.3930, Validation Loss:1.4552, Validation Accuracy:0.3580\n",
    "Epoch #228: Loss:1.3917, Accuracy:0.3877, Validation Loss:1.4620, Validation Accuracy:0.3415\n",
    "Epoch #229: Loss:1.3933, Accuracy:0.3914, Validation Loss:1.4583, Validation Accuracy:0.3481\n",
    "Epoch #230: Loss:1.3911, Accuracy:0.3901, Validation Loss:1.4598, Validation Accuracy:0.3530\n",
    "Epoch #231: Loss:1.3890, Accuracy:0.3955, Validation Loss:1.4615, Validation Accuracy:0.3498\n",
    "Epoch #232: Loss:1.3899, Accuracy:0.3910, Validation Loss:1.4627, Validation Accuracy:0.3530\n",
    "Epoch #233: Loss:1.3879, Accuracy:0.3938, Validation Loss:1.4536, Validation Accuracy:0.3465\n",
    "Epoch #234: Loss:1.3907, Accuracy:0.3979, Validation Loss:1.4713, Validation Accuracy:0.3498\n",
    "Epoch #235: Loss:1.3953, Accuracy:0.3938, Validation Loss:1.4672, Validation Accuracy:0.3580\n",
    "Epoch #236: Loss:1.3927, Accuracy:0.3930, Validation Loss:1.4572, Validation Accuracy:0.3448\n",
    "Epoch #237: Loss:1.3906, Accuracy:0.3951, Validation Loss:1.4559, Validation Accuracy:0.3481\n",
    "Epoch #238: Loss:1.3863, Accuracy:0.3926, Validation Loss:1.4653, Validation Accuracy:0.3481\n",
    "Epoch #239: Loss:1.3841, Accuracy:0.3914, Validation Loss:1.4570, Validation Accuracy:0.3514\n",
    "Epoch #240: Loss:1.3889, Accuracy:0.3975, Validation Loss:1.4541, Validation Accuracy:0.3465\n",
    "Epoch #241: Loss:1.3865, Accuracy:0.3922, Validation Loss:1.4711, Validation Accuracy:0.3514\n",
    "Epoch #242: Loss:1.3906, Accuracy:0.3910, Validation Loss:1.4608, Validation Accuracy:0.3547\n",
    "Epoch #243: Loss:1.3894, Accuracy:0.3926, Validation Loss:1.4543, Validation Accuracy:0.3498\n",
    "Epoch #244: Loss:1.3843, Accuracy:0.3914, Validation Loss:1.4588, Validation Accuracy:0.3498\n",
    "Epoch #245: Loss:1.3853, Accuracy:0.3914, Validation Loss:1.4589, Validation Accuracy:0.3498\n",
    "Epoch #246: Loss:1.3840, Accuracy:0.3922, Validation Loss:1.4569, Validation Accuracy:0.3432\n",
    "Epoch #247: Loss:1.3828, Accuracy:0.3934, Validation Loss:1.4549, Validation Accuracy:0.3530\n",
    "Epoch #248: Loss:1.3840, Accuracy:0.3910, Validation Loss:1.4645, Validation Accuracy:0.3580\n",
    "Epoch #249: Loss:1.3829, Accuracy:0.3922, Validation Loss:1.4541, Validation Accuracy:0.3547\n",
    "Epoch #250: Loss:1.3836, Accuracy:0.3955, Validation Loss:1.4541, Validation Accuracy:0.3530\n",
    "Epoch #251: Loss:1.3821, Accuracy:0.3926, Validation Loss:1.4563, Validation Accuracy:0.3530\n",
    "Epoch #252: Loss:1.3824, Accuracy:0.3930, Validation Loss:1.4608, Validation Accuracy:0.3530\n",
    "Epoch #253: Loss:1.3812, Accuracy:0.3943, Validation Loss:1.4561, Validation Accuracy:0.3448\n",
    "Epoch #254: Loss:1.3883, Accuracy:0.3975, Validation Loss:1.4657, Validation Accuracy:0.3596\n",
    "Epoch #255: Loss:1.3990, Accuracy:0.3922, Validation Loss:1.4696, Validation Accuracy:0.3563\n",
    "Epoch #256: Loss:1.3906, Accuracy:0.3926, Validation Loss:1.4588, Validation Accuracy:0.3580\n",
    "Epoch #257: Loss:1.3836, Accuracy:0.3984, Validation Loss:1.4577, Validation Accuracy:0.3530\n",
    "Epoch #258: Loss:1.3839, Accuracy:0.3930, Validation Loss:1.4669, Validation Accuracy:0.3563\n",
    "Epoch #259: Loss:1.3873, Accuracy:0.3922, Validation Loss:1.4560, Validation Accuracy:0.3547\n",
    "Epoch #260: Loss:1.3850, Accuracy:0.3992, Validation Loss:1.4595, Validation Accuracy:0.3612\n",
    "Epoch #261: Loss:1.3852, Accuracy:0.3897, Validation Loss:1.4601, Validation Accuracy:0.3596\n",
    "Epoch #262: Loss:1.3809, Accuracy:0.3951, Validation Loss:1.4550, Validation Accuracy:0.3580\n",
    "Epoch #263: Loss:1.3807, Accuracy:0.3967, Validation Loss:1.4637, Validation Accuracy:0.3563\n",
    "Epoch #264: Loss:1.3824, Accuracy:0.3943, Validation Loss:1.4550, Validation Accuracy:0.3498\n",
    "Epoch #265: Loss:1.3819, Accuracy:0.3938, Validation Loss:1.4586, Validation Accuracy:0.3547\n",
    "Epoch #266: Loss:1.3868, Accuracy:0.3963, Validation Loss:1.4688, Validation Accuracy:0.3612\n",
    "Epoch #267: Loss:1.3884, Accuracy:0.3963, Validation Loss:1.4532, Validation Accuracy:0.3563\n",
    "Epoch #268: Loss:1.3852, Accuracy:0.3947, Validation Loss:1.4562, Validation Accuracy:0.3465\n",
    "Epoch #269: Loss:1.3860, Accuracy:0.3906, Validation Loss:1.4796, Validation Accuracy:0.3465\n",
    "Epoch #270: Loss:1.3897, Accuracy:0.3971, Validation Loss:1.4558, Validation Accuracy:0.3563\n",
    "Epoch #271: Loss:1.3852, Accuracy:0.3963, Validation Loss:1.4542, Validation Accuracy:0.3481\n",
    "Epoch #272: Loss:1.3868, Accuracy:0.3947, Validation Loss:1.4689, Validation Accuracy:0.3432\n",
    "Epoch #273: Loss:1.3810, Accuracy:0.3979, Validation Loss:1.4553, Validation Accuracy:0.3465\n",
    "Epoch #274: Loss:1.3796, Accuracy:0.3963, Validation Loss:1.4558, Validation Accuracy:0.3580\n",
    "Epoch #275: Loss:1.3781, Accuracy:0.3971, Validation Loss:1.4560, Validation Accuracy:0.3465\n",
    "Epoch #276: Loss:1.3788, Accuracy:0.3992, Validation Loss:1.4562, Validation Accuracy:0.3547\n",
    "Epoch #277: Loss:1.3785, Accuracy:0.3979, Validation Loss:1.4564, Validation Accuracy:0.3530\n",
    "Epoch #278: Loss:1.3781, Accuracy:0.3979, Validation Loss:1.4550, Validation Accuracy:0.3514\n",
    "Epoch #279: Loss:1.3774, Accuracy:0.3918, Validation Loss:1.4607, Validation Accuracy:0.3629\n",
    "Epoch #280: Loss:1.3796, Accuracy:0.4012, Validation Loss:1.4581, Validation Accuracy:0.3432\n",
    "Epoch #281: Loss:1.3832, Accuracy:0.3934, Validation Loss:1.4660, Validation Accuracy:0.3612\n",
    "Epoch #282: Loss:1.3755, Accuracy:0.3996, Validation Loss:1.4564, Validation Accuracy:0.3596\n",
    "Epoch #283: Loss:1.3798, Accuracy:0.4021, Validation Loss:1.4556, Validation Accuracy:0.3465\n",
    "Epoch #284: Loss:1.3780, Accuracy:0.3955, Validation Loss:1.4599, Validation Accuracy:0.3563\n",
    "Epoch #285: Loss:1.3813, Accuracy:0.4008, Validation Loss:1.4555, Validation Accuracy:0.3432\n",
    "Epoch #286: Loss:1.3791, Accuracy:0.4004, Validation Loss:1.4573, Validation Accuracy:0.3580\n",
    "Epoch #287: Loss:1.3786, Accuracy:0.3947, Validation Loss:1.4607, Validation Accuracy:0.3547\n",
    "Epoch #288: Loss:1.3794, Accuracy:0.3996, Validation Loss:1.4547, Validation Accuracy:0.3645\n",
    "Epoch #289: Loss:1.3779, Accuracy:0.3996, Validation Loss:1.4541, Validation Accuracy:0.3530\n",
    "Epoch #290: Loss:1.3787, Accuracy:0.3996, Validation Loss:1.4713, Validation Accuracy:0.3596\n",
    "Epoch #291: Loss:1.3896, Accuracy:0.4012, Validation Loss:1.4588, Validation Accuracy:0.3514\n",
    "Epoch #292: Loss:1.3827, Accuracy:0.3988, Validation Loss:1.4529, Validation Accuracy:0.3596\n",
    "Epoch #293: Loss:1.3780, Accuracy:0.3996, Validation Loss:1.4642, Validation Accuracy:0.3547\n",
    "Epoch #294: Loss:1.3774, Accuracy:0.4000, Validation Loss:1.4562, Validation Accuracy:0.3448\n",
    "Epoch #295: Loss:1.3830, Accuracy:0.3975, Validation Loss:1.4563, Validation Accuracy:0.3547\n",
    "Epoch #296: Loss:1.3778, Accuracy:0.4000, Validation Loss:1.4600, Validation Accuracy:0.3530\n",
    "Epoch #297: Loss:1.3760, Accuracy:0.4016, Validation Loss:1.4565, Validation Accuracy:0.3580\n",
    "Epoch #298: Loss:1.3743, Accuracy:0.4021, Validation Loss:1.4544, Validation Accuracy:0.3612\n",
    "Epoch #299: Loss:1.3797, Accuracy:0.3992, Validation Loss:1.4669, Validation Accuracy:0.3547\n",
    "Epoch #300: Loss:1.3836, Accuracy:0.3992, Validation Loss:1.4569, Validation Accuracy:0.3580\n",
    "\n",
    "Test:\n",
    "Test Loss:1.45688629, Accuracy:0.3580\n",
    "Labels: ['04', '05', '03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      04  05  03  01  02\n",
    "t:04  20   7  28  30  27\n",
    "t:05   3  96   0  33  10\n",
    "t:03  17  10  35  27  26\n",
    "t:01   8  36  10  45  27\n",
    "t:02  16  25  14  37  22\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.31      0.18      0.23       112\n",
    "          05       0.55      0.68      0.61       142\n",
    "          03       0.40      0.30      0.35       115\n",
    "          01       0.26      0.36      0.30       126\n",
    "          02       0.20      0.19      0.19       114\n",
    "\n",
    "    accuracy                           0.36       609\n",
    "   macro avg       0.34      0.34      0.34       609\n",
    "weighted avg       0.35      0.36      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 03:45:41 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 44 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6058592876581528, 1.6055320734265206, 1.60542706198293, 1.605387539111922, 1.6052993567314837, 1.6053618687910007, 1.6052562035559041, 1.6051670070156479, 1.6051082109974326, 1.605042985311674, 1.6049518375756902, 1.6047856406429521, 1.6045778435830804, 1.604278506316575, 1.6039302137684939, 1.603274224035454, 1.6023331994102115, 1.6008016657946733, 1.5982709811826057, 1.593700900453652, 1.5857449807165487, 1.5718517663639362, 1.5502849343570777, 1.5222029662484606, 1.5089632317741908, 1.494019509927784, 1.4797595338085405, 1.4775119939656876, 1.47770550254922, 1.4753405650456746, 1.4746719383449585, 1.4806568082330263, 1.4869197509167424, 1.473325749727697, 1.4734748192804397, 1.4720511628293442, 1.4730364704758467, 1.472635610741739, 1.477560866642468, 1.4761851558152874, 1.4726457259142145, 1.4732480767520972, 1.4753361564551668, 1.4822440609360368, 1.4728978580637715, 1.4735347224378037, 1.4759438394129962, 1.4749338121836997, 1.4735954029023746, 1.4760201762266738, 1.4730550747591091, 1.475202841907495, 1.4755153581622396, 1.4777133703623304, 1.486051580197314, 1.472778301520888, 1.473801003105339, 1.4771881790583945, 1.4769118176696727, 1.4738506027826144, 1.4742040569559107, 1.4730635799210647, 1.4745595216359606, 1.472807208892747, 1.475540856226716, 1.478378445252605, 1.4749937427454982, 1.4786508560963647, 1.47325290032404, 1.4742660426545418, 1.4774695127859883, 1.4737190286122714, 1.4743361189251853, 1.4764358208488753, 1.4777360414636547, 1.472323531391977, 1.4734131778989519, 1.4729773095871623, 1.4741797801504777, 1.4731122175069473, 1.4715393010422906, 1.4786505483837158, 1.4776160065176451, 1.4745833214084894, 1.4713771378465474, 1.478120831237442, 1.4745185036573112, 1.4733189344406128, 1.472028780257565, 1.4725513693147105, 1.4731642213361016, 1.4716139679471847, 1.4743781581105073, 1.4798210105676761, 1.4844796567519114, 1.4770380826223464, 1.4729166533950906, 1.4713369543329249, 1.4748365346629828, 1.47877961035041, 1.4730629169294986, 1.4698323655402523, 1.4732347756183792, 1.4720459074418142, 1.474609974765621, 1.4713905748279614, 1.4698320544999222, 1.4700018144006213, 1.4706599579264574, 1.4712484605206644, 1.4693505241365856, 1.470328561973885, 1.4686650527130403, 1.4703592972215174, 1.4696211556495704, 1.4698243241004756, 1.4709792280040548, 1.4689790253177262, 1.4688348169201504, 1.4685491014192453, 1.4681998520649124, 1.4689488880740011, 1.4690911728760292, 1.467663341946594, 1.4681225292788351, 1.4713412419524294, 1.4732864545092403, 1.4666043304653198, 1.4679056965854564, 1.4690412429753195, 1.4664449666325485, 1.4662845397034694, 1.4671150522279035, 1.4652521815793267, 1.4653872607768268, 1.4656773972002353, 1.4664936241845192, 1.4651329407746765, 1.4641070731950707, 1.4666159436816262, 1.4659725470691676, 1.4651626800668651, 1.4694296765601498, 1.465219478693306, 1.4649549258753585, 1.4635197358765626, 1.4715398080243265, 1.4641476121833563, 1.4668926818813206, 1.4665994336843882, 1.463469083869007, 1.4648877446874609, 1.466337287758763, 1.4666602041921004, 1.4624483501186902, 1.4625784552156045, 1.4680752971489441, 1.4640240769080928, 1.465409350708396, 1.4626840953952183, 1.4649223541391307, 1.4614719847348718, 1.4609362307831963, 1.46513947653653, 1.4602627846212026, 1.4640254140487445, 1.4680907857437635, 1.459006623485797, 1.4623797777642562, 1.463846285354915, 1.468998855166443, 1.4589694411492309, 1.4589496574965604, 1.4617274210762312, 1.4604359407143053, 1.462694496552541, 1.4588044024453375, 1.4580515274664843, 1.4730109573389314, 1.4591805155837085, 1.4609322213187006, 1.4790082637508124, 1.4729803896498406, 1.4627523490752297, 1.457668052518309, 1.4614238030413298, 1.4648699077283611, 1.4594998860789834, 1.4602034951274228, 1.4656490784364773, 1.461994999930972, 1.4638470904580478, 1.4582373951064738, 1.460517069780572, 1.4585244776971626, 1.4612316680071977, 1.458553511716658, 1.4572425203761836, 1.4650940450737238, 1.4693367307017786, 1.466090557414714, 1.4585431297424392, 1.4603790054571844, 1.4602055809963708, 1.4617162545522053, 1.4590771302018064, 1.4621791046828472, 1.4570052148086097, 1.4589281307261175, 1.460893378664903, 1.4593523844513792, 1.456807807748541, 1.466743798091494, 1.4574678179078502, 1.4564433914100008, 1.4611032077635842, 1.4591949363843169, 1.459190649743542, 1.4596677861973177, 1.4574379780022382, 1.4569199441493244, 1.4577181116113522, 1.4571705099397105, 1.4595931457181281, 1.459772087866058, 1.4552683166682427, 1.455164031050671, 1.4619823610058362, 1.4583169687753437, 1.4598216268620858, 1.4615393326983273, 1.4626597044698906, 1.4536140158845874, 1.4712785200532434, 1.467210800385436, 1.4571718799656834, 1.4559314143285766, 1.4652742861918433, 1.4570432971850993, 1.4540918364704927, 1.4710824675552168, 1.460825073503704, 1.4543130374307116, 1.45878215689573, 1.458864512701927, 1.456850596836635, 1.4549175618317327, 1.4644994189586547, 1.45410357124504, 1.4541234850687739, 1.4563369831232407, 1.4607670399160024, 1.4561078315493705, 1.4657317349084689, 1.469570063800843, 1.4588243121584061, 1.4576700941486702, 1.4669499221106468, 1.4560302582084643, 1.4595317032145358, 1.4601218381343022, 1.4549891817550158, 1.4637225363250632, 1.4550026512302592, 1.458590010899824, 1.4688176290546535, 1.453186312724021, 1.4561548321117908, 1.479570711970525, 1.4558493008558777, 1.45418049177317, 1.4689283230034589, 1.4552903764549343, 1.4558025281417546, 1.4560161484481862, 1.4561798670413264, 1.4564057332150062, 1.455034355616139, 1.4607004500766498, 1.4580992898721805, 1.4660385864708811, 1.4564319385096358, 1.455617177858337, 1.45986963355874, 1.4554640475556573, 1.4572833809750811, 1.4606680537288022, 1.454694078473622, 1.4541025057997805, 1.4712933648396007, 1.4587840407548476, 1.4528956646206734, 1.4642019324701996, 1.4561629365817668, 1.456285614489726, 1.459969514109231, 1.4564513594450426, 1.4544225171673277, 1.4669429433756862, 1.4568863523808997], 'val_acc': [0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.2348111656254344, 0.2380952378750239, 0.26929392216632325, 0.2824302113604272, 0.28899835615322506, 0.3037766812763778, 0.3284072234419179, 0.33004925956671266, 0.3201970427200712, 0.3284072234419179, 0.3333333320120481, 0.3300492591752207, 0.32840722314829895, 0.3464696207167871, 0.3497536926727577, 0.33661740387014566, 0.3333333315226832, 0.33661740377227267, 0.3333333315226832, 0.334975367549605, 0.33169129539788844, 0.3333333316205562, 0.3267651870235042, 0.31527093395419503, 0.33169129539788844, 0.3316912952021425, 0.32840722305042597, 0.321839078453374, 0.3185550063016575, 0.3185550063016575, 0.3185550062037845, 0.32840722314829895, 0.3185550062037845, 0.33169129530001545, 0.3267651871213772, 0.32840722314829895, 0.3251231505072176, 0.3218390782576281, 0.3316912952021425, 0.321839078453374, 0.3300492588816018, 0.3251231505072176, 0.33661740347865376, 0.3300492587837288, 0.3300492589794748, 0.3300492588816018, 0.3234811144802958, 0.3333333315226832, 0.32840722285468005, 0.334975367353859, 0.32676518653413933, 0.3234811144802958, 0.3333333312290643, 0.3300492591752207, 0.32840722285468005, 0.334975367353859, 0.3349753677453509, 0.33497536725598603, 0.3316912952021425, 0.3382594396034485, 0.32676518682775824, 0.33169129530001545, 0.3382594397013215, 0.3316912955936344, 0.3431855481735787, 0.32183907864912, 0.33169129510426953, 0.3366174033807808, 0.3333333316205562, 0.3382594397013215, 0.3333333312290643, 0.3300492592730937, 0.3382594396034485, 0.3300492592730937, 0.334975367451732, 0.3349753679410969, 0.33661740347865376, 0.33661740396801865, 0.3382594397991945, 0.33661740367439974, 0.3349753679410969, 0.34482758429837346, 0.33333333171842916, 0.3300492591752207, 0.3333333314248102, 0.3349753678432239, 0.334975367549605, 0.3333333316205562, 0.33169129549576143, 0.33169129530001545, 0.3267651870235042, 0.33333333181630215, 0.32676518682775824, 0.33497536764747793, 0.32840722314829895, 0.3333333316205562, 0.32840722314829895, 0.3300492590773478, 0.3349753680389699, 0.3349753677453509, 0.33497536764747793, 0.3316912957893804, 0.33333333171842916, 0.3333333315226832, 0.3316912957893804, 0.3333333315226832, 0.3333333315226832, 0.3316912958872534, 0.33661740377227267, 0.3300492594688397, 0.33497536764747793, 0.33169129539788844, 0.33333333181630215, 0.3349753679410969, 0.3300492593709667, 0.3316912956915074, 0.32840722314829895, 0.33169129549576143, 0.3267651871213772, 0.32840722314829895, 0.32840722324617194, 0.3316912955936344, 0.32840722334404493, 0.3316912956915074, 0.3333333315226832, 0.3316912957893804, 0.3284072234419179, 0.3284072235397909, 0.3415435122445299, 0.33661740396801865, 0.3316912958872534, 0.33661740406589163, 0.33169129549576143, 0.32676518741499616, 0.3349753679410969, 0.3333333316205562, 0.3300492594688397, 0.33004925956671266, 0.3349753677453509, 0.3316912956915074, 0.33004925966458565, 0.3382594401906864, 0.3382594401906864, 0.3349753680389699, 0.3316912956915074, 0.34154351244027586, 0.3316912957893804, 0.34318554885868957, 0.3431855486629436, 0.3366174041637646, 0.3431855486629436, 0.34646962110827906, 0.3481116571352008, 0.33004925966458565, 0.3349753680389699, 0.33333333171842916, 0.3333333320120481, 0.33333333171842916, 0.3349753680389699, 0.33825944028855937, 0.34482758478773834, 0.3333333320120481, 0.3284072236376639, 0.3431855485650706, 0.3366174042616376, 0.3431855485650706, 0.34482758478773834, 0.33825944038643235, 0.34646962101040607, 0.3316912956915074, 0.33497536823471585, 0.34154351253814885, 0.33497536813684287, 0.3366174043595106, 0.34318554885868957, 0.33333333191417513, 0.33825944038643235, 0.3366174042616376, 0.3333333320120481, 0.3448275849834843, 0.34975369335786854, 0.33497536813684287, 0.3431855486629436, 0.33825944028855937, 0.34154351263602184, 0.3448275850813573, 0.3415435127338948, 0.33825944038643235, 0.3448275850813573, 0.33825944038643235, 0.3448275850813573, 0.34975369335786854, 0.3481116572330738, 0.3513957294826633, 0.34975369335786854, 0.3448275849834843, 0.3448275848856113, 0.35467980183012576, 0.35139572977628225, 0.3513957294826633, 0.3399014764133541, 0.3497536935536145, 0.35139572977628225, 0.34318554885868957, 0.35139572967840926, 0.3481116575266927, 0.3513957294826633, 0.3563218384442854, 0.3579638743733342, 0.34154351263602184, 0.3481116572330738, 0.35303776550958504, 0.34975369335786854, 0.3530377659989499, 0.34646962130402503, 0.34975369335786854, 0.35796387407971525, 0.3448275853749762, 0.3481116575266927, 0.3481116574288198, 0.3513957298741552, 0.34646962120615205, 0.3513957294826633, 0.35467980212374467, 0.3497536938472334, 0.34975369365148745, 0.34975369374936044, 0.34318554885868957, 0.3530377659989499, 0.35796387417758824, 0.35467980231949064, 0.353037765901077, 0.353037765705331, 0.353037765901077, 0.3448275851792303, 0.3596059105960019, 0.3563218378570475, 0.35796387447120714, 0.3530377659989499, 0.3563218378570475, 0.35467980202587174, 0.3612479464271777, 0.35960591040025597, 0.35796387447120714, 0.3563218381506665, 0.34975369345574153, 0.35467980212374467, 0.3612479464271777, 0.3563218380527935, 0.34646962130402503, 0.34646962101040607, 0.3563218381506665, 0.3481116574288198, 0.34318554885868957, 0.34646962120615205, 0.35796387417758824, 0.34646962130402503, 0.35467980212374467, 0.353037765901077, 0.3513957295805363, 0.36288998264984546, 0.3431855490544355, 0.3612479464271777, 0.3596059106938749, 0.34646962120615205, 0.3563218381506665, 0.34318554895656256, 0.35796387417758824, 0.35467980202587174, 0.3645320189703862, 0.353037765803204, 0.359605910302383, 0.35139572977628225, 0.35960591020451, 0.35467980183012576, 0.3448275850813573, 0.35467980202587174, 0.353037765803204, 0.3579638742754612, 0.36124794672079663, 0.35467980192799875, 0.35796387417758824], 'loss': [1.606920541385361, 1.606145241226259, 1.6057735780915685, 1.605661228941696, 1.6056383745381475, 1.6055837457919269, 1.6055375131983043, 1.6054346941824567, 1.6053416972287626, 1.6052827657860163, 1.6052383534962147, 1.605226059273283, 1.6049297439489032, 1.6048460188342806, 1.6044618880234705, 1.6041977978829731, 1.6033247931537198, 1.6022708917300559, 1.6006695036036278, 1.5975341303392603, 1.5923858872674084, 1.5823886309805837, 1.5672411741417291, 1.545151404482628, 1.522618178763184, 1.5001666682946364, 1.4921243313646415, 1.4759486332076777, 1.4705798523137212, 1.4661373148219052, 1.462870700941928, 1.4614437309378716, 1.4638644231663103, 1.4655784237311362, 1.4568743800235724, 1.4579779559337138, 1.4547072696489964, 1.455301992604375, 1.4542151897594915, 1.4529014859600968, 1.4549716888267157, 1.451292597978267, 1.4508414912762337, 1.452664962392568, 1.45043052897561, 1.4488659817090515, 1.4484792145370702, 1.4499218362557569, 1.4495998729670563, 1.4481402622845627, 1.449496953345422, 1.4454332106901635, 1.445925302280293, 1.4477396755492662, 1.4467422801611116, 1.4563094263448852, 1.452918955971328, 1.445658248014274, 1.4441964554835638, 1.4452080827963671, 1.4416298869209367, 1.4429083887801277, 1.4415924646036826, 1.4414550700961197, 1.4429478037773462, 1.43956204082442, 1.4455225066727437, 1.4423442605584553, 1.4438846693880993, 1.4397969070156498, 1.4396674056807093, 1.4413602052038454, 1.4391953935368595, 1.4385080231778187, 1.437709776968437, 1.4371805381970728, 1.4339233976614794, 1.4339295238929608, 1.4338531111300115, 1.4354113163644528, 1.433460949235873, 1.432641243249239, 1.4347871625203126, 1.4349850915051094, 1.4328992492119634, 1.43529641863257, 1.4396218690049722, 1.4291202868769057, 1.4307642476025058, 1.4297116611527712, 1.428224968126912, 1.4290486918337781, 1.4273924365915067, 1.4283812044092763, 1.4359846188547185, 1.4306103676006778, 1.4300620560283779, 1.4299959840226222, 1.4294896027880284, 1.4337430707727858, 1.4301105788111441, 1.4288853150863177, 1.4295244207127629, 1.428442710772677, 1.4257271320178524, 1.4259208849568141, 1.4259535202500266, 1.424313780316582, 1.4214906926027804, 1.423617995544136, 1.4231528251322878, 1.4224958961749223, 1.4206725425054405, 1.4203670190834656, 1.4196745596382407, 1.4194585669456812, 1.4184923000649015, 1.4257616148347483, 1.4201313940161797, 1.41942659521005, 1.4207777398812453, 1.4187323291198919, 1.416877504638578, 1.4230117746936712, 1.419287928761398, 1.416777374905972, 1.422259135559599, 1.417000665850708, 1.4155544855266626, 1.417777694862726, 1.4183347597748837, 1.4175870264824901, 1.4149686020747347, 1.413529630216485, 1.413221840594094, 1.4129835468542895, 1.4130561408076199, 1.412350634872546, 1.4122035452962167, 1.4125549632665804, 1.4122925994822133, 1.413347042610513, 1.4113468071763275, 1.4112220270188192, 1.4107991338020966, 1.409904916966965, 1.4110693468939841, 1.4159411582124306, 1.4101306314096314, 1.4136066101414957, 1.409966859141904, 1.4095139806030712, 1.4115267275784784, 1.4103300131811498, 1.4145535398068125, 1.408287426774262, 1.4072288753560436, 1.4095428178442577, 1.4099905382681186, 1.4122724527206265, 1.410351009878045, 1.408823545414809, 1.4065519880709951, 1.408270255594038, 1.4071458166874409, 1.4082887198645966, 1.4063096806253985, 1.4072416983101157, 1.403763355121965, 1.405536770184182, 1.4048910509144745, 1.4035846516336994, 1.4043897387428206, 1.4057702892859614, 1.4015696985276083, 1.401901068628691, 1.4035051403593968, 1.4019873627891775, 1.4039514687516605, 1.4075766836103718, 1.407547356264792, 1.400315220938571, 1.4145443841906788, 1.4098737287325536, 1.406598632046819, 1.4055593441643999, 1.404433686973133, 1.4028535400817526, 1.401353131819065, 1.4032539040156213, 1.4041815543321614, 1.4009038989793594, 1.397545206179609, 1.3980476481713797, 1.3978223053092096, 1.3965995710733246, 1.3976112248226846, 1.3961403955913911, 1.3971650666524742, 1.3973365877198487, 1.404553031529734, 1.4021843757472734, 1.395158146245279, 1.3954911775902312, 1.3956416314632252, 1.3962810679382858, 1.394124537033222, 1.39470212591747, 1.3940191343334911, 1.3951434071303883, 1.3930579515942803, 1.3930431013968936, 1.3947083183382571, 1.393886361915228, 1.3914150440227815, 1.3919126615387214, 1.392817652347886, 1.3945889275176815, 1.3917214266328597, 1.3885664324985638, 1.389097459898837, 1.3882389429903128, 1.389124269064447, 1.387727601180576, 1.3884355124506862, 1.38844720861995, 1.3899487240358546, 1.3916659543646435, 1.3933079565330209, 1.3911070452578504, 1.3889812183086387, 1.3899441561414965, 1.3879315288159881, 1.3907088507372252, 1.3952858484256438, 1.3927489142153542, 1.390610178295347, 1.3862839166143837, 1.3840673989094259, 1.3889171366329311, 1.3864638649707457, 1.3905770493239102, 1.3894053991815147, 1.3842882766371145, 1.3853427903118565, 1.3839709989344071, 1.3828306035584248, 1.3839502037917808, 1.382850271622503, 1.3835802355341353, 1.382120632684696, 1.3824206222498931, 1.381179738240565, 1.3883123223052132, 1.3990464439627082, 1.3905798067057646, 1.3836359086712282, 1.383906948835698, 1.3872664081976889, 1.3850140990662623, 1.3852172123088484, 1.3808574457677727, 1.3807189560524002, 1.3823632954572016, 1.3819245520558445, 1.3868188689132002, 1.3884176338477792, 1.3852321454386936, 1.3860011232462262, 1.3896694952947157, 1.3851691367445051, 1.3867644262020102, 1.3809613252322532, 1.379617592882082, 1.3780555618861863, 1.378841753613043, 1.378475463708568, 1.378121593306931, 1.3774186378142184, 1.37963004924923, 1.3832090650495807, 1.3754997774805622, 1.379773396926739, 1.3779711288593144, 1.3813391856833894, 1.3791258796774142, 1.3785783437243233, 1.379441974442108, 1.3779077966110418, 1.3787026904691662, 1.3895946921264366, 1.3827304921845391, 1.3780474781255703, 1.3773995453327343, 1.382972499526257, 1.377794037658331, 1.3759850539710732, 1.3743294311010372, 1.3796845957483843, 1.3835792321199265], 'acc': [0.23285420929871545, 0.23285420970872686, 0.2328542083195837, 0.23285420990455322, 0.23285420851541005, 0.23285420890706277, 0.23285420851541005, 0.23285420970872686, 0.23285420970872686, 0.23285421010037957, 0.23285420951290053, 0.23285420892542147, 0.2328542091028891, 0.23285421051039099, 0.2328542091028891, 0.23285421051039099, 0.23285420892542147, 0.23285421049203225, 0.23490759738906453, 0.23778233993224784, 0.2562628327453895, 0.27104722637170636, 0.28418890975828776, 0.2924024642737739, 0.3014373720671362, 0.3129363444306767, 0.3121149909202568, 0.3203285413233896, 0.32854209505557036, 0.32607802911460765, 0.33634497077802855, 0.334702257527463, 0.33921971390869093, 0.34045174774203213, 0.34127310160738733, 0.3420944582143114, 0.34209445367358793, 0.3437371673158062, 0.34537987461814645, 0.3486652969334894, 0.34373716790328523, 0.3449691975875557, 0.3445585235310776, 0.3478439419298936, 0.34250513285826856, 0.3490759733766011, 0.34250513289498596, 0.3515400402599781, 0.3474332670533926, 0.35236139588777043, 0.3507186867862756, 0.35441478397811954, 0.35975359399215884, 0.35071868799795114, 0.342915813450451, 0.3535934297578291, 0.3613963034853064, 0.35687885050656126, 0.3593429166066328, 0.3589322381685402, 0.36057495004832135, 0.36344969040069736, 0.35687885207317205, 0.3593429175857646, 0.35646817265594766, 0.3646817240382122, 0.3634496939622891, 0.3618069811033762, 0.3572895275004346, 0.356057492883788, 0.35687885168151934, 0.35318275014477835, 0.36303901215843104, 0.3601642692603125, 0.3646817230590805, 0.3667351127160403, 0.363860370172857, 0.36386036841041985, 0.3577002031602409, 0.363039014153412, 0.361806983257466, 0.3655030794701782, 0.36427104583266334, 0.36468172384238584, 0.36344969157565543, 0.3667351146743038, 0.36303901650332815, 0.36919917901193827, 0.3687885010022158, 0.3691991808110928, 0.368377823971625, 0.36837782338414593, 0.3679671461577288, 0.3741273086663389, 0.3626283367311685, 0.3667351117369086, 0.37043121167032134, 0.36139630622687524, 0.3737166308157253, 0.37043121010371055, 0.36632443490214417, 0.36837782537912683, 0.37002053326894613, 0.37330595617176815, 0.37002053307311983, 0.3741273114079078, 0.37043121245362676, 0.3704312102995369, 0.36796714576607614, 0.37289527718291393, 0.3716632449161835, 0.3687885023730002, 0.3733059548009837, 0.37330595362602564, 0.37043121167032134, 0.3687885033888494, 0.37412730964547064, 0.3716632437412254, 0.36960985663000806, 0.37577002188018704, 0.372073922925906, 0.37289527737874023, 0.37494866351082584, 0.37494866311917313, 0.36878850021891035, 0.37371663046079007, 0.3696098578049662, 0.37659137731215303, 0.3720739205759898, 0.3708418890925648, 0.37207392233842695, 0.37371663359401164, 0.37412731101625507, 0.3708418910875457, 0.3716632437412254, 0.3733059549968101, 0.37371663202740085, 0.37330595261017646, 0.37535934426211726, 0.37535934484959627, 0.3745379868718878, 0.3741273094129269, 0.3733059571508999, 0.3790554422372665, 0.37741273117750823, 0.3753593424996801, 0.37289527914117737, 0.37084189147919844, 0.37453798984600045, 0.3724845979982333, 0.37700205336361203, 0.37700205120952224, 0.3765913759413686, 0.37659137457058417, 0.37946612044281536, 0.37905544204144015, 0.37987679747340614, 0.37618069632831785, 0.38275154001658945, 0.3811088279776994, 0.37741272961089745, 0.3765913765288476, 0.3806981509471086, 0.37864476559832844, 0.37535934347881184, 0.38193018556375524, 0.37371663261487986, 0.377823408208099, 0.3786447626609332, 0.3806981515345877, 0.37659137731215303, 0.3770020523844803, 0.37330595519263643, 0.3819301841929708, 0.3802874764622604, 0.38069815212206676, 0.3868583165889403, 0.38193018595540795, 0.37741273019837646, 0.37741273254829266, 0.38151950575487814, 0.3827515411915476, 0.3831622156763958, 0.37618069871495147, 0.37618069773581975, 0.3839835750248887, 0.3765913751580632, 0.38151950774985904, 0.380698154312874, 0.38193018654288696, 0.3876796706134044, 0.3806981529053721, 0.38562628412638356, 0.38973305870375347, 0.382340862594346, 0.3811088307192683, 0.3872689922120292, 0.38603696213610605, 0.3835728964276872, 0.3880903490514971, 0.38234086161521424, 0.385626284909689, 0.38850102682867577, 0.3848049275194595, 0.3831622192012701, 0.3934291598488418, 0.3839835730666253, 0.38357289724771004, 0.3848049271278068, 0.38932238010655196, 0.38932238386397, 0.38603696056949527, 0.3815195069665537, 0.38932237971489925, 0.38932238131822744, 0.3917864483974308, 0.38932238210153286, 0.3913757715626664, 0.38973305913212364, 0.3889117048751157, 0.39014373812097786, 0.39178644581497085, 0.39178644937656254, 0.3917864483974308, 0.38685831698059303, 0.390965094727902, 0.39301848203494566, 0.3876796712376009, 0.39137577136684004, 0.39014373514686523, 0.3954825465684064, 0.3909650945320756, 0.39383983511699544, 0.39794661165262885, 0.3938398353128218, 0.393018482818251, 0.39507186715118203, 0.3926078026544387, 0.3913757686252711, 0.39753593266377457, 0.3921971271904587, 0.390965094727902, 0.3926078012469368, 0.3913757696044029, 0.39137577195431905, 0.3921971271904587, 0.39342915769475195, 0.39096509198633306, 0.39219712523219524, 0.395482546531689, 0.39260780382939675, 0.3930184824265983, 0.3942505117559335, 0.39753593270049203, 0.39219712343304064, 0.3926078014794806, 0.39835728950324245, 0.3930184810558139, 0.39219712382469335, 0.39917864630599287, 0.389733059915429, 0.39507186953781565, 0.39671457684015593, 0.3942505146933287, 0.393839833746211, 0.3963039004337616, 0.39630389941791244, 0.39466119113644044, 0.390554415347395, 0.39712525860729647, 0.3963039020003724, 0.39466118914145953, 0.39794661364760975, 0.39630390020121786, 0.39712525684485933, 0.39917864493520844, 0.3979466132559571, 0.3979466114568025, 0.39178644780995175, 0.40123203517964734, 0.3934291574989256, 0.3995893231407573, 0.4020533894366552, 0.39548254637258007, 0.4008213569740985, 0.4004106791602023, 0.3946611927030512, 0.3995893205582973, 0.39958932333658365, 0.3995893236915189, 0.4012320343596245, 0.39876796907957573, 0.3995893213416027, 0.39999999840891093, 0.3975359344629292, 0.39999999895967253, 0.4016427082569937, 0.4020533898283079, 0.39917864434772937, 0.3991786437235329]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
