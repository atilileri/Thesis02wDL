{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf27.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 01:47:55 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F602C48550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F64DE16EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0836, Accuracy:0.3938, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0756, Accuracy:0.3947, Validation Loss:1.0757, Validation Accuracy:0.3695\n",
    "Epoch #3: Loss:1.0753, Accuracy:0.3864, Validation Loss:1.0760, Validation Accuracy:0.3695\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3889, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3951, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #6: Loss:1.0743, Accuracy:0.3930, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0744, Accuracy:0.3938, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #10: Loss:1.0747, Accuracy:0.3947, Validation Loss:1.0746, Validation Accuracy:0.3810\n",
    "Epoch #11: Loss:1.0742, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0743, Accuracy:0.3988, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #20: Loss:1.0745, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #21: Loss:1.0740, Accuracy:0.3959, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0746, Validation Accuracy:0.3711\n",
    "Epoch #25: Loss:1.0743, Accuracy:0.3975, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #26: Loss:1.0743, Accuracy:0.3906, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0737, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #29: Loss:1.0737, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #30: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #31: Loss:1.0737, Accuracy:0.3971, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #32: Loss:1.0736, Accuracy:0.4012, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #33: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #34: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #35: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3826\n",
    "Epoch #36: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #37: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #38: Loss:1.0733, Accuracy:0.4029, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #39: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #40: Loss:1.0736, Accuracy:0.4012, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #41: Loss:1.0735, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3777\n",
    "Epoch #42: Loss:1.0735, Accuracy:0.4033, Validation Loss:1.0746, Validation Accuracy:0.3695\n",
    "Epoch #43: Loss:1.0733, Accuracy:0.4033, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #44: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #45: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #46: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #47: Loss:1.0735, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.3777\n",
    "Epoch #48: Loss:1.0734, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #49: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #50: Loss:1.0729, Accuracy:0.4037, Validation Loss:1.0747, Validation Accuracy:0.3810\n",
    "Epoch #51: Loss:1.0734, Accuracy:0.4008, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #52: Loss:1.0733, Accuracy:0.4053, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #53: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #54: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #55: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #56: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.3810\n",
    "Epoch #57: Loss:1.0742, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #58: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #59: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #60: Loss:1.0738, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0737, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #63: Loss:1.0738, Accuracy:0.3893, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #64: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #65: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #66: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3826\n",
    "Epoch #67: Loss:1.0733, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #68: Loss:1.0731, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #69: Loss:1.0732, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #70: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #71: Loss:1.0737, Accuracy:0.3951, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #72: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #73: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #74: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #75: Loss:1.0734, Accuracy:0.3910, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #76: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #77: Loss:1.0735, Accuracy:0.3971, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #78: Loss:1.0736, Accuracy:0.3959, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #79: Loss:1.0736, Accuracy:0.4016, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #80: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #81: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #82: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #83: Loss:1.0728, Accuracy:0.3996, Validation Loss:1.0753, Validation Accuracy:0.3875\n",
    "Epoch #84: Loss:1.0731, Accuracy:0.3996, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #85: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #86: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #87: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #88: Loss:1.0722, Accuracy:0.3992, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #89: Loss:1.0722, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #90: Loss:1.0725, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #91: Loss:1.0720, Accuracy:0.4000, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #92: Loss:1.0720, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #93: Loss:1.0723, Accuracy:0.3959, Validation Loss:1.0755, Validation Accuracy:0.3957\n",
    "Epoch #94: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0763, Validation Accuracy:0.3990\n",
    "Epoch #95: Loss:1.0724, Accuracy:0.4066, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #96: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #97: Loss:1.0714, Accuracy:0.4074, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #98: Loss:1.0717, Accuracy:0.4066, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #99: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #100: Loss:1.0714, Accuracy:0.4021, Validation Loss:1.0747, Validation Accuracy:0.3826\n",
    "Epoch #101: Loss:1.0717, Accuracy:0.4057, Validation Loss:1.0742, Validation Accuracy:0.3826\n",
    "Epoch #102: Loss:1.0710, Accuracy:0.4045, Validation Loss:1.0740, Validation Accuracy:0.3793\n",
    "Epoch #103: Loss:1.0715, Accuracy:0.4033, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #104: Loss:1.0716, Accuracy:0.4057, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #105: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #106: Loss:1.0722, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #107: Loss:1.0713, Accuracy:0.3979, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #108: Loss:1.0721, Accuracy:0.4012, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #109: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #110: Loss:1.0713, Accuracy:0.4000, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #111: Loss:1.0722, Accuracy:0.3975, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #112: Loss:1.0711, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #113: Loss:1.0718, Accuracy:0.3955, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #114: Loss:1.0714, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #115: Loss:1.0718, Accuracy:0.4016, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #116: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #117: Loss:1.0707, Accuracy:0.4037, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #118: Loss:1.0709, Accuracy:0.4029, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #119: Loss:1.0708, Accuracy:0.4000, Validation Loss:1.0746, Validation Accuracy:0.4039\n",
    "Epoch #120: Loss:1.0712, Accuracy:0.3963, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #121: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #122: Loss:1.0708, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #123: Loss:1.0702, Accuracy:0.4025, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #124: Loss:1.0703, Accuracy:0.3959, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #125: Loss:1.0714, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #126: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #127: Loss:1.0718, Accuracy:0.4041, Validation Loss:1.0756, Validation Accuracy:0.3908\n",
    "Epoch #128: Loss:1.0705, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.3859\n",
    "Epoch #129: Loss:1.0718, Accuracy:0.4062, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #130: Loss:1.0717, Accuracy:0.4012, Validation Loss:1.0797, Validation Accuracy:0.3810\n",
    "Epoch #131: Loss:1.0747, Accuracy:0.3955, Validation Loss:1.0768, Validation Accuracy:0.3941\n",
    "Epoch #132: Loss:1.0739, Accuracy:0.3996, Validation Loss:1.0754, Validation Accuracy:0.3908\n",
    "Epoch #133: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0748, Validation Accuracy:0.3727\n",
    "Epoch #134: Loss:1.0720, Accuracy:0.4078, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #135: Loss:1.0716, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #136: Loss:1.0715, Accuracy:0.4070, Validation Loss:1.0735, Validation Accuracy:0.3892\n",
    "Epoch #137: Loss:1.0713, Accuracy:0.4070, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #138: Loss:1.0715, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #139: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #140: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3777\n",
    "Epoch #141: Loss:1.0711, Accuracy:0.4004, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #142: Loss:1.0713, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #143: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #144: Loss:1.0717, Accuracy:0.3996, Validation Loss:1.0757, Validation Accuracy:0.3859\n",
    "Epoch #145: Loss:1.0713, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #146: Loss:1.0708, Accuracy:0.4016, Validation Loss:1.0743, Validation Accuracy:0.3793\n",
    "Epoch #147: Loss:1.0710, Accuracy:0.4000, Validation Loss:1.0742, Validation Accuracy:0.3810\n",
    "Epoch #148: Loss:1.0708, Accuracy:0.4037, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #149: Loss:1.0716, Accuracy:0.4057, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #150: Loss:1.0710, Accuracy:0.4008, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #151: Loss:1.0709, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.4039\n",
    "Epoch #152: Loss:1.0718, Accuracy:0.4012, Validation Loss:1.0736, Validation Accuracy:0.4023\n",
    "Epoch #153: Loss:1.0721, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #154: Loss:1.0718, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.4138\n",
    "Epoch #155: Loss:1.0707, Accuracy:0.3947, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #156: Loss:1.0710, Accuracy:0.4053, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #157: Loss:1.0713, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #158: Loss:1.0713, Accuracy:0.4057, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #159: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #160: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #161: Loss:1.0712, Accuracy:0.4045, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #162: Loss:1.0709, Accuracy:0.3992, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #163: Loss:1.0706, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #164: Loss:1.0715, Accuracy:0.4025, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #165: Loss:1.0721, Accuracy:0.3996, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #166: Loss:1.0709, Accuracy:0.3971, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #167: Loss:1.0711, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3810\n",
    "Epoch #168: Loss:1.0707, Accuracy:0.4057, Validation Loss:1.0733, Validation Accuracy:0.3859\n",
    "Epoch #169: Loss:1.0702, Accuracy:0.4074, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #170: Loss:1.0706, Accuracy:0.4025, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #171: Loss:1.0707, Accuracy:0.4000, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #172: Loss:1.0700, Accuracy:0.4057, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #173: Loss:1.0706, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #174: Loss:1.0704, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.4089\n",
    "Epoch #175: Loss:1.0709, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.4072\n",
    "Epoch #176: Loss:1.0712, Accuracy:0.3975, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #177: Loss:1.0708, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.4154\n",
    "Epoch #178: Loss:1.0718, Accuracy:0.3893, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #179: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0734, Validation Accuracy:0.3924\n",
    "Epoch #180: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0762, Validation Accuracy:0.4089\n",
    "Epoch #181: Loss:1.0709, Accuracy:0.4016, Validation Loss:1.0774, Validation Accuracy:0.3859\n",
    "Epoch #182: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #183: Loss:1.0707, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3826\n",
    "Epoch #184: Loss:1.0706, Accuracy:0.4029, Validation Loss:1.0744, Validation Accuracy:0.3974\n",
    "Epoch #185: Loss:1.0704, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #186: Loss:1.0700, Accuracy:0.4025, Validation Loss:1.0754, Validation Accuracy:0.3974\n",
    "Epoch #187: Loss:1.0699, Accuracy:0.4033, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #188: Loss:1.0711, Accuracy:0.4025, Validation Loss:1.0758, Validation Accuracy:0.3859\n",
    "Epoch #189: Loss:1.0703, Accuracy:0.4033, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #190: Loss:1.0710, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #191: Loss:1.0705, Accuracy:0.4025, Validation Loss:1.0760, Validation Accuracy:0.3859\n",
    "Epoch #192: Loss:1.0703, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #193: Loss:1.0704, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #194: Loss:1.0703, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3826\n",
    "Epoch #195: Loss:1.0702, Accuracy:0.4037, Validation Loss:1.0753, Validation Accuracy:0.3859\n",
    "Epoch #196: Loss:1.0707, Accuracy:0.4041, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #197: Loss:1.0711, Accuracy:0.3992, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #198: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0754, Validation Accuracy:0.3859\n",
    "Epoch #199: Loss:1.0704, Accuracy:0.4025, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #200: Loss:1.0709, Accuracy:0.4025, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #201: Loss:1.0706, Accuracy:0.3996, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #202: Loss:1.0705, Accuracy:0.4074, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #203: Loss:1.0706, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #204: Loss:1.0704, Accuracy:0.4025, Validation Loss:1.0757, Validation Accuracy:0.3826\n",
    "Epoch #205: Loss:1.0707, Accuracy:0.4045, Validation Loss:1.0760, Validation Accuracy:0.3859\n",
    "Epoch #206: Loss:1.0708, Accuracy:0.4016, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #207: Loss:1.0702, Accuracy:0.4025, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #208: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0760, Validation Accuracy:0.3842\n",
    "Epoch #209: Loss:1.0701, Accuracy:0.3951, Validation Loss:1.0766, Validation Accuracy:0.4039\n",
    "Epoch #210: Loss:1.0702, Accuracy:0.3959, Validation Loss:1.0757, Validation Accuracy:0.3842\n",
    "Epoch #211: Loss:1.0700, Accuracy:0.3971, Validation Loss:1.0757, Validation Accuracy:0.3859\n",
    "Epoch #212: Loss:1.0699, Accuracy:0.4004, Validation Loss:1.0764, Validation Accuracy:0.3810\n",
    "Epoch #213: Loss:1.0695, Accuracy:0.4008, Validation Loss:1.0773, Validation Accuracy:0.3760\n",
    "Epoch #214: Loss:1.0708, Accuracy:0.4016, Validation Loss:1.0760, Validation Accuracy:0.3744\n",
    "Epoch #215: Loss:1.0699, Accuracy:0.4033, Validation Loss:1.0760, Validation Accuracy:0.3727\n",
    "Epoch #216: Loss:1.0695, Accuracy:0.4049, Validation Loss:1.0764, Validation Accuracy:0.3744\n",
    "Epoch #217: Loss:1.0696, Accuracy:0.4025, Validation Loss:1.0766, Validation Accuracy:0.3727\n",
    "Epoch #218: Loss:1.0704, Accuracy:0.3992, Validation Loss:1.0770, Validation Accuracy:0.3727\n",
    "Epoch #219: Loss:1.0705, Accuracy:0.4066, Validation Loss:1.0776, Validation Accuracy:0.3842\n",
    "Epoch #220: Loss:1.0701, Accuracy:0.4049, Validation Loss:1.0788, Validation Accuracy:0.3777\n",
    "Epoch #221: Loss:1.0690, Accuracy:0.3992, Validation Loss:1.0794, Validation Accuracy:0.3777\n",
    "Epoch #222: Loss:1.0691, Accuracy:0.4057, Validation Loss:1.0783, Validation Accuracy:0.3826\n",
    "Epoch #223: Loss:1.0688, Accuracy:0.4049, Validation Loss:1.0789, Validation Accuracy:0.3826\n",
    "Epoch #224: Loss:1.0690, Accuracy:0.4000, Validation Loss:1.0814, Validation Accuracy:0.3711\n",
    "Epoch #225: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0799, Validation Accuracy:0.3744\n",
    "Epoch #226: Loss:1.0696, Accuracy:0.4033, Validation Loss:1.0774, Validation Accuracy:0.3793\n",
    "Epoch #227: Loss:1.0694, Accuracy:0.4029, Validation Loss:1.0777, Validation Accuracy:0.3875\n",
    "Epoch #228: Loss:1.0703, Accuracy:0.4037, Validation Loss:1.0776, Validation Accuracy:0.3875\n",
    "Epoch #229: Loss:1.0699, Accuracy:0.4021, Validation Loss:1.0778, Validation Accuracy:0.3711\n",
    "Epoch #230: Loss:1.0698, Accuracy:0.3975, Validation Loss:1.0777, Validation Accuracy:0.3826\n",
    "Epoch #231: Loss:1.0690, Accuracy:0.4049, Validation Loss:1.0777, Validation Accuracy:0.3777\n",
    "Epoch #232: Loss:1.0685, Accuracy:0.4074, Validation Loss:1.0778, Validation Accuracy:0.3777\n",
    "Epoch #233: Loss:1.0689, Accuracy:0.4074, Validation Loss:1.0776, Validation Accuracy:0.3760\n",
    "Epoch #234: Loss:1.0690, Accuracy:0.4033, Validation Loss:1.0776, Validation Accuracy:0.3744\n",
    "Epoch #235: Loss:1.0686, Accuracy:0.4066, Validation Loss:1.0782, Validation Accuracy:0.3859\n",
    "Epoch #236: Loss:1.0682, Accuracy:0.4041, Validation Loss:1.0778, Validation Accuracy:0.3777\n",
    "Epoch #237: Loss:1.0689, Accuracy:0.4033, Validation Loss:1.0776, Validation Accuracy:0.3662\n",
    "Epoch #238: Loss:1.0683, Accuracy:0.4037, Validation Loss:1.0788, Validation Accuracy:0.3793\n",
    "Epoch #239: Loss:1.0690, Accuracy:0.4033, Validation Loss:1.0780, Validation Accuracy:0.3859\n",
    "Epoch #240: Loss:1.0684, Accuracy:0.4025, Validation Loss:1.0775, Validation Accuracy:0.3760\n",
    "Epoch #241: Loss:1.0683, Accuracy:0.4016, Validation Loss:1.0782, Validation Accuracy:0.3678\n",
    "Epoch #242: Loss:1.0677, Accuracy:0.4012, Validation Loss:1.0779, Validation Accuracy:0.3695\n",
    "Epoch #243: Loss:1.0678, Accuracy:0.4078, Validation Loss:1.0779, Validation Accuracy:0.3892\n",
    "Epoch #244: Loss:1.0676, Accuracy:0.4045, Validation Loss:1.0785, Validation Accuracy:0.3892\n",
    "Epoch #245: Loss:1.0678, Accuracy:0.3979, Validation Loss:1.0779, Validation Accuracy:0.3711\n",
    "Epoch #246: Loss:1.0677, Accuracy:0.4041, Validation Loss:1.0781, Validation Accuracy:0.3711\n",
    "Epoch #247: Loss:1.0678, Accuracy:0.4037, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #248: Loss:1.0686, Accuracy:0.4021, Validation Loss:1.0780, Validation Accuracy:0.3793\n",
    "Epoch #249: Loss:1.0677, Accuracy:0.4037, Validation Loss:1.0787, Validation Accuracy:0.3711\n",
    "Epoch #250: Loss:1.0673, Accuracy:0.4037, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #251: Loss:1.0675, Accuracy:0.4041, Validation Loss:1.0776, Validation Accuracy:0.3711\n",
    "Epoch #252: Loss:1.0684, Accuracy:0.4041, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #253: Loss:1.0682, Accuracy:0.4037, Validation Loss:1.0782, Validation Accuracy:0.3777\n",
    "Epoch #254: Loss:1.0669, Accuracy:0.4041, Validation Loss:1.0784, Validation Accuracy:0.3793\n",
    "Epoch #255: Loss:1.0674, Accuracy:0.4012, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #256: Loss:1.0672, Accuracy:0.4041, Validation Loss:1.0784, Validation Accuracy:0.3777\n",
    "Epoch #257: Loss:1.0672, Accuracy:0.4033, Validation Loss:1.0782, Validation Accuracy:0.3711\n",
    "Epoch #258: Loss:1.0668, Accuracy:0.4033, Validation Loss:1.0784, Validation Accuracy:0.3777\n",
    "Epoch #259: Loss:1.0679, Accuracy:0.4041, Validation Loss:1.0786, Validation Accuracy:0.3711\n",
    "Epoch #260: Loss:1.0683, Accuracy:0.4037, Validation Loss:1.0781, Validation Accuracy:0.3711\n",
    "Epoch #261: Loss:1.0677, Accuracy:0.4041, Validation Loss:1.0784, Validation Accuracy:0.3711\n",
    "Epoch #262: Loss:1.0668, Accuracy:0.4037, Validation Loss:1.0790, Validation Accuracy:0.3711\n",
    "Epoch #263: Loss:1.0674, Accuracy:0.4045, Validation Loss:1.0783, Validation Accuracy:0.3711\n",
    "Epoch #264: Loss:1.0674, Accuracy:0.4041, Validation Loss:1.0783, Validation Accuracy:0.3711\n",
    "Epoch #265: Loss:1.0670, Accuracy:0.4037, Validation Loss:1.0792, Validation Accuracy:0.3711\n",
    "Epoch #266: Loss:1.0665, Accuracy:0.4045, Validation Loss:1.0788, Validation Accuracy:0.3711\n",
    "Epoch #267: Loss:1.0684, Accuracy:0.4041, Validation Loss:1.0786, Validation Accuracy:0.3711\n",
    "Epoch #268: Loss:1.0723, Accuracy:0.3984, Validation Loss:1.0812, Validation Accuracy:0.3924\n",
    "Epoch #269: Loss:1.0696, Accuracy:0.4016, Validation Loss:1.0809, Validation Accuracy:0.3711\n",
    "Epoch #270: Loss:1.0683, Accuracy:0.4041, Validation Loss:1.0783, Validation Accuracy:0.3711\n",
    "Epoch #271: Loss:1.0672, Accuracy:0.4045, Validation Loss:1.0789, Validation Accuracy:0.3859\n",
    "Epoch #272: Loss:1.0668, Accuracy:0.4070, Validation Loss:1.0791, Validation Accuracy:0.3777\n",
    "Epoch #273: Loss:1.0679, Accuracy:0.4066, Validation Loss:1.0787, Validation Accuracy:0.3777\n",
    "Epoch #274: Loss:1.0684, Accuracy:0.4066, Validation Loss:1.0807, Validation Accuracy:0.3826\n",
    "Epoch #275: Loss:1.0675, Accuracy:0.4078, Validation Loss:1.0789, Validation Accuracy:0.3744\n",
    "Epoch #276: Loss:1.0673, Accuracy:0.4033, Validation Loss:1.0779, Validation Accuracy:0.3744\n",
    "Epoch #277: Loss:1.0670, Accuracy:0.4033, Validation Loss:1.0790, Validation Accuracy:0.3793\n",
    "Epoch #278: Loss:1.0665, Accuracy:0.4008, Validation Loss:1.0790, Validation Accuracy:0.3744\n",
    "Epoch #279: Loss:1.0665, Accuracy:0.3988, Validation Loss:1.0791, Validation Accuracy:0.3826\n",
    "Epoch #280: Loss:1.0660, Accuracy:0.4016, Validation Loss:1.0790, Validation Accuracy:0.3744\n",
    "Epoch #281: Loss:1.0664, Accuracy:0.4029, Validation Loss:1.0787, Validation Accuracy:0.3744\n",
    "Epoch #282: Loss:1.0661, Accuracy:0.4041, Validation Loss:1.0786, Validation Accuracy:0.3744\n",
    "Epoch #283: Loss:1.0664, Accuracy:0.4008, Validation Loss:1.0787, Validation Accuracy:0.3760\n",
    "Epoch #284: Loss:1.0662, Accuracy:0.4033, Validation Loss:1.0787, Validation Accuracy:0.3744\n",
    "Epoch #285: Loss:1.0660, Accuracy:0.4041, Validation Loss:1.0783, Validation Accuracy:0.3760\n",
    "Epoch #286: Loss:1.0660, Accuracy:0.4049, Validation Loss:1.0777, Validation Accuracy:0.3760\n",
    "Epoch #287: Loss:1.0669, Accuracy:0.4029, Validation Loss:1.0780, Validation Accuracy:0.3974\n",
    "Epoch #288: Loss:1.0669, Accuracy:0.4082, Validation Loss:1.0780, Validation Accuracy:0.3777\n",
    "Epoch #289: Loss:1.0676, Accuracy:0.4074, Validation Loss:1.0782, Validation Accuracy:0.3777\n",
    "Epoch #290: Loss:1.0659, Accuracy:0.4078, Validation Loss:1.0782, Validation Accuracy:0.3760\n",
    "Epoch #291: Loss:1.0658, Accuracy:0.4107, Validation Loss:1.0776, Validation Accuracy:0.3941\n",
    "Epoch #292: Loss:1.0679, Accuracy:0.4066, Validation Loss:1.0762, Validation Accuracy:0.3859\n",
    "Epoch #293: Loss:1.0672, Accuracy:0.4086, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #294: Loss:1.0667, Accuracy:0.4086, Validation Loss:1.0776, Validation Accuracy:0.3892\n",
    "Epoch #295: Loss:1.0660, Accuracy:0.4057, Validation Loss:1.0771, Validation Accuracy:0.3826\n",
    "Epoch #296: Loss:1.0667, Accuracy:0.4062, Validation Loss:1.0774, Validation Accuracy:0.3842\n",
    "Epoch #297: Loss:1.0663, Accuracy:0.4094, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #298: Loss:1.0664, Accuracy:0.4074, Validation Loss:1.0777, Validation Accuracy:0.3908\n",
    "Epoch #299: Loss:1.0666, Accuracy:0.4099, Validation Loss:1.0778, Validation Accuracy:0.3793\n",
    "Epoch #300: Loss:1.0683, Accuracy:0.4045, Validation Loss:1.0783, Validation Accuracy:0.3810\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07833219, Accuracy:0.3810\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  76  151   0\n",
    "t:01  84  156   0\n",
    "t:03  40  102   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.38      0.33      0.36       227\n",
    "          01       0.38      0.65      0.48       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.25      0.33      0.28       609\n",
    "weighted avg       0.29      0.38      0.32       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 02:28:37 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 42 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0752593061606872, 1.0757084390017004, 1.0760237552066547, 1.074521317466335, 1.074378397664413, 1.074784211141527, 1.0747232016279975, 1.0746125793222137, 1.0746454266687528, 1.0745732813633133, 1.074538035737274, 1.0748283796514, 1.074553798963675, 1.074642378904158, 1.0745974389594568, 1.0742241324266582, 1.074363458332757, 1.074385770044499, 1.074388735008553, 1.0741693987243477, 1.0740825773655682, 1.0744640840881172, 1.0746655315405433, 1.0746479686257875, 1.0748071594191302, 1.0745577185807753, 1.074458980599452, 1.07447879537573, 1.0744131999258533, 1.0746561643133805, 1.0746744307391163, 1.0748428484097685, 1.0749213088713647, 1.0748102563159612, 1.0747156834171714, 1.0748388025365243, 1.0746946740033003, 1.0748251371195752, 1.0748832055500575, 1.0745134042401618, 1.0745060445835632, 1.0745544273082064, 1.0746753957666983, 1.074634840335752, 1.0747490079923607, 1.0746992963483963, 1.0744938253377654, 1.0747421159728603, 1.0744746982170443, 1.0746818033149481, 1.0747076670328777, 1.0747384200933923, 1.07458074868019, 1.0743671228928715, 1.0742261444993795, 1.074137260761167, 1.0744112489258715, 1.0742931113454508, 1.0740686463213516, 1.0741756796249615, 1.0742489753294069, 1.0742344316003358, 1.0742561584231498, 1.0745255727877563, 1.0747177289624519, 1.0747032363230764, 1.074467356569074, 1.0744222913469588, 1.074626604520237, 1.0748335033018992, 1.0743306023733956, 1.0738522520989229, 1.0742030094801303, 1.074254860040198, 1.074523492008203, 1.0744636328936799, 1.0745912289188804, 1.0743186289845232, 1.0747214839572, 1.0747441916630185, 1.0748868857698488, 1.075085841571952, 1.075303573718016, 1.0749993245981402, 1.0747679156818608, 1.0749067909807604, 1.0745653792946601, 1.0743716945397639, 1.0742969250639867, 1.074115223093769, 1.074626697303822, 1.0748384981515569, 1.0754839968798784, 1.0762745425814675, 1.0749546491062307, 1.0747874252901877, 1.074295763115969, 1.0744356975962572, 1.0743377705904456, 1.0746578904012545, 1.0741920249998471, 1.074019968803293, 1.0743442204198226, 1.0743813518624392, 1.0748011981716687, 1.0740961361009695, 1.0746818219108143, 1.0745999077075026, 1.0756375685896975, 1.0750671196453676, 1.07398034394864, 1.0741476376459909, 1.0737328407995415, 1.0743963837819341, 1.0739970753345582, 1.074320312987016, 1.074436951936368, 1.0744294591725165, 1.0746196058191886, 1.0745405062470335, 1.0745962817093422, 1.0740210042994207, 1.0744507293200063, 1.0742696310303286, 1.0743285364705353, 1.0746309357910908, 1.075602129370904, 1.0753069691274357, 1.0743969788496521, 1.079668954088183, 1.0768222299898396, 1.07536814991868, 1.074779148759513, 1.0744251776211367, 1.0741096233890952, 1.0735242932496596, 1.073412820623426, 1.074213191597724, 1.0743098016247177, 1.075314482248867, 1.0744521116779746, 1.0746623365749866, 1.0748008372161189, 1.0756604113602286, 1.074488923075947, 1.0742583306160662, 1.0741966316852662, 1.0744453973958057, 1.0742117156731867, 1.0746806658351755, 1.0754609244993363, 1.0736296423550309, 1.074814284572069, 1.0744347944243984, 1.0751014115970905, 1.0748094320297241, 1.074348723555629, 1.0747382439220285, 1.0759123080273958, 1.0747788637533955, 1.0748042392809012, 1.0748160263196196, 1.0741724271100925, 1.0738198747384333, 1.0741210657191786, 1.0744701886216212, 1.0744157005804904, 1.0733267614994142, 1.074786687523665, 1.0749552058077407, 1.0755707804596875, 1.0735453388765333, 1.0740307179969324, 1.0739662598310824, 1.0739419796979681, 1.0738107469085794, 1.0739144629054078, 1.0747841488943115, 1.0734444185235035, 1.0761714674569116, 1.0774484183792215, 1.0751433673946338, 1.0747429674677857, 1.074440120867712, 1.074527427089234, 1.0754293420631897, 1.0754614314813724, 1.0757778804681963, 1.0756082939984175, 1.07510987878433, 1.076034631243676, 1.0740687222707839, 1.073925451496356, 1.0742496829510517, 1.075268585106422, 1.0755110616950174, 1.0760104078768902, 1.0754056372274514, 1.07491165879129, 1.0748974599666001, 1.0752179969120494, 1.0754038688584502, 1.075198598683174, 1.0756531092529422, 1.0760231965476852, 1.075623495433914, 1.075687846135232, 1.0759879147086433, 1.0766471248542147, 1.0756989443439178, 1.0757281756753405, 1.076377121294269, 1.077293598788908, 1.0759911674192582, 1.0759687272981666, 1.0764056516594096, 1.0766005968225414, 1.0770131463096255, 1.0776129418797484, 1.0788297017023873, 1.0794066821021595, 1.0783194986665974, 1.0789062857432123, 1.0814152339409138, 1.0799356011921548, 1.0773757081509419, 1.0776724421919273, 1.0776309047035004, 1.0777840761128317, 1.0776677413527014, 1.077679040592488, 1.0777829415692484, 1.077619687285525, 1.0776272875139083, 1.0782122796000715, 1.077784605997145, 1.0775637632520327, 1.0788277469832321, 1.077991457016793, 1.077507651889657, 1.078214128420662, 1.077925770740791, 1.0779322669619607, 1.0784864993322463, 1.0778912460471217, 1.0780937904599068, 1.0779660701360216, 1.0780165451892296, 1.0786537382207284, 1.0780115540587452, 1.0775600577810127, 1.0780399577762498, 1.0782484101935952, 1.0783665616719789, 1.0780010765604982, 1.0783545487424226, 1.078172826610371, 1.078417723010522, 1.0786255028447493, 1.0780789319713324, 1.078409975385431, 1.0790190381565312, 1.078319674837961, 1.078258007030769, 1.079167930950672, 1.0788160811112628, 1.0785956713562137, 1.0812154491546706, 1.0808856808297544, 1.0782944281112972, 1.0788866590788015, 1.0791055909518539, 1.0787355770618456, 1.0806686978034785, 1.0788543224334717, 1.077918426743869, 1.078951343526981, 1.079003627272858, 1.0791116641660041, 1.079004768276058, 1.0786689406349546, 1.0785924752161813, 1.0786514207843099, 1.0786658631169737, 1.0782671624608031, 1.07767857295539, 1.0780441516334396, 1.0780076724163612, 1.078238367251379, 1.078217898879341, 1.0775653172792081, 1.0762083089997616, 1.0766849660716817, 1.0776040094043626, 1.0771059342010072, 1.0773934125900269, 1.0768625515043637, 1.0777268112195144, 1.0778360864016028, 1.0783320123143187], 'val_acc': [0.39408866921669156, 0.3694581275405163, 0.3694581275405163, 0.3940886690209456, 0.39573070573297825, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.38587848849484485, 0.38095238002258763, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.3924466331897698, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.38752052491325856, 0.3760262722354413, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.37110016386105704, 0.3760262721375683, 0.39408866921669156, 0.39408866921669156, 0.3957307013776306, 0.39573070573297825, 0.3891625612337993, 0.3891625612337993, 0.390804597358594, 0.39080459716284804, 0.39080459716284804, 0.38259441673462025, 0.38259441673462025, 0.39244663348338876, 0.38752052491325856, 0.38752052491325856, 0.38752052520687746, 0.377668308555982, 0.3694581278341353, 0.39901477788469475, 0.39080459716284804, 0.3891625610380533, 0.3858784888863368, 0.377668308458109, 0.39408866921669156, 0.3924466331897698, 0.3809523804140796, 0.39080459745646695, 0.3891625611359263, 0.39080459716284804, 0.3891625612337993, 0.39080459745646695, 0.3809523806098255, 0.3858784890820827, 0.39080459745646695, 0.39573070573297825, 0.39408866941243753, 0.3940886696081835, 0.3957307014755036, 0.39737273769817133, 0.39080459716284804, 0.39080459716284804, 0.38259441673462025, 0.384236452957288, 0.390804597358594, 0.3858784891799557, 0.38423645276154206, 0.3891625613316722, 0.3891625614295452, 0.3858784890820827, 0.3891625613316722, 0.38752052520687746, 0.38752052530475045, 0.38423645276154206, 0.38752052530475045, 0.39244663348338876, 0.3891625613316722, 0.3891625613316722, 0.390804597358594, 0.38752052520687746, 0.38752052520687746, 0.3891625613316722, 0.3891625613316722, 0.39573070573297825, 0.38752052520687746, 0.38752052520687746, 0.38752052520687746, 0.38587848898420973, 0.39244663348338876, 0.39573070563510526, 0.3990147780804407, 0.39573070583085124, 0.39573070583085124, 0.39080459745646695, 0.38752052530475045, 0.38423645315303395, 0.3825944170282392, 0.38259441237927266, 0.3793103401296832, 0.3940886695103105, 0.3875205207536569, 0.3973727420535189, 0.3924466335812617, 0.39080459706497506, 0.39080459716284804, 0.3924466336791347, 0.39408866941243753, 0.3924466333855158, 0.3924466333855158, 0.38752052491325856, 0.38752052481538557, 0.38752052481538557, 0.38752052481538557, 0.38752052481538557, 0.38752052501113154, 0.40394088655269794, 0.39080459716284804, 0.39080459706497506, 0.4022988504279032, 0.38423645315303395, 0.38752052501113154, 0.3842364525657961, 0.3924466335812617, 0.39080459745646695, 0.3858784890820827, 0.38752052481538557, 0.3809523806098255, 0.39408866941243753, 0.39080459726072103, 0.37274220018159776, 0.3908045976522129, 0.3858784888863368, 0.3891625611359263, 0.38752052520687746, 0.390804597358594, 0.3858784891799557, 0.377668308555982, 0.38423645276154206, 0.3858784888863368, 0.38259441673462025, 0.3858784888863368, 0.3825944166367473, 0.37931034428928484, 0.3809523805119525, 0.38752052510900453, 0.38752052491325856, 0.3858784888863368, 0.40394088596546, 0.40229885013428424, 0.3891625611359263, 0.4137931032035934, 0.3858784888863368, 0.3858784887884638, 0.38752052481538557, 0.3891625609401803, 0.3940886695103105, 0.38752052530475045, 0.3825944170282392, 0.39244663348338876, 0.39573070573297825, 0.39244663348338876, 0.3891625614295452, 0.3891625612337993, 0.3809523804140796, 0.3858784887884638, 0.3891625612337993, 0.3940886697060565, 0.38752052540262344, 0.3940886697060565, 0.39080459716284804, 0.4088669946334632, 0.40722495850866847, 0.3891625610380533, 0.4154351391326422, 0.39737274156415403, 0.39244663348338876, 0.4088669948292092, 0.3858784888863368, 0.38752052520687746, 0.3825944169303662, 0.3973727420535189, 0.39901477778682176, 0.397372741662027, 0.3858784888863368, 0.3858784888863368, 0.397372741857773, 0.39573070573297825, 0.38587848462886215, 0.390804597358594, 0.39573070573297825, 0.3825944166367473, 0.3858784890820827, 0.3858784890820827, 0.3940886696081835, 0.38587848898420973, 0.38752052510900453, 0.38752052510900453, 0.3858784892778287, 0.3940886696081835, 0.3891625612337993, 0.38259441683249323, 0.3858784892778287, 0.3891625612337993, 0.3891625612337993, 0.384236452957288, 0.403940886161206, 0.384236452859415, 0.38587848898420973, 0.38095238090344447, 0.37602627233331426, 0.3743842362085195, 0.3727422000837248, 0.3743842364042655, 0.37274220027947075, 0.3727421999858518, 0.38423645276154206, 0.377668308555982, 0.377668308458109, 0.38259441683249323, 0.3825944166367473, 0.37110016376318405, 0.3743842364042655, 0.37931034448503076, 0.38752052501113154, 0.38752052491325856, 0.37110016395893003, 0.3825944165388743, 0.377668308555982, 0.377668308555982, 0.37602627243118725, 0.3743842363063925, 0.3858784886905908, 0.377668308555982, 0.3661740555845458, 0.37931034468077673, 0.3858784886905908, 0.37602627243118725, 0.36781609170934054, 0.3694581278341353, 0.3891625611359263, 0.3891625611359263, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.3793103447786497, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.377668308360236, 0.37931034438715777, 0.371100164056803, 0.377668308360236, 0.371100164056803, 0.377668308360236, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.371100164056803, 0.3924466331897698, 0.371100164056803, 0.371100164056803, 0.3858784890820827, 0.377668308653855, 0.377668308653855, 0.38259441683249323, 0.3743842363063925, 0.3743842363063925, 0.37931034448503076, 0.3743842363063925, 0.3825944170282392, 0.3743842363063925, 0.3743842363063925, 0.3743842363063925, 0.3760262722354413, 0.3743842363063925, 0.37602627243118725, 0.37602627243118725, 0.39737274156415403, 0.377668308555982, 0.377668308555982, 0.37602627243118725, 0.39408866931456454, 0.3858784892778287, 0.3793103447786497, 0.38916255687845164, 0.3825944170282392, 0.38423645315303395, 0.39408866941243753, 0.39080459726072103, 0.3793103447786497, 0.38095238090344447], 'loss': [1.08361784418016, 1.0756163024314864, 1.075335977209667, 1.0746180942171164, 1.0743820951704617, 1.0743354136449355, 1.074276133876072, 1.0740921205563712, 1.0743594176960187, 1.0747388728590228, 1.0742300205896522, 1.074337750000141, 1.07411282385644, 1.0739024525550356, 1.0741096958242649, 1.0740049706347425, 1.0743923219567206, 1.0742456141438572, 1.0743348833471842, 1.0744503292459726, 1.0739881198264245, 1.0739364712145294, 1.0740689213515797, 1.073917055227918, 1.074291591918444, 1.0743117523389185, 1.0736871490243525, 1.0737407877215126, 1.0737295112570697, 1.0737018727179182, 1.0737177258889044, 1.0736075310736466, 1.0737484295999735, 1.0736732987163002, 1.0736196505215623, 1.07358343684453, 1.073456232748482, 1.073318410605131, 1.0736304025140877, 1.073594667142911, 1.073467371155349, 1.0734896448848184, 1.0733424071168998, 1.073548070263324, 1.0734535211410365, 1.073506396115438, 1.0735136234784763, 1.073397564349478, 1.0733128640196408, 1.072918856853822, 1.0733667234620519, 1.0733490886139918, 1.0732642182089709, 1.0731554472960485, 1.0732240934881097, 1.0736414351747265, 1.0742280369666568, 1.0735563390798393, 1.0734701778854432, 1.0738040176015615, 1.073708732319074, 1.0734950775972871, 1.0737614050048578, 1.0736418793088847, 1.0734006070992785, 1.073305602582818, 1.0732532828250705, 1.0731174925269533, 1.0732233639860056, 1.0736608097440652, 1.0737324914893085, 1.0736814802921772, 1.07343913226157, 1.0733194799638626, 1.0733999833434025, 1.0735214180035757, 1.0734710117140345, 1.0735616197331486, 1.0735951718363674, 1.0732560956257815, 1.073033211901937, 1.0729420547367856, 1.0727920441656877, 1.0731297667755972, 1.0728076420280723, 1.072997027451987, 1.0728004241626121, 1.0721924546318133, 1.0722172324417554, 1.0724924284329893, 1.0720183043998859, 1.0719938607186508, 1.072294910585611, 1.072109146676269, 1.0723528476221607, 1.0715007000635293, 1.0714230346483862, 1.0716576838640217, 1.071864953422938, 1.0714262661747864, 1.071739993839538, 1.0710284242395014, 1.0715465672941424, 1.07162379112087, 1.0715637443491566, 1.0721796692764, 1.071269025792821, 1.0720505405010874, 1.0711816632527345, 1.0713001271292903, 1.072241227778566, 1.0710644945226901, 1.0718019531003258, 1.0714197601381024, 1.0718045727183443, 1.0709418277720897, 1.070674113179624, 1.0709249187543897, 1.070832287163705, 1.0711688396132701, 1.0709284274240294, 1.0708268845595375, 1.070197104085887, 1.070282866528881, 1.0714200129988747, 1.0714093540728216, 1.0718402498801387, 1.0704966530418005, 1.0717629741594288, 1.0716814717228162, 1.0747421677842033, 1.0738677737649216, 1.0709467023794657, 1.0719515729489022, 1.0715533539989401, 1.071474166964114, 1.0712876223440777, 1.0715468592712276, 1.0712610709838555, 1.0708256677925219, 1.0711142767136592, 1.0713338832835642, 1.071297215044621, 1.0717015969435049, 1.0712784126309154, 1.0708379160941748, 1.07100284226866, 1.0707546636554006, 1.0715956230183157, 1.0710277204396055, 1.0709401091511, 1.0717588031561223, 1.0720862965319435, 1.0717686966459365, 1.0707048520904792, 1.071023120576596, 1.071344955452169, 1.0713467608242309, 1.0711469434370005, 1.0711903724337506, 1.071164380941058, 1.0709132582255212, 1.0706432628435765, 1.0715498425877314, 1.0720782499783337, 1.0709457825341508, 1.0711218695865765, 1.0706939158743167, 1.070220764902338, 1.0706483029242169, 1.0707443263741245, 1.0699979528019805, 1.0706305696244602, 1.070434516705037, 1.0709202478553725, 1.0712470505516631, 1.0707862211693484, 1.0718257183901339, 1.0706106243192293, 1.0721603884589257, 1.0709079717953347, 1.07189892807046, 1.070743536655418, 1.070577663317843, 1.070351433656054, 1.070010516530924, 1.0698664251539007, 1.071101414643274, 1.070319053230834, 1.0710332289858275, 1.0704849704335113, 1.0702640555967295, 1.0703962234990552, 1.0702813040794044, 1.0701988541370056, 1.0706941625176025, 1.07112550715891, 1.07089015928382, 1.0704441386816193, 1.070901052418186, 1.0706254714813075, 1.0704844362681896, 1.0705658805443765, 1.0703656498166814, 1.0707405276856627, 1.0707715235696436, 1.070199699078742, 1.0704704318448013, 1.0701071913482227, 1.0702419061680348, 1.07002574442838, 1.0699208635079542, 1.0695306264889068, 1.0708399176842378, 1.0698620449590976, 1.0695139574563968, 1.0695985392623368, 1.0703632945152768, 1.0704666080416105, 1.0701268418858427, 1.0690387733173565, 1.0690675501950713, 1.0688065194006573, 1.0690394306574513, 1.0709329367173526, 1.0695811611426196, 1.0694418235481153, 1.0703477379722517, 1.0699466719519677, 1.0698327391544162, 1.0690020289019637, 1.0685073087347607, 1.0689488303734782, 1.06896516816572, 1.068591200595519, 1.0682064035834717, 1.0689095901023191, 1.0683459655460146, 1.0690477981214896, 1.068439482810316, 1.0683378669515526, 1.0676721480837592, 1.0677853231802124, 1.0675587045093826, 1.0678268251477816, 1.0676739312784873, 1.0678352034312255, 1.0685673472817674, 1.0677326163716874, 1.0672706018483125, 1.067491537344774, 1.0684466072176517, 1.0682282141836272, 1.0668815252472488, 1.067431132455626, 1.0672033314342617, 1.0672238048341975, 1.0668466617928882, 1.0678818522537514, 1.0683248178670048, 1.0677449588168573, 1.0667564472868212, 1.0673528558664498, 1.0673750368231865, 1.0669717048472693, 1.0665298904971177, 1.0684259751494172, 1.0723457134235077, 1.0695614174895707, 1.0682610980783889, 1.0672467467721238, 1.0668163290258796, 1.0679181225735548, 1.06835933798882, 1.0674937602675671, 1.0672600780913961, 1.0669935803149027, 1.0664720031025474, 1.0664949155686083, 1.066041721602485, 1.066421582370813, 1.0660969947153047, 1.0663917839649522, 1.0661587689201935, 1.0660138823902827, 1.0659924210464196, 1.0668982501392248, 1.0668843566514628, 1.0676477266777713, 1.0658959901308378, 1.0658471351286714, 1.067853989444474, 1.0672428942314163, 1.0667298429555716, 1.0659640628454377, 1.0666606416937263, 1.0663135730265592, 1.0664060789946412, 1.0665594314892435, 1.068270088368128], 'acc': [0.3938398366836062, 0.3946611893372859, 0.3864476372084334, 0.38891170327178753, 0.3950718673470084, 0.39301848125164024, 0.3942505152808078, 0.39425051391002336, 0.3938398378585643, 0.39466118992476495, 0.39917864450683826, 0.394250513714197, 0.39425051391002336, 0.39425051351837065, 0.39958932094995003, 0.39383983805439066, 0.39425051250252147, 0.39425051156010715, 0.39876796590963676, 0.39548254633586266, 0.3958932227789744, 0.3942505146933287, 0.39425051195175986, 0.39753593642119267, 0.39753593563788725, 0.39055441397661056, 0.3954825471558855, 0.395893222583148, 0.4004106775935915, 0.3967145804017476, 0.3971252564164892, 0.401232034983821, 0.39589322496978163, 0.3983572902865478, 0.39999999993880425, 0.39794661008601806, 0.3987679668887685, 0.4028747422861612, 0.40082135482000864, 0.40123203377214545, 0.40082135736575114, 0.403285422486691, 0.40328541912092564, 0.3963039035669832, 0.3999999995471516, 0.3987679666929421, 0.402874743852772, 0.3999999999755217, 0.3954825448059693, 0.4036960987339764, 0.4008213532166804, 0.4053388109686928, 0.39835728989489516, 0.39794661266847803, 0.4012320338088629, 0.3987679653221577, 0.4024640662714196, 0.4008213530208541, 0.40000000095465343, 0.40164270907701655, 0.3975359328963184, 0.39876796712131224, 0.389322379947443, 0.39630390180454605, 0.39671457981426855, 0.39876796512633134, 0.40287474404859835, 0.4024640646680914, 0.39958932392406266, 0.3987679688837494, 0.39507186992946836, 0.3987679663012894, 0.40041067681028614, 0.39876796931211955, 0.39096509237798577, 0.394250514301676, 0.39712525606155397, 0.3958932261447397, 0.4016427118185854, 0.3967145809892267, 0.39917864689347193, 0.399178644151903, 0.3995893245482592, 0.39958932294493094, 0.3991786437602503, 0.3983572889157634, 0.3954825465684064, 0.3991786425485748, 0.4028747436936631, 0.4016427104110835, 0.39999999837219347, 0.4008213547832912, 0.3958932239906499, 0.3979466122401079, 0.40657084401872856, 0.40410677831030967, 0.40739219788408376, 0.40657084303959684, 0.40082135619079307, 0.40205339041578697, 0.40574948799928356, 0.4045174551450741, 0.403285418729273, 0.40574948467023564, 0.4036961004964135, 0.39917864336859765, 0.3979466132559571, 0.4012320320097083, 0.4049281292015522, 0.40000000212961156, 0.39753593266377457, 0.3991786457185138, 0.3954825475475382, 0.4045174531500932, 0.4016427082569937, 0.40369610030058717, 0.40369609853815003, 0.40287474150285585, 0.39999999895967253, 0.3963039000053915, 0.39958932294493094, 0.3950718673470084, 0.40246406368895965, 0.39589322238732166, 0.3999999981763671, 0.40041067896437593, 0.404106778506136, 0.4020533864625426, 0.40616016205576166, 0.40123203400468926, 0.39548254398594646, 0.39958932431571537, 0.4036960977181272, 0.40780287350717265, 0.4020533902199606, 0.40698152144097205, 0.40698152046184033, 0.4012320310305766, 0.40328542189921196, 0.4004106779852442, 0.4004106785727233, 0.4028747436936631, 0.403285418729273, 0.3995893247073681, 0.404106778506136, 0.4016427082569937, 0.4000000019337852, 0.4036960977181272, 0.40574948858676263, 0.40082135579914036, 0.3942505129308916, 0.4012320324380785, 0.3942505156724605, 0.4000000005630007, 0.3946611917239195, 0.40533880779875375, 0.402464063884786, 0.4057494864326728, 0.39794661008601806, 0.4016427127977171, 0.40451745475342143, 0.399178643564424, 0.40123203083475023, 0.40246406803385676, 0.3995893207908411, 0.39712525566990126, 0.40246406803385676, 0.40574948522099724, 0.40739219588910286, 0.40246406368895965, 0.3999999985680198, 0.4057494864326728, 0.4000000015421325, 0.3963039020003724, 0.3946611927030512, 0.3975359350504082, 0.4020533862667162, 0.38932238112240114, 0.39794661149351995, 0.40041067818107057, 0.40164271201441176, 0.4004106775935915, 0.39753593661701897, 0.4028747413070295, 0.40082135619079307, 0.4024640642764387, 0.4032854220950383, 0.40246406603887586, 0.4032854189250993, 0.4012320314222293, 0.4024640678380304, 0.40205338747839175, 0.402874743265293, 0.4012320334172102, 0.4036960983423237, 0.404106778506136, 0.39917864313605383, 0.40123203181388195, 0.40246406705472504, 0.40246406349313335, 0.39958932333658365, 0.4073921976882574, 0.4004106779852442, 0.402464066467246, 0.4045174559283795, 0.4016427124060645, 0.40246406803385676, 0.40287474209033486, 0.39507187032112107, 0.3958932255572607, 0.3971252574323384, 0.40041067896437593, 0.4008213565824458, 0.4016427114269327, 0.40328541951257835, 0.4049281323714912, 0.40246406744637775, 0.39917864630599287, 0.4065708424521178, 0.40492813217566487, 0.3991786427811186, 0.40574948482934453, 0.40492812939737854, 0.40000000232543786, 0.4008213569740985, 0.40328542107918913, 0.4028747456519266, 0.4036961004964135, 0.4020533868541952, 0.3975359338754501, 0.4049281306090541, 0.40739219510579744, 0.40739219886321554, 0.4032854206875364, 0.4065708436270759, 0.4041067771353516, 0.40328542287834374, 0.40369609634734277, 0.40328542131173295, 0.40246406349313335, 0.4016427118185854, 0.4012320310305766, 0.4078028725280409, 0.40451745612420587, 0.39794661008601806, 0.4041067741612389, 0.4036961001047608, 0.40205339041578697, 0.4036960971306482, 0.40369609752230085, 0.40410677811448337, 0.4041067755320234, 0.4036960983423237, 0.4041067739654126, 0.4012320331846664, 0.4041067765478726, 0.403285419316752, 0.4032854201367748, 0.404106777918657, 0.40369609634734277, 0.40410677533619704, 0.4036960967389955, 0.4045174549492478, 0.4041067755320234, 0.40369609736319195, 0.4045174531500932, 0.40410677478543544, 0.3983572902865478, 0.4016427127977171, 0.4041067767069815, 0.404517453382637, 0.40698152046184033, 0.4065708428437705, 0.4065708432354232, 0.40780287628545897, 0.4032854232699964, 0.40328542229086467, 0.40082135325339785, 0.39876796790461766, 0.4016427090402991, 0.4028747446727948, 0.40410677494454433, 0.40082135619079307, 0.40328542092008024, 0.4041067739654126, 0.40492812939737854, 0.4028747440853158, 0.4082135546868342, 0.4073921949099711, 0.4078028725280409, 0.4106776164420087, 0.40657084104461594, 0.40862423210907767, 0.40862423073829324, 0.4057494883909363, 0.4061601644056778, 0.4094455833919729, 0.40739219549745015, 0.40985626120586904, 0.40451745573255316]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
