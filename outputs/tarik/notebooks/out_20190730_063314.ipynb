{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf7.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 06:33:14 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '2', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ds', 'ck', 'sg', 'mb', 'yd', 'eo', 'sk', 'ce', 'by', 'ek', 'ib', 'aa', 'eb', 'eg', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001704096D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001703B156EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7155, Accuracy:0.0780, Validation Loss:2.7067, Validation Accuracy:0.0690\n",
    "Epoch #2: Loss:2.7030, Accuracy:0.0768, Validation Loss:2.6969, Validation Accuracy:0.0837\n",
    "Epoch #3: Loss:2.6947, Accuracy:0.0920, Validation Loss:2.6908, Validation Accuracy:0.0903\n",
    "Epoch #4: Loss:2.6899, Accuracy:0.0817, Validation Loss:2.6857, Validation Accuracy:0.0821\n",
    "Epoch #5: Loss:2.6847, Accuracy:0.0813, Validation Loss:2.6811, Validation Accuracy:0.0821\n",
    "Epoch #6: Loss:2.6802, Accuracy:0.0817, Validation Loss:2.6761, Validation Accuracy:0.0854\n",
    "Epoch #7: Loss:2.6758, Accuracy:0.1055, Validation Loss:2.6710, Validation Accuracy:0.1314\n",
    "Epoch #8: Loss:2.6705, Accuracy:0.1158, Validation Loss:2.6656, Validation Accuracy:0.1100\n",
    "Epoch #9: Loss:2.6651, Accuracy:0.1072, Validation Loss:2.6591, Validation Accuracy:0.1067\n",
    "Epoch #10: Loss:2.6587, Accuracy:0.1088, Validation Loss:2.6514, Validation Accuracy:0.1084\n",
    "Epoch #11: Loss:2.6508, Accuracy:0.1179, Validation Loss:2.6422, Validation Accuracy:0.1445\n",
    "Epoch #12: Loss:2.6415, Accuracy:0.1384, Validation Loss:2.6305, Validation Accuracy:0.1642\n",
    "Epoch #13: Loss:2.6295, Accuracy:0.1515, Validation Loss:2.6169, Validation Accuracy:0.1527\n",
    "Epoch #14: Loss:2.6157, Accuracy:0.1569, Validation Loss:2.6012, Validation Accuracy:0.1544\n",
    "Epoch #15: Loss:2.6000, Accuracy:0.1634, Validation Loss:2.5841, Validation Accuracy:0.1576\n",
    "Epoch #16: Loss:2.5839, Accuracy:0.1671, Validation Loss:2.5673, Validation Accuracy:0.1658\n",
    "Epoch #17: Loss:2.5697, Accuracy:0.1598, Validation Loss:2.5572, Validation Accuracy:0.1544\n",
    "Epoch #18: Loss:2.5565, Accuracy:0.1515, Validation Loss:2.5409, Validation Accuracy:0.1494\n",
    "Epoch #19: Loss:2.5446, Accuracy:0.1524, Validation Loss:2.5320, Validation Accuracy:0.1511\n",
    "Epoch #20: Loss:2.5333, Accuracy:0.1507, Validation Loss:2.5207, Validation Accuracy:0.1560\n",
    "Epoch #21: Loss:2.5278, Accuracy:0.1565, Validation Loss:2.5171, Validation Accuracy:0.1511\n",
    "Epoch #22: Loss:2.5212, Accuracy:0.1593, Validation Loss:2.5007, Validation Accuracy:0.1741\n",
    "Epoch #23: Loss:2.5104, Accuracy:0.1647, Validation Loss:2.4956, Validation Accuracy:0.1658\n",
    "Epoch #24: Loss:2.5048, Accuracy:0.1647, Validation Loss:2.4874, Validation Accuracy:0.1741\n",
    "Epoch #25: Loss:2.4992, Accuracy:0.1659, Validation Loss:2.4829, Validation Accuracy:0.1576\n",
    "Epoch #26: Loss:2.4942, Accuracy:0.1680, Validation Loss:2.4764, Validation Accuracy:0.1642\n",
    "Epoch #27: Loss:2.4914, Accuracy:0.1667, Validation Loss:2.4717, Validation Accuracy:0.1773\n",
    "Epoch #28: Loss:2.4871, Accuracy:0.1647, Validation Loss:2.4708, Validation Accuracy:0.1741\n",
    "Epoch #29: Loss:2.4844, Accuracy:0.1713, Validation Loss:2.4658, Validation Accuracy:0.1757\n",
    "Epoch #30: Loss:2.4827, Accuracy:0.1680, Validation Loss:2.4641, Validation Accuracy:0.1675\n",
    "Epoch #31: Loss:2.4775, Accuracy:0.1749, Validation Loss:2.4606, Validation Accuracy:0.1757\n",
    "Epoch #32: Loss:2.4755, Accuracy:0.1713, Validation Loss:2.4586, Validation Accuracy:0.1741\n",
    "Epoch #33: Loss:2.4725, Accuracy:0.1758, Validation Loss:2.4567, Validation Accuracy:0.1757\n",
    "Epoch #34: Loss:2.4733, Accuracy:0.1717, Validation Loss:2.4550, Validation Accuracy:0.1741\n",
    "Epoch #35: Loss:2.4690, Accuracy:0.1741, Validation Loss:2.4521, Validation Accuracy:0.1806\n",
    "Epoch #36: Loss:2.4671, Accuracy:0.1782, Validation Loss:2.4502, Validation Accuracy:0.1773\n",
    "Epoch #37: Loss:2.4672, Accuracy:0.1700, Validation Loss:2.4516, Validation Accuracy:0.1839\n",
    "Epoch #38: Loss:2.4691, Accuracy:0.1766, Validation Loss:2.4494, Validation Accuracy:0.1872\n",
    "Epoch #39: Loss:2.4687, Accuracy:0.1708, Validation Loss:2.4495, Validation Accuracy:0.1757\n",
    "Epoch #40: Loss:2.4711, Accuracy:0.1733, Validation Loss:2.4469, Validation Accuracy:0.1757\n",
    "Epoch #41: Loss:2.4658, Accuracy:0.1717, Validation Loss:2.4463, Validation Accuracy:0.1872\n",
    "Epoch #42: Loss:2.4594, Accuracy:0.1754, Validation Loss:2.4478, Validation Accuracy:0.1856\n",
    "Epoch #43: Loss:2.4586, Accuracy:0.1774, Validation Loss:2.4463, Validation Accuracy:0.1823\n",
    "Epoch #44: Loss:2.4584, Accuracy:0.1762, Validation Loss:2.4449, Validation Accuracy:0.1823\n",
    "Epoch #45: Loss:2.4564, Accuracy:0.1778, Validation Loss:2.4451, Validation Accuracy:0.1905\n",
    "Epoch #46: Loss:2.4548, Accuracy:0.1762, Validation Loss:2.4453, Validation Accuracy:0.1823\n",
    "Epoch #47: Loss:2.4540, Accuracy:0.1758, Validation Loss:2.4445, Validation Accuracy:0.1905\n",
    "Epoch #48: Loss:2.4529, Accuracy:0.1770, Validation Loss:2.4426, Validation Accuracy:0.1856\n",
    "Epoch #49: Loss:2.4519, Accuracy:0.1749, Validation Loss:2.4420, Validation Accuracy:0.1921\n",
    "Epoch #50: Loss:2.4508, Accuracy:0.1770, Validation Loss:2.4426, Validation Accuracy:0.1872\n",
    "Epoch #51: Loss:2.4500, Accuracy:0.1770, Validation Loss:2.4424, Validation Accuracy:0.1938\n",
    "Epoch #52: Loss:2.4500, Accuracy:0.1754, Validation Loss:2.4416, Validation Accuracy:0.1905\n",
    "Epoch #53: Loss:2.4485, Accuracy:0.1754, Validation Loss:2.4405, Validation Accuracy:0.1987\n",
    "Epoch #54: Loss:2.4473, Accuracy:0.1778, Validation Loss:2.4408, Validation Accuracy:0.1856\n",
    "Epoch #55: Loss:2.4472, Accuracy:0.1782, Validation Loss:2.4415, Validation Accuracy:0.2003\n",
    "Epoch #56: Loss:2.4497, Accuracy:0.1733, Validation Loss:2.4462, Validation Accuracy:0.1905\n",
    "Epoch #57: Loss:2.4480, Accuracy:0.1754, Validation Loss:2.4443, Validation Accuracy:0.1938\n",
    "Epoch #58: Loss:2.4474, Accuracy:0.1749, Validation Loss:2.4436, Validation Accuracy:0.1872\n",
    "Epoch #59: Loss:2.4439, Accuracy:0.1758, Validation Loss:2.4415, Validation Accuracy:0.1905\n",
    "Epoch #60: Loss:2.4440, Accuracy:0.1749, Validation Loss:2.4402, Validation Accuracy:0.1938\n",
    "Epoch #61: Loss:2.4451, Accuracy:0.1828, Validation Loss:2.4395, Validation Accuracy:0.1987\n",
    "Epoch #62: Loss:2.4424, Accuracy:0.1762, Validation Loss:2.4400, Validation Accuracy:0.1921\n",
    "Epoch #63: Loss:2.4401, Accuracy:0.1778, Validation Loss:2.4402, Validation Accuracy:0.1905\n",
    "Epoch #64: Loss:2.4401, Accuracy:0.1758, Validation Loss:2.4411, Validation Accuracy:0.1921\n",
    "Epoch #65: Loss:2.4388, Accuracy:0.1786, Validation Loss:2.4393, Validation Accuracy:0.2003\n",
    "Epoch #66: Loss:2.4405, Accuracy:0.1770, Validation Loss:2.4483, Validation Accuracy:0.1921\n",
    "Epoch #67: Loss:2.4417, Accuracy:0.1778, Validation Loss:2.4434, Validation Accuracy:0.1938\n",
    "Epoch #68: Loss:2.4415, Accuracy:0.1799, Validation Loss:2.4425, Validation Accuracy:0.1888\n",
    "Epoch #69: Loss:2.4443, Accuracy:0.1733, Validation Loss:2.4402, Validation Accuracy:0.1921\n",
    "Epoch #70: Loss:2.4367, Accuracy:0.1762, Validation Loss:2.4409, Validation Accuracy:0.1938\n",
    "Epoch #71: Loss:2.4363, Accuracy:0.1786, Validation Loss:2.4394, Validation Accuracy:0.1905\n",
    "Epoch #72: Loss:2.4355, Accuracy:0.1795, Validation Loss:2.4400, Validation Accuracy:0.1938\n",
    "Epoch #73: Loss:2.4348, Accuracy:0.1799, Validation Loss:2.4391, Validation Accuracy:0.1954\n",
    "Epoch #74: Loss:2.4335, Accuracy:0.1774, Validation Loss:2.4405, Validation Accuracy:0.2003\n",
    "Epoch #75: Loss:2.4341, Accuracy:0.1749, Validation Loss:2.4391, Validation Accuracy:0.2020\n",
    "Epoch #76: Loss:2.4322, Accuracy:0.1774, Validation Loss:2.4396, Validation Accuracy:0.2020\n",
    "Epoch #77: Loss:2.4315, Accuracy:0.1782, Validation Loss:2.4443, Validation Accuracy:0.1970\n",
    "Epoch #78: Loss:2.4317, Accuracy:0.1791, Validation Loss:2.4434, Validation Accuracy:0.1856\n",
    "Epoch #79: Loss:2.4342, Accuracy:0.1786, Validation Loss:2.4457, Validation Accuracy:0.1839\n",
    "Epoch #80: Loss:2.4345, Accuracy:0.1774, Validation Loss:2.4408, Validation Accuracy:0.1938\n",
    "Epoch #81: Loss:2.4299, Accuracy:0.1807, Validation Loss:2.4433, Validation Accuracy:0.2069\n",
    "Epoch #82: Loss:2.4289, Accuracy:0.1815, Validation Loss:2.4403, Validation Accuracy:0.1987\n",
    "Epoch #83: Loss:2.4275, Accuracy:0.1782, Validation Loss:2.4410, Validation Accuracy:0.1987\n",
    "Epoch #84: Loss:2.4265, Accuracy:0.1762, Validation Loss:2.4412, Validation Accuracy:0.1987\n",
    "Epoch #85: Loss:2.4259, Accuracy:0.1786, Validation Loss:2.4431, Validation Accuracy:0.2003\n",
    "Epoch #86: Loss:2.4249, Accuracy:0.1832, Validation Loss:2.4461, Validation Accuracy:0.1921\n",
    "Epoch #87: Loss:2.4287, Accuracy:0.1791, Validation Loss:2.4491, Validation Accuracy:0.1790\n",
    "Epoch #88: Loss:2.4328, Accuracy:0.1795, Validation Loss:2.4486, Validation Accuracy:0.1888\n",
    "Epoch #89: Loss:2.4309, Accuracy:0.1786, Validation Loss:2.4452, Validation Accuracy:0.1970\n",
    "Epoch #90: Loss:2.4276, Accuracy:0.1823, Validation Loss:2.4434, Validation Accuracy:0.1954\n",
    "Epoch #91: Loss:2.4250, Accuracy:0.1807, Validation Loss:2.4424, Validation Accuracy:0.1970\n",
    "Epoch #92: Loss:2.4262, Accuracy:0.1840, Validation Loss:2.4423, Validation Accuracy:0.2003\n",
    "Epoch #93: Loss:2.4224, Accuracy:0.1811, Validation Loss:2.4427, Validation Accuracy:0.2003\n",
    "Epoch #94: Loss:2.4223, Accuracy:0.1860, Validation Loss:2.4424, Validation Accuracy:0.1970\n",
    "Epoch #95: Loss:2.4230, Accuracy:0.1836, Validation Loss:2.4438, Validation Accuracy:0.1987\n",
    "Epoch #96: Loss:2.4215, Accuracy:0.1836, Validation Loss:2.4442, Validation Accuracy:0.1921\n",
    "Epoch #97: Loss:2.4226, Accuracy:0.1873, Validation Loss:2.4447, Validation Accuracy:0.1987\n",
    "Epoch #98: Loss:2.4212, Accuracy:0.1844, Validation Loss:2.4443, Validation Accuracy:0.1938\n",
    "Epoch #99: Loss:2.4198, Accuracy:0.1881, Validation Loss:2.4441, Validation Accuracy:0.2003\n",
    "Epoch #100: Loss:2.4193, Accuracy:0.1860, Validation Loss:2.4461, Validation Accuracy:0.1987\n",
    "Epoch #101: Loss:2.4211, Accuracy:0.1877, Validation Loss:2.4475, Validation Accuracy:0.1888\n",
    "Epoch #102: Loss:2.4202, Accuracy:0.1823, Validation Loss:2.4469, Validation Accuracy:0.1987\n",
    "Epoch #103: Loss:2.4178, Accuracy:0.1852, Validation Loss:2.4454, Validation Accuracy:0.1970\n",
    "Epoch #104: Loss:2.4188, Accuracy:0.1873, Validation Loss:2.4456, Validation Accuracy:0.1872\n",
    "Epoch #105: Loss:2.4172, Accuracy:0.1889, Validation Loss:2.4461, Validation Accuracy:0.1921\n",
    "Epoch #106: Loss:2.4157, Accuracy:0.1897, Validation Loss:2.4468, Validation Accuracy:0.1921\n",
    "Epoch #107: Loss:2.4196, Accuracy:0.1852, Validation Loss:2.4574, Validation Accuracy:0.1757\n",
    "Epoch #108: Loss:2.4193, Accuracy:0.1930, Validation Loss:2.4503, Validation Accuracy:0.1872\n",
    "Epoch #109: Loss:2.4159, Accuracy:0.1860, Validation Loss:2.4500, Validation Accuracy:0.1724\n",
    "Epoch #110: Loss:2.4179, Accuracy:0.1864, Validation Loss:2.4449, Validation Accuracy:0.1921\n",
    "Epoch #111: Loss:2.4166, Accuracy:0.1877, Validation Loss:2.4463, Validation Accuracy:0.1856\n",
    "Epoch #112: Loss:2.4153, Accuracy:0.1914, Validation Loss:2.4459, Validation Accuracy:0.1872\n",
    "Epoch #113: Loss:2.4123, Accuracy:0.1877, Validation Loss:2.4462, Validation Accuracy:0.1987\n",
    "Epoch #114: Loss:2.4123, Accuracy:0.1897, Validation Loss:2.4455, Validation Accuracy:0.1938\n",
    "Epoch #115: Loss:2.4109, Accuracy:0.1922, Validation Loss:2.4456, Validation Accuracy:0.1905\n",
    "Epoch #116: Loss:2.4130, Accuracy:0.1864, Validation Loss:2.4471, Validation Accuracy:0.1839\n",
    "Epoch #117: Loss:2.4119, Accuracy:0.1955, Validation Loss:2.4495, Validation Accuracy:0.1888\n",
    "Epoch #118: Loss:2.4148, Accuracy:0.1930, Validation Loss:2.4486, Validation Accuracy:0.1839\n",
    "Epoch #119: Loss:2.4126, Accuracy:0.1943, Validation Loss:2.4487, Validation Accuracy:0.1790\n",
    "Epoch #120: Loss:2.4125, Accuracy:0.1918, Validation Loss:2.4455, Validation Accuracy:0.2003\n",
    "Epoch #121: Loss:2.4111, Accuracy:0.1860, Validation Loss:2.4479, Validation Accuracy:0.1921\n",
    "Epoch #122: Loss:2.4119, Accuracy:0.1873, Validation Loss:2.4571, Validation Accuracy:0.1757\n",
    "Epoch #123: Loss:2.4137, Accuracy:0.1910, Validation Loss:2.4481, Validation Accuracy:0.1954\n",
    "Epoch #124: Loss:2.4085, Accuracy:0.1926, Validation Loss:2.4471, Validation Accuracy:0.1806\n",
    "Epoch #125: Loss:2.4057, Accuracy:0.1943, Validation Loss:2.4478, Validation Accuracy:0.1856\n",
    "Epoch #126: Loss:2.4048, Accuracy:0.1943, Validation Loss:2.4473, Validation Accuracy:0.1970\n",
    "Epoch #127: Loss:2.4048, Accuracy:0.1910, Validation Loss:2.4462, Validation Accuracy:0.1888\n",
    "Epoch #128: Loss:2.4041, Accuracy:0.1963, Validation Loss:2.4494, Validation Accuracy:0.1823\n",
    "Epoch #129: Loss:2.4064, Accuracy:0.1943, Validation Loss:2.4500, Validation Accuracy:0.1839\n",
    "Epoch #130: Loss:2.4085, Accuracy:0.1926, Validation Loss:2.4469, Validation Accuracy:0.1806\n",
    "Epoch #131: Loss:2.4031, Accuracy:0.1959, Validation Loss:2.4568, Validation Accuracy:0.1757\n",
    "Epoch #132: Loss:2.4098, Accuracy:0.1893, Validation Loss:2.4550, Validation Accuracy:0.1757\n",
    "Epoch #133: Loss:2.4087, Accuracy:0.1918, Validation Loss:2.4515, Validation Accuracy:0.1806\n",
    "Epoch #134: Loss:2.4036, Accuracy:0.1901, Validation Loss:2.4505, Validation Accuracy:0.1806\n",
    "Epoch #135: Loss:2.4083, Accuracy:0.1934, Validation Loss:2.4523, Validation Accuracy:0.1773\n",
    "Epoch #136: Loss:2.4067, Accuracy:0.1943, Validation Loss:2.4537, Validation Accuracy:0.1905\n",
    "Epoch #137: Loss:2.4060, Accuracy:0.1918, Validation Loss:2.4468, Validation Accuracy:0.1839\n",
    "Epoch #138: Loss:2.3991, Accuracy:0.1988, Validation Loss:2.4488, Validation Accuracy:0.1757\n",
    "Epoch #139: Loss:2.3992, Accuracy:0.1959, Validation Loss:2.4463, Validation Accuracy:0.1970\n",
    "Epoch #140: Loss:2.3988, Accuracy:0.1955, Validation Loss:2.4529, Validation Accuracy:0.1790\n",
    "Epoch #141: Loss:2.3988, Accuracy:0.2012, Validation Loss:2.4613, Validation Accuracy:0.1675\n",
    "Epoch #142: Loss:2.4042, Accuracy:0.1926, Validation Loss:2.4479, Validation Accuracy:0.1872\n",
    "Epoch #143: Loss:2.3973, Accuracy:0.1988, Validation Loss:2.4485, Validation Accuracy:0.1839\n",
    "Epoch #144: Loss:2.3981, Accuracy:0.1938, Validation Loss:2.4494, Validation Accuracy:0.1823\n",
    "Epoch #145: Loss:2.3974, Accuracy:0.1934, Validation Loss:2.4529, Validation Accuracy:0.1839\n",
    "Epoch #146: Loss:2.3946, Accuracy:0.1971, Validation Loss:2.4504, Validation Accuracy:0.1806\n",
    "Epoch #147: Loss:2.3936, Accuracy:0.1996, Validation Loss:2.4457, Validation Accuracy:0.1856\n",
    "Epoch #148: Loss:2.3960, Accuracy:0.1992, Validation Loss:2.4566, Validation Accuracy:0.1741\n",
    "Epoch #149: Loss:2.4039, Accuracy:0.1910, Validation Loss:2.4508, Validation Accuracy:0.1806\n",
    "Epoch #150: Loss:2.3978, Accuracy:0.1943, Validation Loss:2.4513, Validation Accuracy:0.1839\n",
    "Epoch #151: Loss:2.3949, Accuracy:0.1963, Validation Loss:2.4576, Validation Accuracy:0.1658\n",
    "Epoch #152: Loss:2.3950, Accuracy:0.2000, Validation Loss:2.4573, Validation Accuracy:0.1790\n",
    "Epoch #153: Loss:2.3937, Accuracy:0.1996, Validation Loss:2.4504, Validation Accuracy:0.1921\n",
    "Epoch #154: Loss:2.3906, Accuracy:0.1984, Validation Loss:2.4522, Validation Accuracy:0.1691\n",
    "Epoch #155: Loss:2.3911, Accuracy:0.1992, Validation Loss:2.4483, Validation Accuracy:0.1905\n",
    "Epoch #156: Loss:2.3878, Accuracy:0.2021, Validation Loss:2.4497, Validation Accuracy:0.1888\n",
    "Epoch #157: Loss:2.3865, Accuracy:0.2021, Validation Loss:2.4501, Validation Accuracy:0.1773\n",
    "Epoch #158: Loss:2.3854, Accuracy:0.2057, Validation Loss:2.4478, Validation Accuracy:0.1823\n",
    "Epoch #159: Loss:2.3859, Accuracy:0.1988, Validation Loss:2.4507, Validation Accuracy:0.1790\n",
    "Epoch #160: Loss:2.3883, Accuracy:0.1992, Validation Loss:2.4499, Validation Accuracy:0.1823\n",
    "Epoch #161: Loss:2.3898, Accuracy:0.2004, Validation Loss:2.4501, Validation Accuracy:0.1790\n",
    "Epoch #162: Loss:2.3871, Accuracy:0.1984, Validation Loss:2.4517, Validation Accuracy:0.1757\n",
    "Epoch #163: Loss:2.3877, Accuracy:0.1943, Validation Loss:2.4549, Validation Accuracy:0.1741\n",
    "Epoch #164: Loss:2.3828, Accuracy:0.2025, Validation Loss:2.4497, Validation Accuracy:0.1872\n",
    "Epoch #165: Loss:2.3804, Accuracy:0.2004, Validation Loss:2.4516, Validation Accuracy:0.1823\n",
    "Epoch #166: Loss:2.3795, Accuracy:0.1984, Validation Loss:2.4509, Validation Accuracy:0.1823\n",
    "Epoch #167: Loss:2.3799, Accuracy:0.2008, Validation Loss:2.4501, Validation Accuracy:0.1823\n",
    "Epoch #168: Loss:2.3776, Accuracy:0.2074, Validation Loss:2.4511, Validation Accuracy:0.1839\n",
    "Epoch #169: Loss:2.3792, Accuracy:0.2057, Validation Loss:2.4480, Validation Accuracy:0.1823\n",
    "Epoch #170: Loss:2.3746, Accuracy:0.2029, Validation Loss:2.4510, Validation Accuracy:0.1741\n",
    "Epoch #171: Loss:2.3737, Accuracy:0.2049, Validation Loss:2.4560, Validation Accuracy:0.1790\n",
    "Epoch #172: Loss:2.3810, Accuracy:0.2012, Validation Loss:2.4542, Validation Accuracy:0.1708\n",
    "Epoch #173: Loss:2.3939, Accuracy:0.1959, Validation Loss:2.4555, Validation Accuracy:0.1741\n",
    "Epoch #174: Loss:2.3861, Accuracy:0.2090, Validation Loss:2.4579, Validation Accuracy:0.1823\n",
    "Epoch #175: Loss:2.3844, Accuracy:0.2070, Validation Loss:2.4545, Validation Accuracy:0.1773\n",
    "Epoch #176: Loss:2.3840, Accuracy:0.1988, Validation Loss:2.4753, Validation Accuracy:0.1626\n",
    "Epoch #177: Loss:2.3959, Accuracy:0.1918, Validation Loss:2.4623, Validation Accuracy:0.1642\n",
    "Epoch #178: Loss:2.3874, Accuracy:0.1975, Validation Loss:2.4523, Validation Accuracy:0.1724\n",
    "Epoch #179: Loss:2.3821, Accuracy:0.2025, Validation Loss:2.4579, Validation Accuracy:0.1839\n",
    "Epoch #180: Loss:2.3848, Accuracy:0.2074, Validation Loss:2.4591, Validation Accuracy:0.1823\n",
    "Epoch #181: Loss:2.3800, Accuracy:0.2082, Validation Loss:2.4519, Validation Accuracy:0.1790\n",
    "Epoch #182: Loss:2.3727, Accuracy:0.2041, Validation Loss:2.4534, Validation Accuracy:0.1806\n",
    "Epoch #183: Loss:2.3682, Accuracy:0.2037, Validation Loss:2.4517, Validation Accuracy:0.1806\n",
    "Epoch #184: Loss:2.3711, Accuracy:0.2082, Validation Loss:2.4561, Validation Accuracy:0.1757\n",
    "Epoch #185: Loss:2.3709, Accuracy:0.2045, Validation Loss:2.4609, Validation Accuracy:0.1806\n",
    "Epoch #186: Loss:2.3735, Accuracy:0.2078, Validation Loss:2.4554, Validation Accuracy:0.1757\n",
    "Epoch #187: Loss:2.3735, Accuracy:0.2066, Validation Loss:2.4583, Validation Accuracy:0.1675\n",
    "Epoch #188: Loss:2.3677, Accuracy:0.2078, Validation Loss:2.4583, Validation Accuracy:0.1773\n",
    "Epoch #189: Loss:2.3646, Accuracy:0.2119, Validation Loss:2.4547, Validation Accuracy:0.1806\n",
    "Epoch #190: Loss:2.3615, Accuracy:0.2103, Validation Loss:2.4558, Validation Accuracy:0.1741\n",
    "Epoch #191: Loss:2.3609, Accuracy:0.2086, Validation Loss:2.4574, Validation Accuracy:0.1839\n",
    "Epoch #192: Loss:2.3609, Accuracy:0.2152, Validation Loss:2.4561, Validation Accuracy:0.1790\n",
    "Epoch #193: Loss:2.3578, Accuracy:0.2140, Validation Loss:2.4576, Validation Accuracy:0.1806\n",
    "Epoch #194: Loss:2.3572, Accuracy:0.2082, Validation Loss:2.4653, Validation Accuracy:0.1708\n",
    "Epoch #195: Loss:2.3611, Accuracy:0.2168, Validation Loss:2.4592, Validation Accuracy:0.1773\n",
    "Epoch #196: Loss:2.3588, Accuracy:0.2115, Validation Loss:2.4605, Validation Accuracy:0.1856\n",
    "Epoch #197: Loss:2.3587, Accuracy:0.2168, Validation Loss:2.4593, Validation Accuracy:0.1724\n",
    "Epoch #198: Loss:2.3546, Accuracy:0.2189, Validation Loss:2.4611, Validation Accuracy:0.1839\n",
    "Epoch #199: Loss:2.3528, Accuracy:0.2152, Validation Loss:2.4613, Validation Accuracy:0.1790\n",
    "Epoch #200: Loss:2.3518, Accuracy:0.2193, Validation Loss:2.4607, Validation Accuracy:0.1790\n",
    "Epoch #201: Loss:2.3527, Accuracy:0.2168, Validation Loss:2.4749, Validation Accuracy:0.1658\n",
    "Epoch #202: Loss:2.3599, Accuracy:0.2140, Validation Loss:2.4638, Validation Accuracy:0.1675\n",
    "Epoch #203: Loss:2.3593, Accuracy:0.2144, Validation Loss:2.4715, Validation Accuracy:0.1658\n",
    "Epoch #204: Loss:2.3582, Accuracy:0.2172, Validation Loss:2.4658, Validation Accuracy:0.1839\n",
    "Epoch #205: Loss:2.3504, Accuracy:0.2209, Validation Loss:2.4699, Validation Accuracy:0.1790\n",
    "Epoch #206: Loss:2.3473, Accuracy:0.2201, Validation Loss:2.4645, Validation Accuracy:0.1856\n",
    "Epoch #207: Loss:2.3476, Accuracy:0.2189, Validation Loss:2.4673, Validation Accuracy:0.1757\n",
    "Epoch #208: Loss:2.3431, Accuracy:0.2189, Validation Loss:2.4728, Validation Accuracy:0.1741\n",
    "Epoch #209: Loss:2.3508, Accuracy:0.2218, Validation Loss:2.4672, Validation Accuracy:0.1856\n",
    "Epoch #210: Loss:2.3443, Accuracy:0.2214, Validation Loss:2.4705, Validation Accuracy:0.1790\n",
    "Epoch #211: Loss:2.3432, Accuracy:0.2193, Validation Loss:2.4678, Validation Accuracy:0.1823\n",
    "Epoch #212: Loss:2.3390, Accuracy:0.2222, Validation Loss:2.4697, Validation Accuracy:0.1856\n",
    "Epoch #213: Loss:2.3398, Accuracy:0.2255, Validation Loss:2.4713, Validation Accuracy:0.1856\n",
    "Epoch #214: Loss:2.3389, Accuracy:0.2271, Validation Loss:2.4718, Validation Accuracy:0.1839\n",
    "Epoch #215: Loss:2.3382, Accuracy:0.2246, Validation Loss:2.4721, Validation Accuracy:0.1806\n",
    "Epoch #216: Loss:2.3373, Accuracy:0.2263, Validation Loss:2.4764, Validation Accuracy:0.1856\n",
    "Epoch #217: Loss:2.3414, Accuracy:0.2156, Validation Loss:2.4765, Validation Accuracy:0.1708\n",
    "Epoch #218: Loss:2.3420, Accuracy:0.2251, Validation Loss:2.4753, Validation Accuracy:0.1839\n",
    "Epoch #219: Loss:2.3367, Accuracy:0.2201, Validation Loss:2.4754, Validation Accuracy:0.1872\n",
    "Epoch #220: Loss:2.3325, Accuracy:0.2283, Validation Loss:2.4774, Validation Accuracy:0.1773\n",
    "Epoch #221: Loss:2.3310, Accuracy:0.2279, Validation Loss:2.4798, Validation Accuracy:0.1757\n",
    "Epoch #222: Loss:2.3354, Accuracy:0.2279, Validation Loss:2.4857, Validation Accuracy:0.1757\n",
    "Epoch #223: Loss:2.3333, Accuracy:0.2255, Validation Loss:2.4810, Validation Accuracy:0.1856\n",
    "Epoch #224: Loss:2.3335, Accuracy:0.2251, Validation Loss:2.4824, Validation Accuracy:0.1872\n",
    "Epoch #225: Loss:2.3312, Accuracy:0.2292, Validation Loss:2.4828, Validation Accuracy:0.1790\n",
    "Epoch #226: Loss:2.3285, Accuracy:0.2259, Validation Loss:2.4925, Validation Accuracy:0.1823\n",
    "Epoch #227: Loss:2.3351, Accuracy:0.2259, Validation Loss:2.4862, Validation Accuracy:0.1872\n",
    "Epoch #228: Loss:2.3317, Accuracy:0.2259, Validation Loss:2.4894, Validation Accuracy:0.1741\n",
    "Epoch #229: Loss:2.3278, Accuracy:0.2214, Validation Loss:2.4833, Validation Accuracy:0.1905\n",
    "Epoch #230: Loss:2.3216, Accuracy:0.2304, Validation Loss:2.4850, Validation Accuracy:0.1872\n",
    "Epoch #231: Loss:2.3247, Accuracy:0.2267, Validation Loss:2.4852, Validation Accuracy:0.1954\n",
    "Epoch #232: Loss:2.3223, Accuracy:0.2296, Validation Loss:2.4939, Validation Accuracy:0.1757\n",
    "Epoch #233: Loss:2.3347, Accuracy:0.2222, Validation Loss:2.4908, Validation Accuracy:0.1724\n",
    "Epoch #234: Loss:2.3362, Accuracy:0.2177, Validation Loss:2.4886, Validation Accuracy:0.1872\n",
    "Epoch #235: Loss:2.3293, Accuracy:0.2263, Validation Loss:2.4977, Validation Accuracy:0.1773\n",
    "Epoch #236: Loss:2.3213, Accuracy:0.2271, Validation Loss:2.4908, Validation Accuracy:0.1921\n",
    "Epoch #237: Loss:2.3198, Accuracy:0.2275, Validation Loss:2.4886, Validation Accuracy:0.1921\n",
    "Epoch #238: Loss:2.3157, Accuracy:0.2361, Validation Loss:2.4888, Validation Accuracy:0.1921\n",
    "Epoch #239: Loss:2.3160, Accuracy:0.2357, Validation Loss:2.4948, Validation Accuracy:0.1741\n",
    "Epoch #240: Loss:2.3170, Accuracy:0.2312, Validation Loss:2.4992, Validation Accuracy:0.1839\n",
    "Epoch #241: Loss:2.3152, Accuracy:0.2333, Validation Loss:2.4940, Validation Accuracy:0.1773\n",
    "Epoch #242: Loss:2.3105, Accuracy:0.2345, Validation Loss:2.4976, Validation Accuracy:0.1790\n",
    "Epoch #243: Loss:2.3116, Accuracy:0.2353, Validation Loss:2.4999, Validation Accuracy:0.1773\n",
    "Epoch #244: Loss:2.3116, Accuracy:0.2312, Validation Loss:2.4996, Validation Accuracy:0.1839\n",
    "Epoch #245: Loss:2.3108, Accuracy:0.2283, Validation Loss:2.4990, Validation Accuracy:0.1757\n",
    "Epoch #246: Loss:2.3166, Accuracy:0.2308, Validation Loss:2.5062, Validation Accuracy:0.1757\n",
    "Epoch #247: Loss:2.3152, Accuracy:0.2267, Validation Loss:2.5079, Validation Accuracy:0.1626\n",
    "Epoch #248: Loss:2.3098, Accuracy:0.2271, Validation Loss:2.5005, Validation Accuracy:0.1905\n",
    "Epoch #249: Loss:2.3115, Accuracy:0.2279, Validation Loss:2.5054, Validation Accuracy:0.1806\n",
    "Epoch #250: Loss:2.3137, Accuracy:0.2292, Validation Loss:2.5066, Validation Accuracy:0.1757\n",
    "Epoch #251: Loss:2.3052, Accuracy:0.2435, Validation Loss:2.5050, Validation Accuracy:0.1938\n",
    "Epoch #252: Loss:2.3008, Accuracy:0.2366, Validation Loss:2.5034, Validation Accuracy:0.1741\n",
    "Epoch #253: Loss:2.3039, Accuracy:0.2357, Validation Loss:2.5070, Validation Accuracy:0.1823\n",
    "Epoch #254: Loss:2.2976, Accuracy:0.2394, Validation Loss:2.5055, Validation Accuracy:0.1806\n",
    "Epoch #255: Loss:2.2949, Accuracy:0.2370, Validation Loss:2.5145, Validation Accuracy:0.1806\n",
    "Epoch #256: Loss:2.2984, Accuracy:0.2402, Validation Loss:2.5111, Validation Accuracy:0.1741\n",
    "Epoch #257: Loss:2.2978, Accuracy:0.2390, Validation Loss:2.5160, Validation Accuracy:0.1757\n",
    "Epoch #258: Loss:2.3012, Accuracy:0.2378, Validation Loss:2.5146, Validation Accuracy:0.1773\n",
    "Epoch #259: Loss:2.2968, Accuracy:0.2415, Validation Loss:2.5141, Validation Accuracy:0.1773\n",
    "Epoch #260: Loss:2.3023, Accuracy:0.2333, Validation Loss:2.5173, Validation Accuracy:0.1609\n",
    "Epoch #261: Loss:2.3026, Accuracy:0.2267, Validation Loss:2.5209, Validation Accuracy:0.1806\n",
    "Epoch #262: Loss:2.2968, Accuracy:0.2316, Validation Loss:2.5220, Validation Accuracy:0.1691\n",
    "Epoch #263: Loss:2.3055, Accuracy:0.2263, Validation Loss:2.5210, Validation Accuracy:0.1823\n",
    "Epoch #264: Loss:2.2937, Accuracy:0.2390, Validation Loss:2.5159, Validation Accuracy:0.1806\n",
    "Epoch #265: Loss:2.2960, Accuracy:0.2349, Validation Loss:2.5157, Validation Accuracy:0.1856\n",
    "Epoch #266: Loss:2.2952, Accuracy:0.2435, Validation Loss:2.5201, Validation Accuracy:0.1708\n",
    "Epoch #267: Loss:2.2847, Accuracy:0.2411, Validation Loss:2.5241, Validation Accuracy:0.1856\n",
    "Epoch #268: Loss:2.2902, Accuracy:0.2341, Validation Loss:2.5261, Validation Accuracy:0.1872\n",
    "Epoch #269: Loss:2.2853, Accuracy:0.2435, Validation Loss:2.5218, Validation Accuracy:0.1773\n",
    "Epoch #270: Loss:2.2845, Accuracy:0.2472, Validation Loss:2.5240, Validation Accuracy:0.1839\n",
    "Epoch #271: Loss:2.2809, Accuracy:0.2423, Validation Loss:2.5262, Validation Accuracy:0.1823\n",
    "Epoch #272: Loss:2.2858, Accuracy:0.2427, Validation Loss:2.5287, Validation Accuracy:0.1839\n",
    "Epoch #273: Loss:2.2806, Accuracy:0.2394, Validation Loss:2.5261, Validation Accuracy:0.1609\n",
    "Epoch #274: Loss:2.2874, Accuracy:0.2382, Validation Loss:2.5280, Validation Accuracy:0.1773\n",
    "Epoch #275: Loss:2.2751, Accuracy:0.2456, Validation Loss:2.5303, Validation Accuracy:0.1839\n",
    "Epoch #276: Loss:2.2740, Accuracy:0.2559, Validation Loss:2.5319, Validation Accuracy:0.1773\n",
    "Epoch #277: Loss:2.2739, Accuracy:0.2435, Validation Loss:2.5341, Validation Accuracy:0.1856\n",
    "Epoch #278: Loss:2.2824, Accuracy:0.2345, Validation Loss:2.5353, Validation Accuracy:0.1544\n",
    "Epoch #279: Loss:2.2882, Accuracy:0.2390, Validation Loss:2.5394, Validation Accuracy:0.1773\n",
    "Epoch #280: Loss:2.2792, Accuracy:0.2366, Validation Loss:2.5338, Validation Accuracy:0.1691\n",
    "Epoch #281: Loss:2.2748, Accuracy:0.2431, Validation Loss:2.5374, Validation Accuracy:0.1790\n",
    "Epoch #282: Loss:2.2702, Accuracy:0.2464, Validation Loss:2.5452, Validation Accuracy:0.1921\n",
    "Epoch #283: Loss:2.2710, Accuracy:0.2444, Validation Loss:2.5371, Validation Accuracy:0.1741\n",
    "Epoch #284: Loss:2.2688, Accuracy:0.2439, Validation Loss:2.5378, Validation Accuracy:0.1806\n",
    "Epoch #285: Loss:2.2697, Accuracy:0.2382, Validation Loss:2.5427, Validation Accuracy:0.1741\n",
    "Epoch #286: Loss:2.2760, Accuracy:0.2415, Validation Loss:2.5476, Validation Accuracy:0.1872\n",
    "Epoch #287: Loss:2.2709, Accuracy:0.2505, Validation Loss:2.5449, Validation Accuracy:0.1642\n",
    "Epoch #288: Loss:2.2692, Accuracy:0.2485, Validation Loss:2.5500, Validation Accuracy:0.1757\n",
    "Epoch #289: Loss:2.2736, Accuracy:0.2370, Validation Loss:2.5483, Validation Accuracy:0.1724\n",
    "Epoch #290: Loss:2.2710, Accuracy:0.2427, Validation Loss:2.5553, Validation Accuracy:0.1658\n",
    "Epoch #291: Loss:2.2741, Accuracy:0.2394, Validation Loss:2.5512, Validation Accuracy:0.1757\n",
    "Epoch #292: Loss:2.2716, Accuracy:0.2472, Validation Loss:2.5529, Validation Accuracy:0.1658\n",
    "Epoch #293: Loss:2.2784, Accuracy:0.2394, Validation Loss:2.5579, Validation Accuracy:0.1806\n",
    "Epoch #294: Loss:2.2798, Accuracy:0.2419, Validation Loss:2.5588, Validation Accuracy:0.1576\n",
    "Epoch #295: Loss:2.2749, Accuracy:0.2423, Validation Loss:2.5528, Validation Accuracy:0.1790\n",
    "Epoch #296: Loss:2.2606, Accuracy:0.2526, Validation Loss:2.5486, Validation Accuracy:0.1806\n",
    "Epoch #297: Loss:2.2578, Accuracy:0.2522, Validation Loss:2.5513, Validation Accuracy:0.1806\n",
    "Epoch #298: Loss:2.2546, Accuracy:0.2546, Validation Loss:2.5510, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.2508, Accuracy:0.2517, Validation Loss:2.5550, Validation Accuracy:0.1757\n",
    "Epoch #300: Loss:2.2549, Accuracy:0.2485, Validation Loss:2.5532, Validation Accuracy:0.1642\n",
    "\n",
    "Test:\n",
    "Test Loss:2.55320692, Accuracy:0.1642\n",
    "Labels: ['ds', 'ck', 'sg', 'mb', 'yd', 'eo', 'sk', 'ce', 'by', 'ek', 'ib', 'aa', 'eb', 'eg', 'my']\n",
    "Confusion Matrix:\n",
    "      ds  ck  sg  mb  yd  eo  sk  ce  by  ek  ib  aa  eb  eg  my\n",
    "t:ds   1   0   5   0   0   1   0   0   1   2   0   1   5  15   0\n",
    "t:ck   0   0   0   0   0   2   0   0   3   4   0   1   7   6   0\n",
    "t:sg   0   0  14   3  11   3   0   0   4   4   7   0   3   2   0\n",
    "t:mb   0   0   9   2   8   6   0   0   3   4   9   2   4   5   0\n",
    "t:yd   0   0   3   3  24   8   0   0   1   3  18   0   1   1   0\n",
    "t:eo   0   0   8   0   8   3   0   0   3   2   4   0   3   3   0\n",
    "t:sk   0   0   4   0   8   0   0   0   3   1   2   1   7   7   0\n",
    "t:ce   0   0   4   2   1   3   0   0   3   4   3   0   4   3   0\n",
    "t:by   0   0   4   1   4   4   0   0   9   3   1   1   8   5   0\n",
    "t:ek   0   0   4   1   5   4   0   0   3   3   2   3  13  10   0\n",
    "t:ib   0   0   7   2  23   2   0   0   3   2  11   0   1   3   0\n",
    "t:aa   1   0   2   0   3   0   0   0   2   3   1   5   3  14   0\n",
    "t:eb   0   0   7   0   3   3   0   0   3   6   4   2   7  15   0\n",
    "t:eg   0   0   1   2   1   2   0   0   5   5   0   2  11  21   0\n",
    "t:my   0   0   2   0   2   1   0   0   2   0   3   3   4   3   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ds       0.50      0.03      0.06        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          sg       0.19      0.27      0.22        51\n",
    "          mb       0.12      0.04      0.06        52\n",
    "          yd       0.24      0.39      0.29        62\n",
    "          eo       0.07      0.09      0.08        34\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          by       0.19      0.23      0.20        40\n",
    "          ek       0.07      0.06      0.06        48\n",
    "          ib       0.17      0.20      0.18        54\n",
    "          aa       0.24      0.15      0.18        34\n",
    "          eb       0.09      0.14      0.11        50\n",
    "          eg       0.19      0.42      0.26        50\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.14      0.13      0.11       609\n",
    "weighted avg       0.15      0.16      0.14       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 06:48:50 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7067107993785187, 2.696918860640628, 2.6907550411662835, 2.6857057224549292, 2.681054007635132, 2.6760898705186515, 2.6709602274526714, 2.6655702872816565, 2.6591275396018195, 2.6514184157836613, 2.6421602974188545, 2.6304647491874755, 2.6168539191310236, 2.601200181275166, 2.5840509176645763, 2.567331061966118, 2.557232090992293, 2.5408936889692284, 2.532031013460582, 2.5207443941990144, 2.5170524879825136, 2.5007353455366563, 2.495636373122142, 2.48743612034176, 2.4828771495662494, 2.476441770155833, 2.4717025224406926, 2.470750454807125, 2.465845816241109, 2.464091663094381, 2.460551254463509, 2.4586019038371068, 2.4566865781649385, 2.455008288322411, 2.4521398098010736, 2.450194878335461, 2.4516368663956967, 2.449402885092499, 2.449460376463892, 2.4468841490095667, 2.4462672215572914, 2.4478068191233917, 2.4463467276938053, 2.4448945021198694, 2.4450670486600528, 2.445296264047106, 2.444466711460859, 2.4426433480236134, 2.441995085558085, 2.442647040966892, 2.4423813220902617, 2.4416429135208255, 2.440531791724595, 2.4407876461793245, 2.44145161490918, 2.4462411063058034, 2.4443355929871107, 2.443628261046261, 2.4415420616788817, 2.4401897671578943, 2.4395108767135194, 2.4400205627842295, 2.4401523060790815, 2.4411270160393173, 2.439309965409277, 2.4482813899348717, 2.443403661740433, 2.4424594948052967, 2.440248878131359, 2.4409155630321533, 2.4393606401233643, 2.4399539214636894, 2.439105298914541, 2.4404967120911296, 2.4391088607080267, 2.4396450198538395, 2.444253243444784, 2.4434252118046453, 2.445672256018728, 2.4407929266223376, 2.443338605179184, 2.4403314978031103, 2.44104215585931, 2.4411691374379427, 2.4431354717668055, 2.446095430205021, 2.4491261335820793, 2.4485894733266096, 2.445175022523, 2.4433842437412157, 2.442353787289073, 2.4422864166190865, 2.442662331858292, 2.4423663107240925, 2.44382002944821, 2.4441667464370602, 2.4447016046552235, 2.4442802939704684, 2.444054405090257, 2.4461213078209134, 2.4474920639263584, 2.446934757953011, 2.4454482834914635, 2.445620518404079, 2.4460843756476844, 2.446765461187253, 2.4574347333172075, 2.450313980356226, 2.4499929503267035, 2.4448668710116683, 2.446299900953797, 2.4458702197803066, 2.4461868741046424, 2.445485220754088, 2.4456422426821955, 2.4470960663261474, 2.4494657301158935, 2.4486386314009994, 2.448690662243096, 2.4455455065948035, 2.4478534849602207, 2.457149277375445, 2.4480805514481268, 2.447071329517709, 2.4477528601835905, 2.4473458816265237, 2.4461925632652197, 2.4493964062927196, 2.4499556493680856, 2.4469057002482546, 2.456764050109437, 2.4550064060292613, 2.4515387217203775, 2.4505029045693782, 2.452347247275617, 2.4536522460492765, 2.446815087485979, 2.448821870368494, 2.446275693050942, 2.4528747579734316, 2.4613338358492296, 2.447886591865903, 2.448482688816114, 2.449441358569416, 2.452854469687676, 2.4503677572522844, 2.445674011468496, 2.4566077516583973, 2.4508246976166523, 2.451343077156931, 2.4576135535154044, 2.457283517214269, 2.4503719845820333, 2.4522451602766666, 2.4483125902749046, 2.4496708158984757, 2.450056268663829, 2.447797272201438, 2.450732931910673, 2.449855353053176, 2.450107017761381, 2.4517256826015528, 2.4549124867262315, 2.4497370132671787, 2.4515801415654828, 2.450915256353043, 2.450087123707989, 2.4510841737631313, 2.4479714955015135, 2.451035469036384, 2.4560064482571455, 2.4541703789496463, 2.4554922083524255, 2.457865083550389, 2.4544609990613213, 2.4752864645815444, 2.4623298782041703, 2.4523194983283485, 2.4578530185524072, 2.4590994673605233, 2.4518819689163434, 2.4533872866669704, 2.451729982748799, 2.4560608472338648, 2.4609333725006906, 2.455412024934891, 2.4583364347323213, 2.458340815135411, 2.4546858410921395, 2.4557704949026626, 2.457439041294292, 2.45607100214277, 2.4575595311539122, 2.4653137297857377, 2.459184071896307, 2.4605452681605646, 2.4593493762274683, 2.4611031535419534, 2.461265489581379, 2.4607236686794236, 2.47491649023222, 2.4638478219607, 2.4715330318864344, 2.465752250846775, 2.4699468510883, 2.4644814128750454, 2.467319968885976, 2.472834933176025, 2.4671766769709844, 2.470465156245114, 2.4678354071474624, 2.4696927023638646, 2.4713261017854187, 2.471829589364564, 2.4721498411081497, 2.476362422965039, 2.4765208288170824, 2.4753217094245996, 2.4754329271895936, 2.4774016018571525, 2.479811562301686, 2.485696698644478, 2.480973584702841, 2.4823708776965714, 2.482802723037394, 2.492464657879032, 2.4862190021082684, 2.489406082234751, 2.483299580700879, 2.4849544413179796, 2.485151133122311, 2.4938880030940513, 2.490833625026133, 2.488646246921057, 2.497659418970493, 2.4908292638061473, 2.488574056594047, 2.488776664232777, 2.4948217954933156, 2.4991528345837772, 2.49404247601827, 2.497597537408713, 2.4999002392460365, 2.499566727279638, 2.4990213199202063, 2.506194332354566, 2.5078688028019247, 2.5005181422961757, 2.5054255698506274, 2.5065670678963996, 2.5050234340486073, 2.5034123351812756, 2.506989841977951, 2.5055110023918212, 2.514541569601726, 2.5110840073164264, 2.5159622311396355, 2.514585758077687, 2.514125318167049, 2.517331025479071, 2.520939803475817, 2.5219783598957783, 2.5210106556834453, 2.5158506696447365, 2.5157352480395088, 2.5201193683448877, 2.5240785073372725, 2.5260991670423736, 2.5218282250935222, 2.52402182557117, 2.526190547128812, 2.5287352751432772, 2.526085834002064, 2.5279589490154497, 2.5303186841786203, 2.531875928634493, 2.5340831146647385, 2.53531258720874, 2.539385373564972, 2.5337824011083896, 2.5374051444048953, 2.545239767613278, 2.537056643778859, 2.537847183803815, 2.542732394974807, 2.547603223907145, 2.5448832218282917, 2.5500333551898575, 2.5483204930873926, 2.55530179975851, 2.5512151330562647, 2.552938430767341, 2.557918471068584, 2.5588471122171685, 2.5527827266010354, 2.5486148950110126, 2.551292992577764, 2.5509527254182913, 2.555002938741925, 2.5532069679943015], 'val_acc': [0.06896551683765327, 0.08374384196080598, 0.09031198685759394, 0.08210180623362022, 0.08210180623362022, 0.0853858784832097, 0.1313628894819806, 0.11001642035513089, 0.1067323481055414, 0.10837438423033614, 0.14449917848033858, 0.16420361227149446, 0.15270935860271329, 0.15435139472750803, 0.15763546697709752, 0.1658456478946902, 0.154351394923254, 0.14942528645099679, 0.15106732277153748, 0.15599343085230277, 0.15106732277153748, 0.1740558285186639, 0.1658456477968172, 0.17405582842079093, 0.1576354671728435, 0.16420361167202246, 0.1773399007682534, 0.1740558285186639, 0.17569786464345866, 0.16748768392161195, 0.17569786454558567, 0.17405582832291797, 0.17569786454558567, 0.17405582842079093, 0.18062397291996993, 0.1773399007682534, 0.18390804516955941, 0.1871921175170219, 0.17569786464345866, 0.17569786464345866, 0.1871921175170219, 0.18555008139222715, 0.18226600914263763, 0.18226600894689168, 0.1904761896687384, 0.18226600914263763, 0.1904761895708654, 0.18555008129435416, 0.19211822589140612, 0.1871921175170219, 0.19376026201620086, 0.19047618976661138, 0.19868637048845808, 0.18555008139222715, 0.20032840661325282, 0.1904761896687384, 0.19376026191832788, 0.1871921175170219, 0.1904761896687384, 0.19376026191832788, 0.19868637048845808, 0.19211822589140612, 0.19047618976661138, 0.1921182259892791, 0.20032840651537984, 0.1921182259892791, 0.19376026182045492, 0.18883415354394364, 0.19211822579353313, 0.19376026211407385, 0.1904761896687384, 0.19376026201620086, 0.19540229804312262, 0.2003284067111258, 0.20197044264017458, 0.20197044264017458, 0.19704433446153632, 0.18555008119648117, 0.1839080452674324, 0.19376026191832788, 0.20689655101455884, 0.1986863702927121, 0.1986863702927121, 0.1986863702927121, 0.20032840651537984, 0.19211822579353313, 0.17898193689304814, 0.1888341533481977, 0.19704433416791736, 0.19540229804312262, 0.19704433416791736, 0.20032840641750688, 0.2003284063196339, 0.1970443340700444, 0.1986863702927121, 0.19211822589140612, 0.1986863703905851, 0.19376026191832788, 0.2003284063196339, 0.1986863702927121, 0.18883415344607066, 0.1986863703905851, 0.1970443340700444, 0.1871921175170219, 0.19211822579353313, 0.19211822569566014, 0.1756978644477127, 0.18719211722340295, 0.17241379309121416, 0.19211822579353313, 0.18555008129435416, 0.1871921174191489, 0.1986863703905851, 0.19376026191832788, 0.19047618976661138, 0.18390804516955941, 0.18883415354394364, 0.18390804507168643, 0.17898193679517518, 0.20032840641750688, 0.19211822569566014, 0.1756978644477127, 0.19540229804312262, 0.18062397282209694, 0.18555008129435416, 0.19704433426579035, 0.18883415354394364, 0.18226600894689168, 0.18390804497381344, 0.18062397282209694, 0.1756978644477127, 0.17569786454558567, 0.18062397282209694, 0.18062397311571587, 0.17733990067038044, 0.19047618947299244, 0.18390804507168643, 0.1756978644477127, 0.19704433416791736, 0.17898193759039313, 0.167487683725866, 0.1871921174191489, 0.18390804507168643, 0.1822660088490187, 0.18390804507168643, 0.18062397272422395, 0.18555008109860818, 0.1740558292160089, 0.180623972626351, 0.1839080452674324, 0.16584564750319827, 0.1789819365994292, 0.19211822579353313, 0.16912971975278776, 0.1904761895708654, 0.18883415344607066, 0.17733990037676148, 0.1822660088490187, 0.17898193759039313, 0.1822660088490187, 0.1789819365994292, 0.17569786434983972, 0.17405582842079093, 0.1871921174191489, 0.1822660088490187, 0.1822660088490187, 0.1822660088490187, 0.18390804487594048, 0.1822660088490187, 0.174055828127172, 0.17898193650155622, 0.17077175617120144, 0.174055828127172, 0.18226600875114574, 0.17733990037676148, 0.1625615756451007, 0.16420361236936745, 0.17241379219812322, 0.18390804546317835, 0.1822660088490187, 0.17898193689304814, 0.180623972626351, 0.18062397272422395, 0.17569786464345866, 0.18062397272422395, 0.17569786434983972, 0.16748768362799302, 0.17733990037676148, 0.18062397282209694, 0.174055828127172, 0.18390804516955941, 0.1789819366973022, 0.18062397272422395, 0.1707717558775825, 0.17733990037676148, 0.18555008139222715, 0.17241379239386917, 0.18390804497381344, 0.17898193689304814, 0.17898193689304814, 0.16584564769894422, 0.167487683725866, 0.1658456477968172, 0.18390804516955941, 0.17898193708879412, 0.18555008119648117, 0.17569786474133164, 0.17405582822504498, 0.18555008139222715, 0.1789819366973022, 0.18226600914263763, 0.1855500815879731, 0.18555008139222715, 0.18390804536530536, 0.1806239730178429, 0.1855500814901001, 0.17077175617120144, 0.1839080452674324, 0.18719211781064082, 0.17733990067038044, 0.17569786474133164, 0.1756978649370776, 0.1855500815879731, 0.18719211761489485, 0.17898193708879412, 0.1822660093383836, 0.18719211771276784, 0.17405582842079093, 0.19047618996235732, 0.18719211781064082, 0.19540229863036052, 0.17569786483920463, 0.17241379258961514, 0.18719211771276784, 0.1773399008661264, 0.19211822628289804, 0.192118226478644, 0.19211822638077103, 0.1740558286165369, 0.1839080456589243, 0.17733990096399938, 0.1789819371866671, 0.1773399007682534, 0.1839080452674324, 0.17569786464345866, 0.17569786454558567, 0.16256157544935473, 0.19047618996235732, 0.18062397331146185, 0.17569786483920463, 0.1937602622119468, 0.1740558286165369, 0.1822660093383836, 0.18062397331146185, 0.18062397291996993, 0.1740558286165369, 0.17569786474133164, 0.1773399007682534, 0.17733990067038044, 0.16091953952030597, 0.18062397331146185, 0.1691297205357716, 0.1822660094362566, 0.1806239730178429, 0.18555008178371907, 0.17077175626907443, 0.18555008168584608, 0.18719211771276784, 0.17733990106187233, 0.18390804556105134, 0.1822660093383836, 0.1839080456589243, 0.16091953942243298, 0.1773399007682534, 0.18390804546317835, 0.1773399008661264, 0.1855500815879731, 0.15435139502112696, 0.1773399007682534, 0.16912972024215267, 0.17898193699092113, 0.19211822638077103, 0.17405582881228285, 0.18062397311571587, 0.1740558286165369, 0.18719211761489485, 0.1642036120635144, 0.17569786464345866, 0.17241379249174216, 0.16584564809043614, 0.17569786474133164, 0.1658456478946902, 0.18062397331146185, 0.15763546736858944, 0.17898193708879412, 0.1806239735072078, 0.1806239734093348, 0.1740558287144099, 0.17569786464345866, 0.16420361176989545], 'loss': [2.7155308613786953, 2.7030285641398026, 2.6946862458693173, 2.6898818567303415, 2.6847451355912604, 2.680156699538965, 2.6757943994455515, 2.670533045018723, 2.6650867720648983, 2.6586679449805977, 2.650779030504168, 2.6414740344092587, 2.6294542536843237, 2.6157485192316514, 2.6000339555054963, 2.5839251300882267, 2.5697264949399097, 2.556530902224155, 2.544639160305078, 2.5332555636732974, 2.527848408990817, 2.5212488097087067, 2.5104076990601465, 2.5047751760580703, 2.499153435058907, 2.4941849490210752, 2.4913654860040246, 2.4871428706073173, 2.484416971363326, 2.482749688894597, 2.477543023185808, 2.47547427986192, 2.4724650833396207, 2.473305647769748, 2.4690324960548042, 2.4671244305996436, 2.4671921455884616, 2.469080749135732, 2.468747401678097, 2.471089369560414, 2.465822956449442, 2.4593910861553843, 2.458571564441344, 2.458353498584191, 2.456442399876808, 2.454761034458325, 2.4539598712686153, 2.4528876878397665, 2.451903510925951, 2.450773387474201, 2.44995160249714, 2.4499772838498535, 2.4484737346304515, 2.447265635085057, 2.447172136130519, 2.4496682916089005, 2.4479874192811626, 2.447383396924154, 2.44385791931309, 2.4439526490362273, 2.4451005546953644, 2.4423675070063533, 2.4401426675628097, 2.4400810531522215, 2.4388435821024053, 2.4404959623818523, 2.441696209290679, 2.4414720700751587, 2.4442583477717403, 2.43666183228855, 2.4363412510443028, 2.4355335303155794, 2.434767797791248, 2.433455026419011, 2.4341048320950422, 2.432185423790307, 2.4314503075405804, 2.4316507522575175, 2.4341696091011564, 2.434485234274267, 2.4299326996538917, 2.4288765625297657, 2.4274835399533687, 2.42652854253624, 2.425867829528433, 2.424944619623298, 2.428705959985878, 2.432835175809919, 2.43086108730559, 2.4276324475815163, 2.425045392449632, 2.4262278377642623, 2.422371685725218, 2.422343586798321, 2.4230211969763347, 2.421508597838071, 2.4226005212482242, 2.4211800196332365, 2.4198026018710594, 2.419337683389809, 2.4210982870027515, 2.4201955079542783, 2.4177867386130583, 2.4188147849370814, 2.4172323364007156, 2.4156645462498285, 2.4196200266021477, 2.419278392948409, 2.4159275843132693, 2.4179002291857583, 2.416635619541458, 2.4153266266386124, 2.412305254319365, 2.412311191970073, 2.4109212090102554, 2.4129860422694462, 2.411861445330986, 2.414773977072087, 2.4125775774891127, 2.412462689695417, 2.4111298624739756, 2.411870732493469, 2.4136729117046882, 2.4084728572892455, 2.4056666089279206, 2.4048306587540393, 2.4048190441954063, 2.40413768325743, 2.4064234374240194, 2.408462497683766, 2.4031411852435163, 2.409755721180346, 2.4087031711053553, 2.4035730260108776, 2.4083372054403567, 2.406675453205618, 2.405975894810483, 2.3990621378779164, 2.399152651166035, 2.3987796178343848, 2.3988097646153195, 2.404172154079962, 2.397281686187525, 2.3981457139432307, 2.397378594282961, 2.39456005634958, 2.39356839955465, 2.3960260369694453, 2.403895211170831, 2.3978443495301986, 2.3948638808311133, 2.3949729747106407, 2.393719578670525, 2.390629810474247, 2.39106170744377, 2.3877504880423417, 2.386495909406909, 2.3854418268928295, 2.3859239196385693, 2.388263733822707, 2.3898234063839765, 2.387091488906735, 2.3876769731666516, 2.3828339455308853, 2.3804262591093717, 2.3794639470885666, 2.379882420553564, 2.377594903949839, 2.379241691526691, 2.3745926636689987, 2.37369678211408, 2.3809982994988226, 2.393855763264995, 2.3861363906390367, 2.3843568698091917, 2.383954011439298, 2.3958547340526226, 2.3874188955804407, 2.382067529768425, 2.38483674707354, 2.3800299601388417, 2.3727180786446134, 2.3681933237541872, 2.3710955914530665, 2.3709285534382847, 2.3735038796489487, 2.373543572181059, 2.367725167480093, 2.3646285555445927, 2.3615261577238047, 2.360911746289451, 2.360936446356333, 2.357767494699059, 2.3572006775857974, 2.361055761979591, 2.358797542172536, 2.358692702032947, 2.354570531355527, 2.352785632840417, 2.35177872205417, 2.3526790191995044, 2.3598605274419766, 2.3592576016146056, 2.3581541898559006, 2.350372536618117, 2.347255034808995, 2.3475692444513467, 2.3431040454449352, 2.350803303473784, 2.344262003751751, 2.343218227382558, 2.3390160181683926, 2.3397554490600525, 2.3389211750617998, 2.338151911298842, 2.3372678991705484, 2.3414052939757672, 2.3420447718681006, 2.336666274021783, 2.332489788458822, 2.330961332379915, 2.335385125471581, 2.333288079561394, 2.3334849686593246, 2.3311788970195293, 2.3285225591130816, 2.335120689795492, 2.3316577081073238, 2.3278418347086505, 2.3216107671020945, 2.324693121312825, 2.3222936384486954, 2.3346583119652844, 2.3362002082918703, 2.3293140920525457, 2.321286553474912, 2.319802292661256, 2.31571308631427, 2.316042490906294, 2.317032617318312, 2.315210523057033, 2.310541854208255, 2.311646357209286, 2.311603037875291, 2.3108178570530007, 2.3165964063922484, 2.3152236214898205, 2.3097562127044804, 2.311542333176004, 2.3136938990019185, 2.30515619841934, 2.300775985698191, 2.3038568449705776, 2.2976034667702425, 2.2948796933191757, 2.298399395227922, 2.2977593326960255, 2.3012064254259426, 2.296768797496506, 2.3023357695867395, 2.302586124369251, 2.2967997512778218, 2.305456695223736, 2.293660282843901, 2.295965627870031, 2.295228269604442, 2.2847352735805315, 2.2902261681135676, 2.285342493977635, 2.284509387496071, 2.2808756318180468, 2.285796214031243, 2.2806027645447906, 2.2873775824873843, 2.2750747030520584, 2.274005073492532, 2.273945378082244, 2.2824055273185277, 2.2882355180853935, 2.2791712980250804, 2.2747593568335813, 2.270219215573227, 2.2709915393187035, 2.268836887954931, 2.2697386289279318, 2.2759981305202666, 2.270921235995126, 2.2692005636266126, 2.273557151367532, 2.2710296182417036, 2.2741082084741926, 2.271612725267665, 2.2784012376405376, 2.2797801148964885, 2.2749142785826257, 2.2605913605778123, 2.2577785749455006, 2.2546099674530344, 2.250827281920572, 2.25490031839641], 'acc': [0.0780287472007211, 0.07679671492481134, 0.09199178638338308, 0.08172484635082848, 0.08131416834110597, 0.0817248465282961, 0.10554414836716602, 0.11581108862308506, 0.1071868582611456, 0.10882956932090392, 0.11786447670425478, 0.13839835682444016, 0.1515400409759682, 0.15687884942339675, 0.16344969229164555, 0.167145789446772, 0.15975359351483215, 0.15154003999683646, 0.1523613968179456, 0.15071868652313397, 0.1564681721786209, 0.15934291552346835, 0.16468172475420229, 0.1646817259291604, 0.16591375878336984, 0.1679671452887494, 0.1667351138236831, 0.1646817239708969, 0.1712525658416552, 0.16796714609041352, 0.17494866479593627, 0.17125256701661332, 0.17577002101120762, 0.17166324424303042, 0.17412731014727567, 0.1782340867196265, 0.17002053398493625, 0.17659137663899996, 0.17084188841941175, 0.1733059555169738, 0.1716632448488682, 0.17535934300148512, 0.17741273169767197, 0.17618069706266665, 0.17782340874662145, 0.1761806984334511, 0.17577002006879333, 0.17700205425706977, 0.17494866557924166, 0.1770020531004704, 0.17700205386541706, 0.17535934243236478, 0.17535934241400608, 0.1778234091015567, 0.17823408652380018, 0.17330595453784206, 0.17535934241400608, 0.17494866442264229, 0.1757700209928489, 0.17494866501012132, 0.18275153953926274, 0.1761806980417984, 0.17782340814078368, 0.17577002142121906, 0.1786447633769233, 0.17700205267210026, 0.1778234094932094, 0.17987679621277403, 0.17330595450112463, 0.17618069864763616, 0.1786447649435341, 0.17946611998384737, 0.17987679758355848, 0.1774127307001815, 0.17494866458175118, 0.1774127315018456, 0.1782340865421589, 0.1790554425616039, 0.17864476335856458, 0.1774127299168761, 0.18069815127144603, 0.18151950729089106, 0.1782340867196265, 0.1761806984518098, 0.1786447629669119, 0.18316221874230207, 0.1790554417599398, 0.17946611939636833, 0.17864476394604364, 0.1823408615111815, 0.18069815144891366, 0.18398357396008297, 0.18110883006447395, 0.18603696007380985, 0.18357289555870776, 0.18357289632365445, 0.18726899351549833, 0.18439425140068516, 0.18809034974912844, 0.18603696048382126, 0.18767967154357956, 0.18234086290032467, 0.18521560583516067, 0.18726899273219294, 0.18891170498526808, 0.18973305941974358, 0.18521560681429242, 0.1930184805968459, 0.1860369618546057, 0.1864476394726755, 0.18767967232688496, 0.1913757693045438, 0.18767967076027417, 0.1897330594381023, 0.19219712575235895, 0.1864476374960533, 0.1954825468743851, 0.19301848057848717, 0.19425051402017565, 0.19178644792010408, 0.18603696167713807, 0.18726899431716246, 0.19096509286143207, 0.1926078031562437, 0.19425051403853438, 0.19425051361016424, 0.19096509131318, 0.19630390212888346, 0.19425051284521758, 0.19260780235457958, 0.19589322429662856, 0.18932238182003247, 0.19178644654931964, 0.1901437372703572, 0.19342915760907795, 0.19425051441182836, 0.19178644852594184, 0.19876796605650648, 0.19589322310331175, 0.1954825463052648, 0.20123203331317746, 0.1926078019812856, 0.19876796642980046, 0.1938398358146268, 0.19342915739489286, 0.1971252561717063, 0.19958932227177786, 0.19917864543701344, 0.19096509188230032, 0.19425051343269661, 0.19630390211052473, 0.20000000026314166, 0.19958932209431024, 0.19835728902591573, 0.1991786446169906, 0.2020533885676758, 0.20205338917351356, 0.20574948672029272, 0.19876796683981188, 0.19917864424369663, 0.2004106782912229, 0.19835728881173065, 0.1942505122577385, 0.2024640661857456, 0.20041067850540795, 0.19835729039670016, 0.2008213551259873, 0.20739219701510436, 0.20574948691611908, 0.2028747422004872, 0.20492813187580577, 0.20123203174656668, 0.19589322314002922, 0.20903490786067758, 0.20698151939703455, 0.19876796703563823, 0.19178644735098374, 0.19753593416307008, 0.20246406518825516, 0.20739219662345165, 0.20821355242871162, 0.20410677505469665, 0.20369609784663825, 0.2082135528203643, 0.204517453884442, 0.20780287580813225, 0.20657084193807362, 0.2078028736356837, 0.2119096515971777, 0.2102669405190607, 0.20862423063426047, 0.21519507152588704, 0.2139630381025573, 0.20821355264289668, 0.21683778182069868, 0.2114989731958025, 0.21683778183905741, 0.21889117108600586, 0.21519507154424578, 0.219301847939129, 0.2168377819981663, 0.21396303949170045, 0.2143737157206271, 0.21724846102373802, 0.22094455956800763, 0.22012320415440037, 0.21889117069435315, 0.21889116932356872, 0.2217659146083209, 0.2213552354236403, 0.21930184811659661, 0.22217659024976852, 0.2254620133667756, 0.2271047232332171, 0.22464065734733057, 0.22628336821126252, 0.21560574996397971, 0.2250513349654004, 0.2201232027652572, 0.2283367551082948, 0.22792607690274594, 0.22792607788187766, 0.22546201317094924, 0.2250513349654004, 0.22915811014860807, 0.22587268881239686, 0.2258726898098873, 0.2258726905931927, 0.22135523718607744, 0.23039014341282893, 0.22669404563350598, 0.22956878835415692, 0.2221765918163793, 0.2176591386234491, 0.22628336703630444, 0.2271047236432285, 0.22751540202624498, 0.23613962965579494, 0.23572895283938922, 0.23121149823895715, 0.23326488652513258, 0.2344969197709947, 0.2353182754355045, 0.231211500019753, 0.2283367547350008, 0.23080082240168318, 0.22669404561514728, 0.22710472342904345, 0.22792607868354178, 0.2291581113602836, 0.2435318263893989, 0.2365503074696911, 0.23572895225191018, 0.23942505236279057, 0.23696098628107773, 0.24024640742146258, 0.23901437260898967, 0.23778234192722877, 0.24147844048985712, 0.23326488793263445, 0.2266940460251587, 0.23162217742363775, 0.22628336643046668, 0.2390143727864573, 0.2349075979949023, 0.24353182817019475, 0.24106776285342857, 0.23408624197545727, 0.2435318263893989, 0.24722792730194343, 0.24229979353518946, 0.24271047271987006, 0.2394250523811493, 0.23819301774614401, 0.24558521526305338, 0.25585215610645146, 0.24353182893514144, 0.2344969199851798, 0.23901437317810997, 0.23655030709639713, 0.24312114955463449, 0.24640657071337807, 0.2443531826046703, 0.24394250655321126, 0.2381930193127548, 0.24147844047149838, 0.2505133460924121, 0.24845995996032652, 0.23696098489193457, 0.24271047254240244, 0.23942505060035346, 0.24722792573533264, 0.2394250507961798, 0.2418891180895682, 0.24229979394520088, 0.25256673692433007, 0.25215605715217043, 0.2546201240355474, 0.2517453807274174, 0.24845995937284748]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
