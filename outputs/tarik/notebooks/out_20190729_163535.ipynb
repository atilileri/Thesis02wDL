{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf14.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 16:35:35 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'All', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '03', '01', '02', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001D8024ACE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001D85A386EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6153, Accuracy:0.1877, Validation Loss:1.6113, Validation Accuracy:0.1888\n",
    "Epoch #2: Loss:1.6080, Accuracy:0.2172, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6053, Accuracy:0.2357, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6025, Accuracy:0.2329, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6026, Accuracy:0.2329, Validation Loss:1.6015, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6023, Accuracy:0.2329, Validation Loss:1.6013, Validation Accuracy:0.2447\n",
    "Epoch #23: Loss:1.6022, Accuracy:0.2357, Validation Loss:1.6012, Validation Accuracy:0.2463\n",
    "Epoch #24: Loss:1.6017, Accuracy:0.2370, Validation Loss:1.6011, Validation Accuracy:0.2463\n",
    "Epoch #25: Loss:1.6015, Accuracy:0.2366, Validation Loss:1.6009, Validation Accuracy:0.2447\n",
    "Epoch #26: Loss:1.6011, Accuracy:0.2366, Validation Loss:1.6008, Validation Accuracy:0.2463\n",
    "Epoch #27: Loss:1.6006, Accuracy:0.2382, Validation Loss:1.6028, Validation Accuracy:0.2365\n",
    "Epoch #28: Loss:1.6030, Accuracy:0.2402, Validation Loss:1.6028, Validation Accuracy:0.2315\n",
    "Epoch #29: Loss:1.6021, Accuracy:0.2370, Validation Loss:1.6017, Validation Accuracy:0.2463\n",
    "Epoch #30: Loss:1.6016, Accuracy:0.2370, Validation Loss:1.6019, Validation Accuracy:0.2463\n",
    "Epoch #31: Loss:1.6013, Accuracy:0.2394, Validation Loss:1.6021, Validation Accuracy:0.2463\n",
    "Epoch #32: Loss:1.6021, Accuracy:0.2366, Validation Loss:1.6051, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.6040, Accuracy:0.2386, Validation Loss:1.6032, Validation Accuracy:0.2397\n",
    "Epoch #34: Loss:1.6019, Accuracy:0.2378, Validation Loss:1.6030, Validation Accuracy:0.2414\n",
    "Epoch #35: Loss:1.6024, Accuracy:0.2366, Validation Loss:1.6031, Validation Accuracy:0.2430\n",
    "Epoch #36: Loss:1.6018, Accuracy:0.2402, Validation Loss:1.6019, Validation Accuracy:0.2463\n",
    "Epoch #37: Loss:1.6013, Accuracy:0.2382, Validation Loss:1.6019, Validation Accuracy:0.2463\n",
    "Epoch #38: Loss:1.6013, Accuracy:0.2386, Validation Loss:1.6011, Validation Accuracy:0.2447\n",
    "Epoch #39: Loss:1.6008, Accuracy:0.2435, Validation Loss:1.6010, Validation Accuracy:0.2430\n",
    "Epoch #40: Loss:1.6003, Accuracy:0.2431, Validation Loss:1.6009, Validation Accuracy:0.2430\n",
    "Epoch #41: Loss:1.6004, Accuracy:0.2427, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #42: Loss:1.6002, Accuracy:0.2411, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #43: Loss:1.6002, Accuracy:0.2386, Validation Loss:1.6005, Validation Accuracy:0.2299\n",
    "Epoch #44: Loss:1.6001, Accuracy:0.2398, Validation Loss:1.6005, Validation Accuracy:0.2299\n",
    "Epoch #45: Loss:1.5999, Accuracy:0.2419, Validation Loss:1.6001, Validation Accuracy:0.2397\n",
    "Epoch #46: Loss:1.6001, Accuracy:0.2419, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #47: Loss:1.5999, Accuracy:0.2419, Validation Loss:1.6002, Validation Accuracy:0.2381\n",
    "Epoch #48: Loss:1.5997, Accuracy:0.2394, Validation Loss:1.5997, Validation Accuracy:0.2315\n",
    "Epoch #49: Loss:1.5998, Accuracy:0.2402, Validation Loss:1.5991, Validation Accuracy:0.2299\n",
    "Epoch #50: Loss:1.5997, Accuracy:0.2398, Validation Loss:1.5990, Validation Accuracy:0.2315\n",
    "Epoch #51: Loss:1.5996, Accuracy:0.2407, Validation Loss:1.5993, Validation Accuracy:0.2365\n",
    "Epoch #52: Loss:1.5995, Accuracy:0.2444, Validation Loss:1.5994, Validation Accuracy:0.2332\n",
    "Epoch #53: Loss:1.5997, Accuracy:0.2407, Validation Loss:1.5997, Validation Accuracy:0.2282\n",
    "Epoch #54: Loss:1.5993, Accuracy:0.2394, Validation Loss:1.5996, Validation Accuracy:0.2315\n",
    "Epoch #55: Loss:1.5991, Accuracy:0.2398, Validation Loss:1.5997, Validation Accuracy:0.2315\n",
    "Epoch #56: Loss:1.5991, Accuracy:0.2394, Validation Loss:1.5997, Validation Accuracy:0.2315\n",
    "Epoch #57: Loss:1.5991, Accuracy:0.2398, Validation Loss:1.5997, Validation Accuracy:0.2332\n",
    "Epoch #58: Loss:1.5990, Accuracy:0.2390, Validation Loss:1.5994, Validation Accuracy:0.2282\n",
    "Epoch #59: Loss:1.5993, Accuracy:0.2390, Validation Loss:1.5994, Validation Accuracy:0.2282\n",
    "Epoch #60: Loss:1.5990, Accuracy:0.2357, Validation Loss:1.5996, Validation Accuracy:0.2430\n",
    "Epoch #61: Loss:1.5991, Accuracy:0.2402, Validation Loss:1.5996, Validation Accuracy:0.2299\n",
    "Epoch #62: Loss:1.5991, Accuracy:0.2378, Validation Loss:1.5993, Validation Accuracy:0.2332\n",
    "Epoch #63: Loss:1.5989, Accuracy:0.2411, Validation Loss:1.5992, Validation Accuracy:0.2332\n",
    "Epoch #64: Loss:1.5988, Accuracy:0.2419, Validation Loss:1.5988, Validation Accuracy:0.2332\n",
    "Epoch #65: Loss:1.5985, Accuracy:0.2407, Validation Loss:1.5993, Validation Accuracy:0.2299\n",
    "Epoch #66: Loss:1.5986, Accuracy:0.2402, Validation Loss:1.5982, Validation Accuracy:0.2332\n",
    "Epoch #67: Loss:1.5986, Accuracy:0.2402, Validation Loss:1.5978, Validation Accuracy:0.2348\n",
    "Epoch #68: Loss:1.5984, Accuracy:0.2390, Validation Loss:1.5998, Validation Accuracy:0.2299\n",
    "Epoch #69: Loss:1.5984, Accuracy:0.2361, Validation Loss:1.6004, Validation Accuracy:0.2200\n",
    "Epoch #70: Loss:1.5988, Accuracy:0.2402, Validation Loss:1.5989, Validation Accuracy:0.2217\n",
    "Epoch #71: Loss:1.5982, Accuracy:0.2402, Validation Loss:1.5986, Validation Accuracy:0.2332\n",
    "Epoch #72: Loss:1.5983, Accuracy:0.2402, Validation Loss:1.5989, Validation Accuracy:0.2332\n",
    "Epoch #73: Loss:1.5981, Accuracy:0.2382, Validation Loss:1.5984, Validation Accuracy:0.2332\n",
    "Epoch #74: Loss:1.5984, Accuracy:0.2378, Validation Loss:1.5989, Validation Accuracy:0.2332\n",
    "Epoch #75: Loss:1.5977, Accuracy:0.2386, Validation Loss:1.5989, Validation Accuracy:0.2332\n",
    "Epoch #76: Loss:1.5982, Accuracy:0.2419, Validation Loss:1.5991, Validation Accuracy:0.2250\n",
    "Epoch #77: Loss:1.5979, Accuracy:0.2419, Validation Loss:1.5987, Validation Accuracy:0.2250\n",
    "Epoch #78: Loss:1.5978, Accuracy:0.2402, Validation Loss:1.5991, Validation Accuracy:0.2315\n",
    "Epoch #79: Loss:1.5979, Accuracy:0.2394, Validation Loss:1.5987, Validation Accuracy:0.2332\n",
    "Epoch #80: Loss:1.5976, Accuracy:0.2398, Validation Loss:1.5992, Validation Accuracy:0.2299\n",
    "Epoch #81: Loss:1.5979, Accuracy:0.2386, Validation Loss:1.5988, Validation Accuracy:0.2299\n",
    "Epoch #82: Loss:1.5977, Accuracy:0.2382, Validation Loss:1.5989, Validation Accuracy:0.2266\n",
    "Epoch #83: Loss:1.5979, Accuracy:0.2398, Validation Loss:1.5989, Validation Accuracy:0.2282\n",
    "Epoch #84: Loss:1.5972, Accuracy:0.2390, Validation Loss:1.5986, Validation Accuracy:0.2299\n",
    "Epoch #85: Loss:1.5971, Accuracy:0.2390, Validation Loss:1.5987, Validation Accuracy:0.2315\n",
    "Epoch #86: Loss:1.5975, Accuracy:0.2390, Validation Loss:1.5987, Validation Accuracy:0.2282\n",
    "Epoch #87: Loss:1.5974, Accuracy:0.2357, Validation Loss:1.5999, Validation Accuracy:0.2365\n",
    "Epoch #88: Loss:1.5978, Accuracy:0.2444, Validation Loss:1.6002, Validation Accuracy:0.2332\n",
    "Epoch #89: Loss:1.5985, Accuracy:0.2398, Validation Loss:1.6000, Validation Accuracy:0.2135\n",
    "Epoch #90: Loss:1.5979, Accuracy:0.2448, Validation Loss:1.6002, Validation Accuracy:0.2299\n",
    "Epoch #91: Loss:1.5973, Accuracy:0.2390, Validation Loss:1.5998, Validation Accuracy:0.2430\n",
    "Epoch #92: Loss:1.5970, Accuracy:0.2419, Validation Loss:1.5991, Validation Accuracy:0.2315\n",
    "Epoch #93: Loss:1.5965, Accuracy:0.2390, Validation Loss:1.5986, Validation Accuracy:0.2282\n",
    "Epoch #94: Loss:1.5964, Accuracy:0.2448, Validation Loss:1.5987, Validation Accuracy:0.2299\n",
    "Epoch #95: Loss:1.5962, Accuracy:0.2435, Validation Loss:1.5988, Validation Accuracy:0.2315\n",
    "Epoch #96: Loss:1.5962, Accuracy:0.2394, Validation Loss:1.5988, Validation Accuracy:0.2348\n",
    "Epoch #97: Loss:1.5955, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2348\n",
    "Epoch #98: Loss:1.5964, Accuracy:0.2419, Validation Loss:1.5987, Validation Accuracy:0.2282\n",
    "Epoch #99: Loss:1.5964, Accuracy:0.2435, Validation Loss:1.5983, Validation Accuracy:0.2348\n",
    "Epoch #100: Loss:1.5969, Accuracy:0.2407, Validation Loss:1.6005, Validation Accuracy:0.2447\n",
    "Epoch #101: Loss:1.5971, Accuracy:0.2444, Validation Loss:1.5999, Validation Accuracy:0.2233\n",
    "Epoch #102: Loss:1.5999, Accuracy:0.2439, Validation Loss:1.6212, Validation Accuracy:0.2003\n",
    "Epoch #103: Loss:1.6190, Accuracy:0.1996, Validation Loss:1.6166, Validation Accuracy:0.2299\n",
    "Epoch #104: Loss:1.6099, Accuracy:0.2320, Validation Loss:1.6080, Validation Accuracy:0.2315\n",
    "Epoch #105: Loss:1.6112, Accuracy:0.2251, Validation Loss:1.6074, Validation Accuracy:0.2365\n",
    "Epoch #106: Loss:1.6051, Accuracy:0.2337, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #107: Loss:1.6044, Accuracy:0.2333, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #108: Loss:1.6065, Accuracy:0.2271, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #109: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #110: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #111: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #112: Loss:1.6051, Accuracy:0.2341, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #113: Loss:1.6049, Accuracy:0.2341, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #114: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #115: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #116: Loss:1.6042, Accuracy:0.2337, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #117: Loss:1.6041, Accuracy:0.2370, Validation Loss:1.6041, Validation Accuracy:0.2447\n",
    "Epoch #118: Loss:1.6039, Accuracy:0.2378, Validation Loss:1.6039, Validation Accuracy:0.2447\n",
    "Epoch #119: Loss:1.6037, Accuracy:0.2337, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #120: Loss:1.6036, Accuracy:0.2337, Validation Loss:1.6033, Validation Accuracy:0.2447\n",
    "Epoch #121: Loss:1.6027, Accuracy:0.2378, Validation Loss:1.6030, Validation Accuracy:0.2463\n",
    "Epoch #122: Loss:1.6024, Accuracy:0.2370, Validation Loss:1.6028, Validation Accuracy:0.2397\n",
    "Epoch #123: Loss:1.6022, Accuracy:0.2382, Validation Loss:1.6024, Validation Accuracy:0.2430\n",
    "Epoch #124: Loss:1.6015, Accuracy:0.2407, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #125: Loss:1.6053, Accuracy:0.2349, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #126: Loss:1.6039, Accuracy:0.2341, Validation Loss:1.6034, Validation Accuracy:0.2447\n",
    "Epoch #127: Loss:1.6027, Accuracy:0.2378, Validation Loss:1.6028, Validation Accuracy:0.2315\n",
    "Epoch #128: Loss:1.6023, Accuracy:0.2435, Validation Loss:1.6026, Validation Accuracy:0.2315\n",
    "Epoch #129: Loss:1.6017, Accuracy:0.2394, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #130: Loss:1.6015, Accuracy:0.2382, Validation Loss:1.6024, Validation Accuracy:0.2447\n",
    "Epoch #131: Loss:1.6012, Accuracy:0.2386, Validation Loss:1.6021, Validation Accuracy:0.2447\n",
    "Epoch #132: Loss:1.6005, Accuracy:0.2398, Validation Loss:1.6020, Validation Accuracy:0.2332\n",
    "Epoch #133: Loss:1.6004, Accuracy:0.2435, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #134: Loss:1.6002, Accuracy:0.2435, Validation Loss:1.6016, Validation Accuracy:0.2266\n",
    "Epoch #135: Loss:1.5997, Accuracy:0.2435, Validation Loss:1.6016, Validation Accuracy:0.2299\n",
    "Epoch #136: Loss:1.5997, Accuracy:0.2427, Validation Loss:1.6014, Validation Accuracy:0.2414\n",
    "Epoch #137: Loss:1.5993, Accuracy:0.2435, Validation Loss:1.6014, Validation Accuracy:0.2250\n",
    "Epoch #138: Loss:1.5992, Accuracy:0.2423, Validation Loss:1.6006, Validation Accuracy:0.2184\n",
    "Epoch #139: Loss:1.5990, Accuracy:0.2374, Validation Loss:1.6009, Validation Accuracy:0.2299\n",
    "Epoch #140: Loss:1.5989, Accuracy:0.2357, Validation Loss:1.6004, Validation Accuracy:0.2299\n",
    "Epoch #141: Loss:1.5989, Accuracy:0.2374, Validation Loss:1.6009, Validation Accuracy:0.2151\n",
    "Epoch #142: Loss:1.5985, Accuracy:0.2378, Validation Loss:1.6005, Validation Accuracy:0.2332\n",
    "Epoch #143: Loss:1.5989, Accuracy:0.2357, Validation Loss:1.6009, Validation Accuracy:0.2447\n",
    "Epoch #144: Loss:1.5979, Accuracy:0.2398, Validation Loss:1.6009, Validation Accuracy:0.2118\n",
    "Epoch #145: Loss:1.5992, Accuracy:0.2407, Validation Loss:1.6009, Validation Accuracy:0.2266\n",
    "Epoch #146: Loss:1.6006, Accuracy:0.2386, Validation Loss:1.6036, Validation Accuracy:0.2463\n",
    "Epoch #147: Loss:1.6023, Accuracy:0.2349, Validation Loss:1.6035, Validation Accuracy:0.2167\n",
    "Epoch #148: Loss:1.5991, Accuracy:0.2402, Validation Loss:1.6023, Validation Accuracy:0.2348\n",
    "Epoch #149: Loss:1.5993, Accuracy:0.2361, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #150: Loss:1.5987, Accuracy:0.2361, Validation Loss:1.6018, Validation Accuracy:0.2315\n",
    "Epoch #151: Loss:1.5983, Accuracy:0.2415, Validation Loss:1.6018, Validation Accuracy:0.2151\n",
    "Epoch #152: Loss:1.5979, Accuracy:0.2407, Validation Loss:1.6012, Validation Accuracy:0.2315\n",
    "Epoch #153: Loss:1.5982, Accuracy:0.2419, Validation Loss:1.6015, Validation Accuracy:0.2299\n",
    "Epoch #154: Loss:1.5976, Accuracy:0.2431, Validation Loss:1.6009, Validation Accuracy:0.2315\n",
    "Epoch #155: Loss:1.5984, Accuracy:0.2419, Validation Loss:1.6008, Validation Accuracy:0.2184\n",
    "Epoch #156: Loss:1.5979, Accuracy:0.2431, Validation Loss:1.6007, Validation Accuracy:0.2315\n",
    "Epoch #157: Loss:1.5983, Accuracy:0.2419, Validation Loss:1.6010, Validation Accuracy:0.2315\n",
    "Epoch #158: Loss:1.5980, Accuracy:0.2448, Validation Loss:1.6000, Validation Accuracy:0.2266\n",
    "Epoch #159: Loss:1.5981, Accuracy:0.2448, Validation Loss:1.5995, Validation Accuracy:0.2282\n",
    "Epoch #160: Loss:1.5981, Accuracy:0.2435, Validation Loss:1.5998, Validation Accuracy:0.2332\n",
    "Epoch #161: Loss:1.5978, Accuracy:0.2439, Validation Loss:1.6002, Validation Accuracy:0.2315\n",
    "Epoch #162: Loss:1.5977, Accuracy:0.2435, Validation Loss:1.6000, Validation Accuracy:0.2299\n",
    "Epoch #163: Loss:1.5979, Accuracy:0.2464, Validation Loss:1.6003, Validation Accuracy:0.2315\n",
    "Epoch #164: Loss:1.5985, Accuracy:0.2439, Validation Loss:1.6002, Validation Accuracy:0.2315\n",
    "Epoch #165: Loss:1.5982, Accuracy:0.2460, Validation Loss:1.6002, Validation Accuracy:0.2299\n",
    "Epoch #166: Loss:1.5976, Accuracy:0.2444, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #167: Loss:1.5973, Accuracy:0.2456, Validation Loss:1.6000, Validation Accuracy:0.2315\n",
    "Epoch #168: Loss:1.5971, Accuracy:0.2464, Validation Loss:1.6000, Validation Accuracy:0.2332\n",
    "Epoch #169: Loss:1.5973, Accuracy:0.2460, Validation Loss:1.5998, Validation Accuracy:0.2315\n",
    "Epoch #170: Loss:1.5970, Accuracy:0.2456, Validation Loss:1.5995, Validation Accuracy:0.2332\n",
    "Epoch #171: Loss:1.5975, Accuracy:0.2419, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #172: Loss:1.5976, Accuracy:0.2407, Validation Loss:1.6021, Validation Accuracy:0.2233\n",
    "Epoch #173: Loss:1.5984, Accuracy:0.2382, Validation Loss:1.6083, Validation Accuracy:0.2200\n",
    "Epoch #174: Loss:1.6021, Accuracy:0.2324, Validation Loss:1.6042, Validation Accuracy:0.2135\n",
    "Epoch #175: Loss:1.5985, Accuracy:0.2423, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #176: Loss:1.5993, Accuracy:0.2386, Validation Loss:1.6022, Validation Accuracy:0.2348\n",
    "Epoch #177: Loss:1.5977, Accuracy:0.2402, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #178: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #179: Loss:1.5975, Accuracy:0.2439, Validation Loss:1.6013, Validation Accuracy:0.2365\n",
    "Epoch #180: Loss:1.5969, Accuracy:0.2435, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #181: Loss:1.5969, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2365\n",
    "Epoch #182: Loss:1.5967, Accuracy:0.2431, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #183: Loss:1.5970, Accuracy:0.2407, Validation Loss:1.6010, Validation Accuracy:0.2299\n",
    "Epoch #184: Loss:1.5970, Accuracy:0.2411, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #185: Loss:1.5966, Accuracy:0.2435, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #186: Loss:1.5967, Accuracy:0.2427, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #187: Loss:1.5969, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2332\n",
    "Epoch #188: Loss:1.5965, Accuracy:0.2427, Validation Loss:1.6003, Validation Accuracy:0.2332\n",
    "Epoch #189: Loss:1.5964, Accuracy:0.2468, Validation Loss:1.6003, Validation Accuracy:0.2348\n",
    "Epoch #190: Loss:1.5969, Accuracy:0.2431, Validation Loss:1.6002, Validation Accuracy:0.2348\n",
    "Epoch #191: Loss:1.5965, Accuracy:0.2448, Validation Loss:1.6001, Validation Accuracy:0.2348\n",
    "Epoch #192: Loss:1.5969, Accuracy:0.2452, Validation Loss:1.6006, Validation Accuracy:0.2332\n",
    "Epoch #193: Loss:1.5964, Accuracy:0.2456, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #194: Loss:1.5961, Accuracy:0.2448, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #195: Loss:1.5962, Accuracy:0.2419, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #196: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.6004, Validation Accuracy:0.2365\n",
    "Epoch #197: Loss:1.5957, Accuracy:0.2394, Validation Loss:1.6007, Validation Accuracy:0.2299\n",
    "Epoch #198: Loss:1.5958, Accuracy:0.2423, Validation Loss:1.6008, Validation Accuracy:0.2332\n",
    "Epoch #199: Loss:1.5959, Accuracy:0.2439, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #200: Loss:1.5956, Accuracy:0.2448, Validation Loss:1.6001, Validation Accuracy:0.2348\n",
    "Epoch #201: Loss:1.5958, Accuracy:0.2448, Validation Loss:1.6007, Validation Accuracy:0.2332\n",
    "Epoch #202: Loss:1.5953, Accuracy:0.2460, Validation Loss:1.6002, Validation Accuracy:0.2332\n",
    "Epoch #203: Loss:1.5958, Accuracy:0.2423, Validation Loss:1.6007, Validation Accuracy:0.2315\n",
    "Epoch #204: Loss:1.5957, Accuracy:0.2452, Validation Loss:1.6003, Validation Accuracy:0.2315\n",
    "Epoch #205: Loss:1.5955, Accuracy:0.2452, Validation Loss:1.5993, Validation Accuracy:0.2282\n",
    "Epoch #206: Loss:1.5953, Accuracy:0.2464, Validation Loss:1.5998, Validation Accuracy:0.2365\n",
    "Epoch #207: Loss:1.5953, Accuracy:0.2476, Validation Loss:1.5991, Validation Accuracy:0.2348\n",
    "Epoch #208: Loss:1.5950, Accuracy:0.2489, Validation Loss:1.5998, Validation Accuracy:0.2397\n",
    "Epoch #209: Loss:1.5948, Accuracy:0.2501, Validation Loss:1.6008, Validation Accuracy:0.2332\n",
    "Epoch #210: Loss:1.5954, Accuracy:0.2460, Validation Loss:1.5992, Validation Accuracy:0.2332\n",
    "Epoch #211: Loss:1.5955, Accuracy:0.2485, Validation Loss:1.5998, Validation Accuracy:0.2332\n",
    "Epoch #212: Loss:1.5962, Accuracy:0.2476, Validation Loss:1.5996, Validation Accuracy:0.2348\n",
    "Epoch #213: Loss:1.5963, Accuracy:0.2460, Validation Loss:1.5998, Validation Accuracy:0.2365\n",
    "Epoch #214: Loss:1.5963, Accuracy:0.2431, Validation Loss:1.6007, Validation Accuracy:0.2299\n",
    "Epoch #215: Loss:1.5973, Accuracy:0.2423, Validation Loss:1.5994, Validation Accuracy:0.2332\n",
    "Epoch #216: Loss:1.5974, Accuracy:0.2407, Validation Loss:1.5991, Validation Accuracy:0.2332\n",
    "Epoch #217: Loss:1.5971, Accuracy:0.2423, Validation Loss:1.6008, Validation Accuracy:0.2102\n",
    "Epoch #218: Loss:1.5978, Accuracy:0.2456, Validation Loss:1.6000, Validation Accuracy:0.2151\n",
    "Epoch #219: Loss:1.5977, Accuracy:0.2419, Validation Loss:1.5991, Validation Accuracy:0.2348\n",
    "Epoch #220: Loss:1.5969, Accuracy:0.2398, Validation Loss:1.6002, Validation Accuracy:0.2299\n",
    "Epoch #221: Loss:1.5971, Accuracy:0.2431, Validation Loss:1.5993, Validation Accuracy:0.2299\n",
    "Epoch #222: Loss:1.5966, Accuracy:0.2435, Validation Loss:1.5997, Validation Accuracy:0.2332\n",
    "Epoch #223: Loss:1.5969, Accuracy:0.2378, Validation Loss:1.5992, Validation Accuracy:0.2365\n",
    "Epoch #224: Loss:1.5961, Accuracy:0.2386, Validation Loss:1.6001, Validation Accuracy:0.2299\n",
    "Epoch #225: Loss:1.5971, Accuracy:0.2439, Validation Loss:1.5997, Validation Accuracy:0.2348\n",
    "Epoch #226: Loss:1.5968, Accuracy:0.2398, Validation Loss:1.5992, Validation Accuracy:0.2365\n",
    "Epoch #227: Loss:1.5966, Accuracy:0.2411, Validation Loss:1.5996, Validation Accuracy:0.2348\n",
    "Epoch #228: Loss:1.5965, Accuracy:0.2423, Validation Loss:1.5992, Validation Accuracy:0.2315\n",
    "Epoch #229: Loss:1.5969, Accuracy:0.2402, Validation Loss:1.5987, Validation Accuracy:0.2348\n",
    "Epoch #230: Loss:1.5959, Accuracy:0.2448, Validation Loss:1.5980, Validation Accuracy:0.2381\n",
    "Epoch #231: Loss:1.5963, Accuracy:0.2452, Validation Loss:1.5982, Validation Accuracy:0.2463\n",
    "Epoch #232: Loss:1.5958, Accuracy:0.2415, Validation Loss:1.5995, Validation Accuracy:0.2463\n",
    "Epoch #233: Loss:1.5960, Accuracy:0.2394, Validation Loss:1.5991, Validation Accuracy:0.2348\n",
    "Epoch #234: Loss:1.5956, Accuracy:0.2472, Validation Loss:1.5989, Validation Accuracy:0.2233\n",
    "Epoch #235: Loss:1.5956, Accuracy:0.2485, Validation Loss:1.5981, Validation Accuracy:0.2397\n",
    "Epoch #236: Loss:1.5956, Accuracy:0.2444, Validation Loss:1.5982, Validation Accuracy:0.2348\n",
    "Epoch #237: Loss:1.5965, Accuracy:0.2431, Validation Loss:1.5987, Validation Accuracy:0.2430\n",
    "Epoch #238: Loss:1.5961, Accuracy:0.2444, Validation Loss:1.5979, Validation Accuracy:0.2315\n",
    "Epoch #239: Loss:1.5959, Accuracy:0.2435, Validation Loss:1.5976, Validation Accuracy:0.2365\n",
    "Epoch #240: Loss:1.5948, Accuracy:0.2468, Validation Loss:1.5989, Validation Accuracy:0.2381\n",
    "Epoch #241: Loss:1.5952, Accuracy:0.2456, Validation Loss:1.5987, Validation Accuracy:0.2381\n",
    "Epoch #242: Loss:1.5944, Accuracy:0.2497, Validation Loss:1.5984, Validation Accuracy:0.2233\n",
    "Epoch #243: Loss:1.5939, Accuracy:0.2476, Validation Loss:1.5983, Validation Accuracy:0.2332\n",
    "Epoch #244: Loss:1.5941, Accuracy:0.2464, Validation Loss:1.5976, Validation Accuracy:0.2430\n",
    "Epoch #245: Loss:1.5943, Accuracy:0.2448, Validation Loss:1.5975, Validation Accuracy:0.2397\n",
    "Epoch #246: Loss:1.5938, Accuracy:0.2472, Validation Loss:1.5992, Validation Accuracy:0.2365\n",
    "Epoch #247: Loss:1.5939, Accuracy:0.2468, Validation Loss:1.5986, Validation Accuracy:0.2381\n",
    "Epoch #248: Loss:1.5950, Accuracy:0.2439, Validation Loss:1.5994, Validation Accuracy:0.2266\n",
    "Epoch #249: Loss:1.5934, Accuracy:0.2431, Validation Loss:1.6004, Validation Accuracy:0.2266\n",
    "Epoch #250: Loss:1.5957, Accuracy:0.2423, Validation Loss:1.6000, Validation Accuracy:0.2479\n",
    "Epoch #251: Loss:1.5961, Accuracy:0.2349, Validation Loss:1.5992, Validation Accuracy:0.2118\n",
    "Epoch #252: Loss:1.5969, Accuracy:0.2464, Validation Loss:1.6005, Validation Accuracy:0.2250\n",
    "Epoch #253: Loss:1.5961, Accuracy:0.2485, Validation Loss:1.6009, Validation Accuracy:0.2167\n",
    "Epoch #254: Loss:1.5960, Accuracy:0.2464, Validation Loss:1.6015, Validation Accuracy:0.2200\n",
    "Epoch #255: Loss:1.5967, Accuracy:0.2439, Validation Loss:1.6015, Validation Accuracy:0.2397\n",
    "Epoch #256: Loss:1.5964, Accuracy:0.2534, Validation Loss:1.6019, Validation Accuracy:0.2200\n",
    "Epoch #257: Loss:1.5953, Accuracy:0.2472, Validation Loss:1.6014, Validation Accuracy:0.2200\n",
    "Epoch #258: Loss:1.5961, Accuracy:0.2427, Validation Loss:1.6030, Validation Accuracy:0.2217\n",
    "Epoch #259: Loss:1.5968, Accuracy:0.2480, Validation Loss:1.6026, Validation Accuracy:0.2184\n",
    "Epoch #260: Loss:1.5959, Accuracy:0.2353, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #261: Loss:1.5953, Accuracy:0.2513, Validation Loss:1.6017, Validation Accuracy:0.2200\n",
    "Epoch #262: Loss:1.5953, Accuracy:0.2435, Validation Loss:1.5996, Validation Accuracy:0.2414\n",
    "Epoch #263: Loss:1.5944, Accuracy:0.2509, Validation Loss:1.5990, Validation Accuracy:0.2414\n",
    "Epoch #264: Loss:1.5947, Accuracy:0.2530, Validation Loss:1.5996, Validation Accuracy:0.2184\n",
    "Epoch #265: Loss:1.5952, Accuracy:0.2452, Validation Loss:1.5986, Validation Accuracy:0.2299\n",
    "Epoch #266: Loss:1.5948, Accuracy:0.2468, Validation Loss:1.5992, Validation Accuracy:0.2266\n",
    "Epoch #267: Loss:1.5942, Accuracy:0.2530, Validation Loss:1.6001, Validation Accuracy:0.2135\n",
    "Epoch #268: Loss:1.5951, Accuracy:0.2509, Validation Loss:1.5988, Validation Accuracy:0.2167\n",
    "Epoch #269: Loss:1.5948, Accuracy:0.2444, Validation Loss:1.5999, Validation Accuracy:0.2365\n",
    "Epoch #270: Loss:1.5952, Accuracy:0.2460, Validation Loss:1.5986, Validation Accuracy:0.2397\n",
    "Epoch #271: Loss:1.5955, Accuracy:0.2468, Validation Loss:1.6004, Validation Accuracy:0.2184\n",
    "Epoch #272: Loss:1.5953, Accuracy:0.2468, Validation Loss:1.6005, Validation Accuracy:0.2365\n",
    "Epoch #273: Loss:1.5950, Accuracy:0.2444, Validation Loss:1.6001, Validation Accuracy:0.2562\n",
    "Epoch #274: Loss:1.5951, Accuracy:0.2460, Validation Loss:1.5996, Validation Accuracy:0.2151\n",
    "Epoch #275: Loss:1.5947, Accuracy:0.2468, Validation Loss:1.5991, Validation Accuracy:0.2250\n",
    "Epoch #276: Loss:1.5938, Accuracy:0.2530, Validation Loss:1.5993, Validation Accuracy:0.2217\n",
    "Epoch #277: Loss:1.5945, Accuracy:0.2423, Validation Loss:1.5994, Validation Accuracy:0.2184\n",
    "Epoch #278: Loss:1.5932, Accuracy:0.2464, Validation Loss:1.6003, Validation Accuracy:0.2250\n",
    "Epoch #279: Loss:1.5936, Accuracy:0.2517, Validation Loss:1.6007, Validation Accuracy:0.2250\n",
    "Epoch #280: Loss:1.5932, Accuracy:0.2513, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #281: Loss:1.5934, Accuracy:0.2464, Validation Loss:1.6000, Validation Accuracy:0.2365\n",
    "Epoch #282: Loss:1.5930, Accuracy:0.2452, Validation Loss:1.5997, Validation Accuracy:0.2578\n",
    "Epoch #283: Loss:1.5933, Accuracy:0.2480, Validation Loss:1.6001, Validation Accuracy:0.2578\n",
    "Epoch #284: Loss:1.5939, Accuracy:0.2509, Validation Loss:1.6004, Validation Accuracy:0.2381\n",
    "Epoch #285: Loss:1.5941, Accuracy:0.2407, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #286: Loss:1.5933, Accuracy:0.2460, Validation Loss:1.6009, Validation Accuracy:0.2545\n",
    "Epoch #287: Loss:1.5945, Accuracy:0.2468, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #288: Loss:1.5941, Accuracy:0.2509, Validation Loss:1.6011, Validation Accuracy:0.2233\n",
    "Epoch #289: Loss:1.5934, Accuracy:0.2513, Validation Loss:1.6005, Validation Accuracy:0.2233\n",
    "Epoch #290: Loss:1.5940, Accuracy:0.2444, Validation Loss:1.6004, Validation Accuracy:0.2381\n",
    "Epoch #291: Loss:1.5930, Accuracy:0.2501, Validation Loss:1.5998, Validation Accuracy:0.2381\n",
    "Epoch #292: Loss:1.5934, Accuracy:0.2517, Validation Loss:1.5994, Validation Accuracy:0.2200\n",
    "Epoch #293: Loss:1.5928, Accuracy:0.2517, Validation Loss:1.6013, Validation Accuracy:0.2200\n",
    "Epoch #294: Loss:1.5933, Accuracy:0.2501, Validation Loss:1.5997, Validation Accuracy:0.2562\n",
    "Epoch #295: Loss:1.5928, Accuracy:0.2534, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #296: Loss:1.5933, Accuracy:0.2493, Validation Loss:1.5998, Validation Accuracy:0.2430\n",
    "Epoch #297: Loss:1.5931, Accuracy:0.2480, Validation Loss:1.5993, Validation Accuracy:0.2562\n",
    "Epoch #298: Loss:1.5930, Accuracy:0.2513, Validation Loss:1.5993, Validation Accuracy:0.2578\n",
    "Epoch #299: Loss:1.5926, Accuracy:0.2501, Validation Loss:1.6004, Validation Accuracy:0.2562\n",
    "Epoch #300: Loss:1.5921, Accuracy:0.2526, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "\n",
    "Test:\n",
    "Test Loss:1.59986639, Accuracy:0.2381\n",
    "Labels: ['05', '03', '01', '02', '04']\n",
    "Confusion Matrix:\n",
    "       05  03  01  02  04\n",
    "t:05  121   7   0   0  14\n",
    "t:03   96   5   0   0  14\n",
    "t:01  109   7   0   0  10\n",
    "t:02   89   9   0   0  16\n",
    "t:04   87   6   0   0  19\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.24      0.85      0.38       142\n",
    "          03       0.15      0.04      0.07       115\n",
    "          01       0.00      0.00      0.00       126\n",
    "          02       0.00      0.00      0.00       114\n",
    "          04       0.26      0.17      0.21       112\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.13      0.21      0.13       609\n",
    "weighted avg       0.13      0.24      0.14       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 17:16:11 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6112642290165466, 1.6067858699507314, 1.6055993614917123, 1.6054098103042502, 1.6053194116880545, 1.6045746012470015, 1.6043386786246339, 1.6041762918870046, 1.6042919769662942, 1.6046461835870602, 1.604095073756326, 1.6040073053785928, 1.6040658592590558, 1.6034057022902766, 1.6031810540479587, 1.6036953256635242, 1.6030101237821657, 1.6024312029526935, 1.6019477969515696, 1.601478504821389, 1.6013796423456352, 1.601281679518313, 1.6011598611308633, 1.6011123843185224, 1.60091583403852, 1.6007915465115325, 1.6028295295383348, 1.60280715401341, 1.601700611106672, 1.601871112884559, 1.6021140271611205, 1.6050780835410057, 1.6032064209626422, 1.6030458816753819, 1.6030633285127838, 1.6018657020747369, 1.601889129734196, 1.6010805045442629, 1.6010092784618508, 1.6009299024964005, 1.600718953926575, 1.6007619755608695, 1.600455905024837, 1.6005105418328973, 1.6001265961156885, 1.6003226482222233, 1.6002375731131517, 1.5996769061816738, 1.5990890233191755, 1.5990044127152667, 1.5992919760580329, 1.599434568376964, 1.5996627267358339, 1.599551240015891, 1.5997474796470554, 1.5997403944262927, 1.599735999733748, 1.5994358665641697, 1.5993867497921772, 1.5996416237553939, 1.5995814398983232, 1.5992819528861586, 1.5992062630128783, 1.5987947498049055, 1.5993079134983381, 1.598166876434301, 1.597792927463262, 1.5998064659303437, 1.6003956240777704, 1.598919923669599, 1.598623747700345, 1.5989252767343631, 1.598416303766185, 1.5988517473092416, 1.5989317586660776, 1.5991218628358763, 1.5987054826003577, 1.5991081126609263, 1.598741021453845, 1.5991702721819698, 1.598754311430043, 1.5989221193520307, 1.598863622629388, 1.5985693802387255, 1.5987154677975157, 1.598714463033504, 1.599879967363793, 1.6002400185674281, 1.600025996981779, 1.6001592332310668, 1.5997586299241666, 1.5990982858222498, 1.5986215868606943, 1.598740601578761, 1.5988464312404638, 1.5988471838836795, 1.5983193607753134, 1.5987460607378354, 1.5982907239243707, 1.6005193130136124, 1.5999002908838207, 1.6212290036071502, 1.616567197691631, 1.6079903557187034, 1.607381981190398, 1.6050407508715425, 1.6036775415558338, 1.6062491754397188, 1.6056094024764689, 1.6052128423023693, 1.6056489157559248, 1.6054759197830175, 1.605223463869643, 1.6051418002211597, 1.6047447307160727, 1.604476106577906, 1.6041457034488422, 1.6039490032274344, 1.6038192156304671, 1.6032715519073562, 1.6029509043654393, 1.6027948276945718, 1.6023502021000302, 1.604036925657238, 1.6049012106236173, 1.6033659272984722, 1.602795285544372, 1.6026202011578188, 1.6022398961197175, 1.6024292656549288, 1.6021493217236498, 1.6020364788756973, 1.6017781214173792, 1.6016120511322773, 1.601630674011406, 1.6014454069200212, 1.6013927673079893, 1.6005796452461205, 1.6008985524107082, 1.600370081970453, 1.6009189955315175, 1.600517842569962, 1.6009015154173025, 1.6008728666258563, 1.6008850921355249, 1.6036105600288153, 1.6035342134278396, 1.6023296546466246, 1.6032937095670277, 1.6017799244334154, 1.6017991580399387, 1.6011897934285682, 1.601527744521844, 1.600864179224412, 1.6008189450735333, 1.600657918574579, 1.6009648727078742, 1.5999669613705088, 1.5995103529912889, 1.5997681191010624, 1.600172953456885, 1.6000231966400773, 1.6002754587649517, 1.6001984840151908, 1.600244314995501, 1.600379057705696, 1.6000197397664262, 1.6000214556755104, 1.5998045251091517, 1.5995298422420359, 1.6011344268795695, 1.602069051394909, 1.6082974883723142, 1.6041891498518694, 1.6007024175036326, 1.602213392899737, 1.59986695280216, 1.6009040462168176, 1.6012501491505915, 1.6003186626387347, 1.600570730387871, 1.600761105665824, 1.6009623753809186, 1.600973928307469, 1.6006831791992062, 1.6005471831276303, 1.600572232346621, 1.600333244734014, 1.600271935924912, 1.600210204696029, 1.6001222084699984, 1.6005757340460967, 1.6005057280482526, 1.6007599961777235, 1.6005683773256876, 1.6003866266147257, 1.6007455620663897, 1.600800170491286, 1.600827816280434, 1.6001088302123723, 1.600661009599031, 1.6002188917059812, 1.600684414160467, 1.600268460063903, 1.5992741068008498, 1.5998431467657606, 1.5991198672058156, 1.5998064635813922, 1.6008286910691285, 1.5992277357574363, 1.599780198192753, 1.5996103584277024, 1.5998493151124475, 1.600746263424164, 1.5994313084237486, 1.599080594889636, 1.6007880428545973, 1.5999524814546207, 1.5991377613227356, 1.6001948188678385, 1.5993019720212187, 1.5997406207086222, 1.5991618263310399, 1.600079766439491, 1.5997360760746722, 1.5992031737501398, 1.5996314435952599, 1.5992167632176566, 1.5986735339235203, 1.5979915700718295, 1.598222461044299, 1.5995255810482356, 1.5990700508377627, 1.5988985760067091, 1.598055416335809, 1.5981931050226998, 1.5987477337785543, 1.5978627604216777, 1.5976083715169496, 1.598912527017014, 1.598746569873077, 1.598441115741072, 1.5982887973926339, 1.597573112971677, 1.5975488130682207, 1.5991847168635853, 1.598630433012112, 1.5993596272319799, 1.6004007261962139, 1.5999634870754673, 1.5992383213074532, 1.6004738651081454, 1.6008979526450873, 1.6015134875606043, 1.6015297955480114, 1.6018563281922114, 1.601431039166568, 1.6030143814525386, 1.6025762783091253, 1.6014984990967123, 1.6017234343026072, 1.5995771589341814, 1.598952211182693, 1.599602914991833, 1.5985968555331427, 1.5991558891603317, 1.6001298347325943, 1.5988299145878633, 1.5998945514165317, 1.5985777182336316, 1.6004082209175248, 1.6005293657431265, 1.6000843490481573, 1.5995647308274443, 1.5991267772339444, 1.5993458713803972, 1.5994320638074075, 1.6002551062745218, 1.6007445067999202, 1.6003480270774102, 1.6000129943606498, 1.5996927400723662, 1.600079786209833, 1.6003674282424751, 1.5999211209943924, 1.6009410232158716, 1.6007587778548693, 1.6010815284913789, 1.6005142506315986, 1.6003531055105926, 1.5997682220634373, 1.5993966375078474, 1.6013147053851675, 1.5996535701313237, 1.6004431719458945, 1.5998267518671472, 1.5992505047317405, 1.5993265698500259, 1.6003832574352646, 1.5998666219914879], 'val_acc': [0.18883415344607066, 0.23316912969638562, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2446633822763299, 0.24630541849899762, 0.24630541849899762, 0.24466338247207586, 0.2463054185968706, 0.23645320165235617, 0.23152709318009895, 0.24630541849899762, 0.2463054185968706, 0.24630541849899762, 0.23645319967042833, 0.2397372719200178, 0.24137931032035934, 0.2430213441696073, 0.2463054185968706, 0.2463054185968706, 0.24466338029440204, 0.24302134644515408, 0.24302134644515408, 0.2397372741955646, 0.24137931032035934, 0.22988505517124935, 0.22988505517124935, 0.2397372741955646, 0.23645320184810212, 0.23809523807076985, 0.23152709149179004, 0.2298850552691223, 0.23152709149179004, 0.23645319967042833, 0.23316912742083884, 0.22824301914432757, 0.23152709139391706, 0.23152709139391706, 0.23152709139391706, 0.2331691276165848, 0.22824301924220056, 0.22824301924220056, 0.24302134644515408, 0.2298850553669953, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2298850553669953, 0.2331691276165848, 0.23481116374137953, 0.2298850552691223, 0.22003283861822684, 0.22167487474302158, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.22495894689473808, 0.22495894699261107, 0.23152709139391706, 0.2331691276165848, 0.2298850553669953, 0.2298850553669953, 0.22660098301953283, 0.22824301904645458, 0.22988505517124935, 0.2315270912960441, 0.2282430213220014, 0.23645319967042833, 0.2331691276165848, 0.2134646939233019, 0.22988505517124935, 0.24302134644515408, 0.23152709139391706, 0.2282430213220014, 0.22988505744679613, 0.2315270912960441, 0.23481116364350654, 0.23481116364350654, 0.22824301914432757, 0.23481116364350654, 0.24466338029440204, 0.22331691076994334, 0.2003284051206899, 0.2298850571531772, 0.23152709308222597, 0.23645320165235617, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.24466338247207586, 0.24466338247207586, 0.2331691293048937, 0.24466338247207586, 0.24630541849899762, 0.23973727390194566, 0.24302134634728112, 0.23316912940276668, 0.2331691293048937, 0.24466338247207586, 0.2315270934737179, 0.2315270933758449, 0.24302134634728112, 0.24466338247207586, 0.24466338247207586, 0.23316912969638562, 0.23316912969638562, 0.22660098500146067, 0.22988505734892314, 0.2413793101246134, 0.22495894887666593, 0.21839080229768612, 0.22988505744679613, 0.22988505744679613, 0.2151067323236434, 0.23316912742083884, 0.24466338247207586, 0.211822659780435, 0.22660098500146067, 0.2463054185968706, 0.21674876835056517, 0.23481116354563358, 0.23316912742083884, 0.2315270912960441, 0.21510673212789747, 0.23152709357159088, 0.22988505734892314, 0.23152709139391706, 0.2183908044753599, 0.2315270912960441, 0.2315270912960441, 0.22660098509933366, 0.2282430212241284, 0.23316912742083884, 0.23152709357159088, 0.22988505517124935, 0.23152709357159088, 0.2315270912960441, 0.22988505734892314, 0.23316912742083884, 0.2315270912960441, 0.23316912969638562, 0.2315270912960441, 0.2331691275187118, 0.23809523589309606, 0.22331691294761713, 0.2200328404044087, 0.21346469610097568, 0.23481116582118036, 0.23481116582118036, 0.23809523589309606, 0.23316912969638562, 0.23645319976830131, 0.23645319986617427, 0.23645319986617427, 0.23645319986617427, 0.22988505517124935, 0.2331691275187118, 0.23481116364350654, 0.23481116364350654, 0.23316912742083884, 0.23316912742083884, 0.23481116354563358, 0.23481116354563358, 0.23481116354563358, 0.23316912742083884, 0.23481116354563358, 0.23645319976830131, 0.23481116364350654, 0.23645319976830131, 0.22988505517124935, 0.2331691275187118, 0.23481116364350654, 0.23481116364350654, 0.23316912742083884, 0.2331691275187118, 0.2315270912960441, 0.23152709357159088, 0.22824301904645458, 0.23645319976830131, 0.23481116374137953, 0.23973727211576376, 0.23316912742083884, 0.23316912742083884, 0.23316912969638562, 0.23481116354563358, 0.23645319986617427, 0.22988505734892314, 0.23316912742083884, 0.23316912742083884, 0.21018062355776726, 0.2151067319321515, 0.23481116582118036, 0.22988505734892314, 0.22988505734892314, 0.23316912959851263, 0.23645319967042833, 0.22988505734892314, 0.23481116354563358, 0.23645319976830131, 0.23481116364350654, 0.23152709139391706, 0.23481116364350654, 0.23809523797289686, 0.24630541651706978, 0.24630541651706978, 0.2348111656254344, 0.22331691275187118, 0.2397372740976916, 0.23481116354563358, 0.24302134634728112, 0.2315270933758449, 0.2364532019459751, 0.23809523589309606, 0.23809523589309606, 0.22331691294761713, 0.23316912969638562, 0.2430213442674803, 0.2397372719200178, 0.23645319967042833, 0.23809523579522307, 0.22660098509933366, 0.22660098480571472, 0.2479474548195383, 0.21182265987830795, 0.2249589467968651, 0.21674876795907325, 0.22003284020866276, 0.23973727390194566, 0.22003284030653572, 0.2200328404044087, 0.2216748763334575, 0.21839080408386802, 0.23645320175022916, 0.22003284020866276, 0.2413793101246134, 0.24137931022248635, 0.21839080418174098, 0.22988505734892314, 0.22660098500146067, 0.21346469570948376, 0.21674876795907325, 0.23645320165235617, 0.23973727390194566, 0.21839080398599503, 0.23645320135873724, 0.2561576351498931, 0.21510673154065957, 0.224958948485174, 0.22167487613771153, 0.21839080388812204, 0.224958948485174, 0.224958948583047, 0.23645320155448318, 0.23645320165235617, 0.25779967127468784, 0.2577996713725608, 0.23809523777715091, 0.23809523777715091, 0.25451559902509835, 0.23645320175022916, 0.2233169125561252, 0.22331691245825225, 0.23809523777715091, 0.23809523758140494, 0.22003284020866276, 0.22003284020866276, 0.2561576352477661, 0.24137930983099445, 0.2430213459557892, 0.2561576351498931, 0.25779967127468784, 0.2561576351498931, 0.23809523777715091], 'loss': [1.615268987160199, 1.6080066500258396, 1.6052960764945654, 1.6054293639850812, 1.60526471647149, 1.6048380385189331, 1.604514976056939, 1.604206228794748, 1.603963256127046, 1.6042667483402229, 1.6039672258208664, 1.6040062346742383, 1.6037355782314981, 1.6035086850610847, 1.603390774981442, 1.6031542833825645, 1.6030594700415766, 1.6026596488404323, 1.6025207630662703, 1.6025868395270753, 1.602361015666437, 1.6022855937848102, 1.6021995145437409, 1.6016913429178006, 1.6014888989607168, 1.6011334070679588, 1.6006386349578168, 1.6029759874578864, 1.6020798612179452, 1.601590379307647, 1.6013282869385987, 1.6021470278929881, 1.6039615961070912, 1.6018603396366753, 1.6024074992115247, 1.6017558813095092, 1.6012812104802847, 1.6013311088941915, 1.6008112577931837, 1.600347618204857, 1.6004124918023175, 1.6002152400339897, 1.6001511389225171, 1.6000944281995173, 1.5998670052698751, 1.6000935778725562, 1.5999191952437102, 1.5997330756647632, 1.5997729526652937, 1.5996646041498046, 1.5996343826611183, 1.599513851249977, 1.599663219314826, 1.5993314192280388, 1.5991058753011653, 1.5990659479243066, 1.5990824726817545, 1.5990409669934846, 1.5992760961305434, 1.599028357539089, 1.5990758285874949, 1.5990830516423533, 1.5989096219534746, 1.5987856054208116, 1.598468880486929, 1.598588227148663, 1.5985742249772779, 1.5983505032635323, 1.5983953888656177, 1.5988322268766055, 1.598196484224997, 1.5982733525289892, 1.5980759233419901, 1.5983561857525084, 1.5976992709435966, 1.5982248142269848, 1.5978786359822237, 1.597820624140009, 1.5978806868715698, 1.5975519250795336, 1.5978878159787375, 1.5977468699645212, 1.5978539391464766, 1.5972009046366573, 1.5971015011750207, 1.5974918613198845, 1.597410672156473, 1.5978319917126602, 1.5984560147447997, 1.597868073500647, 1.5973168894985128, 1.5969675502737934, 1.5965024052214574, 1.5964016384167838, 1.596156814602611, 1.5961879670497574, 1.595476933426436, 1.596424575997574, 1.596387743117628, 1.596873620403376, 1.5970839217458173, 1.5999092406560753, 1.619009611347128, 1.6099126774672365, 1.6111936902608226, 1.605149588154082, 1.6043659997916564, 1.606511915146203, 1.6058034545831856, 1.6054301490529117, 1.604974873403749, 1.6050892302632576, 1.6048513724818612, 1.604737307697351, 1.6043852506476997, 1.604170539149024, 1.6041109627032426, 1.603893512866825, 1.6037361000597599, 1.6035848599439775, 1.6027349209149027, 1.6023664629679686, 1.6022075417594988, 1.60148581699669, 1.605262682521123, 1.6039359364911026, 1.6026926981105452, 1.602324880319944, 1.6017342685918787, 1.6014518114575615, 1.6011789132437422, 1.6005235511419464, 1.6004461727592734, 1.600240782790605, 1.5996813948883903, 1.5997161385459822, 1.599348167080654, 1.5992481756993633, 1.5990271028307184, 1.5989177444877076, 1.5989330306435023, 1.5984796547057447, 1.5989069761926389, 1.597936981120883, 1.599226515640713, 1.6006335322617016, 1.602308430368161, 1.599055906389773, 1.599327080597378, 1.5986656208547478, 1.5983127187164903, 1.5979384366981302, 1.5981762114001985, 1.5975782002266916, 1.5983805815542014, 1.5978826936020742, 1.5983334785124605, 1.598039503802509, 1.598147004487823, 1.5980736863196998, 1.597799271137073, 1.5976968902337232, 1.5978533236153072, 1.5985463939408258, 1.5982236608587497, 1.597625475987272, 1.5972764829835362, 1.597067433656853, 1.5973198265510418, 1.5970030185378308, 1.5974650939142434, 1.5976238115612242, 1.5983701707401314, 1.602083294837137, 1.598498257229705, 1.5992807814717538, 1.5976918497614303, 1.5971606442570931, 1.5975228921588687, 1.5968997566117398, 1.5969447910418502, 1.5966902348050347, 1.5970034812265352, 1.596960807532011, 1.5965819085158361, 1.5967150484022419, 1.5968508050182273, 1.5965358009573372, 1.5963518271456019, 1.5968945713258622, 1.596465333971889, 1.5969132321081612, 1.596392809585869, 1.5960757538033707, 1.5961520854452553, 1.5960268068117773, 1.5957314969088263, 1.5957634072528972, 1.595867726543356, 1.5956327593057307, 1.5957918604786145, 1.5952651013094297, 1.5958195128724806, 1.5956765384889482, 1.5954766263217652, 1.5953265390356952, 1.5953358905760904, 1.5950439791904583, 1.5948475233583235, 1.595418734276319, 1.5955285423834955, 1.5962238415555543, 1.5962502153502354, 1.5963356354887726, 1.5973364926951132, 1.597357135876493, 1.5971245077845986, 1.597836404561507, 1.597747003860787, 1.5969026907758301, 1.5971347842617936, 1.5966479553089494, 1.5968718743666976, 1.5961125195148789, 1.5970806571737206, 1.5968029878467505, 1.5966093514734225, 1.5965233707330064, 1.5968551222548593, 1.5958853234011046, 1.5963108979701017, 1.5957811981745569, 1.5960402641453049, 1.595626894404511, 1.5956313872973777, 1.5955747640597993, 1.596543600280182, 1.596099245278987, 1.5959269819807957, 1.5947522023375273, 1.5952002355939798, 1.594414066387153, 1.5938838836838332, 1.5940507167663418, 1.5943396175176945, 1.5937884198566727, 1.5939027025959085, 1.594986539259094, 1.5934386400226694, 1.5956839718613047, 1.5961058312617777, 1.5968763302483842, 1.5961015017370423, 1.5960252676166793, 1.5966942087091214, 1.5964224854533922, 1.5952925510230251, 1.5960722258448357, 1.5968320651220835, 1.5959364318260176, 1.5953438557148958, 1.5952707578514147, 1.594360151183189, 1.5946686349610284, 1.5951742809655975, 1.5947745372627304, 1.5942294645603188, 1.595074326889226, 1.5948478640961696, 1.5952491794523518, 1.5955360680879753, 1.5952789880901392, 1.5949549629947732, 1.5951286687008899, 1.5946618910442878, 1.5937741413743098, 1.5944993868990356, 1.5932237019039521, 1.5935761244634827, 1.593213172076419, 1.5934156901048193, 1.5929539100834966, 1.5932892502700524, 1.5939455661930342, 1.5940986959841217, 1.5932551351171254, 1.594523854617955, 1.5940548957006153, 1.5933789262536615, 1.5939536764881204, 1.5930022871714598, 1.5933746244383544, 1.5928107633727777, 1.5933232354921971, 1.5928167745562793, 1.5933427557073825, 1.5931300186278639, 1.5930014850667369, 1.5925746656786, 1.5920623506608684], 'acc': [0.18767967035026276, 0.21724845981206248, 0.23572895477929398, 0.23285420992291195, 0.23285421047367355, 0.23285420851541005, 0.23285421010037957, 0.23285420890706277, 0.23285420892542147, 0.23285421164863163, 0.23285421029620593, 0.2328542087112364, 0.23285421049203225, 0.2328542087112364, 0.23285420970872686, 0.23285420912124782, 0.2328542094945418, 0.23285420970872686, 0.2328542087112364, 0.23285421029620593, 0.2328542087112364, 0.23285421051039099, 0.2357289504343969, 0.23696098508776092, 0.2365503074880498, 0.23655030805717014, 0.23819301913528718, 0.24024640742146258, 0.23696098685019804, 0.2369609874376771, 0.23942505099200614, 0.2365503074696911, 0.23860369714500967, 0.23778234169468498, 0.2365503078613438, 0.24024640779475656, 0.23819301755031766, 0.23860369673499826, 0.24353182854348873, 0.2431211499462872, 0.24271047428648085, 0.24106776167847047, 0.23860369536421383, 0.23983572939338135, 0.24188911787538314, 0.24188911711043645, 0.24188911771627422, 0.23942505101036488, 0.24024640544484038, 0.23983572900172867, 0.2406570834545629, 0.24435318162553854, 0.24065708365038924, 0.23942505218532295, 0.2398357299808604, 0.23942505138365885, 0.2398357303725131, 0.23901437298228365, 0.23901437396141537, 0.2357289518602575, 0.2402464070298099, 0.2377823401280742, 0.2410677626576022, 0.24188911728790408, 0.24065708445205336, 0.24024640742146258, 0.24024640605067815, 0.2390143743714268, 0.23613963045745903, 0.2402464068156248, 0.24024640761728894, 0.24024640720727752, 0.23819301794197034, 0.23778234032390053, 0.23860369536421383, 0.2418891180895682, 0.24188911728790408, 0.24024640722563625, 0.2394250523811493, 0.2398357295892077, 0.2386036955783989, 0.23819301911692844, 0.23983572999921912, 0.23901437417560045, 0.23901437417560045, 0.23901437494054711, 0.23572895303521557, 0.24435318203554995, 0.23983572921591373, 0.24476386198517722, 0.23901437339229506, 0.24188911789374185, 0.23901437478143822, 0.24476386004527248, 0.2435318263893989, 0.2394250511878325, 0.2418891173062628, 0.24188911650459868, 0.2435318263893989, 0.24065708521700002, 0.2443531818213649, 0.2439425063573849, 0.1995893224492455, 0.23203285525589262, 0.22505133435956262, 0.2336755637515497, 0.23326488752262303, 0.22710472326993453, 0.2328542091028891, 0.23285421086532623, 0.2328542091028891, 0.234086243327883, 0.23408624391536204, 0.23285420912124782, 0.23285420931707418, 0.2336755637515497, 0.23696098685019804, 0.23778234149885863, 0.2336755641615611, 0.23367556531816047, 0.23778234130303227, 0.2369609866727304, 0.2381930189211021, 0.24065708365038924, 0.23490759779907594, 0.2340862425629363, 0.23778234151721736, 0.24353182932679412, 0.23942505020870075, 0.23819301952693986, 0.23860369694918332, 0.23983572802259692, 0.2435318271727043, 0.2435318263893989, 0.24353182893514144, 0.24271047174073831, 0.2435318263893989, 0.2422997941226685, 0.23737166211835167, 0.23572895203772512, 0.23737166446826785, 0.2377823397364215, 0.23572895342686828, 0.23983572960756644, 0.24065708443369463, 0.23860369657588934, 0.23490759915150167, 0.24024640722563625, 0.23613963004744762, 0.2361396306532854, 0.24147844084479236, 0.24065708464787972, 0.241889116112946, 0.2431211503562986, 0.24188911632713106, 0.2431211499462872, 0.2418891173062628, 0.2447638602227401, 0.2447638610060455, 0.2435318279560097, 0.24394250578826457, 0.2435318287393151, 0.24640657147832476, 0.2439425054149706, 0.24599589268529684, 0.2443531837979871, 0.245585214479748, 0.24640657069501937, 0.24599589407444, 0.24558521526305338, 0.24188911750208916, 0.2406570830629102, 0.23819301794197034, 0.23244353287396244, 0.2422997941226685, 0.23860369614751922, 0.24024640701145117, 0.24271047312988148, 0.2439425053966119, 0.24353182952262048, 0.24271047332570783, 0.24312114955463449, 0.24065708619613177, 0.24106776185593812, 0.24353182776018334, 0.2427104715632707, 0.24271047254240244, 0.24271047174073831, 0.24681724909639458, 0.24312115033793988, 0.2447638602227401, 0.2451745374491572, 0.24558521722131685, 0.24476386002691375, 0.2418891176979155, 0.24394250420329508, 0.23942505255861693, 0.24229979373101582, 0.24394250576990587, 0.2447638606143928, 0.2447638614160569, 0.24599589386025494, 0.2422997945143212, 0.2451745384650064, 0.24517453960324703, 0.24640657245745648, 0.24763860354922881, 0.24887063720510236, 0.2501026700409531, 0.2459958932911346, 0.2484599585895421, 0.2476386051158396, 0.24599589268529684, 0.2431211503562986, 0.24229979510180025, 0.24065708325873655, 0.24229979373101582, 0.24558521665219654, 0.241889116112946, 0.23983572861007596, 0.24312114878968782, 0.24353182699523668, 0.23778234190887004, 0.23860369616587793, 0.24394250559243824, 0.239835729197555, 0.24106776207012318, 0.24229979471014756, 0.2402464062465045, 0.24476386218100357, 0.24517453805499498, 0.24147843966983426, 0.2394250511878325, 0.24722792692864945, 0.24845996054780556, 0.24435318221301758, 0.24312114896715545, 0.2443531830146817, 0.24353182756435698, 0.24681724870474187, 0.24558521586889115, 0.249691993402015, 0.24763860354922881, 0.2464065720658038, 0.2447638606143928, 0.24722792651863804, 0.24681724733395743, 0.2439425055740795, 0.24312115016047225, 0.2422997953159853, 0.23490759660575913, 0.24640657149668346, 0.24845995917702113, 0.2464065708908457, 0.2439425043991214, 0.253388091768817, 0.2472279255395063, 0.24271047232821738, 0.24804928236061544, 0.23531827482966672, 0.25133470031270255, 0.24353182893514144, 0.2509240235146556, 0.2529774129757891, 0.245174539015768, 0.24681724772561012, 0.2529774121924837, 0.2509240260603981, 0.2443531822313763, 0.24599589268529684, 0.24681724792143647, 0.2468172492922209, 0.24435318338797568, 0.24599589427026636, 0.24681724833144789, 0.25297741358162684, 0.2422997953159853, 0.24640657030336666, 0.25174538013993836, 0.25133470172020445, 0.24640656971588762, 0.24517453565000263, 0.24804928077564592, 0.25092402529545144, 0.24065708600030541, 0.24599589268529684, 0.24681724772561012, 0.25092402429796096, 0.2513347028951625, 0.24435318203554995, 0.2501026712159112, 0.2517453807090587, 0.2517453818840168, 0.2501026684743423, 0.2533880898105535, 0.24928131304237633, 0.24804928253808306, 0.2513347009001816, 0.2501026692576477, 0.252566735749372]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
