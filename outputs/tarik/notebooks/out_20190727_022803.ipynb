{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf24.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 02:28:03 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '2Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001E8003E8128>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001E855E87EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1027, Accuracy:0.3043, Validation Loss:1.0803, Validation Accuracy:0.3547\n",
    "Epoch #2: Loss:1.0757, Accuracy:0.4000, Validation Loss:1.0766, Validation Accuracy:0.3678\n",
    "Epoch #3: Loss:1.0734, Accuracy:0.4045, Validation Loss:1.0775, Validation Accuracy:0.3826\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3996, Validation Loss:1.0769, Validation Accuracy:0.3859\n",
    "Epoch #5: Loss:1.0746, Accuracy:0.3930, Validation Loss:1.0764, Validation Accuracy:0.3727\n",
    "Epoch #6: Loss:1.0740, Accuracy:0.4004, Validation Loss:1.0760, Validation Accuracy:0.3530\n",
    "Epoch #7: Loss:1.0735, Accuracy:0.3992, Validation Loss:1.0756, Validation Accuracy:0.3563\n",
    "Epoch #8: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0753, Validation Accuracy:0.3908\n",
    "Epoch #9: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #10: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #11: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #13: Loss:1.0734, Accuracy:0.4000, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #14: Loss:1.0733, Accuracy:0.4045, Validation Loss:1.0748, Validation Accuracy:0.3777\n",
    "Epoch #15: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #16: Loss:1.0731, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #17: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.3760\n",
    "Epoch #18: Loss:1.0729, Accuracy:0.4057, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #19: Loss:1.0728, Accuracy:0.4041, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #20: Loss:1.0729, Accuracy:0.4041, Validation Loss:1.0756, Validation Accuracy:0.3810\n",
    "Epoch #21: Loss:1.0729, Accuracy:0.4021, Validation Loss:1.0756, Validation Accuracy:0.3810\n",
    "Epoch #22: Loss:1.0728, Accuracy:0.4053, Validation Loss:1.0757, Validation Accuracy:0.3760\n",
    "Epoch #23: Loss:1.0727, Accuracy:0.4000, Validation Loss:1.0757, Validation Accuracy:0.3760\n",
    "Epoch #24: Loss:1.0726, Accuracy:0.4074, Validation Loss:1.0759, Validation Accuracy:0.3760\n",
    "Epoch #25: Loss:1.0726, Accuracy:0.4074, Validation Loss:1.0761, Validation Accuracy:0.3744\n",
    "Epoch #26: Loss:1.0727, Accuracy:0.4045, Validation Loss:1.0759, Validation Accuracy:0.3777\n",
    "Epoch #27: Loss:1.0723, Accuracy:0.4037, Validation Loss:1.0760, Validation Accuracy:0.3777\n",
    "Epoch #28: Loss:1.0723, Accuracy:0.4049, Validation Loss:1.0762, Validation Accuracy:0.3711\n",
    "Epoch #29: Loss:1.0722, Accuracy:0.4070, Validation Loss:1.0765, Validation Accuracy:0.3695\n",
    "Epoch #30: Loss:1.0720, Accuracy:0.4057, Validation Loss:1.0765, Validation Accuracy:0.3793\n",
    "Epoch #31: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0773, Validation Accuracy:0.3530\n",
    "Epoch #32: Loss:1.0720, Accuracy:0.3988, Validation Loss:1.0774, Validation Accuracy:0.3612\n",
    "Epoch #33: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0774, Validation Accuracy:0.3777\n",
    "Epoch #34: Loss:1.0718, Accuracy:0.4070, Validation Loss:1.0769, Validation Accuracy:0.3793\n",
    "Epoch #35: Loss:1.0718, Accuracy:0.4070, Validation Loss:1.0773, Validation Accuracy:0.3481\n",
    "Epoch #36: Loss:1.0718, Accuracy:0.4037, Validation Loss:1.0773, Validation Accuracy:0.3481\n",
    "Epoch #37: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0768, Validation Accuracy:0.3727\n",
    "Epoch #38: Loss:1.0716, Accuracy:0.4045, Validation Loss:1.0769, Validation Accuracy:0.3777\n",
    "Epoch #39: Loss:1.0713, Accuracy:0.4066, Validation Loss:1.0780, Validation Accuracy:0.3481\n",
    "Epoch #40: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0783, Validation Accuracy:0.3415\n",
    "Epoch #41: Loss:1.0712, Accuracy:0.4131, Validation Loss:1.0783, Validation Accuracy:0.3777\n",
    "Epoch #42: Loss:1.0711, Accuracy:0.4082, Validation Loss:1.0784, Validation Accuracy:0.3596\n",
    "Epoch #43: Loss:1.0708, Accuracy:0.4123, Validation Loss:1.0790, Validation Accuracy:0.3432\n",
    "Epoch #44: Loss:1.0707, Accuracy:0.4057, Validation Loss:1.0789, Validation Accuracy:0.3530\n",
    "Epoch #45: Loss:1.0708, Accuracy:0.4094, Validation Loss:1.0779, Validation Accuracy:0.3612\n",
    "Epoch #46: Loss:1.0701, Accuracy:0.4111, Validation Loss:1.0778, Validation Accuracy:0.3530\n",
    "Epoch #47: Loss:1.0699, Accuracy:0.4078, Validation Loss:1.0790, Validation Accuracy:0.3498\n",
    "Epoch #48: Loss:1.0696, Accuracy:0.4078, Validation Loss:1.0784, Validation Accuracy:0.3662\n",
    "Epoch #49: Loss:1.0699, Accuracy:0.4115, Validation Loss:1.0774, Validation Accuracy:0.3727\n",
    "Epoch #50: Loss:1.0699, Accuracy:0.4107, Validation Loss:1.0776, Validation Accuracy:0.3760\n",
    "Epoch #51: Loss:1.0702, Accuracy:0.4094, Validation Loss:1.0773, Validation Accuracy:0.3695\n",
    "Epoch #52: Loss:1.0714, Accuracy:0.4094, Validation Loss:1.0781, Validation Accuracy:0.3826\n",
    "Epoch #53: Loss:1.0710, Accuracy:0.3959, Validation Loss:1.0791, Validation Accuracy:0.3924\n",
    "Epoch #54: Loss:1.0704, Accuracy:0.4115, Validation Loss:1.0808, Validation Accuracy:0.3727\n",
    "Epoch #55: Loss:1.0711, Accuracy:0.4086, Validation Loss:1.0801, Validation Accuracy:0.3645\n",
    "Epoch #56: Loss:1.0701, Accuracy:0.4090, Validation Loss:1.0789, Validation Accuracy:0.3629\n",
    "Epoch #57: Loss:1.0700, Accuracy:0.4090, Validation Loss:1.0781, Validation Accuracy:0.3629\n",
    "Epoch #58: Loss:1.0696, Accuracy:0.4148, Validation Loss:1.0790, Validation Accuracy:0.3645\n",
    "Epoch #59: Loss:1.0698, Accuracy:0.4160, Validation Loss:1.0794, Validation Accuracy:0.3645\n",
    "Epoch #60: Loss:1.0691, Accuracy:0.4172, Validation Loss:1.0798, Validation Accuracy:0.3645\n",
    "Epoch #61: Loss:1.0692, Accuracy:0.4103, Validation Loss:1.0816, Validation Accuracy:0.3645\n",
    "Epoch #62: Loss:1.0695, Accuracy:0.4115, Validation Loss:1.0825, Validation Accuracy:0.3662\n",
    "Epoch #63: Loss:1.0707, Accuracy:0.4066, Validation Loss:1.0839, Validation Accuracy:0.3563\n",
    "Epoch #64: Loss:1.0698, Accuracy:0.4074, Validation Loss:1.0844, Validation Accuracy:0.3629\n",
    "Epoch #65: Loss:1.0695, Accuracy:0.4078, Validation Loss:1.0828, Validation Accuracy:0.3629\n",
    "Epoch #66: Loss:1.0688, Accuracy:0.4115, Validation Loss:1.0830, Validation Accuracy:0.3645\n",
    "Epoch #67: Loss:1.0692, Accuracy:0.4111, Validation Loss:1.0839, Validation Accuracy:0.3695\n",
    "Epoch #68: Loss:1.0682, Accuracy:0.4189, Validation Loss:1.0820, Validation Accuracy:0.3645\n",
    "Epoch #69: Loss:1.0677, Accuracy:0.4119, Validation Loss:1.0826, Validation Accuracy:0.3678\n",
    "Epoch #70: Loss:1.0681, Accuracy:0.4086, Validation Loss:1.0820, Validation Accuracy:0.3760\n",
    "Epoch #71: Loss:1.0711, Accuracy:0.3992, Validation Loss:1.0799, Validation Accuracy:0.3957\n",
    "Epoch #72: Loss:1.0687, Accuracy:0.4107, Validation Loss:1.0828, Validation Accuracy:0.3842\n",
    "Epoch #73: Loss:1.0695, Accuracy:0.4099, Validation Loss:1.0822, Validation Accuracy:0.4023\n",
    "Epoch #74: Loss:1.0677, Accuracy:0.4152, Validation Loss:1.0808, Validation Accuracy:0.3662\n",
    "Epoch #75: Loss:1.0679, Accuracy:0.4168, Validation Loss:1.0834, Validation Accuracy:0.3793\n",
    "Epoch #76: Loss:1.0664, Accuracy:0.4205, Validation Loss:1.0826, Validation Accuracy:0.3662\n",
    "Epoch #77: Loss:1.0674, Accuracy:0.4168, Validation Loss:1.0831, Validation Accuracy:0.3547\n",
    "Epoch #78: Loss:1.0651, Accuracy:0.4168, Validation Loss:1.0846, Validation Accuracy:0.3629\n",
    "Epoch #79: Loss:1.0646, Accuracy:0.4136, Validation Loss:1.0891, Validation Accuracy:0.3662\n",
    "Epoch #80: Loss:1.0662, Accuracy:0.4279, Validation Loss:1.0874, Validation Accuracy:0.3514\n",
    "Epoch #81: Loss:1.0674, Accuracy:0.4160, Validation Loss:1.0872, Validation Accuracy:0.3514\n",
    "Epoch #82: Loss:1.0673, Accuracy:0.4201, Validation Loss:1.0869, Validation Accuracy:0.3744\n",
    "Epoch #83: Loss:1.0653, Accuracy:0.4127, Validation Loss:1.0864, Validation Accuracy:0.3744\n",
    "Epoch #84: Loss:1.0651, Accuracy:0.4181, Validation Loss:1.0889, Validation Accuracy:0.3580\n",
    "Epoch #85: Loss:1.0686, Accuracy:0.4115, Validation Loss:1.0844, Validation Accuracy:0.3760\n",
    "Epoch #86: Loss:1.0668, Accuracy:0.4201, Validation Loss:1.0881, Validation Accuracy:0.3760\n",
    "Epoch #87: Loss:1.0654, Accuracy:0.4193, Validation Loss:1.0869, Validation Accuracy:0.3859\n",
    "Epoch #88: Loss:1.0662, Accuracy:0.4168, Validation Loss:1.0840, Validation Accuracy:0.3662\n",
    "Epoch #89: Loss:1.0684, Accuracy:0.4177, Validation Loss:1.0823, Validation Accuracy:0.3908\n",
    "Epoch #90: Loss:1.0691, Accuracy:0.4082, Validation Loss:1.0779, Validation Accuracy:0.3875\n",
    "Epoch #91: Loss:1.0716, Accuracy:0.4041, Validation Loss:1.0820, Validation Accuracy:0.3760\n",
    "Epoch #92: Loss:1.0704, Accuracy:0.4094, Validation Loss:1.0829, Validation Accuracy:0.3727\n",
    "Epoch #93: Loss:1.0682, Accuracy:0.4094, Validation Loss:1.0848, Validation Accuracy:0.3432\n",
    "Epoch #94: Loss:1.0658, Accuracy:0.4234, Validation Loss:1.0840, Validation Accuracy:0.3727\n",
    "Epoch #95: Loss:1.0678, Accuracy:0.4164, Validation Loss:1.0841, Validation Accuracy:0.3695\n",
    "Epoch #96: Loss:1.0672, Accuracy:0.4193, Validation Loss:1.0834, Validation Accuracy:0.3530\n",
    "Epoch #97: Loss:1.0663, Accuracy:0.4127, Validation Loss:1.0812, Validation Accuracy:0.3678\n",
    "Epoch #98: Loss:1.0672, Accuracy:0.4127, Validation Loss:1.0823, Validation Accuracy:0.3711\n",
    "Epoch #99: Loss:1.0658, Accuracy:0.4140, Validation Loss:1.0851, Validation Accuracy:0.3563\n",
    "Epoch #100: Loss:1.0645, Accuracy:0.4177, Validation Loss:1.0849, Validation Accuracy:0.3678\n",
    "Epoch #101: Loss:1.0658, Accuracy:0.4144, Validation Loss:1.0864, Validation Accuracy:0.3678\n",
    "Epoch #102: Loss:1.0644, Accuracy:0.4209, Validation Loss:1.0875, Validation Accuracy:0.3662\n",
    "Epoch #103: Loss:1.0648, Accuracy:0.4209, Validation Loss:1.0863, Validation Accuracy:0.3744\n",
    "Epoch #104: Loss:1.0643, Accuracy:0.4144, Validation Loss:1.0875, Validation Accuracy:0.3793\n",
    "Epoch #105: Loss:1.0637, Accuracy:0.4234, Validation Loss:1.0912, Validation Accuracy:0.3711\n",
    "Epoch #106: Loss:1.0639, Accuracy:0.4246, Validation Loss:1.0894, Validation Accuracy:0.3744\n",
    "Epoch #107: Loss:1.0642, Accuracy:0.4246, Validation Loss:1.0884, Validation Accuracy:0.3760\n",
    "Epoch #108: Loss:1.0631, Accuracy:0.4296, Validation Loss:1.0917, Validation Accuracy:0.3612\n",
    "Epoch #109: Loss:1.0632, Accuracy:0.4238, Validation Loss:1.0896, Validation Accuracy:0.3678\n",
    "Epoch #110: Loss:1.0625, Accuracy:0.4123, Validation Loss:1.0890, Validation Accuracy:0.3711\n",
    "Epoch #111: Loss:1.0632, Accuracy:0.4259, Validation Loss:1.0896, Validation Accuracy:0.3777\n",
    "Epoch #112: Loss:1.0616, Accuracy:0.4251, Validation Loss:1.0922, Validation Accuracy:0.3711\n",
    "Epoch #113: Loss:1.0600, Accuracy:0.4201, Validation Loss:1.0932, Validation Accuracy:0.3777\n",
    "Epoch #114: Loss:1.0593, Accuracy:0.4333, Validation Loss:1.0917, Validation Accuracy:0.3629\n",
    "Epoch #115: Loss:1.0588, Accuracy:0.4222, Validation Loss:1.0911, Validation Accuracy:0.3695\n",
    "Epoch #116: Loss:1.0580, Accuracy:0.4287, Validation Loss:1.0923, Validation Accuracy:0.3678\n",
    "Epoch #117: Loss:1.0591, Accuracy:0.4267, Validation Loss:1.0949, Validation Accuracy:0.3711\n",
    "Epoch #118: Loss:1.0609, Accuracy:0.4222, Validation Loss:1.0961, Validation Accuracy:0.3744\n",
    "Epoch #119: Loss:1.0592, Accuracy:0.4320, Validation Loss:1.0927, Validation Accuracy:0.3678\n",
    "Epoch #120: Loss:1.0602, Accuracy:0.4292, Validation Loss:1.0916, Validation Accuracy:0.3678\n",
    "Epoch #121: Loss:1.0588, Accuracy:0.4230, Validation Loss:1.0945, Validation Accuracy:0.3875\n",
    "Epoch #122: Loss:1.0596, Accuracy:0.4234, Validation Loss:1.0911, Validation Accuracy:0.3695\n",
    "Epoch #123: Loss:1.0604, Accuracy:0.4246, Validation Loss:1.0948, Validation Accuracy:0.3859\n",
    "Epoch #124: Loss:1.0584, Accuracy:0.4201, Validation Loss:1.0974, Validation Accuracy:0.3744\n",
    "Epoch #125: Loss:1.0586, Accuracy:0.4218, Validation Loss:1.0960, Validation Accuracy:0.3892\n",
    "Epoch #126: Loss:1.0568, Accuracy:0.4246, Validation Loss:1.0907, Validation Accuracy:0.3957\n",
    "Epoch #127: Loss:1.0813, Accuracy:0.4172, Validation Loss:1.1002, Validation Accuracy:0.3842\n",
    "Epoch #128: Loss:1.0738, Accuracy:0.4131, Validation Loss:1.0870, Validation Accuracy:0.3842\n",
    "Epoch #129: Loss:1.0617, Accuracy:0.4119, Validation Loss:1.0879, Validation Accuracy:0.3498\n",
    "Epoch #130: Loss:1.0646, Accuracy:0.4177, Validation Loss:1.0843, Validation Accuracy:0.3695\n",
    "Epoch #131: Loss:1.0608, Accuracy:0.4218, Validation Loss:1.0861, Validation Accuracy:0.3793\n",
    "Epoch #132: Loss:1.0627, Accuracy:0.4168, Validation Loss:1.0851, Validation Accuracy:0.3711\n",
    "Epoch #133: Loss:1.0615, Accuracy:0.4234, Validation Loss:1.0851, Validation Accuracy:0.3498\n",
    "Epoch #134: Loss:1.0612, Accuracy:0.4287, Validation Loss:1.0875, Validation Accuracy:0.3695\n",
    "Epoch #135: Loss:1.0596, Accuracy:0.4279, Validation Loss:1.0879, Validation Accuracy:0.3678\n",
    "Epoch #136: Loss:1.0578, Accuracy:0.4246, Validation Loss:1.0866, Validation Accuracy:0.3711\n",
    "Epoch #137: Loss:1.0569, Accuracy:0.4304, Validation Loss:1.0894, Validation Accuracy:0.3711\n",
    "Epoch #138: Loss:1.0556, Accuracy:0.4324, Validation Loss:1.0921, Validation Accuracy:0.3744\n",
    "Epoch #139: Loss:1.0561, Accuracy:0.4320, Validation Loss:1.0917, Validation Accuracy:0.3875\n",
    "Epoch #140: Loss:1.0585, Accuracy:0.4251, Validation Loss:1.0886, Validation Accuracy:0.3892\n",
    "Epoch #141: Loss:1.0675, Accuracy:0.4000, Validation Loss:1.0813, Validation Accuracy:0.4039\n",
    "Epoch #142: Loss:1.0753, Accuracy:0.4000, Validation Loss:1.0893, Validation Accuracy:0.3810\n",
    "Epoch #143: Loss:1.0835, Accuracy:0.3741, Validation Loss:1.0786, Validation Accuracy:0.4056\n",
    "Epoch #144: Loss:1.0718, Accuracy:0.3918, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #145: Loss:1.0679, Accuracy:0.3885, Validation Loss:1.0764, Validation Accuracy:0.3662\n",
    "Epoch #146: Loss:1.0707, Accuracy:0.3955, Validation Loss:1.0769, Validation Accuracy:0.4039\n",
    "Epoch #147: Loss:1.0712, Accuracy:0.4021, Validation Loss:1.0811, Validation Accuracy:0.3990\n",
    "Epoch #148: Loss:1.0710, Accuracy:0.4008, Validation Loss:1.0789, Validation Accuracy:0.3810\n",
    "Epoch #149: Loss:1.0693, Accuracy:0.4008, Validation Loss:1.0781, Validation Accuracy:0.3629\n",
    "Epoch #150: Loss:1.0690, Accuracy:0.3864, Validation Loss:1.0765, Validation Accuracy:0.3777\n",
    "Epoch #151: Loss:1.0700, Accuracy:0.3873, Validation Loss:1.0789, Validation Accuracy:0.3727\n",
    "Epoch #152: Loss:1.0691, Accuracy:0.4119, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #153: Loss:1.0702, Accuracy:0.4074, Validation Loss:1.0774, Validation Accuracy:0.3941\n",
    "Epoch #154: Loss:1.0706, Accuracy:0.4045, Validation Loss:1.0775, Validation Accuracy:0.3892\n",
    "Epoch #155: Loss:1.0697, Accuracy:0.4062, Validation Loss:1.0775, Validation Accuracy:0.3793\n",
    "Epoch #156: Loss:1.0679, Accuracy:0.3992, Validation Loss:1.0782, Validation Accuracy:0.3826\n",
    "Epoch #157: Loss:1.0665, Accuracy:0.4197, Validation Loss:1.0738, Validation Accuracy:0.4039\n",
    "Epoch #158: Loss:1.0661, Accuracy:0.4177, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #159: Loss:1.0651, Accuracy:0.4156, Validation Loss:1.0762, Validation Accuracy:0.3924\n",
    "Epoch #160: Loss:1.0660, Accuracy:0.4131, Validation Loss:1.0782, Validation Accuracy:0.3941\n",
    "Epoch #161: Loss:1.0668, Accuracy:0.4205, Validation Loss:1.0772, Validation Accuracy:0.3842\n",
    "Epoch #162: Loss:1.0672, Accuracy:0.4144, Validation Loss:1.0783, Validation Accuracy:0.3859\n",
    "Epoch #163: Loss:1.0661, Accuracy:0.4099, Validation Loss:1.0776, Validation Accuracy:0.3892\n",
    "Epoch #164: Loss:1.0652, Accuracy:0.4136, Validation Loss:1.0783, Validation Accuracy:0.3859\n",
    "Epoch #165: Loss:1.0662, Accuracy:0.4082, Validation Loss:1.0788, Validation Accuracy:0.3678\n",
    "Epoch #166: Loss:1.0644, Accuracy:0.4131, Validation Loss:1.0790, Validation Accuracy:0.3514\n",
    "Epoch #167: Loss:1.0623, Accuracy:0.4246, Validation Loss:1.0807, Validation Accuracy:0.3793\n",
    "Epoch #168: Loss:1.0629, Accuracy:0.4197, Validation Loss:1.0844, Validation Accuracy:0.3678\n",
    "Epoch #169: Loss:1.0622, Accuracy:0.4197, Validation Loss:1.0859, Validation Accuracy:0.3645\n",
    "Epoch #170: Loss:1.0602, Accuracy:0.4283, Validation Loss:1.0858, Validation Accuracy:0.3645\n",
    "Epoch #171: Loss:1.0590, Accuracy:0.4197, Validation Loss:1.0828, Validation Accuracy:0.3793\n",
    "Epoch #172: Loss:1.0603, Accuracy:0.4209, Validation Loss:1.0833, Validation Accuracy:0.3810\n",
    "Epoch #173: Loss:1.0605, Accuracy:0.4131, Validation Loss:1.0841, Validation Accuracy:0.3793\n",
    "Epoch #174: Loss:1.0616, Accuracy:0.4131, Validation Loss:1.0824, Validation Accuracy:0.3924\n",
    "Epoch #175: Loss:1.0642, Accuracy:0.4090, Validation Loss:1.0849, Validation Accuracy:0.3810\n",
    "Epoch #176: Loss:1.0602, Accuracy:0.4119, Validation Loss:1.0869, Validation Accuracy:0.3744\n",
    "Epoch #177: Loss:1.0615, Accuracy:0.4160, Validation Loss:1.0828, Validation Accuracy:0.3760\n",
    "Epoch #178: Loss:1.0628, Accuracy:0.4205, Validation Loss:1.0876, Validation Accuracy:0.3842\n",
    "Epoch #179: Loss:1.0628, Accuracy:0.4131, Validation Loss:1.0867, Validation Accuracy:0.3810\n",
    "Epoch #180: Loss:1.0641, Accuracy:0.4131, Validation Loss:1.0853, Validation Accuracy:0.3810\n",
    "Epoch #181: Loss:1.0620, Accuracy:0.4156, Validation Loss:1.0858, Validation Accuracy:0.3924\n",
    "Epoch #182: Loss:1.0609, Accuracy:0.4181, Validation Loss:1.0853, Validation Accuracy:0.3908\n",
    "Epoch #183: Loss:1.0614, Accuracy:0.4197, Validation Loss:1.0869, Validation Accuracy:0.3727\n",
    "Epoch #184: Loss:1.0627, Accuracy:0.4140, Validation Loss:1.0861, Validation Accuracy:0.3777\n",
    "Epoch #185: Loss:1.0615, Accuracy:0.4177, Validation Loss:1.0873, Validation Accuracy:0.3908\n",
    "Epoch #186: Loss:1.0617, Accuracy:0.4177, Validation Loss:1.0846, Validation Accuracy:0.3810\n",
    "Epoch #187: Loss:1.0611, Accuracy:0.4181, Validation Loss:1.0849, Validation Accuracy:0.3924\n",
    "Epoch #188: Loss:1.0631, Accuracy:0.4172, Validation Loss:1.0832, Validation Accuracy:0.3810\n",
    "Epoch #189: Loss:1.0632, Accuracy:0.4057, Validation Loss:1.0824, Validation Accuracy:0.4122\n",
    "Epoch #190: Loss:1.0624, Accuracy:0.4189, Validation Loss:1.0835, Validation Accuracy:0.3842\n",
    "Epoch #191: Loss:1.0619, Accuracy:0.4152, Validation Loss:1.0816, Validation Accuracy:0.3793\n",
    "Epoch #192: Loss:1.0617, Accuracy:0.4049, Validation Loss:1.0809, Validation Accuracy:0.3826\n",
    "Epoch #193: Loss:1.0606, Accuracy:0.4209, Validation Loss:1.0826, Validation Accuracy:0.3760\n",
    "Epoch #194: Loss:1.0610, Accuracy:0.4214, Validation Loss:1.0827, Validation Accuracy:0.3645\n",
    "Epoch #195: Loss:1.0600, Accuracy:0.4197, Validation Loss:1.0838, Validation Accuracy:0.3826\n",
    "Epoch #196: Loss:1.0615, Accuracy:0.4172, Validation Loss:1.0857, Validation Accuracy:0.3826\n",
    "Epoch #197: Loss:1.0625, Accuracy:0.4119, Validation Loss:1.0855, Validation Accuracy:0.3892\n",
    "Epoch #198: Loss:1.0619, Accuracy:0.4148, Validation Loss:1.0893, Validation Accuracy:0.3695\n",
    "Epoch #199: Loss:1.0598, Accuracy:0.4242, Validation Loss:1.0865, Validation Accuracy:0.3826\n",
    "Epoch #200: Loss:1.0581, Accuracy:0.4193, Validation Loss:1.0852, Validation Accuracy:0.3711\n",
    "Epoch #201: Loss:1.0582, Accuracy:0.4234, Validation Loss:1.0860, Validation Accuracy:0.3760\n",
    "Epoch #202: Loss:1.0584, Accuracy:0.4238, Validation Loss:1.0896, Validation Accuracy:0.3596\n",
    "Epoch #203: Loss:1.0578, Accuracy:0.4214, Validation Loss:1.0901, Validation Accuracy:0.3645\n",
    "Epoch #204: Loss:1.0568, Accuracy:0.4238, Validation Loss:1.0960, Validation Accuracy:0.3711\n",
    "Epoch #205: Loss:1.0568, Accuracy:0.4226, Validation Loss:1.0991, Validation Accuracy:0.3695\n",
    "Epoch #206: Loss:1.0552, Accuracy:0.4292, Validation Loss:1.1013, Validation Accuracy:0.3629\n",
    "Epoch #207: Loss:1.0535, Accuracy:0.4296, Validation Loss:1.1050, Validation Accuracy:0.3580\n",
    "Epoch #208: Loss:1.0538, Accuracy:0.4234, Validation Loss:1.1075, Validation Accuracy:0.3596\n",
    "Epoch #209: Loss:1.0541, Accuracy:0.4283, Validation Loss:1.1027, Validation Accuracy:0.3662\n",
    "Epoch #210: Loss:1.0554, Accuracy:0.4361, Validation Loss:1.1011, Validation Accuracy:0.3711\n",
    "Epoch #211: Loss:1.0567, Accuracy:0.4251, Validation Loss:1.1003, Validation Accuracy:0.3810\n",
    "Epoch #212: Loss:1.0562, Accuracy:0.4177, Validation Loss:1.1008, Validation Accuracy:0.3645\n",
    "Epoch #213: Loss:1.0566, Accuracy:0.4226, Validation Loss:1.1000, Validation Accuracy:0.3777\n",
    "Epoch #214: Loss:1.0562, Accuracy:0.4148, Validation Loss:1.0975, Validation Accuracy:0.3760\n",
    "Epoch #215: Loss:1.0536, Accuracy:0.4271, Validation Loss:1.0963, Validation Accuracy:0.3645\n",
    "Epoch #216: Loss:1.0535, Accuracy:0.4283, Validation Loss:1.0908, Validation Accuracy:0.3662\n",
    "Epoch #217: Loss:1.0546, Accuracy:0.4292, Validation Loss:1.0933, Validation Accuracy:0.3678\n",
    "Epoch #218: Loss:1.0543, Accuracy:0.4234, Validation Loss:1.0950, Validation Accuracy:0.3892\n",
    "Epoch #219: Loss:1.0540, Accuracy:0.4287, Validation Loss:1.0957, Validation Accuracy:0.3645\n",
    "Epoch #220: Loss:1.0527, Accuracy:0.4226, Validation Loss:1.0965, Validation Accuracy:0.3612\n",
    "Epoch #221: Loss:1.0532, Accuracy:0.4267, Validation Loss:1.0974, Validation Accuracy:0.3596\n",
    "Epoch #222: Loss:1.0526, Accuracy:0.4251, Validation Loss:1.0997, Validation Accuracy:0.3645\n",
    "Epoch #223: Loss:1.0552, Accuracy:0.4222, Validation Loss:1.0966, Validation Accuracy:0.3957\n",
    "Epoch #224: Loss:1.0546, Accuracy:0.4177, Validation Loss:1.0982, Validation Accuracy:0.3842\n",
    "Epoch #225: Loss:1.0534, Accuracy:0.4242, Validation Loss:1.0979, Validation Accuracy:0.3760\n",
    "Epoch #226: Loss:1.0532, Accuracy:0.4320, Validation Loss:1.0918, Validation Accuracy:0.3842\n",
    "Epoch #227: Loss:1.0518, Accuracy:0.4300, Validation Loss:1.0918, Validation Accuracy:0.3875\n",
    "Epoch #228: Loss:1.0526, Accuracy:0.4337, Validation Loss:1.0940, Validation Accuracy:0.3793\n",
    "Epoch #229: Loss:1.0532, Accuracy:0.4287, Validation Loss:1.0947, Validation Accuracy:0.3760\n",
    "Epoch #230: Loss:1.0509, Accuracy:0.4353, Validation Loss:1.1029, Validation Accuracy:0.3662\n",
    "Epoch #231: Loss:1.0482, Accuracy:0.4345, Validation Loss:1.1085, Validation Accuracy:0.3974\n",
    "Epoch #232: Loss:1.0496, Accuracy:0.4341, Validation Loss:1.1076, Validation Accuracy:0.3629\n",
    "Epoch #233: Loss:1.0505, Accuracy:0.4312, Validation Loss:1.1037, Validation Accuracy:0.3990\n",
    "Epoch #234: Loss:1.0509, Accuracy:0.4353, Validation Loss:1.1038, Validation Accuracy:0.3892\n",
    "Epoch #235: Loss:1.0519, Accuracy:0.4251, Validation Loss:1.1018, Validation Accuracy:0.3678\n",
    "Epoch #236: Loss:1.0523, Accuracy:0.4234, Validation Loss:1.1014, Validation Accuracy:0.3974\n",
    "Epoch #237: Loss:1.0537, Accuracy:0.4246, Validation Loss:1.1033, Validation Accuracy:0.3547\n",
    "Epoch #238: Loss:1.0503, Accuracy:0.4366, Validation Loss:1.1034, Validation Accuracy:0.3941\n",
    "Epoch #239: Loss:1.0487, Accuracy:0.4271, Validation Loss:1.1100, Validation Accuracy:0.3645\n",
    "Epoch #240: Loss:1.0477, Accuracy:0.4382, Validation Loss:1.1058, Validation Accuracy:0.3957\n",
    "Epoch #241: Loss:1.0470, Accuracy:0.4353, Validation Loss:1.1086, Validation Accuracy:0.3645\n",
    "Epoch #242: Loss:1.0498, Accuracy:0.4292, Validation Loss:1.1127, Validation Accuracy:0.3859\n",
    "Epoch #243: Loss:1.0477, Accuracy:0.4304, Validation Loss:1.1132, Validation Accuracy:0.3892\n",
    "Epoch #244: Loss:1.0513, Accuracy:0.4218, Validation Loss:1.1184, Validation Accuracy:0.3547\n",
    "Epoch #245: Loss:1.0495, Accuracy:0.4238, Validation Loss:1.1125, Validation Accuracy:0.3793\n",
    "Epoch #246: Loss:1.0517, Accuracy:0.4283, Validation Loss:1.1145, Validation Accuracy:0.3645\n",
    "Epoch #247: Loss:1.0477, Accuracy:0.4423, Validation Loss:1.1139, Validation Accuracy:0.3711\n",
    "Epoch #248: Loss:1.0494, Accuracy:0.4357, Validation Loss:1.1082, Validation Accuracy:0.3695\n",
    "Epoch #249: Loss:1.0494, Accuracy:0.4246, Validation Loss:1.1080, Validation Accuracy:0.3678\n",
    "Epoch #250: Loss:1.0491, Accuracy:0.4411, Validation Loss:1.1026, Validation Accuracy:0.3908\n",
    "Epoch #251: Loss:1.0469, Accuracy:0.4378, Validation Loss:1.1087, Validation Accuracy:0.3695\n",
    "Epoch #252: Loss:1.0483, Accuracy:0.4349, Validation Loss:1.1122, Validation Accuracy:0.3711\n",
    "Epoch #253: Loss:1.0494, Accuracy:0.4349, Validation Loss:1.1205, Validation Accuracy:0.3612\n",
    "Epoch #254: Loss:1.0504, Accuracy:0.4296, Validation Loss:1.1229, Validation Accuracy:0.3481\n",
    "Epoch #255: Loss:1.0532, Accuracy:0.4263, Validation Loss:1.1233, Validation Accuracy:0.3580\n",
    "Epoch #256: Loss:1.0650, Accuracy:0.4127, Validation Loss:1.1250, Validation Accuracy:0.3695\n",
    "Epoch #257: Loss:1.0734, Accuracy:0.4082, Validation Loss:1.1085, Validation Accuracy:0.3875\n",
    "Epoch #258: Loss:1.0679, Accuracy:0.4021, Validation Loss:1.0985, Validation Accuracy:0.3711\n",
    "Epoch #259: Loss:1.0674, Accuracy:0.4103, Validation Loss:1.0978, Validation Accuracy:0.3695\n",
    "Epoch #260: Loss:1.0888, Accuracy:0.3659, Validation Loss:1.1240, Validation Accuracy:0.3153\n",
    "Epoch #261: Loss:1.0753, Accuracy:0.3864, Validation Loss:1.0965, Validation Accuracy:0.3957\n",
    "Epoch #262: Loss:1.0806, Accuracy:0.3988, Validation Loss:1.0924, Validation Accuracy:0.3563\n",
    "Epoch #263: Loss:1.0690, Accuracy:0.4115, Validation Loss:1.0824, Validation Accuracy:0.3629\n",
    "Epoch #264: Loss:1.0637, Accuracy:0.4152, Validation Loss:1.0854, Validation Accuracy:0.3777\n",
    "Epoch #265: Loss:1.0672, Accuracy:0.4062, Validation Loss:1.0857, Validation Accuracy:0.3826\n",
    "Epoch #266: Loss:1.0654, Accuracy:0.4049, Validation Loss:1.0841, Validation Accuracy:0.3974\n",
    "Epoch #267: Loss:1.0629, Accuracy:0.4090, Validation Loss:1.0834, Validation Accuracy:0.3596\n",
    "Epoch #268: Loss:1.0624, Accuracy:0.4123, Validation Loss:1.0853, Validation Accuracy:0.3596\n",
    "Epoch #269: Loss:1.0620, Accuracy:0.4090, Validation Loss:1.0848, Validation Accuracy:0.3662\n",
    "Epoch #270: Loss:1.0604, Accuracy:0.4074, Validation Loss:1.0861, Validation Accuracy:0.3777\n",
    "Epoch #271: Loss:1.0599, Accuracy:0.4131, Validation Loss:1.0865, Validation Accuracy:0.3695\n",
    "Epoch #272: Loss:1.0591, Accuracy:0.4090, Validation Loss:1.0878, Validation Accuracy:0.3629\n",
    "Epoch #273: Loss:1.0578, Accuracy:0.4160, Validation Loss:1.0904, Validation Accuracy:0.3678\n",
    "Epoch #274: Loss:1.0563, Accuracy:0.4177, Validation Loss:1.0940, Validation Accuracy:0.3612\n",
    "Epoch #275: Loss:1.0553, Accuracy:0.4119, Validation Loss:1.0936, Validation Accuracy:0.3662\n",
    "Epoch #276: Loss:1.0551, Accuracy:0.4234, Validation Loss:1.0988, Validation Accuracy:0.3842\n",
    "Epoch #277: Loss:1.0535, Accuracy:0.4341, Validation Loss:1.0969, Validation Accuracy:0.3596\n",
    "Epoch #278: Loss:1.0515, Accuracy:0.4300, Validation Loss:1.0998, Validation Accuracy:0.3744\n",
    "Epoch #279: Loss:1.0485, Accuracy:0.4349, Validation Loss:1.0978, Validation Accuracy:0.3777\n",
    "Epoch #280: Loss:1.0501, Accuracy:0.4279, Validation Loss:1.1055, Validation Accuracy:0.3924\n",
    "Epoch #281: Loss:1.0533, Accuracy:0.4222, Validation Loss:1.1120, Validation Accuracy:0.3957\n",
    "Epoch #282: Loss:1.0596, Accuracy:0.4193, Validation Loss:1.1107, Validation Accuracy:0.3563\n",
    "Epoch #283: Loss:1.0650, Accuracy:0.4045, Validation Loss:1.1077, Validation Accuracy:0.3383\n",
    "Epoch #284: Loss:1.0613, Accuracy:0.4193, Validation Loss:1.0963, Validation Accuracy:0.3530\n",
    "Epoch #285: Loss:1.0591, Accuracy:0.4156, Validation Loss:1.0860, Validation Accuracy:0.3727\n",
    "Epoch #286: Loss:1.0567, Accuracy:0.4082, Validation Loss:1.0816, Validation Accuracy:0.3875\n",
    "Epoch #287: Loss:1.0542, Accuracy:0.4275, Validation Loss:1.0861, Validation Accuracy:0.3711\n",
    "Epoch #288: Loss:1.0583, Accuracy:0.4148, Validation Loss:1.0872, Validation Accuracy:0.3744\n",
    "Epoch #289: Loss:1.0547, Accuracy:0.4304, Validation Loss:1.0896, Validation Accuracy:0.3612\n",
    "Epoch #290: Loss:1.0502, Accuracy:0.4193, Validation Loss:1.0911, Validation Accuracy:0.3678\n",
    "Epoch #291: Loss:1.0474, Accuracy:0.4345, Validation Loss:1.0949, Validation Accuracy:0.3612\n",
    "Epoch #292: Loss:1.0447, Accuracy:0.4382, Validation Loss:1.0966, Validation Accuracy:0.3530\n",
    "Epoch #293: Loss:1.0423, Accuracy:0.4444, Validation Loss:1.1002, Validation Accuracy:0.3465\n",
    "Epoch #294: Loss:1.0414, Accuracy:0.4468, Validation Loss:1.1067, Validation Accuracy:0.3711\n",
    "Epoch #295: Loss:1.0478, Accuracy:0.4308, Validation Loss:1.1087, Validation Accuracy:0.3662\n",
    "Epoch #296: Loss:1.0426, Accuracy:0.4349, Validation Loss:1.1083, Validation Accuracy:0.3760\n",
    "Epoch #297: Loss:1.0487, Accuracy:0.4337, Validation Loss:1.1092, Validation Accuracy:0.3842\n",
    "Epoch #298: Loss:1.0451, Accuracy:0.4361, Validation Loss:1.1080, Validation Accuracy:0.3645\n",
    "Epoch #299: Loss:1.0410, Accuracy:0.4370, Validation Loss:1.1060, Validation Accuracy:0.3612\n",
    "Epoch #300: Loss:1.0442, Accuracy:0.4370, Validation Loss:1.1012, Validation Accuracy:0.3957\n",
    "\n",
    "Test:\n",
    "Test Loss:1.10119343, Accuracy:0.3957\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "       02   01  03\n",
    "t:02  119   97  11\n",
    "t:01  114  117   9\n",
    "t:03   84   53   5\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.38      0.52      0.44       227\n",
    "          01       0.44      0.49      0.46       240\n",
    "          03       0.20      0.04      0.06       142\n",
    "\n",
    "    accuracy                           0.40       609\n",
    "   macro avg       0.34      0.35      0.32       609\n",
    "weighted avg       0.36      0.40      0.36       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 03:08:52 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.080339574852992, 1.0766169584443417, 1.077477641097822, 1.0768646776969797, 1.0764107588672482, 1.076033320137237, 1.0755828675769625, 1.075337973134271, 1.0750056178307494, 1.074552211268195, 1.0745524968615503, 1.074672038136249, 1.0747842653631576, 1.0747517752529951, 1.074872066430466, 1.074815152313909, 1.0750796469953063, 1.0752790412683597, 1.0754996144712852, 1.0756384823318381, 1.0755926955901147, 1.0757476165768352, 1.0757163665173284, 1.0759259215716659, 1.0761377003001071, 1.0759382453458062, 1.0759722665808666, 1.076194347419175, 1.0765187599388837, 1.0764872542351533, 1.0773254607502853, 1.0773888497517026, 1.0774464397790593, 1.0768894515014047, 1.0773288195552106, 1.0772845715724777, 1.0768046709900028, 1.0769185125338425, 1.078040429524013, 1.078272906625995, 1.0783204820942995, 1.0784461962178422, 1.0790412725486191, 1.0788630307797336, 1.0778691792135755, 1.07784229252726, 1.0789840325150388, 1.0783544510651888, 1.0774349904021214, 1.0775823031348744, 1.0772727549761192, 1.0780588202484331, 1.0791328902706527, 1.0807706659846315, 1.080084595382703, 1.078881288984139, 1.0781202815436377, 1.0790432073016865, 1.0793581674447397, 1.0797763833858696, 1.0815647740669438, 1.082533934433472, 1.08385224158345, 1.0844256449215517, 1.0828353297729993, 1.0829879209913056, 1.083864699247827, 1.081990912434307, 1.0826491035264114, 1.0819793501119503, 1.0798782235491649, 1.0828073852755167, 1.0822147596841571, 1.0808224910977242, 1.0833729722816956, 1.0825625849866318, 1.0831398333626232, 1.0846050928770419, 1.089125112359747, 1.0874319186155823, 1.087186975040655, 1.0869439395973444, 1.0863586580029065, 1.0889071817272793, 1.0843635715287308, 1.0881368156724376, 1.0868688896176066, 1.0839890819073505, 1.082349424291714, 1.077876368962681, 1.0819739144423912, 1.0828859745379544, 1.0847533040837505, 1.0839825044516076, 1.0840535105155606, 1.083404025029275, 1.0812071215343007, 1.0823262820298645, 1.0851263451850277, 1.0848557872725237, 1.0863557466732456, 1.0874537560348636, 1.0863032065001614, 1.0874691001691645, 1.0911720450875795, 1.0894214177170802, 1.0883873587562924, 1.0916763704594328, 1.0896367594134828, 1.0889825014449497, 1.0896007417653777, 1.0921912696365457, 1.0932364316996683, 1.0917141985619205, 1.091138232909204, 1.0922774242845859, 1.0948538576636604, 1.0961023903832647, 1.0927429522199583, 1.0916119334341465, 1.0945172309875488, 1.0910723906236721, 1.0948250336795802, 1.0974331687040908, 1.0960243049709277, 1.0906944180944282, 1.1002443844853167, 1.0869888928527707, 1.0878553134075721, 1.0843015247573602, 1.0860851169219745, 1.0851103732934335, 1.085060895370145, 1.0874963550536307, 1.0878638222887012, 1.0866431172062414, 1.089370007781168, 1.09205043629081, 1.0916524017581408, 1.0885576704648523, 1.0813316218371463, 1.0893095647564466, 1.0786316518125862, 1.0745959769328828, 1.0763501572882992, 1.0768759730218471, 1.0811327316099395, 1.0789389430204244, 1.0781056109711846, 1.0764858294003115, 1.0789349362963723, 1.0759595748043216, 1.0773669770981487, 1.0775416149881674, 1.077472048440003, 1.0782283939947244, 1.0738134202111531, 1.0757326238065321, 1.076199369673267, 1.078211272095616, 1.0772411737144483, 1.0783384028326701, 1.0775844831576293, 1.0782675566931663, 1.0787645441362228, 1.0789652336602924, 1.0806942947196647, 1.0844483199377952, 1.085875104409329, 1.0857728028728066, 1.0827572915354386, 1.0833255451888286, 1.084127460011512, 1.0824168828516367, 1.0848504976294506, 1.0869286285441107, 1.0827904493350702, 1.0876029015370385, 1.0866562654623648, 1.0853392801848538, 1.0858125645538856, 1.0853214667152693, 1.0868620678709058, 1.0860968963266007, 1.0873008189334463, 1.0846381432121415, 1.0849319917619327, 1.0831540204425556, 1.0824225461737471, 1.083455756380053, 1.0816149369053456, 1.0809469786770824, 1.0826079943301448, 1.0826591726985864, 1.0837914638331372, 1.0856696795947447, 1.0854546634238733, 1.0893351426852749, 1.0865207256746214, 1.0852217985491448, 1.0859961198468513, 1.0896088227458385, 1.0900987709684324, 1.095952746511876, 1.0991435403307084, 1.1013445627121699, 1.1049640256978803, 1.1074589126803018, 1.102725386228076, 1.1011495670465805, 1.100340480287674, 1.1007796304762265, 1.1000414317464593, 1.0974675593117775, 1.0963218535108519, 1.090808946119349, 1.0932867061132672, 1.0950333927261027, 1.0957225862590747, 1.0965059377094013, 1.097393088544335, 1.0997215401754394, 1.0966442300768322, 1.0981650550181448, 1.097920556178038, 1.0917643518087703, 1.0918047825495403, 1.093988391761905, 1.094718677656991, 1.1029104428925538, 1.108538719037875, 1.1075989530591541, 1.1037079226990247, 1.1038325790114003, 1.1017691394182654, 1.1013584371858043, 1.103265599468463, 1.1034433234892846, 1.1099951153709775, 1.105784789094784, 1.1085622784343652, 1.1126595500654775, 1.1131742724839886, 1.11844353350904, 1.1124540602632345, 1.1144852716543012, 1.113867368604162, 1.108206867388708, 1.10798191828485, 1.1026156800133842, 1.1086534210809542, 1.1122076564234467, 1.1205285940264247, 1.1228513353563883, 1.1233358590669429, 1.1249687199913614, 1.1085314967949402, 1.0985058747684622, 1.0978439697882616, 1.1239831705985985, 1.096528669491973, 1.0924159147469281, 1.0823764750131442, 1.0853909412628324, 1.0857270895358182, 1.0841340347268116, 1.0834408587422864, 1.0853261421075204, 1.0848181919119824, 1.0861166230171968, 1.0865015539238214, 1.0878162613055977, 1.0904080071081277, 1.093959819702875, 1.0935692816532303, 1.0988210191084637, 1.0968576941779877, 1.0998378236501283, 1.0977672639934497, 1.1055401199556925, 1.111963465100243, 1.110736657833231, 1.1076782618837404, 1.0962924150802036, 1.0859768688189377, 1.081558937705405, 1.0861327971143675, 1.0872429192359812, 1.0896363943472676, 1.0911214151993174, 1.0948520387922014, 1.096613991045208, 1.1001981966601218, 1.1067305390274975, 1.1087097156615484, 1.1082934497416705, 1.1091794086794549, 1.1080301700554458, 1.1059812753658576, 1.1011935846363186], 'val_acc': [0.3546798013407609, 0.36781609033911883, 0.3825944160495094, 0.3858784882990989, 0.37274219910499495, 0.35303776531383907, 0.35632183736768264, 0.3908045965756102, 0.3842364520764312, 0.38916256064656135, 0.39408866892307265, 0.3908045965756102, 0.38259441585376347, 0.37766830747937924, 0.379310343604174, 0.38095237963109574, 0.37602627096309255, 0.3825944156580175, 0.38095237963109574, 0.3809523797289687, 0.3809523793374768, 0.37602627086521956, 0.3760262711588385, 0.37602627135458444, 0.3743842352297897, 0.3776683068921413, 0.3776683068921413, 0.3711001628823273, 0.36945812656178656, 0.37931034301693606, 0.3530377647266012, 0.3612479456441939, 0.37766830738150625, 0.379310343310555, 0.34811165645008996, 0.34811165645008996, 0.37274219871350306, 0.37766830728363326, 0.348111656254344, 0.341543511755165, 0.37766830747937924, 0.3596059092257802, 0.34318554797783274, 0.3530377648244742, 0.3612479456441939, 0.3530377647266012, 0.3497536924770117, 0.3661740543121971, 0.3727421985177571, 0.37602627086521956, 0.3694581262681676, 0.3825944157558905, 0.39244663240678596, 0.372742199007122, 0.36453201789378337, 0.3628899820626076, 0.36288998167111564, 0.36453201828527526, 0.3645320177959104, 0.36453201799165635, 0.3645320176980374, 0.366174054605816, 0.35632183726980965, 0.3628899821604805, 0.3628899817689886, 0.3645320177959104, 0.3694581261702946, 0.36453201838314825, 0.36781609024124584, 0.37602627096309255, 0.3957307049499944, 0.3842364519785582, 0.4022988493513004, 0.3661740542143241, 0.379310343310555, 0.3661740542143241, 0.3546798011450149, 0.3628899818668616, 0.366174054507943, 0.3513957291890443, 0.3513957291890443, 0.3743842348382978, 0.37438423513191676, 0.3579638738839693, 0.3760262712567115, 0.3760262716482034, 0.38587848839697186, 0.36617405441007006, 0.39080459667348316, 0.3875205240324017, 0.3760262715503304, 0.372742199007122, 0.34318554885868957, 0.37274219910499495, 0.36945812705115144, 0.35303776521596614, 0.36781609024124584, 0.3711001628823273, 0.35632183756342856, 0.3678160904369918, 0.3678160905348648, 0.36617405441007006, 0.3743842350340438, 0.379310343408428, 0.3711001629802002, 0.3743842350340438, 0.37602627135458444, 0.3612479460356858, 0.3678160908284837, 0.3711001631759462, 0.37766830738150625, 0.3711001627844543, 0.37766830747937924, 0.3628899818668616, 0.3694581263660406, 0.36781609063273774, 0.3711001626865813, 0.3743842349361708, 0.3678160904369918, 0.3678160908284837, 0.3875205246196396, 0.36945812705115144, 0.3858784882990989, 0.37438423571915463, 0.3891625608423073, 0.3957307052436133, 0.3842364522721771, 0.38423645217430413, 0.3497536930642496, 0.36945812685540547, 0.3793103439956659, 0.3711001629802002, 0.3497536930642496, 0.36945812656178656, 0.36781609063273774, 0.3711001625887083, 0.3711001627844543, 0.3743842350340438, 0.3875205242281477, 0.3891625600593235, 0.4039408856718411, 0.38095237982684166, 0.40558292169876287, 0.39080459716284804, 0.3661740543121971, 0.4039408855739681, 0.3990147771995839, 0.38095237943534976, 0.3628899818668616, 0.37766830767512516, 0.37274219861563007, 0.3825944156580175, 0.3940886690209456, 0.38916256035294244, 0.37931034321268203, 0.3825944156580175, 0.4039408855739681, 0.38916256015719647, 0.39244663240678596, 0.39408866882519966, 0.38423645168493925, 0.38587848810335296, 0.38916256045081543, 0.385878487907607, 0.3678160899476269, 0.3513957284060605, 0.379310343506301, 0.36781609004549987, 0.3645320177959104, 0.36453201828527526, 0.379310343310555, 0.38095237992471465, 0.379310343604174, 0.3924466327004049, 0.3809523797289687, 0.3743842348382978, 0.37602627096309255, 0.3842364518806852, 0.3809523797289687, 0.38095237963109574, 0.39244663240678596, 0.3908045962819912, 0.37274219890924903, 0.3776683071857603, 0.3908045963798642, 0.3809523793374768, 0.39244663230891297, 0.3809523793374768, 0.4121510664915608, 0.3842364518806852, 0.37931034321268203, 0.3825944155601445, 0.3760262711588385, 0.36453201789378337, 0.3825944156580175, 0.3825944156580175, 0.38916256035294244, 0.3694581267575325, 0.3825944157558905, 0.3711001626865813, 0.37602627096309255, 0.35960590951939914, 0.36453201799165635, 0.37110016249083533, 0.3694581260724216, 0.3628899817689886, 0.3579638732967314, 0.3596059097151451, 0.3661740543121971, 0.3711001628823273, 0.38095237943534976, 0.36453201838314825, 0.37766830777299815, 0.37602627145245743, 0.3645320177959104, 0.3661740541164511, 0.36781609014337285, 0.38916256035294244, 0.3645320177959104, 0.3612479455463209, 0.35960590932365316, 0.36453201799165635, 0.39573070475424843, 0.38423645178281224, 0.37602627086521956, 0.38423645178281224, 0.3875205240324017, 0.379310343408428, 0.37602627096309255, 0.3661740543121971, 0.39737274136840806, 0.3628899819647346, 0.3990147772974569, 0.38916256054868836, 0.36781609014337285, 0.3973727412705351, 0.3546798012428879, 0.39408866911881857, 0.36453201799165635, 0.3957307052436133, 0.3645320177959104, 0.385878487907607, 0.38916256015719647, 0.3546798014386338, 0.3793103437999199, 0.36453201828527526, 0.3711001630780732, 0.36945812714902443, 0.36781609063273774, 0.3908045968692291, 0.36945812695327845, 0.3711001631759462, 0.3612479464271777, 0.34811165693945484, 0.3579638734924774, 0.36945812665965955, 0.3875205242281477, 0.3711001627844543, 0.36945812665965955, 0.315270934247814, 0.3957307050478674, 0.3563218381506665, 0.36288998167111564, 0.37766830728363326, 0.3825944159516364, 0.3973727412705351, 0.3596059096172721, 0.35960590951939914, 0.3661740543121971, 0.37766830738150625, 0.36945812656178656, 0.3628899824540995, 0.36781609063273774, 0.3612479456441939, 0.3661740541164511, 0.38423645168493925, 0.3596059096172721, 0.3743842349361708, 0.37766830747937924, 0.39244663250465894, 0.3957307050478674, 0.3563218374655556, 0.3382594397013215, 0.35303776511809315, 0.37274219910499495, 0.3875205240324017, 0.3711001626865813, 0.3743842352297897, 0.3612479454484479, 0.36781609024124584, 0.36124794574206687, 0.35303776511809315, 0.3464696203252952, 0.3711001625887083, 0.3661740543121971, 0.3760262711588385, 0.38423645158706626, 0.36453201848102124, 0.36124794632930474, 0.3957307051457404], 'loss': [1.1026848981512645, 1.0757087296773766, 1.0734359125826638, 1.0746131216965662, 1.0746394960297696, 1.0740145916811494, 1.0734773816024499, 1.0734107424835895, 1.0736697355579792, 1.0736063851957693, 1.073573864656797, 1.0733693976177083, 1.0734069845759648, 1.0732766632671473, 1.0731144491407172, 1.0731292837209525, 1.0730267631444599, 1.072942944718582, 1.0727634568968347, 1.0729105419691582, 1.0728555272002485, 1.0728374918383494, 1.0726758858016874, 1.0725603921212699, 1.0726166034381248, 1.0726578192054859, 1.0722986682484528, 1.072290665413075, 1.0721894459068408, 1.0719849306455138, 1.0719096845180347, 1.0719682541226458, 1.0721482308738286, 1.0718374481436164, 1.0717999788769952, 1.0718081959953543, 1.0716484558900523, 1.0716381049498884, 1.0713486972530764, 1.0717267066791072, 1.0712137690315011, 1.0711193359363251, 1.070825896958306, 1.0706790300365345, 1.0707618488668171, 1.07013408217342, 1.0698901448651261, 1.0696000370401622, 1.0699491118503548, 1.0698910213349047, 1.0701709218583313, 1.0714258647307724, 1.0709851851943093, 1.070447172864017, 1.0711183057918197, 1.0700550602691619, 1.0700363664411667, 1.0696093742362773, 1.0697748444653146, 1.0690596459582602, 1.0692244770590529, 1.0695153193307363, 1.0706516761309801, 1.069847374628212, 1.0695448139609742, 1.0688230709373583, 1.0691824633972355, 1.068237012073979, 1.0677303809649645, 1.0680960981752838, 1.0710995719173362, 1.0687032611463103, 1.069467202398077, 1.0676634607373812, 1.0679335610822485, 1.0663728270932145, 1.0674049337297005, 1.0651365006484046, 1.0645949798442988, 1.0661652571856364, 1.067353281250235, 1.0673408799592474, 1.0652836742342375, 1.065135182296471, 1.0685896746186994, 1.0667925197240997, 1.0653902318198578, 1.066156767868653, 1.0684223727768696, 1.0690561967708736, 1.0715966233482597, 1.070363812378055, 1.068209756424295, 1.0657701828151758, 1.0677527918707908, 1.0671887301811203, 1.0662896350668687, 1.0672073894947216, 1.0658054113388062, 1.0644631748081967, 1.0657608673068288, 1.064392109769081, 1.0647861667727054, 1.064333301794847, 1.0636782167873344, 1.0639397325946565, 1.0641593198756663, 1.0631495766081605, 1.0631747256069457, 1.062511889303, 1.0631981996540172, 1.0615984759046801, 1.0600407173011828, 1.0592929836171363, 1.0588037260748768, 1.0579552928525076, 1.0591049187482016, 1.0609068160184354, 1.0592065633444816, 1.0602273191025124, 1.0588133986725698, 1.0596080131354517, 1.0604262001460583, 1.0584495275172365, 1.0585844084957052, 1.0568266457355977, 1.0812647054327587, 1.0737573959009847, 1.061671555899007, 1.064634780325684, 1.0608299131510928, 1.0626997692629052, 1.0614749765494031, 1.0612075209862397, 1.0595789949996761, 1.0578410034062191, 1.05690434860253, 1.055590302890331, 1.05612798706462, 1.058461669534139, 1.0675176327723008, 1.0752829470918408, 1.0834692186398673, 1.0718243929884517, 1.0678773205872678, 1.0706775509601256, 1.0712113013257725, 1.0709890341611858, 1.0692905146483278, 1.0690367711398145, 1.0700284709675845, 1.069072988586504, 1.0702016830933903, 1.0705725573416363, 1.069735917369443, 1.0678577881329359, 1.0665213609378197, 1.066087094175742, 1.0650949063976687, 1.0659804396071229, 1.0668434873498684, 1.0672385780228726, 1.0661346735161188, 1.0651783557398364, 1.0662420261077568, 1.0643882930645951, 1.0622866997239035, 1.062853054197417, 1.06222702433686, 1.0602004838919983, 1.0589926501808715, 1.060288186778278, 1.060452805652266, 1.0615716215031836, 1.0642200357860119, 1.0601671254120812, 1.0614734243808097, 1.0627665327804534, 1.062805008594505, 1.064124247817288, 1.0620311365969617, 1.060872032118529, 1.0613776590789858, 1.0627301540218095, 1.0615490640213356, 1.0617320815640554, 1.0611169030289385, 1.0631231773070975, 1.0632491484314999, 1.062365752421855, 1.0618572125934234, 1.0616654395077998, 1.0605770498820155, 1.060959366019012, 1.0599564658053358, 1.0614587776469988, 1.062465725052773, 1.0618961593208862, 1.0598448367579028, 1.0580705681865465, 1.0582113020718709, 1.058398007367426, 1.0577658103476804, 1.0567989671499578, 1.0568086328447721, 1.0551798199236515, 1.0534598867506462, 1.053844628539663, 1.0540514784916715, 1.0554194902247718, 1.0567060137186697, 1.056231879404683, 1.056631531069166, 1.0562273258056485, 1.0536064413783486, 1.0534950066885664, 1.0545597824472666, 1.0543042373363487, 1.05396222025462, 1.052688121697741, 1.0531820932208145, 1.0526365589556999, 1.0552380034566171, 1.054557612103848, 1.053447378587429, 1.0531747471870094, 1.0517762526839176, 1.0525945428950096, 1.0532177970149923, 1.05094124159529, 1.0481664581710064, 1.0495767106265748, 1.0505463888023423, 1.0508923625554392, 1.05190109389029, 1.0523284985544257, 1.0537490816331743, 1.0502884294462889, 1.0486645043508227, 1.0476901169430304, 1.046982701750017, 1.049771484457247, 1.0477463430447744, 1.0513263238773698, 1.0494802951323179, 1.0517279173069667, 1.0476975527142598, 1.0493838557472464, 1.0493553173370676, 1.049059680012462, 1.0468503532468416, 1.0483126363715107, 1.049358588322477, 1.0503722940382283, 1.0531579872910737, 1.0649766278707515, 1.0733670601365013, 1.0679432089079088, 1.067371676687832, 1.0887931235272292, 1.0752946418903202, 1.0806359205402634, 1.068970979655303, 1.06373817201023, 1.0671778290668308, 1.0653938385495414, 1.0628857337473845, 1.0623570064254855, 1.0620219036783771, 1.060352635383606, 1.0598859129989906, 1.0591174551593694, 1.0578074567371816, 1.0563231012414858, 1.055310633735735, 1.0551369796298613, 1.053492853871606, 1.051541098872739, 1.048536522334606, 1.0500831646596136, 1.0533325140970689, 1.0595932318199832, 1.064994826796608, 1.061320096364501, 1.0590837254416527, 1.056668542249002, 1.054234366201522, 1.0582613997390873, 1.054741532944556, 1.0502443639159937, 1.047372556467076, 1.0446520905230325, 1.0423115199106674, 1.0413541913277315, 1.0477916065427557, 1.0426253301651816, 1.0486740548018312, 1.0451041599563504, 1.04104793571104, 1.0441559789606678], 'acc': [0.30431211574856015, 0.39999999935132524, 0.4045174555367268, 0.39958932372823636, 0.3930184806641612, 0.4004106799435077, 0.39917864532686115, 0.39753593563788725, 0.3963038996137388, 0.39425051547663414, 0.3950718681303138, 0.4012320314222293, 0.4000000003671744, 0.4045174551450741, 0.40287474388948946, 0.4020533858750635, 0.39917864450683826, 0.4057494883909363, 0.40410677592367605, 0.404106777918657, 0.4020533892408289, 0.4053388074071011, 0.40000000212961156, 0.40739219572999397, 0.4073921953016238, 0.4045174551450741, 0.4036960995172818, 0.40492813158818586, 0.40698151905433844, 0.4057494874118045, 0.4020533888491762, 0.39876796907957573, 0.400410679551855, 0.40698152104931934, 0.40698151752444506, 0.4036960967389955, 0.4028747448686212, 0.40451745240350523, 0.40657084006548416, 0.40451745416594237, 0.41314168175877486, 0.4082135546868342, 0.41232032789341966, 0.4057494878034572, 0.40944558323286395, 0.41108829742584385, 0.40780287491467454, 0.4078028733113463, 0.41149897347730285, 0.41067761702948774, 0.4094455855460627, 0.4094455833919729, 0.39589322301151814, 0.41149897226562737, 0.4086242305424669, 0.40903490616555577, 0.40903490913966845, 0.41478439556010205, 0.41601642488943724, 0.41724846208854377, 0.4102669421897042, 0.41149897246145367, 0.40657084398201115, 0.40739219628075557, 0.4078028727238673, 0.41149897109066924, 0.41108829386425216, 0.4188911682159259, 0.41190965027534987, 0.40862422893913863, 0.39917864689347193, 0.4106776168336614, 0.4098562610100427, 0.4151950702407766, 0.41683778407882127, 0.42053388299638483, 0.41683778090888224, 0.41683778368716856, 0.4135523623509573, 0.427926078163378, 0.41601642649276543, 0.42012320286928995, 0.4127310043365314, 0.41806981395891807, 0.41149897543556635, 0.4201232020492671, 0.4193018460298221, 0.41683778231638413, 0.4176591384949381, 0.4082135501461107, 0.4041067739654126, 0.40944558714939094, 0.4094455869535646, 0.4234086255762856, 0.41642710329081245, 0.4193018468131275, 0.41273100731064405, 0.4127310059031422, 0.4139630403606799, 0.4176591363408482, 0.41437371660796524, 0.4209445578728858, 0.4209445581054296, 0.41437371856622873, 0.4234086224063466, 0.42464065843049503, 0.42464065780629856, 0.4295687904348119, 0.42381930041606913, 0.41232033083081493, 0.42587268968137626, 0.42505133526525946, 0.42012320146178805, 0.4332648862191539, 0.4221765903354425, 0.42874743559032497, 0.4266940473041495, 0.42217659213459713, 0.43203285453990253, 0.4291581123883719, 0.4229979457674085, 0.42340862538045926, 0.42464065565220876, 0.42012320146178805, 0.42176591608313807, 0.42464065565220876, 0.41724845875949584, 0.41314168610367197, 0.4119096504711762, 0.41765913575336916, 0.42176591608313807, 0.41683778466630034, 0.4234086251479155, 0.4287474320287332, 0.4279260767925936, 0.4246406562396878, 0.4303901425010125, 0.43244353235379873, 0.4320328551273816, 0.4250513344819541, 0.39999999895967253, 0.39999999778471446, 0.3741273107837113, 0.39178644796906065, 0.3885010284687215, 0.3954825455892747, 0.40205338963248155, 0.40082135677827213, 0.4008213530575715, 0.38644763995000225, 0.3872689922120292, 0.4119096504711762, 0.4073921992548682, 0.4045174519751351, 0.40616016264324073, 0.39917864532686115, 0.4197125259978081, 0.41765913555754286, 0.4156057480546728, 0.4131416815629485, 0.42053388283727594, 0.41437371895788144, 0.40985626218500076, 0.41355235918101835, 0.4082135501461107, 0.41314168293373293, 0.42464065565220876, 0.41971252345206556, 0.41971252501867634, 0.42833675656475323, 0.4197125252145027, 0.42094455806871217, 0.41314168332538564, 0.4131416831295593, 0.4090349055780767, 0.4119096492962181, 0.4160164262969391, 0.42053387966733696, 0.4131416843045174, 0.41314168473288754, 0.4156057506371328, 0.41806981572135515, 0.41971252384371827, 0.4139630383656989, 0.4176591383358291, 0.41765913872748184, 0.41806981317561265, 0.4172484604852155, 0.4057494846335182, 0.41889117193662656, 0.4151950696532976, 0.4049281315514684, 0.42094456084699844, 0.4213552382692419, 0.41971252443119733, 0.417248460521933, 0.41190965324946255, 0.41478439203522777, 0.42422997725083356, 0.4193018503747192, 0.42340862260217293, 0.42381930257015893, 0.4213552378775892, 0.42381930394094336, 0.4225872707317987, 0.42915811121341385, 0.4295687864815675, 0.4234086235813047, 0.42833675441066343, 0.4361396305614918, 0.4250513342494103, 0.4176591383358291, 0.4225872691651879, 0.4147843934060122, 0.4271047211648013, 0.4283367551939688, 0.4291581086676713, 0.42340862495208914, 0.42874743559032497, 0.42258726952012315, 0.4266940453091686, 0.4250513350327157, 0.4221765928811851, 0.41765913814000283, 0.42422997846250904, 0.43203285375659717, 0.4299794648829427, 0.43367556599131357, 0.4287474310496015, 0.43531827509280835, 0.4344969184858843, 0.43408624360938336, 0.43121149891211025, 0.43531827411367663, 0.4250513334661049, 0.4234086222105202, 0.42464065823466873, 0.4365503087303232, 0.4271047217522803, 0.43819301959425516, 0.4353182731345449, 0.42915811043010843, 0.4303901458667779, 0.4217659138923308, 0.42381930257015893, 0.4283367558181653, 0.44229979342503717, 0.4357289531025309, 0.4246406554563824, 0.44106776076665405, 0.43778233923461646, 0.43490759888224045, 0.4349075992371757, 0.4295687870690465, 0.42628336710361975, 0.41273100437324883, 0.40821355429518147, 0.4020533870500216, 0.4102669384322862, 0.36591375767572704, 0.3864476397541759, 0.3987679673171386, 0.41149897226562737, 0.4151950726274103, 0.40616016323071974, 0.404928133350623, 0.4090349085521894, 0.412320330671706, 0.40903490616555577, 0.40739219588910286, 0.41314168215042757, 0.40903490773216655, 0.41601642704352704, 0.41765913911913455, 0.41190964890456544, 0.42340862456043643, 0.43408624400103607, 0.4299794639038109, 0.4349075953206487, 0.4279260801216415, 0.42217659072709524, 0.41930184955469635, 0.40451745475342143, 0.41930184638475737, 0.41560575083295914, 0.40821355092941614, 0.42751539995782917, 0.4147843947767967, 0.4303901414851633, 0.4193018477922592, 0.43449691907336335, 0.43819301704851266, 0.4443531822986916, 0.44681724957372126, 0.430800819923256, 0.4349075984538703, 0.433675566028031, 0.4361396295456426, 0.43696098674004574, 0.43696098439012954]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
