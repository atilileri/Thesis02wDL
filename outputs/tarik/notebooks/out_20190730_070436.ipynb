{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf9.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 07:04:36 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '2', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001FF011E5E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001FF59637EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0908, Accuracy:0.3943, Validation Loss:1.0833, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0803, Accuracy:0.3943, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0753, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0738, Accuracy:0.3959, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0738, Accuracy:0.3971, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #24: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #25: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #26: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #28: Loss:1.0733, Accuracy:0.3951, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #29: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #30: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #31: Loss:1.0730, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #32: Loss:1.0728, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #33: Loss:1.0730, Accuracy:0.3906, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #34: Loss:1.0725, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #35: Loss:1.0720, Accuracy:0.4012, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #36: Loss:1.0722, Accuracy:0.4090, Validation Loss:1.0745, Validation Accuracy:0.3744\n",
    "Epoch #37: Loss:1.0726, Accuracy:0.3934, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #39: Loss:1.0711, Accuracy:0.4119, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #40: Loss:1.0710, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #41: Loss:1.0700, Accuracy:0.4123, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0697, Accuracy:0.4177, Validation Loss:1.0754, Validation Accuracy:0.3859\n",
    "Epoch #43: Loss:1.0699, Accuracy:0.4033, Validation Loss:1.0757, Validation Accuracy:0.3777\n",
    "Epoch #44: Loss:1.0706, Accuracy:0.4021, Validation Loss:1.0758, Validation Accuracy:0.3826\n",
    "Epoch #45: Loss:1.0691, Accuracy:0.4049, Validation Loss:1.0763, Validation Accuracy:0.3875\n",
    "Epoch #46: Loss:1.0673, Accuracy:0.4197, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #47: Loss:1.0671, Accuracy:0.4230, Validation Loss:1.0772, Validation Accuracy:0.3760\n",
    "Epoch #48: Loss:1.0666, Accuracy:0.4205, Validation Loss:1.0782, Validation Accuracy:0.4007\n",
    "Epoch #49: Loss:1.0650, Accuracy:0.4234, Validation Loss:1.0789, Validation Accuracy:0.3810\n",
    "Epoch #50: Loss:1.0644, Accuracy:0.4271, Validation Loss:1.0809, Validation Accuracy:0.3777\n",
    "Epoch #51: Loss:1.0630, Accuracy:0.4287, Validation Loss:1.0813, Validation Accuracy:0.3711\n",
    "Epoch #52: Loss:1.0628, Accuracy:0.4324, Validation Loss:1.0831, Validation Accuracy:0.3810\n",
    "Epoch #53: Loss:1.0614, Accuracy:0.4271, Validation Loss:1.0839, Validation Accuracy:0.3842\n",
    "Epoch #54: Loss:1.0607, Accuracy:0.4185, Validation Loss:1.0923, Validation Accuracy:0.3908\n",
    "Epoch #55: Loss:1.0658, Accuracy:0.4242, Validation Loss:1.0875, Validation Accuracy:0.3711\n",
    "Epoch #56: Loss:1.0672, Accuracy:0.4144, Validation Loss:1.0825, Validation Accuracy:0.3481\n",
    "Epoch #57: Loss:1.0626, Accuracy:0.4193, Validation Loss:1.0857, Validation Accuracy:0.3810\n",
    "Epoch #58: Loss:1.0603, Accuracy:0.4296, Validation Loss:1.0829, Validation Accuracy:0.3580\n",
    "Epoch #59: Loss:1.0586, Accuracy:0.4382, Validation Loss:1.0841, Validation Accuracy:0.3695\n",
    "Epoch #60: Loss:1.0594, Accuracy:0.4361, Validation Loss:1.0860, Validation Accuracy:0.3596\n",
    "Epoch #61: Loss:1.0598, Accuracy:0.4263, Validation Loss:1.0864, Validation Accuracy:0.3908\n",
    "Epoch #62: Loss:1.0576, Accuracy:0.4386, Validation Loss:1.0863, Validation Accuracy:0.3777\n",
    "Epoch #63: Loss:1.0557, Accuracy:0.4316, Validation Loss:1.0892, Validation Accuracy:0.3908\n",
    "Epoch #64: Loss:1.0558, Accuracy:0.4398, Validation Loss:1.0907, Validation Accuracy:0.3711\n",
    "Epoch #65: Loss:1.0585, Accuracy:0.4300, Validation Loss:1.0902, Validation Accuracy:0.3662\n",
    "Epoch #66: Loss:1.0559, Accuracy:0.4382, Validation Loss:1.0923, Validation Accuracy:0.3563\n",
    "Epoch #67: Loss:1.0556, Accuracy:0.4366, Validation Loss:1.0916, Validation Accuracy:0.3662\n",
    "Epoch #68: Loss:1.0530, Accuracy:0.4402, Validation Loss:1.0920, Validation Accuracy:0.3678\n",
    "Epoch #69: Loss:1.0531, Accuracy:0.4366, Validation Loss:1.0924, Validation Accuracy:0.3678\n",
    "Epoch #70: Loss:1.0519, Accuracy:0.4419, Validation Loss:1.0922, Validation Accuracy:0.3711\n",
    "Epoch #71: Loss:1.0510, Accuracy:0.4398, Validation Loss:1.0936, Validation Accuracy:0.3695\n",
    "Epoch #72: Loss:1.0501, Accuracy:0.4517, Validation Loss:1.0932, Validation Accuracy:0.3645\n",
    "Epoch #73: Loss:1.0483, Accuracy:0.4464, Validation Loss:1.1019, Validation Accuracy:0.3892\n",
    "Epoch #74: Loss:1.0512, Accuracy:0.4415, Validation Loss:1.0966, Validation Accuracy:0.3645\n",
    "Epoch #75: Loss:1.0493, Accuracy:0.4501, Validation Loss:1.0946, Validation Accuracy:0.3711\n",
    "Epoch #76: Loss:1.0454, Accuracy:0.4493, Validation Loss:1.0986, Validation Accuracy:0.3777\n",
    "Epoch #77: Loss:1.0504, Accuracy:0.4435, Validation Loss:1.0998, Validation Accuracy:0.3645\n",
    "Epoch #78: Loss:1.0461, Accuracy:0.4522, Validation Loss:1.0981, Validation Accuracy:0.3793\n",
    "Epoch #79: Loss:1.0468, Accuracy:0.4464, Validation Loss:1.0966, Validation Accuracy:0.3711\n",
    "Epoch #80: Loss:1.0425, Accuracy:0.4538, Validation Loss:1.0992, Validation Accuracy:0.3678\n",
    "Epoch #81: Loss:1.0414, Accuracy:0.4616, Validation Loss:1.1005, Validation Accuracy:0.3760\n",
    "Epoch #82: Loss:1.0406, Accuracy:0.4628, Validation Loss:1.1015, Validation Accuracy:0.3711\n",
    "Epoch #83: Loss:1.0415, Accuracy:0.4604, Validation Loss:1.1039, Validation Accuracy:0.3793\n",
    "Epoch #84: Loss:1.0413, Accuracy:0.4583, Validation Loss:1.1047, Validation Accuracy:0.3678\n",
    "Epoch #85: Loss:1.0395, Accuracy:0.4530, Validation Loss:1.1076, Validation Accuracy:0.3777\n",
    "Epoch #86: Loss:1.0418, Accuracy:0.4587, Validation Loss:1.1033, Validation Accuracy:0.3612\n",
    "Epoch #87: Loss:1.0411, Accuracy:0.4550, Validation Loss:1.1026, Validation Accuracy:0.3826\n",
    "Epoch #88: Loss:1.0364, Accuracy:0.4612, Validation Loss:1.1027, Validation Accuracy:0.3678\n",
    "Epoch #89: Loss:1.0340, Accuracy:0.4747, Validation Loss:1.1108, Validation Accuracy:0.3875\n",
    "Epoch #90: Loss:1.0300, Accuracy:0.4665, Validation Loss:1.1070, Validation Accuracy:0.3711\n",
    "Epoch #91: Loss:1.0299, Accuracy:0.4756, Validation Loss:1.1084, Validation Accuracy:0.3678\n",
    "Epoch #92: Loss:1.0364, Accuracy:0.4686, Validation Loss:1.1213, Validation Accuracy:0.3612\n",
    "Epoch #93: Loss:1.0348, Accuracy:0.4616, Validation Loss:1.1125, Validation Accuracy:0.3514\n",
    "Epoch #94: Loss:1.0302, Accuracy:0.4682, Validation Loss:1.1173, Validation Accuracy:0.3727\n",
    "Epoch #95: Loss:1.0303, Accuracy:0.4690, Validation Loss:1.1141, Validation Accuracy:0.3678\n",
    "Epoch #96: Loss:1.0263, Accuracy:0.4694, Validation Loss:1.1160, Validation Accuracy:0.3530\n",
    "Epoch #97: Loss:1.0260, Accuracy:0.4756, Validation Loss:1.1129, Validation Accuracy:0.3875\n",
    "Epoch #98: Loss:1.0245, Accuracy:0.4768, Validation Loss:1.1136, Validation Accuracy:0.3514\n",
    "Epoch #99: Loss:1.0229, Accuracy:0.4817, Validation Loss:1.1159, Validation Accuracy:0.3777\n",
    "Epoch #100: Loss:1.0235, Accuracy:0.4772, Validation Loss:1.1128, Validation Accuracy:0.3842\n",
    "Epoch #101: Loss:1.0202, Accuracy:0.4752, Validation Loss:1.1191, Validation Accuracy:0.3580\n",
    "Epoch #102: Loss:1.0174, Accuracy:0.4883, Validation Loss:1.1227, Validation Accuracy:0.3711\n",
    "Epoch #103: Loss:1.0158, Accuracy:0.4867, Validation Loss:1.1251, Validation Accuracy:0.3793\n",
    "Epoch #104: Loss:1.0139, Accuracy:0.4887, Validation Loss:1.1253, Validation Accuracy:0.3695\n",
    "Epoch #105: Loss:1.0164, Accuracy:0.4858, Validation Loss:1.1271, Validation Accuracy:0.3645\n",
    "Epoch #106: Loss:1.0127, Accuracy:0.4850, Validation Loss:1.1288, Validation Accuracy:0.3662\n",
    "Epoch #107: Loss:1.0095, Accuracy:0.4994, Validation Loss:1.1317, Validation Accuracy:0.3678\n",
    "Epoch #108: Loss:1.0089, Accuracy:0.4932, Validation Loss:1.1298, Validation Accuracy:0.3465\n",
    "Epoch #109: Loss:1.0092, Accuracy:0.4887, Validation Loss:1.1285, Validation Accuracy:0.3777\n",
    "Epoch #110: Loss:1.0041, Accuracy:0.5002, Validation Loss:1.1426, Validation Accuracy:0.3498\n",
    "Epoch #111: Loss:1.0079, Accuracy:0.4850, Validation Loss:1.1430, Validation Accuracy:0.3645\n",
    "Epoch #112: Loss:1.0034, Accuracy:0.5060, Validation Loss:1.1398, Validation Accuracy:0.3793\n",
    "Epoch #113: Loss:1.0079, Accuracy:0.4838, Validation Loss:1.1402, Validation Accuracy:0.3498\n",
    "Epoch #114: Loss:1.0048, Accuracy:0.4957, Validation Loss:1.1408, Validation Accuracy:0.3974\n",
    "Epoch #115: Loss:1.0090, Accuracy:0.4916, Validation Loss:1.1372, Validation Accuracy:0.3810\n",
    "Epoch #116: Loss:1.0080, Accuracy:0.4793, Validation Loss:1.1336, Validation Accuracy:0.3448\n",
    "Epoch #117: Loss:1.0036, Accuracy:0.4940, Validation Loss:1.1284, Validation Accuracy:0.3678\n",
    "Epoch #118: Loss:1.0014, Accuracy:0.4982, Validation Loss:1.1384, Validation Accuracy:0.3859\n",
    "Epoch #119: Loss:0.9968, Accuracy:0.5064, Validation Loss:1.1449, Validation Accuracy:0.3612\n",
    "Epoch #120: Loss:0.9946, Accuracy:0.5092, Validation Loss:1.1505, Validation Accuracy:0.3842\n",
    "Epoch #121: Loss:0.9948, Accuracy:0.5060, Validation Loss:1.1388, Validation Accuracy:0.3662\n",
    "Epoch #122: Loss:0.9978, Accuracy:0.5039, Validation Loss:1.1488, Validation Accuracy:0.3498\n",
    "Epoch #123: Loss:0.9918, Accuracy:0.5080, Validation Loss:1.1636, Validation Accuracy:0.3629\n",
    "Epoch #124: Loss:0.9965, Accuracy:0.4977, Validation Loss:1.1545, Validation Accuracy:0.3810\n",
    "Epoch #125: Loss:0.9914, Accuracy:0.5105, Validation Loss:1.1471, Validation Accuracy:0.3629\n",
    "Epoch #126: Loss:0.9855, Accuracy:0.5117, Validation Loss:1.1560, Validation Accuracy:0.3727\n",
    "Epoch #127: Loss:0.9884, Accuracy:0.5023, Validation Loss:1.1588, Validation Accuracy:0.3498\n",
    "Epoch #128: Loss:0.9885, Accuracy:0.5043, Validation Loss:1.1578, Validation Accuracy:0.3629\n",
    "Epoch #129: Loss:0.9838, Accuracy:0.5129, Validation Loss:1.1574, Validation Accuracy:0.3793\n",
    "Epoch #130: Loss:0.9813, Accuracy:0.5117, Validation Loss:1.1669, Validation Accuracy:0.3711\n",
    "Epoch #131: Loss:0.9810, Accuracy:0.5133, Validation Loss:1.1659, Validation Accuracy:0.3432\n",
    "Epoch #132: Loss:0.9845, Accuracy:0.5138, Validation Loss:1.1691, Validation Accuracy:0.3826\n",
    "Epoch #133: Loss:0.9823, Accuracy:0.5166, Validation Loss:1.1697, Validation Accuracy:0.3727\n",
    "Epoch #134: Loss:0.9813, Accuracy:0.5084, Validation Loss:1.1688, Validation Accuracy:0.3678\n",
    "Epoch #135: Loss:0.9794, Accuracy:0.5109, Validation Loss:1.1697, Validation Accuracy:0.3580\n",
    "Epoch #136: Loss:0.9862, Accuracy:0.5064, Validation Loss:1.1607, Validation Accuracy:0.3859\n",
    "Epoch #137: Loss:0.9860, Accuracy:0.5109, Validation Loss:1.1694, Validation Accuracy:0.3563\n",
    "Epoch #138: Loss:0.9752, Accuracy:0.5129, Validation Loss:1.1748, Validation Accuracy:0.3711\n",
    "Epoch #139: Loss:0.9720, Accuracy:0.5195, Validation Loss:1.1762, Validation Accuracy:0.3563\n",
    "Epoch #140: Loss:0.9732, Accuracy:0.5170, Validation Loss:1.1836, Validation Accuracy:0.3580\n",
    "Epoch #141: Loss:0.9718, Accuracy:0.5220, Validation Loss:1.1775, Validation Accuracy:0.3580\n",
    "Epoch #142: Loss:0.9661, Accuracy:0.5298, Validation Loss:1.1900, Validation Accuracy:0.3760\n",
    "Epoch #143: Loss:0.9683, Accuracy:0.5162, Validation Loss:1.1901, Validation Accuracy:0.3760\n",
    "Epoch #144: Loss:0.9652, Accuracy:0.5175, Validation Loss:1.2062, Validation Accuracy:0.3760\n",
    "Epoch #145: Loss:0.9655, Accuracy:0.5261, Validation Loss:1.2005, Validation Accuracy:0.3662\n",
    "Epoch #146: Loss:0.9622, Accuracy:0.5232, Validation Loss:1.1993, Validation Accuracy:0.3530\n",
    "Epoch #147: Loss:0.9645, Accuracy:0.5129, Validation Loss:1.2029, Validation Accuracy:0.3678\n",
    "Epoch #148: Loss:0.9621, Accuracy:0.5290, Validation Loss:1.2137, Validation Accuracy:0.3727\n",
    "Epoch #149: Loss:0.9661, Accuracy:0.5199, Validation Loss:1.2016, Validation Accuracy:0.3629\n",
    "Epoch #150: Loss:0.9593, Accuracy:0.5203, Validation Loss:1.1813, Validation Accuracy:0.3415\n",
    "Epoch #151: Loss:0.9662, Accuracy:0.5133, Validation Loss:1.2033, Validation Accuracy:0.3662\n",
    "Epoch #152: Loss:0.9643, Accuracy:0.5224, Validation Loss:1.2036, Validation Accuracy:0.3629\n",
    "Epoch #153: Loss:0.9678, Accuracy:0.5191, Validation Loss:1.1841, Validation Accuracy:0.3547\n",
    "Epoch #154: Loss:0.9638, Accuracy:0.5175, Validation Loss:1.2037, Validation Accuracy:0.3612\n",
    "Epoch #155: Loss:0.9615, Accuracy:0.5211, Validation Loss:1.2214, Validation Accuracy:0.3777\n",
    "Epoch #156: Loss:0.9626, Accuracy:0.5191, Validation Loss:1.2109, Validation Accuracy:0.3645\n",
    "Epoch #157: Loss:0.9653, Accuracy:0.5154, Validation Loss:1.1930, Validation Accuracy:0.3563\n",
    "Epoch #158: Loss:0.9543, Accuracy:0.5248, Validation Loss:1.2013, Validation Accuracy:0.3612\n",
    "Epoch #159: Loss:0.9494, Accuracy:0.5257, Validation Loss:1.2179, Validation Accuracy:0.3580\n",
    "Epoch #160: Loss:0.9506, Accuracy:0.5269, Validation Loss:1.2296, Validation Accuracy:0.3711\n",
    "Epoch #161: Loss:0.9533, Accuracy:0.5199, Validation Loss:1.2195, Validation Accuracy:0.3629\n",
    "Epoch #162: Loss:0.9464, Accuracy:0.5277, Validation Loss:1.2166, Validation Accuracy:0.3481\n",
    "Epoch #163: Loss:0.9454, Accuracy:0.5257, Validation Loss:1.2241, Validation Accuracy:0.3662\n",
    "Epoch #164: Loss:0.9461, Accuracy:0.5240, Validation Loss:1.2121, Validation Accuracy:0.3399\n",
    "Epoch #165: Loss:0.9462, Accuracy:0.5347, Validation Loss:1.2191, Validation Accuracy:0.3514\n",
    "Epoch #166: Loss:0.9491, Accuracy:0.5261, Validation Loss:1.2320, Validation Accuracy:0.3596\n",
    "Epoch #167: Loss:0.9430, Accuracy:0.5392, Validation Loss:1.2320, Validation Accuracy:0.3645\n",
    "Epoch #168: Loss:0.9431, Accuracy:0.5314, Validation Loss:1.2354, Validation Accuracy:0.3629\n",
    "Epoch #169: Loss:0.9410, Accuracy:0.5372, Validation Loss:1.2251, Validation Accuracy:0.3547\n",
    "Epoch #170: Loss:0.9391, Accuracy:0.5322, Validation Loss:1.2309, Validation Accuracy:0.3629\n",
    "Epoch #171: Loss:0.9392, Accuracy:0.5335, Validation Loss:1.2407, Validation Accuracy:0.3612\n",
    "Epoch #172: Loss:0.9398, Accuracy:0.5343, Validation Loss:1.2512, Validation Accuracy:0.3662\n",
    "Epoch #173: Loss:0.9420, Accuracy:0.5306, Validation Loss:1.2549, Validation Accuracy:0.3662\n",
    "Epoch #174: Loss:0.9477, Accuracy:0.5306, Validation Loss:1.2230, Validation Accuracy:0.3481\n",
    "Epoch #175: Loss:0.9303, Accuracy:0.5335, Validation Loss:1.2369, Validation Accuracy:0.3530\n",
    "Epoch #176: Loss:0.9293, Accuracy:0.5396, Validation Loss:1.2349, Validation Accuracy:0.3432\n",
    "Epoch #177: Loss:0.9292, Accuracy:0.5396, Validation Loss:1.2591, Validation Accuracy:0.3563\n",
    "Epoch #178: Loss:0.9293, Accuracy:0.5285, Validation Loss:1.2656, Validation Accuracy:0.3366\n",
    "Epoch #179: Loss:0.9282, Accuracy:0.5359, Validation Loss:1.2760, Validation Accuracy:0.3498\n",
    "Epoch #180: Loss:0.9274, Accuracy:0.5363, Validation Loss:1.2734, Validation Accuracy:0.3530\n",
    "Epoch #181: Loss:0.9306, Accuracy:0.5437, Validation Loss:1.2466, Validation Accuracy:0.3415\n",
    "Epoch #182: Loss:0.9275, Accuracy:0.5417, Validation Loss:1.2289, Validation Accuracy:0.3251\n",
    "Epoch #183: Loss:0.9381, Accuracy:0.5417, Validation Loss:1.2816, Validation Accuracy:0.3563\n",
    "Epoch #184: Loss:0.9289, Accuracy:0.5294, Validation Loss:1.2354, Validation Accuracy:0.3383\n",
    "Epoch #185: Loss:0.9225, Accuracy:0.5405, Validation Loss:1.2498, Validation Accuracy:0.3300\n",
    "Epoch #186: Loss:0.9219, Accuracy:0.5355, Validation Loss:1.2671, Validation Accuracy:0.3465\n",
    "Epoch #187: Loss:0.9194, Accuracy:0.5433, Validation Loss:1.2680, Validation Accuracy:0.3415\n",
    "Epoch #188: Loss:0.9126, Accuracy:0.5470, Validation Loss:1.2890, Validation Accuracy:0.3530\n",
    "Epoch #189: Loss:0.9086, Accuracy:0.5556, Validation Loss:1.2961, Validation Accuracy:0.3366\n",
    "Epoch #190: Loss:0.9117, Accuracy:0.5368, Validation Loss:1.3039, Validation Accuracy:0.3530\n",
    "Epoch #191: Loss:0.9093, Accuracy:0.5466, Validation Loss:1.2988, Validation Accuracy:0.3481\n",
    "Epoch #192: Loss:0.9105, Accuracy:0.5446, Validation Loss:1.3012, Validation Accuracy:0.3481\n",
    "Epoch #193: Loss:0.9101, Accuracy:0.5520, Validation Loss:1.2962, Validation Accuracy:0.3333\n",
    "Epoch #194: Loss:0.9009, Accuracy:0.5540, Validation Loss:1.2834, Validation Accuracy:0.3481\n",
    "Epoch #195: Loss:0.9019, Accuracy:0.5556, Validation Loss:1.3291, Validation Accuracy:0.3415\n",
    "Epoch #196: Loss:0.9205, Accuracy:0.5368, Validation Loss:1.3071, Validation Accuracy:0.3415\n",
    "Epoch #197: Loss:0.9032, Accuracy:0.5446, Validation Loss:1.2851, Validation Accuracy:0.3202\n",
    "Epoch #198: Loss:0.8993, Accuracy:0.5589, Validation Loss:1.2938, Validation Accuracy:0.3366\n",
    "Epoch #199: Loss:0.8980, Accuracy:0.5536, Validation Loss:1.3072, Validation Accuracy:0.3202\n",
    "Epoch #200: Loss:0.8921, Accuracy:0.5528, Validation Loss:1.3026, Validation Accuracy:0.3169\n",
    "Epoch #201: Loss:0.8913, Accuracy:0.5585, Validation Loss:1.3153, Validation Accuracy:0.3530\n",
    "Epoch #202: Loss:0.8843, Accuracy:0.5585, Validation Loss:1.3236, Validation Accuracy:0.3202\n",
    "Epoch #203: Loss:0.8812, Accuracy:0.5684, Validation Loss:1.2950, Validation Accuracy:0.3383\n",
    "Epoch #204: Loss:0.9017, Accuracy:0.5437, Validation Loss:1.3235, Validation Accuracy:0.3399\n",
    "Epoch #205: Loss:0.8857, Accuracy:0.5565, Validation Loss:1.3211, Validation Accuracy:0.3120\n",
    "Epoch #206: Loss:0.8771, Accuracy:0.5614, Validation Loss:1.3564, Validation Accuracy:0.3383\n",
    "Epoch #207: Loss:0.8802, Accuracy:0.5663, Validation Loss:1.3228, Validation Accuracy:0.3284\n",
    "Epoch #208: Loss:0.8786, Accuracy:0.5639, Validation Loss:1.3476, Validation Accuracy:0.3218\n",
    "Epoch #209: Loss:0.8988, Accuracy:0.5528, Validation Loss:1.3543, Validation Accuracy:0.3383\n",
    "Epoch #210: Loss:0.8816, Accuracy:0.5602, Validation Loss:1.3251, Validation Accuracy:0.3120\n",
    "Epoch #211: Loss:0.8708, Accuracy:0.5692, Validation Loss:1.3425, Validation Accuracy:0.3547\n",
    "Epoch #212: Loss:0.8768, Accuracy:0.5667, Validation Loss:1.3508, Validation Accuracy:0.3268\n",
    "Epoch #213: Loss:0.8744, Accuracy:0.5630, Validation Loss:1.3400, Validation Accuracy:0.3120\n",
    "Epoch #214: Loss:0.8741, Accuracy:0.5647, Validation Loss:1.3694, Validation Accuracy:0.3563\n",
    "Epoch #215: Loss:0.8754, Accuracy:0.5589, Validation Loss:1.3347, Validation Accuracy:0.3399\n",
    "Epoch #216: Loss:0.8999, Accuracy:0.5598, Validation Loss:1.3617, Validation Accuracy:0.3596\n",
    "Epoch #217: Loss:0.8843, Accuracy:0.5622, Validation Loss:1.3271, Validation Accuracy:0.3038\n",
    "Epoch #218: Loss:0.8872, Accuracy:0.5745, Validation Loss:1.3550, Validation Accuracy:0.3481\n",
    "Epoch #219: Loss:0.8788, Accuracy:0.5585, Validation Loss:1.3521, Validation Accuracy:0.3350\n",
    "Epoch #220: Loss:0.8705, Accuracy:0.5598, Validation Loss:1.3531, Validation Accuracy:0.3366\n",
    "Epoch #221: Loss:0.8854, Accuracy:0.5515, Validation Loss:1.3465, Validation Accuracy:0.3350\n",
    "Epoch #222: Loss:0.8862, Accuracy:0.5561, Validation Loss:1.3490, Validation Accuracy:0.3235\n",
    "Epoch #223: Loss:0.8618, Accuracy:0.5799, Validation Loss:1.3568, Validation Accuracy:0.3268\n",
    "Epoch #224: Loss:0.8563, Accuracy:0.5762, Validation Loss:1.3891, Validation Accuracy:0.3383\n",
    "Epoch #225: Loss:0.8608, Accuracy:0.5782, Validation Loss:1.3802, Validation Accuracy:0.3251\n",
    "Epoch #226: Loss:0.8480, Accuracy:0.5840, Validation Loss:1.3851, Validation Accuracy:0.3202\n",
    "Epoch #227: Loss:0.8464, Accuracy:0.5844, Validation Loss:1.3920, Validation Accuracy:0.3547\n",
    "Epoch #228: Loss:0.8639, Accuracy:0.5754, Validation Loss:1.3824, Validation Accuracy:0.3103\n",
    "Epoch #229: Loss:0.8561, Accuracy:0.5754, Validation Loss:1.3956, Validation Accuracy:0.3465\n",
    "Epoch #230: Loss:0.8574, Accuracy:0.5762, Validation Loss:1.3905, Validation Accuracy:0.3333\n",
    "Epoch #231: Loss:0.8860, Accuracy:0.5602, Validation Loss:1.3847, Validation Accuracy:0.3333\n",
    "Epoch #232: Loss:0.8869, Accuracy:0.5589, Validation Loss:1.3744, Validation Accuracy:0.3432\n",
    "Epoch #233: Loss:0.8643, Accuracy:0.5692, Validation Loss:1.3731, Validation Accuracy:0.3366\n",
    "Epoch #234: Loss:0.8578, Accuracy:0.5815, Validation Loss:1.3726, Validation Accuracy:0.3120\n",
    "Epoch #235: Loss:0.8434, Accuracy:0.5848, Validation Loss:1.3847, Validation Accuracy:0.3202\n",
    "Epoch #236: Loss:0.8458, Accuracy:0.5803, Validation Loss:1.3959, Validation Accuracy:0.3202\n",
    "Epoch #237: Loss:0.8512, Accuracy:0.5799, Validation Loss:1.4103, Validation Accuracy:0.3350\n",
    "Epoch #238: Loss:0.8499, Accuracy:0.5749, Validation Loss:1.4239, Validation Accuracy:0.3366\n",
    "Epoch #239: Loss:0.8521, Accuracy:0.5786, Validation Loss:1.4101, Validation Accuracy:0.3103\n",
    "Epoch #240: Loss:0.8532, Accuracy:0.5737, Validation Loss:1.4069, Validation Accuracy:0.3432\n",
    "Epoch #241: Loss:0.8720, Accuracy:0.5598, Validation Loss:1.3589, Validation Accuracy:0.3268\n",
    "Epoch #242: Loss:0.8529, Accuracy:0.5725, Validation Loss:1.3864, Validation Accuracy:0.3186\n",
    "Epoch #243: Loss:0.8568, Accuracy:0.5729, Validation Loss:1.4144, Validation Accuracy:0.3399\n",
    "Epoch #244: Loss:0.8296, Accuracy:0.5959, Validation Loss:1.4035, Validation Accuracy:0.3268\n",
    "Epoch #245: Loss:0.8270, Accuracy:0.5992, Validation Loss:1.4344, Validation Accuracy:0.3268\n",
    "Epoch #246: Loss:0.8208, Accuracy:0.6021, Validation Loss:1.4431, Validation Accuracy:0.3383\n",
    "Epoch #247: Loss:0.8179, Accuracy:0.6037, Validation Loss:1.4442, Validation Accuracy:0.3300\n",
    "Epoch #248: Loss:0.8186, Accuracy:0.6012, Validation Loss:1.4399, Validation Accuracy:0.3202\n",
    "Epoch #249: Loss:0.8113, Accuracy:0.6053, Validation Loss:1.4663, Validation Accuracy:0.3366\n",
    "Epoch #250: Loss:0.8167, Accuracy:0.6021, Validation Loss:1.4220, Validation Accuracy:0.3136\n",
    "Epoch #251: Loss:0.8199, Accuracy:0.6000, Validation Loss:1.4607, Validation Accuracy:0.3514\n",
    "Epoch #252: Loss:0.8307, Accuracy:0.5906, Validation Loss:1.4147, Validation Accuracy:0.3136\n",
    "Epoch #253: Loss:0.8648, Accuracy:0.5823, Validation Loss:1.4233, Validation Accuracy:0.3448\n",
    "Epoch #254: Loss:0.8730, Accuracy:0.5663, Validation Loss:1.3989, Validation Accuracy:0.3153\n",
    "Epoch #255: Loss:0.8790, Accuracy:0.5577, Validation Loss:1.4284, Validation Accuracy:0.3547\n",
    "Epoch #256: Loss:0.8610, Accuracy:0.5811, Validation Loss:1.4119, Validation Accuracy:0.3415\n",
    "Epoch #257: Loss:0.8223, Accuracy:0.5984, Validation Loss:1.4161, Validation Accuracy:0.3251\n",
    "Epoch #258: Loss:0.8378, Accuracy:0.5840, Validation Loss:1.4537, Validation Accuracy:0.3547\n",
    "Epoch #259: Loss:0.8254, Accuracy:0.5947, Validation Loss:1.4416, Validation Accuracy:0.3284\n",
    "Epoch #260: Loss:0.8304, Accuracy:0.5951, Validation Loss:1.4571, Validation Accuracy:0.3415\n",
    "Epoch #261: Loss:0.8436, Accuracy:0.5844, Validation Loss:1.3895, Validation Accuracy:0.3202\n",
    "Epoch #262: Loss:0.8478, Accuracy:0.5708, Validation Loss:1.4207, Validation Accuracy:0.3268\n",
    "Epoch #263: Loss:0.8539, Accuracy:0.5811, Validation Loss:1.4009, Validation Accuracy:0.3300\n",
    "Epoch #264: Loss:0.8427, Accuracy:0.5852, Validation Loss:1.4261, Validation Accuracy:0.3186\n",
    "Epoch #265: Loss:0.8327, Accuracy:0.5897, Validation Loss:1.4337, Validation Accuracy:0.3136\n",
    "Epoch #266: Loss:0.8214, Accuracy:0.6021, Validation Loss:1.4459, Validation Accuracy:0.3235\n",
    "Epoch #267: Loss:0.8198, Accuracy:0.5996, Validation Loss:1.4436, Validation Accuracy:0.3317\n",
    "Epoch #268: Loss:0.8190, Accuracy:0.6057, Validation Loss:1.4409, Validation Accuracy:0.3268\n",
    "Epoch #269: Loss:0.8080, Accuracy:0.6103, Validation Loss:1.4483, Validation Accuracy:0.3235\n",
    "Epoch #270: Loss:0.8069, Accuracy:0.6115, Validation Loss:1.4760, Validation Accuracy:0.3366\n",
    "Epoch #271: Loss:0.8040, Accuracy:0.6115, Validation Loss:1.4423, Validation Accuracy:0.3169\n",
    "Epoch #272: Loss:0.8002, Accuracy:0.6053, Validation Loss:1.4722, Validation Accuracy:0.3629\n",
    "Epoch #273: Loss:0.8150, Accuracy:0.6045, Validation Loss:1.4508, Validation Accuracy:0.3284\n",
    "Epoch #274: Loss:0.7991, Accuracy:0.6090, Validation Loss:1.4852, Validation Accuracy:0.3350\n",
    "Epoch #275: Loss:0.8025, Accuracy:0.6070, Validation Loss:1.4609, Validation Accuracy:0.3333\n",
    "Epoch #276: Loss:0.8139, Accuracy:0.6045, Validation Loss:1.4800, Validation Accuracy:0.3448\n",
    "Epoch #277: Loss:0.8141, Accuracy:0.6021, Validation Loss:1.4491, Validation Accuracy:0.3153\n",
    "Epoch #278: Loss:0.7966, Accuracy:0.6144, Validation Loss:1.4954, Validation Accuracy:0.3498\n",
    "Epoch #279: Loss:0.7827, Accuracy:0.6292, Validation Loss:1.4878, Validation Accuracy:0.3300\n",
    "Epoch #280: Loss:0.7808, Accuracy:0.6263, Validation Loss:1.4691, Validation Accuracy:0.3415\n",
    "Epoch #281: Loss:0.7826, Accuracy:0.6271, Validation Loss:1.4818, Validation Accuracy:0.3268\n",
    "Epoch #282: Loss:0.7763, Accuracy:0.6287, Validation Loss:1.5251, Validation Accuracy:0.3251\n",
    "Epoch #283: Loss:0.7718, Accuracy:0.6390, Validation Loss:1.5168, Validation Accuracy:0.3317\n",
    "Epoch #284: Loss:0.7761, Accuracy:0.6345, Validation Loss:1.5063, Validation Accuracy:0.3498\n",
    "Epoch #285: Loss:0.8028, Accuracy:0.6090, Validation Loss:1.5237, Validation Accuracy:0.3235\n",
    "Epoch #286: Loss:0.7978, Accuracy:0.6172, Validation Loss:1.4732, Validation Accuracy:0.3268\n",
    "Epoch #287: Loss:0.7729, Accuracy:0.6324, Validation Loss:1.5365, Validation Accuracy:0.3465\n",
    "Epoch #288: Loss:0.7672, Accuracy:0.6349, Validation Loss:1.5076, Validation Accuracy:0.3300\n",
    "Epoch #289: Loss:0.7725, Accuracy:0.6259, Validation Loss:1.5377, Validation Accuracy:0.3481\n",
    "Epoch #290: Loss:0.8138, Accuracy:0.6053, Validation Loss:1.4565, Validation Accuracy:0.3268\n",
    "Epoch #291: Loss:0.8290, Accuracy:0.6012, Validation Loss:1.4448, Validation Accuracy:0.3186\n",
    "Epoch #292: Loss:0.8031, Accuracy:0.6164, Validation Loss:1.5566, Validation Accuracy:0.3678\n",
    "Epoch #293: Loss:0.7812, Accuracy:0.6205, Validation Loss:1.5263, Validation Accuracy:0.3120\n",
    "Epoch #294: Loss:0.7829, Accuracy:0.6287, Validation Loss:1.5184, Validation Accuracy:0.3366\n",
    "Epoch #295: Loss:0.7854, Accuracy:0.6246, Validation Loss:1.5123, Validation Accuracy:0.3120\n",
    "Epoch #296: Loss:0.7861, Accuracy:0.6242, Validation Loss:1.5511, Validation Accuracy:0.3514\n",
    "Epoch #297: Loss:0.7782, Accuracy:0.6312, Validation Loss:1.5379, Validation Accuracy:0.3350\n",
    "Epoch #298: Loss:0.7698, Accuracy:0.6333, Validation Loss:1.5194, Validation Accuracy:0.3235\n",
    "Epoch #299: Loss:0.7600, Accuracy:0.6452, Validation Loss:1.5300, Validation Accuracy:0.3317\n",
    "Epoch #300: Loss:0.7687, Accuracy:0.6312, Validation Loss:1.5528, Validation Accuracy:0.3218\n",
    "\n",
    "Test:\n",
    "Test Loss:1.55279255, Accuracy:0.3218\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  03   01\n",
    "t:02  69  54  104\n",
    "t:03  50  29   63\n",
    "t:01  84  58   98\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.34      0.30      0.32       227\n",
    "          03       0.21      0.20      0.20       142\n",
    "          01       0.37      0.41      0.39       240\n",
    "\n",
    "    accuracy                           0.32       609\n",
    "   macro avg       0.31      0.31      0.30       609\n",
    "weighted avg       0.32      0.32      0.32       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 07:20:12 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0832998864169192, 1.0768902618896785, 1.074586857911597, 1.0747349457983508, 1.0750863900521315, 1.074944774897032, 1.0744555693346096, 1.0744256360581748, 1.0744763397426635, 1.0743707543719188, 1.0743410170371896, 1.0743864001507437, 1.0743675960108565, 1.074383878355543, 1.0743754763517082, 1.0743762446546006, 1.0743619972849128, 1.0743228557270343, 1.074321039987511, 1.074359957024773, 1.0742935564717635, 1.074362282682522, 1.0743117851185289, 1.0742678021758256, 1.0742819516725337, 1.0742791256881112, 1.0743766065888805, 1.074255001368781, 1.0743211315966201, 1.0743370884157755, 1.0742352081245585, 1.074344189491961, 1.0741463909399724, 1.074240140139763, 1.074457307754479, 1.0744625373035426, 1.0748893040154368, 1.0749884160672893, 1.0745750990602967, 1.0746715423117326, 1.0752064034660853, 1.0754266421391654, 1.0756756979452173, 1.075784873884104, 1.0763491523285413, 1.076751718575927, 1.0771985621679396, 1.0782096243061259, 1.0788986193526946, 1.0809243422227932, 1.0813258577058664, 1.0831153077640752, 1.0838937561695994, 1.0923318643679565, 1.087462623326845, 1.0824897387149104, 1.0857362882257096, 1.0829446726831897, 1.084140824958413, 1.0859675994647549, 1.0863572856279822, 1.0863451806978248, 1.0892326563645662, 1.090740334811469, 1.09019805331927, 1.0923055198979494, 1.0916435730281135, 1.0919952474791428, 1.092436198726272, 1.0921946168924592, 1.0935503420571389, 1.0931662567730607, 1.1018875751197827, 1.0965738969874892, 1.0946433140922258, 1.0985885721513595, 1.0998208802713354, 1.0981180716813688, 1.0966156143664532, 1.0992469407850494, 1.1004941273596878, 1.1015302872618626, 1.1039260633669072, 1.1046778952155403, 1.1076153491322434, 1.103332173256647, 1.102607090093428, 1.1027109346953519, 1.1107765426385188, 1.1070444916660953, 1.1083881602498697, 1.1212711915594016, 1.1125457680284097, 1.117302783800072, 1.1141096183231898, 1.1160348121364325, 1.1129326904544299, 1.1136057797715386, 1.1158943315249163, 1.1127833584063551, 1.119097322274507, 1.1226557763339264, 1.1250887561118466, 1.1252635408113352, 1.1271371655472002, 1.1288036317465144, 1.1317124762167092, 1.1298074651821493, 1.1284503482636952, 1.142620933858436, 1.1429927030024662, 1.1397914123065367, 1.1402020994665587, 1.1408171095871573, 1.1371577253874103, 1.1336188169535746, 1.1283804673475193, 1.1383953039673553, 1.1449156193114658, 1.1505486078450244, 1.1388486964362008, 1.1488363962063843, 1.1635514478182363, 1.1544928548762756, 1.1471177854365708, 1.1559683149084081, 1.1587535324942302, 1.1578401497432165, 1.1574107204947761, 1.1668663931206138, 1.1658983120973083, 1.1690529759098547, 1.1697415763325683, 1.1687639618937802, 1.1697332244396992, 1.1607091622595325, 1.1694139919453261, 1.1747568383788436, 1.1761601347054167, 1.1836424844801328, 1.1775406557938148, 1.189970309511194, 1.1900552288064816, 1.206204789025443, 1.2005314124237336, 1.1992751900198424, 1.2029150585431378, 1.2137199939765366, 1.2015602058182013, 1.1813274551494954, 1.2032956728598558, 1.2036365162954346, 1.1840717050633798, 1.2037402183942998, 1.2213683948532505, 1.2109420277997973, 1.192961836879085, 1.2012650780685625, 1.217915200834791, 1.2296239973484784, 1.2195420801541683, 1.2165630494041004, 1.2241222363191677, 1.2121257995345518, 1.2191066111641369, 1.2320232829828373, 1.232009188099252, 1.2354195094460925, 1.2250624343092218, 1.2308719986178018, 1.2406712380927576, 1.251166563315932, 1.2548829798627956, 1.2229790530964266, 1.2368780251207023, 1.2349016167260156, 1.2591005610714991, 1.2656447709291831, 1.2759573097495218, 1.2733637906843414, 1.2466194913500832, 1.2289109960173934, 1.2816065604659332, 1.2353753816513788, 1.249774455632678, 1.2670659977814247, 1.2680200095638656, 1.2890354365550827, 1.2961494556592994, 1.3038812090806382, 1.2988220829094572, 1.3012206607264252, 1.2961526715696738, 1.2833642824530014, 1.3290666343739075, 1.3071393620204457, 1.2850621899556252, 1.2937539348069866, 1.307248036653929, 1.3025934320365267, 1.3153130213419597, 1.3235548264874613, 1.2949814258146364, 1.3235044680988455, 1.3210671967865015, 1.3563993695530006, 1.3228346849309986, 1.347618759755039, 1.3542980585975208, 1.3250641022213965, 1.3425466178477496, 1.3507815777570351, 1.3399775010611623, 1.3694053647553392, 1.3346812854259473, 1.3617034347969519, 1.3270784596895742, 1.3549791245624936, 1.3520795504252117, 1.3530622765739955, 1.3464874426523845, 1.3489743453528493, 1.3568159292875643, 1.3890911959270733, 1.3802008977273024, 1.385082809991633, 1.3920123557543325, 1.3823660522063181, 1.3956034488865894, 1.3905371517579153, 1.3847242076995925, 1.3744138976427527, 1.373124122228137, 1.3726359558810155, 1.3847322548160022, 1.3958895519644952, 1.4103035738902727, 1.4238778548483386, 1.4101143272835241, 1.4069072104048455, 1.3588943363997736, 1.3864126407062674, 1.4144498123519722, 1.4035048048288756, 1.4343820645891387, 1.443063916439689, 1.4441814968738649, 1.4398795626629357, 1.466270743919711, 1.4220039376680096, 1.4606722060878485, 1.4146666861520025, 1.4233069284796127, 1.3989293837586452, 1.428375078148051, 1.4118565221138188, 1.4160732300997956, 1.453651249506595, 1.441594723997445, 1.4570606598517382, 1.38945153291981, 1.4207370126580174, 1.4008693812515935, 1.4261208941000827, 1.4337133157429436, 1.4459354487937464, 1.4435669843394965, 1.4409288396976265, 1.4483041675219983, 1.47603881085056, 1.442286647599319, 1.4721639677025806, 1.4507768909723693, 1.4852168810582904, 1.4608682272665214, 1.4799641582178953, 1.4490786358249208, 1.4954463186718168, 1.4878332497451106, 1.4691410937724247, 1.4818307466694873, 1.5251038397474241, 1.5168163876228145, 1.5062800514678454, 1.5236590452773622, 1.473221135061167, 1.5365439559438545, 1.5075892329411749, 1.5377268869497114, 1.4565271180251549, 1.4447932742499365, 1.5566266367979629, 1.5263443960149103, 1.518380909521983, 1.512343324463943, 1.5510560090122942, 1.5378987789154053, 1.5193895618316575, 1.5300248851525569, 1.5527925810398922], 'val_acc': [0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.39408866892307265, 0.3924466327982779, 0.3924466327982779, 0.3957307050478674, 0.39408866892307265, 0.38916256035294244, 0.3924466327982779, 0.38916256045081543, 0.3908045964777372, 0.3924466327004049, 0.3957307048521214, 0.38587848839697186, 0.3875205243260207, 0.3825944159516364, 0.3743842353276627, 0.39408866892307265, 0.38259441624525536, 0.3875205245217666, 0.3908045967713561, 0.39408866921669156, 0.38587848839697186, 0.37766830777299815, 0.38259441624525536, 0.3875205246196396, 0.37931034419141185, 0.37602627203969535, 0.4006568140094895, 0.38095238002258763, 0.37766830767512516, 0.37110016337169216, 0.3809523806098255, 0.38423645266366907, 0.39080459667348316, 0.3711001629802002, 0.3481116570373278, 0.38095238002258763, 0.35796387447120714, 0.36945812685540547, 0.35960591020451, 0.3908045965756102, 0.3776683080666171, 0.3908045965756102, 0.3711001631759462, 0.36617405470368897, 0.3563218381506665, 0.3661740550951809, 0.3678160913178486, 0.36781609121997566, 0.3711001631759462, 0.3694581273447704, 0.3645320188725132, 0.38916256045081543, 0.3645320189703862, 0.37110016337169216, 0.3776683081644901, 0.3645320186767672, 0.379310343604174, 0.3711001631759462, 0.36781609121997566, 0.3760262717460764, 0.37110016346956515, 0.3793103437020469, 0.3678160904369918, 0.37766830738150625, 0.36124794613355876, 0.3825944161473824, 0.3678160905348648, 0.3875205244238936, 0.3711001629802002, 0.3678160904369918, 0.36124794623143175, 0.3513957292869173, 0.37274219910499495, 0.3678160910242297, 0.35303776531383907, 0.3875205244238936, 0.3513957291890443, 0.37766830757725217, 0.3842364520764312, 0.3579638738839693, 0.3711001629802002, 0.3793103439956659, 0.36945812665965955, 0.36453201818740233, 0.366174054605816, 0.3678160907306107, 0.34646962101040607, 0.37766830738150625, 0.34975369316212257, 0.3645320185788942, 0.3793103438977929, 0.34975369316212257, 0.3973727407811702, 0.38095238002258763, 0.3448275844941194, 0.36781609063273774, 0.38587848800547997, 0.3612479460356858, 0.3842364518806852, 0.366174054507943, 0.34975369316212257, 0.3628899819647346, 0.38095237953322275, 0.3628899821604805, 0.37274219910499495, 0.3497536929663766, 0.36288998255197247, 0.379310343408428, 0.3711001629802002, 0.3431855486629436, 0.3825944156580175, 0.37274219881137605, 0.3678160908284837, 0.35796387407971525, 0.38587848810335296, 0.35632183766130154, 0.3711001628823273, 0.3563218380527935, 0.3579638738839693, 0.3579638737860963, 0.37602627135458444, 0.37602627145245743, 0.3760262715503304, 0.366174054605816, 0.35303776541171206, 0.3678160908284837, 0.372742199007122, 0.3628899822583535, 0.34154351263602184, 0.3661740542143241, 0.3628899823562265, 0.3546798015365068, 0.36124794623143175, 0.37766830747937924, 0.36453201818740233, 0.3563218379549205, 0.36124794583993985, 0.35796387398184226, 0.3711001630780732, 0.3628899824540995, 0.3481116571352008, 0.36617405499730793, 0.3399014765112271, 0.35139572967840926, 0.359605910302383, 0.36453201848102124, 0.3628899824540995, 0.3546798016343798, 0.3628899821604805, 0.36124794593781284, 0.366174054507943, 0.366174054507943, 0.34811165693945484, 0.353037765901077, 0.3431855482714517, 0.3563218380527935, 0.33661740387014566, 0.34975369316212257, 0.353037765803204, 0.34154351263602184, 0.32512315089870947, 0.35632183775917453, 0.33825944048430534, 0.33004925976245864, 0.3464696208146601, 0.34154351253814885, 0.353037765803204, 0.3366174041637646, 0.35303776560745803, 0.3481116572330738, 0.3481116576245657, 0.33333333230566703, 0.3481116571352008, 0.34154351253814885, 0.3415435127338948, 0.3201970432094361, 0.33661740406589163, 0.3201970430136901, 0.31691297076410063, 0.35303776550958504, 0.3201970429158172, 0.3382594400928134, 0.3399014764133541, 0.3119868619982245, 0.33825944048430534, 0.3284072236376639, 0.32183907894273894, 0.33825944038643235, 0.3119868622918434, 0.35467980183012576, 0.32676518741499616, 0.3119868619982245, 0.3563218380527935, 0.3399014764133541, 0.35960591020451, 0.3037766812763778, 0.3481116572330738, 0.3349753678432239, 0.3366174042616376, 0.33497536813684287, 0.3234811150675337, 0.3267651870235042, 0.33825944068005126, 0.32512315119232843, 0.3201970428179442, 0.3546798014386338, 0.31034482616704867, 0.3464696209125331, 0.3333333320120481, 0.33333333191417513, 0.3431855486629436, 0.3366174041637646, 0.3119868621939704, 0.3201970428179442, 0.3201970429158172, 0.3349753679410969, 0.3366174042616376, 0.31034482587342976, 0.34318554895656256, 0.32676518751286915, 0.31855500669314946, 0.33990147670697307, 0.3267651873171232, 0.3267651873171232, 0.33825944068005126, 0.33004925966458565, 0.32019704252432524, 0.33661740445738353, 0.31362889812301925, 0.3513957293847903, 0.3136288979272733, 0.34482758478773834, 0.315270934247814, 0.35467980202587174, 0.34154351244027586, 0.32512315089870947, 0.35467980212374467, 0.3284072234419179, 0.34154351263602184, 0.32019704232857926, 0.3267651872192502, 0.33004925956671266, 0.3185550063995305, 0.31362889802514626, 0.3234811147739147, 0.3316912957893804, 0.3267651871213772, 0.3234811148717877, 0.33661740445738353, 0.31691297017686276, 0.3628899823562265, 0.32840722324617194, 0.3349753683325888, 0.33333333191417513, 0.3448275848856113, 0.315270934345687, 0.34975369325999556, 0.3300492593709667, 0.34154351253814885, 0.3267651872192502, 0.3251231512902014, 0.33169129549576143, 0.34975369325999556, 0.3234811148717877, 0.3267651871213772, 0.34646962120615205, 0.3300492593709667, 0.3481116572330738, 0.3267651870235042, 0.3185550060080386, 0.36781609121997566, 0.31198686180247853, 0.3366174041637646, 0.3119868619003515, 0.3513957295805363, 0.33497536823471585, 0.3234811147739147, 0.3316912957893804, 0.32183907864912], 'loss': [1.0908386263269663, 1.0802656931064456, 1.0753199148961405, 1.0741633481803126, 1.074535019980319, 1.0748676333828873, 1.0744932653478039, 1.074386692291902, 1.0743832159825664, 1.074348394728784, 1.074160079691689, 1.0742657326085367, 1.0741880416870118, 1.0741372716500284, 1.0742123967568242, 1.0741095227627293, 1.074107679743052, 1.0740707255486834, 1.0738900487183056, 1.0740305875116305, 1.0738633192050628, 1.073770076929911, 1.073768014878463, 1.0735648845010715, 1.0734997113872113, 1.0734015981764276, 1.0733522450409876, 1.0732932028584412, 1.0732433109557604, 1.0730318079738892, 1.073035767533696, 1.072776815876579, 1.072975361224807, 1.0725028992678352, 1.0720387496987407, 1.0722265585736817, 1.0725543319322246, 1.0713359071488742, 1.0710772179969772, 1.0710319328112279, 1.070036949856815, 1.0696968069801096, 1.0699349012218218, 1.070609285062833, 1.0691124706542467, 1.0672706007712676, 1.0671485763800463, 1.06656347941569, 1.0650113832779244, 1.0643795876532365, 1.0629982234516182, 1.0628176576058233, 1.0614195207306003, 1.0606786316669943, 1.0657928477567324, 1.067242299213057, 1.0626385528204132, 1.0602993453552592, 1.058642092622526, 1.0593663117234466, 1.0597852343161738, 1.0576353538207695, 1.0557476069648164, 1.0557511477499772, 1.058532759541114, 1.0558566998162553, 1.0556309707355696, 1.0530449489303684, 1.0530964550296384, 1.051927748206215, 1.0509791616052082, 1.0501335498978226, 1.048274625693993, 1.051230350020485, 1.0493413171239456, 1.045390455531878, 1.0503860425165792, 1.0461248048277116, 1.0468295395986256, 1.0425387052539927, 1.0414472062974496, 1.040559477336108, 1.0414996763519193, 1.0413489944880994, 1.0394714984071327, 1.0417569101223956, 1.0411388091238127, 1.0363930813340925, 1.0340222327860964, 1.0299599211318782, 1.0299082756532045, 1.0364330433232583, 1.0347666216092433, 1.0301635201706778, 1.0302564102521423, 1.0262529535215248, 1.0260459275216292, 1.0245365848286685, 1.0228588082707148, 1.0235199334929856, 1.0201620555021924, 1.0173675644813867, 1.0158354944272208, 1.0139462249724527, 1.01638894722447, 1.0126959859468119, 1.0095447867313205, 1.008905253273261, 1.009202407419804, 1.0041415822334603, 1.0078802266894424, 1.0033998038489715, 1.0078944440739845, 1.0047812472378694, 1.0090367683394978, 1.0080269834099365, 1.0036129449672033, 1.0014328197287339, 0.9967917832505776, 0.9945554236367008, 0.9948379104876666, 0.9978322203644002, 0.9918404505238151, 0.996502361748008, 0.9913544941731792, 0.985522669492561, 0.9883747183811493, 0.9884715862097926, 0.9837688443597092, 0.9812981909305408, 0.9810255271208604, 0.9845417005570273, 0.9822798390408072, 0.981289398792588, 0.979354137953302, 0.9862163143970638, 0.9859733611406487, 0.9751581227020562, 0.9720040530394725, 0.9732370082357825, 0.9717784708285478, 0.9661403145878222, 0.9682509756430953, 0.9651753740144217, 0.965455552782611, 0.9622446208519123, 0.9644507582672323, 0.9621314207875998, 0.9660981132754065, 0.9593335456916684, 0.9661790890615334, 0.9642904935676214, 0.9677729831828719, 0.9638050413719192, 0.9615410229263854, 0.9626209690585519, 0.9652897323181497, 0.9543310394522101, 0.9493863490083134, 0.9506042984232032, 0.9532794020748726, 0.9464485853604467, 0.9454325056908311, 0.9461332899099503, 0.9461868943619777, 0.9490825867506023, 0.9430279780951858, 0.9430737548785043, 0.9409783492832458, 0.9390741797198505, 0.9391845235589593, 0.9398372160335831, 0.9419683279442836, 0.9477122996132477, 0.9302629705082465, 0.9293185924847268, 0.9292432238189102, 0.9292996697602086, 0.9282109149917195, 0.9274367193177006, 0.930612781307291, 0.9274954458281734, 0.9381318075456169, 0.9288958104484135, 0.9225489353986737, 0.9219327149939488, 0.9194280333097955, 0.9126194616362789, 0.9085751275507087, 0.9116736458801881, 0.9092735703476156, 0.9104779149227809, 0.9100929795349403, 0.9008937087636709, 0.9018802348348395, 0.920490168643928, 0.9032179708598331, 0.899347851530972, 0.8980489384467107, 0.8920828017855572, 0.891279821009117, 0.8842697752085065, 0.8811936234057072, 0.9017319976916304, 0.8857007568132217, 0.8770917727717139, 0.8801875100733073, 0.8786244861154341, 0.8988443032427246, 0.881621498909819, 0.8708017354628389, 0.8767857356482708, 0.8743963164225741, 0.8741085589054429, 0.8754371696184303, 0.8998965582074081, 0.884339649427598, 0.8871979736939103, 0.8788391386948572, 0.8705296345315185, 0.8853620017088903, 0.8861793756240203, 0.8618249652811634, 0.8563422511245681, 0.8607746503436345, 0.847999263030058, 0.8464397479376509, 0.863864618772354, 0.8561355791787102, 0.8573698543425213, 0.8859824520606525, 0.8869416213623063, 0.8642928771169768, 0.85784255518316, 0.843368897643667, 0.8458476797021635, 0.8511722609247759, 0.8498659498637706, 0.852054286027591, 0.853234483280221, 0.8720444592607095, 0.8528577010979153, 0.8567887012963422, 0.8296149861641243, 0.8269953338272518, 0.8207912460244902, 0.8178558381431157, 0.8185908530037506, 0.8112526664743678, 0.8166854658410779, 0.8199255705124544, 0.8306857460822903, 0.8648205619817887, 0.8729939979694218, 0.8790459170968136, 0.8610064901610419, 0.8223258668392346, 0.8377540423884774, 0.8254400302742051, 0.8303997794950277, 0.8435930877740379, 0.84783957278704, 0.8538723831793611, 0.8427173999790294, 0.8327314023119713, 0.821374143883433, 0.8197666696454465, 0.8190008478977352, 0.8080113189421151, 0.8068578338965743, 0.8039946106180273, 0.800186766589202, 0.8150339273211893, 0.7991299259589193, 0.8025048078942348, 0.8138892318189022, 0.8140750500210991, 0.7965630401820862, 0.7827187559932653, 0.7808369376331384, 0.7825514791927298, 0.7762841660628819, 0.771790138676426, 0.7761421441542294, 0.8028165183517723, 0.797834401649616, 0.7728812007199077, 0.7671844292225534, 0.772526012700686, 0.8137793347086505, 0.8289533885597449, 0.8031140232233052, 0.7812453534324066, 0.7828628350087504, 0.785359040349416, 0.7860543030732956, 0.7781870739660713, 0.769832248976588, 0.759962788156905, 0.7687489354879704], 'acc': [0.39425051156010715, 0.3942505133225443, 0.39425051211086876, 0.3942505119150424, 0.3942505146933287, 0.39425051273506523, 0.39425051391002336, 0.3942505125392389, 0.39425051391002336, 0.39425051508498143, 0.39425051547663414, 0.39425051156010715, 0.3942505146933287, 0.39425051391002336, 0.3942505113275634, 0.39425051116845444, 0.39425051211086876, 0.39425051449750237, 0.3942505156724605, 0.3942505129308916, 0.39425051156010715, 0.39589322575308705, 0.3971252546540521, 0.3975359332879711, 0.3975359350504082, 0.3983572908740269, 0.3983572873124352, 0.3950718685586839, 0.3942505121475862, 0.4004106759902633, 0.3942505113275634, 0.4012320330255575, 0.3905544161307004, 0.39425051449750237, 0.4012320314222293, 0.4090349093354948, 0.39342915710727294, 0.4004106770061125, 0.41190964890456544, 0.40000000173795885, 0.4123203290683778, 0.4176591383358291, 0.4032854232699964, 0.40205339002413426, 0.4049281335097319, 0.4197125256061554, 0.4229979461590612, 0.4205338806464687, 0.4234086241687837, 0.4271047227314121, 0.42874743477030214, 0.43244353039553524, 0.4271047217522803, 0.41848049314359864, 0.4242299801882288, 0.4143737161795951, 0.4193018486122821, 0.4295687880481783, 0.43819301763599167, 0.43613962974146897, 0.426283369294427, 0.4386036952540615, 0.4316221751593956, 0.43983572889157635, 0.4299794648829427, 0.4381930188109498, 0.4365503101378251, 0.4402464057263408, 0.4365503069678861, 0.44188911635772893, 0.4398357302623608, 0.4517453791914045, 0.4464065698015616, 0.44147844073464004, 0.4501026685232989, 0.4492813144988348, 0.4435318256550501, 0.4521560591961079, 0.4464065692140826, 0.4537987688850818, 0.46160164280838545, 0.4628336752709422, 0.46036961136167787, 0.4583162228796761, 0.45297741380805107, 0.4587268987353088, 0.45503079954848397, 0.4611909671485791, 0.47474332423670335, 0.4665297758285515, 0.4755646802561484, 0.46858316192391963, 0.46160164182925373, 0.4681724833634355, 0.46899384114531767, 0.4694045170009503, 0.47556468283860837, 0.47679671232705245, 0.48172484472302196, 0.47720739429001935, 0.4751540067504319, 0.488295689782078, 0.48665297711899147, 0.4887063657968196, 0.48583162070789376, 0.4850102666467122, 0.49938398367080844, 0.4932238192039349, 0.48870636383855615, 0.5002053400819062, 0.4850102680542141, 0.5059548243849674, 0.483778235395831, 0.49568788687061727, 0.49158110755669754, 0.4792607794062558, 0.49404517581085894, 0.49815194924998824, 0.5063655039613008, 0.5092402480710948, 0.5059548265390572, 0.5039014368820973, 0.5080082128669692, 0.4977412739818346, 0.5104722809253043, 0.5117043135836874, 0.5022587277806025, 0.5043121154792989, 0.5129363474170285, 0.5117043126045556, 0.5133470250350983, 0.5137577022615155, 0.516632447742094, 0.508418890485039, 0.510882957955895, 0.5063655018072108, 0.5108829587392004, 0.5129363456545915, 0.5195071871520558, 0.517043122618595, 0.5219712538763238, 0.5297741226347076, 0.5162217693407188, 0.517453799257533, 0.5260780272053007, 0.5232032886520793, 0.5129363472212022, 0.5289527733467932, 0.519917865553431, 0.5203285431715008, 0.5133470213143977, 0.522381932436808, 0.5190965112964231, 0.517453799845012, 0.5211499011492092, 0.5190965111005967, 0.5154004133212737, 0.5248459955994843, 0.5256673541646718, 0.5268993860397495, 0.5199178686866526, 0.5277207430383263, 0.5256673490731868, 0.5240246429090872, 0.5347022586289862, 0.5260780310361537, 0.5392197147776704, 0.5314168388593857, 0.5371663259040159, 0.5322381966412679, 0.5334702294954774, 0.5342915770209545, 0.5305954861689887, 0.530595485777336, 0.5334702281246929, 0.5396303860558621, 0.5396303897765627, 0.5285420959245498, 0.5359342948122435, 0.5363449712553553, 0.5437371675973066, 0.5416837810735683, 0.5416837777445204, 0.5293634540980846, 0.540451749394317, 0.535523615627563, 0.5433264842268378, 0.5470225901084759, 0.5556468202837683, 0.5367556484817724, 0.5466119136653642, 0.5445585228334462, 0.5519507167520464, 0.5540041109864472, 0.5556468191088103, 0.536755651027515, 0.544558523812578, 0.5589322410325005, 0.553593433172551, 0.5527720767614533, 0.558521563022778, 0.5585215564870737, 0.5683778268355854, 0.5437371660306958, 0.5564681747366026, 0.561396299617736, 0.5663244318178792, 0.5638603657178076, 0.5527720692466171, 0.5601642746700154, 0.5691991830508567, 0.566735115775827, 0.5630390166257196, 0.5646817227163844, 0.5589322356717542, 0.5597535970519456, 0.5622176552455284, 0.5745379851584073, 0.5585215584453371, 0.5597535964644665, 0.5515400425364594, 0.5560574925410919, 0.5798767934100094, 0.5761807025580435, 0.578234090844219, 0.5839835690032286, 0.5843942470129511, 0.5753593475177303, 0.5753593467344249, 0.5761806956306865, 0.5601642673510056, 0.558932242403285, 0.5691991739694098, 0.5815195062322048, 0.5848049277642424, 0.5802874785429154, 0.5798767928225304, 0.5749486614056926, 0.5786447678748098, 0.5737166297264412, 0.5597535934536364, 0.5724845960889264, 0.5728952731195172, 0.595893222081343, 0.5991786489741269, 0.602053387919001, 0.6036960944747534, 0.6012320305287716, 0.6053388092552122, 0.602053387331522, 0.6000000044060928, 0.5905544173546151, 0.582340858922602, 0.5663244337761426, 0.5577002022300658, 0.5811088295932668, 0.5983572868106302, 0.583983570961492, 0.5946611915770498, 0.5950718664535507, 0.584394250929478, 0.5708418860082999, 0.5811088248934344, 0.5852156057739649, 0.5897330578102957, 0.6020533845899531, 0.5995893218189294, 0.6057494839358868, 0.6102669418470081, 0.6114989752886966, 0.6114989713721697, 0.6053388090593859, 0.6045174532357672, 0.6090349042929664, 0.6069815150276592, 0.6045174522566354, 0.6020533851774321, 0.614373719006838, 0.6291581097324771, 0.6262833697350363, 0.6271047228170861, 0.6287474291770121, 0.6390143774618113, 0.6344969226839117, 0.60903490409714, 0.6172484564585362, 0.6324435281312931, 0.634907598931197, 0.625872690158703, 0.6053388080802542, 0.6012320309204242, 0.6164271002432649, 0.6205338813196217, 0.6287474360309342, 0.6246406602418888, 0.6242299834805592, 0.6312115021310059, 0.6332648884589177, 0.6451745409495532, 0.6312114995852633]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
