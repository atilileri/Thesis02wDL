{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf3.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 05:30:30 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '0', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023235D64E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002322F576EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0866, Accuracy:0.3943, Validation Loss:1.0788, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0779, Accuracy:0.3943, Validation Loss:1.0762, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0771, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0748, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0748, Accuracy:0.3955, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #6: Loss:1.0744, Accuracy:0.3951, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0730, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0725, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0725, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0721, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0723, Accuracy:0.3938, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #33: Loss:1.0718, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0720, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #35: Loss:1.0714, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #36: Loss:1.0711, Accuracy:0.3938, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #37: Loss:1.0708, Accuracy:0.3959, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0707, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.3892\n",
    "Epoch #39: Loss:1.0714, Accuracy:0.3938, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #40: Loss:1.0699, Accuracy:0.4057, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #41: Loss:1.0701, Accuracy:0.3967, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #42: Loss:1.0701, Accuracy:0.3963, Validation Loss:1.0731, Validation Accuracy:0.3826\n",
    "Epoch #43: Loss:1.0686, Accuracy:0.4045, Validation Loss:1.0741, Validation Accuracy:0.3826\n",
    "Epoch #44: Loss:1.0680, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #45: Loss:1.0678, Accuracy:0.3959, Validation Loss:1.0738, Validation Accuracy:0.3810\n",
    "Epoch #46: Loss:1.0667, Accuracy:0.3959, Validation Loss:1.0737, Validation Accuracy:0.3859\n",
    "Epoch #47: Loss:1.0673, Accuracy:0.4037, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0689, Accuracy:0.4066, Validation Loss:1.0724, Validation Accuracy:0.3875\n",
    "Epoch #49: Loss:1.0690, Accuracy:0.3975, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #50: Loss:1.0656, Accuracy:0.4119, Validation Loss:1.0727, Validation Accuracy:0.3875\n",
    "Epoch #51: Loss:1.0648, Accuracy:0.4053, Validation Loss:1.0722, Validation Accuracy:0.3777\n",
    "Epoch #52: Loss:1.0673, Accuracy:0.4008, Validation Loss:1.0719, Validation Accuracy:0.3957\n",
    "Epoch #53: Loss:1.0636, Accuracy:0.4201, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #54: Loss:1.0633, Accuracy:0.4021, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #55: Loss:1.0621, Accuracy:0.4197, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0617, Accuracy:0.4181, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #57: Loss:1.0595, Accuracy:0.4115, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #58: Loss:1.0590, Accuracy:0.4152, Validation Loss:1.0767, Validation Accuracy:0.4072\n",
    "Epoch #59: Loss:1.0592, Accuracy:0.4242, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #60: Loss:1.0565, Accuracy:0.4238, Validation Loss:1.0763, Validation Accuracy:0.4072\n",
    "Epoch #61: Loss:1.0550, Accuracy:0.4082, Validation Loss:1.0767, Validation Accuracy:0.3990\n",
    "Epoch #62: Loss:1.0531, Accuracy:0.4271, Validation Loss:1.0786, Validation Accuracy:0.4039\n",
    "Epoch #63: Loss:1.0524, Accuracy:0.4320, Validation Loss:1.0908, Validation Accuracy:0.3842\n",
    "Epoch #64: Loss:1.0550, Accuracy:0.4275, Validation Loss:1.0795, Validation Accuracy:0.4039\n",
    "Epoch #65: Loss:1.0489, Accuracy:0.4353, Validation Loss:1.0816, Validation Accuracy:0.3924\n",
    "Epoch #66: Loss:1.0466, Accuracy:0.4390, Validation Loss:1.0835, Validation Accuracy:0.3990\n",
    "Epoch #67: Loss:1.0475, Accuracy:0.4320, Validation Loss:1.0886, Validation Accuracy:0.3727\n",
    "Epoch #68: Loss:1.0459, Accuracy:0.4320, Validation Loss:1.0910, Validation Accuracy:0.3974\n",
    "Epoch #69: Loss:1.0441, Accuracy:0.4353, Validation Loss:1.0894, Validation Accuracy:0.3842\n",
    "Epoch #70: Loss:1.0433, Accuracy:0.4427, Validation Loss:1.0973, Validation Accuracy:0.3859\n",
    "Epoch #71: Loss:1.0414, Accuracy:0.4452, Validation Loss:1.0947, Validation Accuracy:0.3859\n",
    "Epoch #72: Loss:1.0397, Accuracy:0.4349, Validation Loss:1.0972, Validation Accuracy:0.3727\n",
    "Epoch #73: Loss:1.0345, Accuracy:0.4493, Validation Loss:1.1081, Validation Accuracy:0.3678\n",
    "Epoch #74: Loss:1.0356, Accuracy:0.4550, Validation Loss:1.1050, Validation Accuracy:0.3793\n",
    "Epoch #75: Loss:1.0342, Accuracy:0.4563, Validation Loss:1.1069, Validation Accuracy:0.3711\n",
    "Epoch #76: Loss:1.0308, Accuracy:0.4591, Validation Loss:1.1064, Validation Accuracy:0.3826\n",
    "Epoch #77: Loss:1.0267, Accuracy:0.4591, Validation Loss:1.1181, Validation Accuracy:0.3678\n",
    "Epoch #78: Loss:1.0244, Accuracy:0.4645, Validation Loss:1.1173, Validation Accuracy:0.3596\n",
    "Epoch #79: Loss:1.0225, Accuracy:0.4616, Validation Loss:1.1238, Validation Accuracy:0.3563\n",
    "Epoch #80: Loss:1.0268, Accuracy:0.4604, Validation Loss:1.1177, Validation Accuracy:0.3744\n",
    "Epoch #81: Loss:1.0228, Accuracy:0.4657, Validation Loss:1.1192, Validation Accuracy:0.3678\n",
    "Epoch #82: Loss:1.0168, Accuracy:0.4657, Validation Loss:1.1285, Validation Accuracy:0.3284\n",
    "Epoch #83: Loss:1.0161, Accuracy:0.4669, Validation Loss:1.1310, Validation Accuracy:0.3514\n",
    "Epoch #84: Loss:1.0122, Accuracy:0.4739, Validation Loss:1.1322, Validation Accuracy:0.3530\n",
    "Epoch #85: Loss:1.0127, Accuracy:0.4821, Validation Loss:1.1370, Validation Accuracy:0.3383\n",
    "Epoch #86: Loss:1.0104, Accuracy:0.4743, Validation Loss:1.1403, Validation Accuracy:0.3875\n",
    "Epoch #87: Loss:1.0078, Accuracy:0.4772, Validation Loss:1.1500, Validation Accuracy:0.3235\n",
    "Epoch #88: Loss:1.0092, Accuracy:0.4756, Validation Loss:1.1436, Validation Accuracy:0.3596\n",
    "Epoch #89: Loss:1.0080, Accuracy:0.4940, Validation Loss:1.1562, Validation Accuracy:0.3695\n",
    "Epoch #90: Loss:1.0186, Accuracy:0.4776, Validation Loss:1.1352, Validation Accuracy:0.3760\n",
    "Epoch #91: Loss:1.0218, Accuracy:0.4583, Validation Loss:1.1319, Validation Accuracy:0.3711\n",
    "Epoch #92: Loss:1.0126, Accuracy:0.4661, Validation Loss:1.1286, Validation Accuracy:0.3875\n",
    "Epoch #93: Loss:1.0030, Accuracy:0.4867, Validation Loss:1.1317, Validation Accuracy:0.3580\n",
    "Epoch #94: Loss:0.9994, Accuracy:0.4908, Validation Loss:1.1474, Validation Accuracy:0.3612\n",
    "Epoch #95: Loss:0.9951, Accuracy:0.4867, Validation Loss:1.1488, Validation Accuracy:0.3612\n",
    "Epoch #96: Loss:0.9910, Accuracy:0.4961, Validation Loss:1.1600, Validation Accuracy:0.3580\n",
    "Epoch #97: Loss:0.9923, Accuracy:0.5002, Validation Loss:1.1773, Validation Accuracy:0.3580\n",
    "Epoch #98: Loss:0.9987, Accuracy:0.4903, Validation Loss:1.1649, Validation Accuracy:0.3596\n",
    "Epoch #99: Loss:0.9935, Accuracy:0.4965, Validation Loss:1.1649, Validation Accuracy:0.3284\n",
    "Epoch #100: Loss:0.9893, Accuracy:0.4969, Validation Loss:1.1727, Validation Accuracy:0.3612\n",
    "Epoch #101: Loss:0.9869, Accuracy:0.5014, Validation Loss:1.1741, Validation Accuracy:0.3563\n",
    "Epoch #102: Loss:0.9816, Accuracy:0.5014, Validation Loss:1.1788, Validation Accuracy:0.3580\n",
    "Epoch #103: Loss:0.9778, Accuracy:0.5101, Validation Loss:1.1814, Validation Accuracy:0.3514\n",
    "Epoch #104: Loss:0.9801, Accuracy:0.5031, Validation Loss:1.1914, Validation Accuracy:0.3563\n",
    "Epoch #105: Loss:0.9798, Accuracy:0.4998, Validation Loss:1.1902, Validation Accuracy:0.3563\n",
    "Epoch #106: Loss:0.9792, Accuracy:0.5006, Validation Loss:1.1827, Validation Accuracy:0.3596\n",
    "Epoch #107: Loss:0.9749, Accuracy:0.5121, Validation Loss:1.1879, Validation Accuracy:0.3218\n",
    "Epoch #108: Loss:0.9823, Accuracy:0.4924, Validation Loss:1.1705, Validation Accuracy:0.3810\n",
    "Epoch #109: Loss:0.9675, Accuracy:0.5261, Validation Loss:1.1989, Validation Accuracy:0.3695\n",
    "Epoch #110: Loss:0.9705, Accuracy:0.5170, Validation Loss:1.2033, Validation Accuracy:0.3744\n",
    "Epoch #111: Loss:0.9729, Accuracy:0.5158, Validation Loss:1.1769, Validation Accuracy:0.3760\n",
    "Epoch #112: Loss:0.9695, Accuracy:0.5302, Validation Loss:1.1846, Validation Accuracy:0.3777\n",
    "Epoch #113: Loss:0.9705, Accuracy:0.5105, Validation Loss:1.2057, Validation Accuracy:0.3514\n",
    "Epoch #114: Loss:0.9584, Accuracy:0.5207, Validation Loss:1.2076, Validation Accuracy:0.3645\n",
    "Epoch #115: Loss:0.9588, Accuracy:0.5220, Validation Loss:1.2036, Validation Accuracy:0.3547\n",
    "Epoch #116: Loss:0.9643, Accuracy:0.5084, Validation Loss:1.2033, Validation Accuracy:0.3629\n",
    "Epoch #117: Loss:0.9574, Accuracy:0.5183, Validation Loss:1.2052, Validation Accuracy:0.3514\n",
    "Epoch #118: Loss:0.9701, Accuracy:0.5216, Validation Loss:1.1998, Validation Accuracy:0.3481\n",
    "Epoch #119: Loss:0.9667, Accuracy:0.5097, Validation Loss:1.2129, Validation Accuracy:0.3498\n",
    "Epoch #120: Loss:0.9587, Accuracy:0.5142, Validation Loss:1.2236, Validation Accuracy:0.3547\n",
    "Epoch #121: Loss:0.9541, Accuracy:0.5150, Validation Loss:1.1905, Validation Accuracy:0.3793\n",
    "Epoch #122: Loss:0.9458, Accuracy:0.5318, Validation Loss:1.2045, Validation Accuracy:0.3678\n",
    "Epoch #123: Loss:0.9457, Accuracy:0.5376, Validation Loss:1.2339, Validation Accuracy:0.3514\n",
    "Epoch #124: Loss:0.9441, Accuracy:0.5343, Validation Loss:1.2371, Validation Accuracy:0.3760\n",
    "Epoch #125: Loss:0.9369, Accuracy:0.5363, Validation Loss:1.2455, Validation Accuracy:0.3432\n",
    "Epoch #126: Loss:0.9417, Accuracy:0.5363, Validation Loss:1.2294, Validation Accuracy:0.3810\n",
    "Epoch #127: Loss:0.9405, Accuracy:0.5376, Validation Loss:1.2374, Validation Accuracy:0.3727\n",
    "Epoch #128: Loss:0.9349, Accuracy:0.5413, Validation Loss:1.2405, Validation Accuracy:0.3760\n",
    "Epoch #129: Loss:0.9355, Accuracy:0.5450, Validation Loss:1.2406, Validation Accuracy:0.3596\n",
    "Epoch #130: Loss:0.9266, Accuracy:0.5520, Validation Loss:1.2430, Validation Accuracy:0.3810\n",
    "Epoch #131: Loss:0.9299, Accuracy:0.5491, Validation Loss:1.2600, Validation Accuracy:0.3399\n",
    "Epoch #132: Loss:0.9267, Accuracy:0.5499, Validation Loss:1.2619, Validation Accuracy:0.3744\n",
    "Epoch #133: Loss:0.9251, Accuracy:0.5487, Validation Loss:1.2491, Validation Accuracy:0.3793\n",
    "Epoch #134: Loss:0.9277, Accuracy:0.5441, Validation Loss:1.2664, Validation Accuracy:0.3547\n",
    "Epoch #135: Loss:0.9249, Accuracy:0.5458, Validation Loss:1.2613, Validation Accuracy:0.3645\n",
    "Epoch #136: Loss:0.9224, Accuracy:0.5441, Validation Loss:1.2519, Validation Accuracy:0.3612\n",
    "Epoch #137: Loss:0.9264, Accuracy:0.5503, Validation Loss:1.2624, Validation Accuracy:0.3563\n",
    "Epoch #138: Loss:0.9206, Accuracy:0.5495, Validation Loss:1.2842, Validation Accuracy:0.3514\n",
    "Epoch #139: Loss:0.9128, Accuracy:0.5602, Validation Loss:1.2821, Validation Accuracy:0.3465\n",
    "Epoch #140: Loss:0.9100, Accuracy:0.5528, Validation Loss:1.3097, Validation Accuracy:0.3678\n",
    "Epoch #141: Loss:0.9160, Accuracy:0.5540, Validation Loss:1.2742, Validation Accuracy:0.3695\n",
    "Epoch #142: Loss:0.9142, Accuracy:0.5552, Validation Loss:1.2768, Validation Accuracy:0.3695\n",
    "Epoch #143: Loss:0.9109, Accuracy:0.5556, Validation Loss:1.2859, Validation Accuracy:0.3530\n",
    "Epoch #144: Loss:0.9045, Accuracy:0.5585, Validation Loss:1.2874, Validation Accuracy:0.3629\n",
    "Epoch #145: Loss:0.9130, Accuracy:0.5429, Validation Loss:1.2792, Validation Accuracy:0.3580\n",
    "Epoch #146: Loss:0.9031, Accuracy:0.5610, Validation Loss:1.2969, Validation Accuracy:0.3465\n",
    "Epoch #147: Loss:0.9116, Accuracy:0.5552, Validation Loss:1.2913, Validation Accuracy:0.3580\n",
    "Epoch #148: Loss:0.9091, Accuracy:0.5544, Validation Loss:1.3072, Validation Accuracy:0.3662\n",
    "Epoch #149: Loss:0.9075, Accuracy:0.5634, Validation Loss:1.3030, Validation Accuracy:0.3596\n",
    "Epoch #150: Loss:0.9051, Accuracy:0.5524, Validation Loss:1.2975, Validation Accuracy:0.3695\n",
    "Epoch #151: Loss:0.8919, Accuracy:0.5671, Validation Loss:1.3096, Validation Accuracy:0.3629\n",
    "Epoch #152: Loss:0.8929, Accuracy:0.5639, Validation Loss:1.2959, Validation Accuracy:0.3547\n",
    "Epoch #153: Loss:0.9090, Accuracy:0.5618, Validation Loss:1.3049, Validation Accuracy:0.3580\n",
    "Epoch #154: Loss:0.8968, Accuracy:0.5589, Validation Loss:1.2983, Validation Accuracy:0.3662\n",
    "Epoch #155: Loss:0.8946, Accuracy:0.5548, Validation Loss:1.3224, Validation Accuracy:0.3612\n",
    "Epoch #156: Loss:0.9000, Accuracy:0.5659, Validation Loss:1.3549, Validation Accuracy:0.3580\n",
    "Epoch #157: Loss:0.9053, Accuracy:0.5536, Validation Loss:1.3120, Validation Accuracy:0.3596\n",
    "Epoch #158: Loss:0.8990, Accuracy:0.5655, Validation Loss:1.3074, Validation Accuracy:0.3793\n",
    "Epoch #159: Loss:0.9002, Accuracy:0.5614, Validation Loss:1.3097, Validation Accuracy:0.3580\n",
    "Epoch #160: Loss:0.8945, Accuracy:0.5671, Validation Loss:1.3483, Validation Accuracy:0.3580\n",
    "Epoch #161: Loss:0.8856, Accuracy:0.5774, Validation Loss:1.3377, Validation Accuracy:0.3563\n",
    "Epoch #162: Loss:0.8864, Accuracy:0.5680, Validation Loss:1.3679, Validation Accuracy:0.3465\n",
    "Epoch #163: Loss:0.8938, Accuracy:0.5630, Validation Loss:1.3709, Validation Accuracy:0.3580\n",
    "Epoch #164: Loss:0.8804, Accuracy:0.5749, Validation Loss:1.3508, Validation Accuracy:0.3612\n",
    "Epoch #165: Loss:0.8695, Accuracy:0.5864, Validation Loss:1.3437, Validation Accuracy:0.3760\n",
    "Epoch #166: Loss:0.8707, Accuracy:0.5819, Validation Loss:1.3584, Validation Accuracy:0.3629\n",
    "Epoch #167: Loss:0.8655, Accuracy:0.5864, Validation Loss:1.3666, Validation Accuracy:0.3662\n",
    "Epoch #168: Loss:0.8628, Accuracy:0.5791, Validation Loss:1.3758, Validation Accuracy:0.3612\n",
    "Epoch #169: Loss:0.8625, Accuracy:0.5889, Validation Loss:1.3941, Validation Accuracy:0.3678\n",
    "Epoch #170: Loss:0.8606, Accuracy:0.5881, Validation Loss:1.3621, Validation Accuracy:0.3547\n",
    "Epoch #171: Loss:0.8613, Accuracy:0.5869, Validation Loss:1.3630, Validation Accuracy:0.3662\n",
    "Epoch #172: Loss:0.8590, Accuracy:0.5881, Validation Loss:1.3407, Validation Accuracy:0.3399\n",
    "Epoch #173: Loss:0.8698, Accuracy:0.5852, Validation Loss:1.3663, Validation Accuracy:0.3530\n",
    "Epoch #174: Loss:0.8611, Accuracy:0.5873, Validation Loss:1.3896, Validation Accuracy:0.3596\n",
    "Epoch #175: Loss:0.8547, Accuracy:0.5922, Validation Loss:1.3631, Validation Accuracy:0.3514\n",
    "Epoch #176: Loss:0.8562, Accuracy:0.5873, Validation Loss:1.3734, Validation Accuracy:0.3530\n",
    "Epoch #177: Loss:0.8474, Accuracy:0.5996, Validation Loss:1.3778, Validation Accuracy:0.3629\n",
    "Epoch #178: Loss:0.8498, Accuracy:0.5971, Validation Loss:1.3749, Validation Accuracy:0.3580\n",
    "Epoch #179: Loss:0.8472, Accuracy:0.5914, Validation Loss:1.4045, Validation Accuracy:0.3695\n",
    "Epoch #180: Loss:0.8454, Accuracy:0.5959, Validation Loss:1.3638, Validation Accuracy:0.3711\n",
    "Epoch #181: Loss:0.8370, Accuracy:0.6070, Validation Loss:1.3954, Validation Accuracy:0.3678\n",
    "Epoch #182: Loss:0.8373, Accuracy:0.6103, Validation Loss:1.4270, Validation Accuracy:0.3629\n",
    "Epoch #183: Loss:0.8320, Accuracy:0.6004, Validation Loss:1.4267, Validation Accuracy:0.3645\n",
    "Epoch #184: Loss:0.8317, Accuracy:0.6037, Validation Loss:1.4338, Validation Accuracy:0.3530\n",
    "Epoch #185: Loss:0.8492, Accuracy:0.6021, Validation Loss:1.4928, Validation Accuracy:0.3580\n",
    "Epoch #186: Loss:0.8432, Accuracy:0.5926, Validation Loss:1.4175, Validation Accuracy:0.3695\n",
    "Epoch #187: Loss:0.8339, Accuracy:0.6070, Validation Loss:1.4033, Validation Accuracy:0.3629\n",
    "Epoch #188: Loss:0.8291, Accuracy:0.6181, Validation Loss:1.3944, Validation Accuracy:0.3662\n",
    "Epoch #189: Loss:0.8235, Accuracy:0.6197, Validation Loss:1.4009, Validation Accuracy:0.3760\n",
    "Epoch #190: Loss:0.8205, Accuracy:0.6140, Validation Loss:1.4247, Validation Accuracy:0.3711\n",
    "Epoch #191: Loss:0.8182, Accuracy:0.6205, Validation Loss:1.4124, Validation Accuracy:0.3530\n",
    "Epoch #192: Loss:0.8136, Accuracy:0.6168, Validation Loss:1.4265, Validation Accuracy:0.3727\n",
    "Epoch #193: Loss:0.8108, Accuracy:0.6246, Validation Loss:1.4500, Validation Accuracy:0.3711\n",
    "Epoch #194: Loss:0.8242, Accuracy:0.6123, Validation Loss:1.4473, Validation Accuracy:0.3711\n",
    "Epoch #195: Loss:0.8218, Accuracy:0.6094, Validation Loss:1.3907, Validation Accuracy:0.3530\n",
    "Epoch #196: Loss:0.8104, Accuracy:0.6214, Validation Loss:1.4034, Validation Accuracy:0.3563\n",
    "Epoch #197: Loss:0.8133, Accuracy:0.6287, Validation Loss:1.4061, Validation Accuracy:0.3530\n",
    "Epoch #198: Loss:0.8051, Accuracy:0.6197, Validation Loss:1.4248, Validation Accuracy:0.3563\n",
    "Epoch #199: Loss:0.7988, Accuracy:0.6398, Validation Loss:1.4325, Validation Accuracy:0.3645\n",
    "Epoch #200: Loss:0.7989, Accuracy:0.6333, Validation Loss:1.4842, Validation Accuracy:0.3530\n",
    "Epoch #201: Loss:0.8278, Accuracy:0.6107, Validation Loss:1.4684, Validation Accuracy:0.3645\n",
    "Epoch #202: Loss:0.8164, Accuracy:0.6214, Validation Loss:1.4311, Validation Accuracy:0.3547\n",
    "Epoch #203: Loss:0.8158, Accuracy:0.6185, Validation Loss:1.3934, Validation Accuracy:0.3300\n",
    "Epoch #204: Loss:0.8195, Accuracy:0.6222, Validation Loss:1.4778, Validation Accuracy:0.3662\n",
    "Epoch #205: Loss:0.7996, Accuracy:0.6296, Validation Loss:1.4242, Validation Accuracy:0.3563\n",
    "Epoch #206: Loss:0.7926, Accuracy:0.6366, Validation Loss:1.4663, Validation Accuracy:0.3645\n",
    "Epoch #207: Loss:0.7903, Accuracy:0.6444, Validation Loss:1.4362, Validation Accuracy:0.3547\n",
    "Epoch #208: Loss:0.7977, Accuracy:0.6370, Validation Loss:1.4334, Validation Accuracy:0.3612\n",
    "Epoch #209: Loss:0.7972, Accuracy:0.6341, Validation Loss:1.4732, Validation Accuracy:0.3645\n",
    "Epoch #210: Loss:0.7858, Accuracy:0.6378, Validation Loss:1.4784, Validation Accuracy:0.3760\n",
    "Epoch #211: Loss:0.7838, Accuracy:0.6452, Validation Loss:1.4605, Validation Accuracy:0.3678\n",
    "Epoch #212: Loss:0.7835, Accuracy:0.6411, Validation Loss:1.4707, Validation Accuracy:0.3629\n",
    "Epoch #213: Loss:0.7859, Accuracy:0.6415, Validation Loss:1.5077, Validation Accuracy:0.3695\n",
    "Epoch #214: Loss:0.7921, Accuracy:0.6398, Validation Loss:1.4662, Validation Accuracy:0.3596\n",
    "Epoch #215: Loss:0.7858, Accuracy:0.6382, Validation Loss:1.4864, Validation Accuracy:0.3695\n",
    "Epoch #216: Loss:0.7816, Accuracy:0.6448, Validation Loss:1.4752, Validation Accuracy:0.3711\n",
    "Epoch #217: Loss:0.7757, Accuracy:0.6407, Validation Loss:1.4494, Validation Accuracy:0.3629\n",
    "Epoch #218: Loss:0.7758, Accuracy:0.6476, Validation Loss:1.4505, Validation Accuracy:0.3678\n",
    "Epoch #219: Loss:0.7673, Accuracy:0.6550, Validation Loss:1.4456, Validation Accuracy:0.3563\n",
    "Epoch #220: Loss:0.7633, Accuracy:0.6575, Validation Loss:1.4397, Validation Accuracy:0.3530\n",
    "Epoch #221: Loss:0.7692, Accuracy:0.6542, Validation Loss:1.4447, Validation Accuracy:0.3547\n",
    "Epoch #222: Loss:0.7715, Accuracy:0.6505, Validation Loss:1.4378, Validation Accuracy:0.3580\n",
    "Epoch #223: Loss:0.7776, Accuracy:0.6493, Validation Loss:1.4478, Validation Accuracy:0.3530\n",
    "Epoch #224: Loss:0.7608, Accuracy:0.6530, Validation Loss:1.4542, Validation Accuracy:0.3645\n",
    "Epoch #225: Loss:0.7660, Accuracy:0.6546, Validation Loss:1.4466, Validation Accuracy:0.3530\n",
    "Epoch #226: Loss:0.7731, Accuracy:0.6489, Validation Loss:1.4690, Validation Accuracy:0.3612\n",
    "Epoch #227: Loss:0.7704, Accuracy:0.6509, Validation Loss:1.4951, Validation Accuracy:0.3662\n",
    "Epoch #228: Loss:0.7750, Accuracy:0.6452, Validation Loss:1.4808, Validation Accuracy:0.3596\n",
    "Epoch #229: Loss:0.7562, Accuracy:0.6600, Validation Loss:1.5071, Validation Accuracy:0.3695\n",
    "Epoch #230: Loss:0.7555, Accuracy:0.6661, Validation Loss:1.5408, Validation Accuracy:0.3662\n",
    "Epoch #231: Loss:0.7599, Accuracy:0.6612, Validation Loss:1.4948, Validation Accuracy:0.3580\n",
    "Epoch #232: Loss:0.7771, Accuracy:0.6357, Validation Loss:1.4697, Validation Accuracy:0.3580\n",
    "Epoch #233: Loss:0.7530, Accuracy:0.6620, Validation Loss:1.4619, Validation Accuracy:0.3580\n",
    "Epoch #234: Loss:0.7430, Accuracy:0.6690, Validation Loss:1.4576, Validation Accuracy:0.3563\n",
    "Epoch #235: Loss:0.7511, Accuracy:0.6641, Validation Loss:1.4906, Validation Accuracy:0.3662\n",
    "Epoch #236: Loss:0.7441, Accuracy:0.6715, Validation Loss:1.4647, Validation Accuracy:0.3448\n",
    "Epoch #237: Loss:0.7645, Accuracy:0.6489, Validation Loss:1.4221, Validation Accuracy:0.3530\n",
    "Epoch #238: Loss:0.7860, Accuracy:0.6476, Validation Loss:1.5246, Validation Accuracy:0.3596\n",
    "Epoch #239: Loss:0.7921, Accuracy:0.6329, Validation Loss:1.4653, Validation Accuracy:0.3268\n",
    "Epoch #240: Loss:0.7615, Accuracy:0.6534, Validation Loss:1.4390, Validation Accuracy:0.3465\n",
    "Epoch #241: Loss:0.7699, Accuracy:0.6538, Validation Loss:1.4564, Validation Accuracy:0.3415\n",
    "Epoch #242: Loss:0.7876, Accuracy:0.6423, Validation Loss:1.4472, Validation Accuracy:0.3448\n",
    "Epoch #243: Loss:0.7670, Accuracy:0.6497, Validation Loss:1.4431, Validation Accuracy:0.3612\n",
    "Epoch #244: Loss:0.7512, Accuracy:0.6559, Validation Loss:1.4770, Validation Accuracy:0.3612\n",
    "Epoch #245: Loss:0.7325, Accuracy:0.6756, Validation Loss:1.4783, Validation Accuracy:0.3547\n",
    "Epoch #246: Loss:0.7292, Accuracy:0.6805, Validation Loss:1.5265, Validation Accuracy:0.3596\n",
    "Epoch #247: Loss:0.7289, Accuracy:0.6756, Validation Loss:1.5585, Validation Accuracy:0.3530\n",
    "Epoch #248: Loss:0.7290, Accuracy:0.6805, Validation Loss:1.5384, Validation Accuracy:0.3432\n",
    "Epoch #249: Loss:0.7329, Accuracy:0.6830, Validation Loss:1.5929, Validation Accuracy:0.3629\n",
    "Epoch #250: Loss:0.7422, Accuracy:0.6645, Validation Loss:1.5892, Validation Accuracy:0.3563\n",
    "Epoch #251: Loss:0.7377, Accuracy:0.6731, Validation Loss:1.5575, Validation Accuracy:0.3580\n",
    "Epoch #252: Loss:0.7227, Accuracy:0.6747, Validation Loss:1.5359, Validation Accuracy:0.3662\n",
    "Epoch #253: Loss:0.7219, Accuracy:0.6817, Validation Loss:1.5203, Validation Accuracy:0.3645\n",
    "Epoch #254: Loss:0.7133, Accuracy:0.6842, Validation Loss:1.5306, Validation Accuracy:0.3514\n",
    "Epoch #255: Loss:0.7096, Accuracy:0.6838, Validation Loss:1.4931, Validation Accuracy:0.3448\n",
    "Epoch #256: Loss:0.7153, Accuracy:0.6871, Validation Loss:1.4816, Validation Accuracy:0.3563\n",
    "Epoch #257: Loss:0.7144, Accuracy:0.6821, Validation Loss:1.5351, Validation Accuracy:0.3563\n",
    "Epoch #258: Loss:0.7093, Accuracy:0.6867, Validation Loss:1.5160, Validation Accuracy:0.3645\n",
    "Epoch #259: Loss:0.7223, Accuracy:0.6780, Validation Loss:1.4916, Validation Accuracy:0.3481\n",
    "Epoch #260: Loss:0.7260, Accuracy:0.6735, Validation Loss:1.5031, Validation Accuracy:0.3563\n",
    "Epoch #261: Loss:0.7176, Accuracy:0.6756, Validation Loss:1.5083, Validation Accuracy:0.3498\n",
    "Epoch #262: Loss:0.7274, Accuracy:0.6739, Validation Loss:1.5582, Validation Accuracy:0.3596\n",
    "Epoch #263: Loss:0.7082, Accuracy:0.6879, Validation Loss:1.5586, Validation Accuracy:0.3498\n",
    "Epoch #264: Loss:0.7110, Accuracy:0.6809, Validation Loss:1.5674, Validation Accuracy:0.3596\n",
    "Epoch #265: Loss:0.7258, Accuracy:0.6694, Validation Loss:1.5336, Validation Accuracy:0.3563\n",
    "Epoch #266: Loss:0.7231, Accuracy:0.6797, Validation Loss:1.5050, Validation Accuracy:0.3481\n",
    "Epoch #267: Loss:0.7083, Accuracy:0.6842, Validation Loss:1.5348, Validation Accuracy:0.3514\n",
    "Epoch #268: Loss:0.6977, Accuracy:0.6949, Validation Loss:1.5681, Validation Accuracy:0.3547\n",
    "Epoch #269: Loss:0.6915, Accuracy:0.6973, Validation Loss:1.6002, Validation Accuracy:0.3695\n",
    "Epoch #270: Loss:0.6898, Accuracy:0.6957, Validation Loss:1.5792, Validation Accuracy:0.3481\n",
    "Epoch #271: Loss:0.6878, Accuracy:0.7002, Validation Loss:1.5598, Validation Accuracy:0.3563\n",
    "Epoch #272: Loss:0.6820, Accuracy:0.6990, Validation Loss:1.5750, Validation Accuracy:0.3629\n",
    "Epoch #273: Loss:0.6805, Accuracy:0.7014, Validation Loss:1.6012, Validation Accuracy:0.3481\n",
    "Epoch #274: Loss:0.6761, Accuracy:0.7018, Validation Loss:1.5721, Validation Accuracy:0.3547\n",
    "Epoch #275: Loss:0.6761, Accuracy:0.7084, Validation Loss:1.5832, Validation Accuracy:0.3580\n",
    "Epoch #276: Loss:0.6737, Accuracy:0.7035, Validation Loss:1.5841, Validation Accuracy:0.3530\n",
    "Epoch #277: Loss:0.6755, Accuracy:0.7031, Validation Loss:1.6142, Validation Accuracy:0.3596\n",
    "Epoch #278: Loss:0.6813, Accuracy:0.6961, Validation Loss:1.5842, Validation Accuracy:0.3580\n",
    "Epoch #279: Loss:0.6801, Accuracy:0.6961, Validation Loss:1.6115, Validation Accuracy:0.3629\n",
    "Epoch #280: Loss:0.6869, Accuracy:0.6957, Validation Loss:1.5666, Validation Accuracy:0.3481\n",
    "Epoch #281: Loss:0.6893, Accuracy:0.6895, Validation Loss:1.5431, Validation Accuracy:0.3498\n",
    "Epoch #282: Loss:0.6851, Accuracy:0.7006, Validation Loss:1.5608, Validation Accuracy:0.3514\n",
    "Epoch #283: Loss:0.6751, Accuracy:0.7047, Validation Loss:1.6083, Validation Accuracy:0.3547\n",
    "Epoch #284: Loss:0.6726, Accuracy:0.7031, Validation Loss:1.5866, Validation Accuracy:0.3530\n",
    "Epoch #285: Loss:0.6718, Accuracy:0.7014, Validation Loss:1.5966, Validation Accuracy:0.3448\n",
    "Epoch #286: Loss:0.6655, Accuracy:0.7055, Validation Loss:1.6212, Validation Accuracy:0.3547\n",
    "Epoch #287: Loss:0.6599, Accuracy:0.7129, Validation Loss:1.6290, Validation Accuracy:0.3530\n",
    "Epoch #288: Loss:0.6621, Accuracy:0.7055, Validation Loss:1.6501, Validation Accuracy:0.3645\n",
    "Epoch #289: Loss:0.6694, Accuracy:0.7051, Validation Loss:1.5843, Validation Accuracy:0.3432\n",
    "Epoch #290: Loss:0.6673, Accuracy:0.7088, Validation Loss:1.5595, Validation Accuracy:0.3415\n",
    "Epoch #291: Loss:0.6737, Accuracy:0.6949, Validation Loss:1.5866, Validation Accuracy:0.3465\n",
    "Epoch #292: Loss:0.6720, Accuracy:0.6990, Validation Loss:1.6375, Validation Accuracy:0.3547\n",
    "Epoch #293: Loss:0.6690, Accuracy:0.7031, Validation Loss:1.6625, Validation Accuracy:0.3727\n",
    "Epoch #294: Loss:0.6656, Accuracy:0.7064, Validation Loss:1.6236, Validation Accuracy:0.3481\n",
    "Epoch #295: Loss:0.6586, Accuracy:0.7109, Validation Loss:1.5684, Validation Accuracy:0.3448\n",
    "Epoch #296: Loss:0.6698, Accuracy:0.7051, Validation Loss:1.5754, Validation Accuracy:0.3596\n",
    "Epoch #297: Loss:0.6701, Accuracy:0.7060, Validation Loss:1.5598, Validation Accuracy:0.3547\n",
    "Epoch #298: Loss:0.6641, Accuracy:0.7060, Validation Loss:1.6252, Validation Accuracy:0.3514\n",
    "Epoch #299: Loss:0.6545, Accuracy:0.7154, Validation Loss:1.6139, Validation Accuracy:0.3465\n",
    "Epoch #300: Loss:0.6466, Accuracy:0.7146, Validation Loss:1.6370, Validation Accuracy:0.3547\n",
    "\n",
    "Test:\n",
    "Test Loss:1.63702679, Accuracy:0.3547\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03  01  02\n",
    "t:03  36  45  61\n",
    "t:01  77  88  75\n",
    "t:02  56  79  92\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.21      0.25      0.23       142\n",
    "          01       0.42      0.37      0.39       240\n",
    "          02       0.40      0.41      0.40       227\n",
    "\n",
    "    accuracy                           0.35       609\n",
    "   macro avg       0.34      0.34      0.34       609\n",
    "weighted avg       0.36      0.35      0.36       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 05:46:06 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0788284362047569, 1.0762183924613915, 1.0750480465505314, 1.0743452809714331, 1.0747658556513795, 1.074318933369491, 1.074284197661677, 1.0742714825913628, 1.074242388282112, 1.0742416221324251, 1.0742582838327819, 1.0742557790674796, 1.074358880226248, 1.0742162507155846, 1.074187486238276, 1.0742297405484078, 1.074180859650297, 1.0741188019171528, 1.0741142952579192, 1.074112091745649, 1.0741377957348752, 1.0742031731237527, 1.074177564658555, 1.0740584056757159, 1.0740901057551844, 1.0741361487283692, 1.074140059928393, 1.0741200466657115, 1.0741230372724861, 1.0741092859230605, 1.0741766387801648, 1.0740673064402564, 1.0739767338059023, 1.0742189576864634, 1.073838316552549, 1.0737960843617105, 1.0739288431866023, 1.07340264927186, 1.0732833088325162, 1.0746701536898935, 1.0725294018809628, 1.073120000131416, 1.0740555753848824, 1.0741847812248568, 1.0738221019359644, 1.0736992680184752, 1.0733314578364832, 1.0724025919715368, 1.0726386825439378, 1.0726613700879226, 1.0721926405316307, 1.0719253660618573, 1.0749237421893918, 1.0735250361055773, 1.073653583064651, 1.0737994899499201, 1.07472543215321, 1.0766714400258557, 1.0760605006382382, 1.076323429351957, 1.0767284062108382, 1.0785948731041894, 1.0907507443858682, 1.0794888730902585, 1.0816472461462412, 1.0834953765368032, 1.0886011665873536, 1.0910022609143812, 1.089362533025945, 1.0972502795346264, 1.0947174626618184, 1.0971595317076384, 1.1081313730656415, 1.1049538754868782, 1.1069186708610046, 1.106411328456672, 1.11811714767431, 1.117255107522598, 1.1238243722759054, 1.1177452538401036, 1.1191938109390058, 1.128456862884985, 1.1310078349998236, 1.13222053094059, 1.1369784565394736, 1.1403245783008769, 1.1499868966088507, 1.1436431740696598, 1.1562207152299302, 1.1351674405616297, 1.131933229505918, 1.1286489121823866, 1.1316533215918956, 1.147438391284598, 1.1487602615982833, 1.159973026887928, 1.1773301760355632, 1.1648550129484856, 1.1649148980972215, 1.1726910819365277, 1.1740653023539702, 1.1787830336732035, 1.1813924938978624, 1.1914376509796418, 1.1902025012155668, 1.1827488440793918, 1.187860701471714, 1.1704774061447294, 1.1989422382783812, 1.2032525253609092, 1.1769156328758779, 1.1845951323047257, 1.2057355806745331, 1.2075679611494192, 1.2035558076914896, 1.2033081089921773, 1.2052144305459385, 1.1998419327101684, 1.2128513135346286, 1.2235505880393418, 1.1904617412924179, 1.2045296645908325, 1.2338623186246123, 1.23705778666122, 1.2454615829417663, 1.2293861072834684, 1.2374403143946957, 1.2405185133757066, 1.2405779856962131, 1.242954696340514, 1.259982324586126, 1.2618892347479884, 1.249090894493955, 1.2663702283586775, 1.261310387518997, 1.251865248961989, 1.2624046894521352, 1.2841742827583025, 1.2820687153069257, 1.3096946869381934, 1.2741808638784098, 1.2767531828731542, 1.2858661211574411, 1.2874259332130695, 1.2792310477869069, 1.2968889970106052, 1.2913216708720416, 1.3071783458070803, 1.302950353066518, 1.2974718632956443, 1.3095979271655405, 1.2959394045847, 1.3048678466251917, 1.2982604624994087, 1.3223736397738528, 1.3548684985375365, 1.3120095154334759, 1.3074363579695252, 1.3096787330552275, 1.3482563844063795, 1.3376853693099249, 1.3679010028322343, 1.3708754973654285, 1.3507740879293733, 1.3437208074262772, 1.358365505982698, 1.366629334701889, 1.3758110428482833, 1.3940933287045834, 1.3621330126165756, 1.3629913776378912, 1.3406574324825518, 1.3662934870946974, 1.389556791590548, 1.363146849062251, 1.3734262592491062, 1.377781434990894, 1.3749414940772973, 1.4044563703740562, 1.363799225129126, 1.3954377794892134, 1.4269801963530542, 1.426707601703838, 1.4337538331991737, 1.4927630718118452, 1.4175443602312963, 1.4032673136941318, 1.3944168531248722, 1.4009175457194913, 1.4246792370462653, 1.4123864389209717, 1.4264736799966722, 1.4500482567816924, 1.447254220448887, 1.390686312724021, 1.4033640407772094, 1.4060857656162555, 1.4247835578981096, 1.4325245558138944, 1.4842355957955171, 1.4684412755402438, 1.431070387069815, 1.393399520460608, 1.4778349965272475, 1.4241790753867238, 1.4662717948797692, 1.4361529387472494, 1.4334463162962439, 1.4732227971401122, 1.4784238741707136, 1.4605386190618004, 1.4706735677515541, 1.5076756741613002, 1.4662243137610174, 1.486387941441904, 1.475237639862524, 1.4493538037505251, 1.4505334575775222, 1.4455746899684663, 1.439673822697356, 1.444716653800363, 1.4377500844510709, 1.4477997626772852, 1.454164280092775, 1.4466378679024958, 1.4689668628382566, 1.4951377899580205, 1.4808106764979747, 1.5071279816635332, 1.5407701358810826, 1.494765642241304, 1.469661323895008, 1.4618556775483005, 1.4575814748632496, 1.490639592039174, 1.4646741294704244, 1.422136279162515, 1.5245560691470192, 1.465348663001225, 1.4390053553338513, 1.4563563081431272, 1.4471562086850747, 1.4430589564328122, 1.4769870062375499, 1.478260781573153, 1.5264703332888474, 1.5585186275942573, 1.5384463326292868, 1.5928630274896356, 1.5892380546466471, 1.5574779334326683, 1.5358774542612788, 1.520314478130372, 1.5306173701983172, 1.4930841924717468, 1.48158778326069, 1.5351070439678498, 1.515996452818559, 1.4916145527499847, 1.5031449321064063, 1.5082889977347087, 1.5582307295258997, 1.5585997112474614, 1.5674054137200166, 1.5336351269375905, 1.505025795136375, 1.5347705445266122, 1.5681114251586212, 1.600163606587302, 1.5792396910280626, 1.5597685493271927, 1.5749712330954415, 1.601208347209373, 1.572116425863432, 1.5831783365929264, 1.5841393901405272, 1.614234369768102, 1.5841549989233659, 1.6115406420822018, 1.5665720243172105, 1.5430927583932486, 1.5607578621317797, 1.6082586690122858, 1.5866348228626845, 1.596640194186632, 1.621194390827799, 1.6290195031314845, 1.6500678150524646, 1.5842757201547106, 1.5594696618849029, 1.5865501374838191, 1.6375189146580562, 1.662526230702455, 1.62357663932105, 1.5684213534560305, 1.5753507297027287, 1.5597861372974315, 1.6252135514038537, 1.6138592373169898, 1.6370267380634551], 'val_acc': [0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.3957307053414863, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39080459696710207, 0.39408866921669156, 0.4022988498406653, 0.39080459696710207, 0.39737274146628104, 0.39408866921669156, 0.3891625608423073, 0.3924466330918968, 0.38423645158706626, 0.3908045967713561, 0.38259441644100134, 0.38259441634312835, 0.3924466330918968, 0.3809523797289687, 0.385878487809734, 0.3940886685315807, 0.38752052383665575, 0.3940886690209456, 0.3875205243260207, 0.37766830767512516, 0.39573070455850246, 0.3908045968692291, 0.3940886683358347, 0.3940886683358347, 0.38752052393452874, 0.3875205244238936, 0.40722495752993865, 0.39244663211316705, 0.4072249579214306, 0.399014776710219, 0.4039408855739681, 0.3842364519785582, 0.40394088547609514, 0.39244663240678596, 0.3990147771017109, 0.37274219920286794, 0.3973727408790432, 0.38423645178281224, 0.385878487809734, 0.38587848800547997, 0.37274219890924903, 0.3678160905348648, 0.379310343310555, 0.3711001628823273, 0.3825944155601445, 0.3678160904369918, 0.3596059097151451, 0.3563218374655556, 0.3743842350340438, 0.3678160904369918, 0.32840722324617194, 0.35139572879755243, 0.35303776502022016, 0.3382594397991945, 0.3875205240324017, 0.3234811144802958, 0.3596059096172721, 0.36945812695327845, 0.37602627096309255, 0.3711001627844543, 0.3875205240324017, 0.35796387310098543, 0.36124794574206687, 0.36124794574206687, 0.3579638736882233, 0.35796387359035037, 0.35960590942152615, 0.3284072226589341, 0.3612479455463209, 0.35632183717193666, 0.35796387310098543, 0.3513957305837343, 0.35632183736768264, 0.35632183726980965, 0.3596059092257802, 0.3218390781597551, 0.3809523793374768, 0.36945812656178656, 0.3743842348382978, 0.37602627057160065, 0.3776683066963954, 0.35139572850393347, 0.36453201750229147, 0.35467980085139594, 0.36288998147536966, 0.3513957304858613, 0.348111656156471, 0.3497536926727577, 0.3546798015365068, 0.37931034311480905, 0.3678160899476269, 0.35139572879755243, 0.37602627057160065, 0.3431855481735787, 0.3809523791417308, 0.3727421981262652, 0.37602627057160065, 0.35960590903003425, 0.3809523792396038, 0.33990147572824325, 0.3743842344468059, 0.37931034262544416, 0.35467980075352296, 0.3645320177959104, 0.36124794743037575, 0.35632183895811853, 0.35139572879755243, 0.34646962012954924, 0.3678160904369918, 0.36945812646391357, 0.3694581259745487, 0.3530377647266012, 0.36288998137749673, 0.35796387300311244, 0.34646962230722306, 0.3579638750829133, 0.3661740542143241, 0.3596059096172721, 0.3694581260724216, 0.36288998147536966, 0.3546798027354508, 0.3579638750829133, 0.36617405372495915, 0.3612479455463209, 0.3579638736882233, 0.35960590951939914, 0.37931034272331715, 0.35796387300311244, 0.35796387310098543, 0.35632183717193666, 0.3464696208146601, 0.3579638734924774, 0.361247945252702, 0.37602627057160065, 0.36288998147536966, 0.36617405372495915, 0.3612479455463209, 0.36781609014337285, 0.3546798029311968, 0.3661740536270862, 0.3399014777101711, 0.35303776680640203, 0.35960590942152615, 0.3513957284060605, 0.35303776680640203, 0.36288998137749673, 0.35796387300311244, 0.3694581262681676, 0.37110016219721637, 0.36781608984975395, 0.3628899817689886, 0.3645320176980374, 0.35303776502022016, 0.3579638736882233, 0.36945812665965955, 0.36288998167111564, 0.3661740539207051, 0.3760262706694736, 0.37110016239296234, 0.3530377647266012, 0.3727421985177571, 0.3711001625887083, 0.3711001626865813, 0.3530377647266012, 0.35632183717193666, 0.3530377647266012, 0.35632183726980965, 0.3645320177959104, 0.35303776521596614, 0.36453201818740233, 0.3546798010471419, 0.3300492590773478, 0.3661740540185781, 0.3563218370740637, 0.36453201789378337, 0.3546798010471419, 0.3612479456441939, 0.36453201789378337, 0.37602627106096553, 0.36781609014337285, 0.3628899818668616, 0.36945812656178656, 0.35960590951939914, 0.36945812646391357, 0.3711001625887083, 0.3628899819647346, 0.3678160899476269, 0.3563218370740637, 0.3530377648244742, 0.35467980094926893, 0.35796387310098543, 0.3530377648244742, 0.3645320177959104, 0.3530377646287282, 0.36124794593781284, 0.3661740540185781, 0.3596059097151451, 0.3694581263660406, 0.3661740540185781, 0.3579638732967314, 0.3579638736882233, 0.35796387310098543, 0.3563218369761907, 0.36617405382283214, 0.34482758420050047, 0.3530377648244742, 0.35960590942152615, 0.3267651866320123, 0.34646962003167625, 0.34154351383496584, 0.3448275839068815, 0.361247945154829, 0.3612479456441939, 0.35467980094926893, 0.3596059097151451, 0.35303776521596614, 0.3431855482714517, 0.3628899819647346, 0.35632183736768264, 0.3579638734924774, 0.3661740540185781, 0.36453201808952934, 0.35139572869967944, 0.34482758420050047, 0.3563218369761907, 0.35632183726980965, 0.3645320176001644, 0.348111656156471, 0.3563218370740637, 0.3497536926727577, 0.3596059098130181, 0.34975369286850366, 0.35960590951939914, 0.35632183736768264, 0.34811165654796294, 0.3513957289932984, 0.3546798014386338, 0.3694581262681676, 0.3481116567437089, 0.35632183736768264, 0.3628899819647346, 0.34811165684158185, 0.3546798013407609, 0.35796387359035037, 0.35303776521596614, 0.3596059097151451, 0.3579638736882233, 0.3628899820626076, 0.34811165664583593, 0.34975369286850366, 0.3513957288954254, 0.3546798013407609, 0.35303776521596614, 0.3448275844941194, 0.3546798013407609, 0.35303776521596614, 0.36453201808952934, 0.34318554836932463, 0.3415435123424029, 0.3464696204231682, 0.3546798012428879, 0.37274219881137605, 0.34811165693945484, 0.34482758439624644, 0.35960590932365316, 0.3546798010471419, 0.35139572909117134, 0.3464696203252952, 0.3546798013407609], 'loss': [1.086579358063684, 1.077860837501667, 1.0770894364409869, 1.0748152762223073, 1.0747551326144646, 1.0744311666096995, 1.074298593542659, 1.0742630982545858, 1.0742252205431584, 1.0741131911777129, 1.0741326908311315, 1.0741284558415658, 1.0742619423895645, 1.074196733292613, 1.0739964521885896, 1.0739475328085115, 1.0739146907716317, 1.0740075727263025, 1.073770903415014, 1.0738396109986355, 1.0736327284904965, 1.07351686729298, 1.0735950046985792, 1.0733670428058695, 1.073438799111995, 1.0730575231066475, 1.0730101896262512, 1.07286612561596, 1.0724884625577829, 1.0725332941117962, 1.072120554246452, 1.0722565537850226, 1.0718118773348768, 1.0719784417436353, 1.0713581894947029, 1.071145198134671, 1.0707766139776556, 1.070697430761443, 1.07144781128337, 1.0698841195331708, 1.0700533031193382, 1.070072708384457, 1.0685702454138095, 1.0679800788969476, 1.0678237614445618, 1.066667636610889, 1.0672609580370924, 1.0689412355422974, 1.0689703814547655, 1.0655653636313562, 1.0647667436384323, 1.0672899104241718, 1.0636311342094469, 1.0632638057399335, 1.0620581327277776, 1.0616683462072447, 1.0594632447867423, 1.059032471811502, 1.059199446525417, 1.0564938097763845, 1.0550103421573522, 1.0531117970448989, 1.0523869319618115, 1.0549984896207492, 1.048925013316975, 1.0465559566779792, 1.047484735639678, 1.0459419113899404, 1.044117344625187, 1.043318331021303, 1.041435200526729, 1.0397112523261036, 1.0344821054098299, 1.0356040686307746, 1.0341559760134813, 1.0308161576915327, 1.0266983895575976, 1.0243646912506228, 1.0224572022592753, 1.0267980369454293, 1.0228117290218752, 1.016765551792278, 1.0161381641697345, 1.0122466510815786, 1.0126600237597676, 1.010442981142283, 1.0077857825300778, 1.0091974654971207, 1.007988105760218, 1.018580950799664, 1.0217729030448552, 1.0126454223597563, 1.0030447872757178, 0.9994371236961725, 0.9950908612911217, 0.9910415346862355, 0.9923204978878248, 0.9987295225170848, 0.9934800002364408, 0.989292151727226, 0.9868906845791874, 0.9816332264112986, 0.9777694584407846, 0.9800760490693595, 0.979842969573254, 0.9792455066890442, 0.9748884831610646, 0.9823455508484733, 0.967527403185255, 0.9705372361187083, 0.9728681468376144, 0.9694548506756338, 0.9704708513782744, 0.9584067852834901, 0.9588106548761685, 0.9642669861076794, 0.9573642700115024, 0.9701039185024629, 0.966661542201189, 0.9587285070203903, 0.9540880097500842, 0.9458332524162543, 0.9456617819210342, 0.9440942366265174, 0.9369011619007808, 0.9416891202299992, 0.9405301918239319, 0.9348838708483953, 0.9355299157283634, 0.9265597265359068, 0.929933083008447, 0.9267456534217271, 0.9251328527805007, 0.9277454117729923, 0.924860621233006, 0.9223849480157026, 0.9263562230603651, 0.9206324001112512, 0.9127940185995317, 0.9099826404446204, 0.9159786302450991, 0.91421566102539, 0.9108609758363367, 0.904531708292403, 0.9130458857489318, 0.9031357884407043, 0.9115779923707308, 0.9091168992573231, 0.907507310633297, 0.9051405233768957, 0.8918775093873669, 0.8929224397857086, 0.9089994382564537, 0.8968466394001453, 0.8945791118198841, 0.8999999003243887, 0.9053255897038281, 0.8990352785562833, 0.9001559470958044, 0.8944692539238587, 0.8855983265371538, 0.8863586145749572, 0.8937632188415136, 0.8804427734145883, 0.8694595410593726, 0.8707146292104858, 0.8654534594723821, 0.8628376343901397, 0.8624503044866684, 0.8606338066976418, 0.8613060164745339, 0.8589689190138047, 0.8697542245383135, 0.861146375016755, 0.854681006643072, 0.8561683124830101, 0.8474438665583883, 0.849781443720236, 0.8471893213857615, 0.8453672524595163, 0.8369640230887725, 0.8372637162463131, 0.8320019995896968, 0.8316550332173185, 0.8492490897433224, 0.8431791731709083, 0.8339088887649395, 0.8291048845960863, 0.8235255315563272, 0.8204772973697043, 0.818226310212999, 0.8136067937286973, 0.8107848201933827, 0.8241753262416048, 0.8217834758317936, 0.8103902905139101, 0.8133423946231787, 0.8051444603921941, 0.7987832559941975, 0.7988885489332602, 0.8278351769310248, 0.816408481661544, 0.8158240593923926, 0.8195241229245306, 0.7995941697939221, 0.7926460324861185, 0.7903007584430843, 0.7977428547165967, 0.7971760542730532, 0.7858202066020065, 0.7837925499224809, 0.7834734597000498, 0.785921524437546, 0.7921053719961179, 0.7858375342230043, 0.7815625741986034, 0.7756518525264591, 0.7758158739097799, 0.7672771082766492, 0.7633131818849693, 0.7691796240130979, 0.7714895288067921, 0.7776190935953442, 0.7607921178580799, 0.7660391285434152, 0.7731000301529495, 0.7703841499968965, 0.7750388502095514, 0.756180564040276, 0.7554812704023639, 0.759919025765797, 0.7771180315428936, 0.7529528223268794, 0.7430258359997179, 0.751136139751215, 0.7441436457927711, 0.7645465417074716, 0.7859781937677512, 0.792078564005466, 0.7614738416378013, 0.7698728845349573, 0.787555486098452, 0.7669592647581864, 0.7511730995266345, 0.7325024590110387, 0.7291602680325753, 0.7289331101294171, 0.7290066480636597, 0.7329040025049166, 0.7421521481302484, 0.7376530187085913, 0.7227126231428534, 0.721924340724945, 0.7132541617084088, 0.7096277872150194, 0.7153064257555184, 0.714433759830326, 0.7092939780722898, 0.7223267961086923, 0.7260070902121385, 0.717611051119818, 0.7273561913374758, 0.7082272172708531, 0.7109575194255038, 0.7258010661087976, 0.7231027597274624, 0.7082986146517602, 0.6976509028146889, 0.6914560920893534, 0.6897533981951844, 0.6878289666998313, 0.6820377672477425, 0.6804738018791778, 0.6761078451203615, 0.6761176582234596, 0.6737392641190876, 0.6754729696612584, 0.6813388988222674, 0.6801145602055888, 0.6868722244944171, 0.6893080322404662, 0.6850587853660819, 0.6750981984197236, 0.6725976201543084, 0.671829317066459, 0.6655307801107606, 0.6598513292580904, 0.6620566886063719, 0.669447275525001, 0.6672785164883984, 0.6736865642379197, 0.6720329533612214, 0.6690105124665482, 0.6656162114113998, 0.6586106170863831, 0.6698014206954831, 0.6700507675108234, 0.6640622909064166, 0.6544797689273372, 0.646578275227204], 'acc': [0.39425051391002336, 0.39425051391002336, 0.39425051508498143, 0.39425051508498143, 0.39548254496507823, 0.39507186953781565, 0.3942505148891551, 0.39425051171921605, 0.3942505119150424, 0.39425051113173704, 0.3942505156724605, 0.3942505121475862, 0.3942505142649586, 0.3942505146933287, 0.3942505152808078, 0.3942505119150424, 0.3942505148891551, 0.3942505110950196, 0.3942505113275634, 0.39425051449750237, 0.3942505152808078, 0.394250514301676, 0.39425051312671794, 0.39425051171921605, 0.3942505152808078, 0.3942505113275634, 0.39425051113173704, 0.3942505138733059, 0.39425051547663414, 0.3942505152808078, 0.39425051391002336, 0.3938398362919535, 0.3938398349211691, 0.39425051547663414, 0.39466119231139857, 0.3938398349211691, 0.3958932247739553, 0.3950718677386611, 0.3938398361328446, 0.4057494863959553, 0.39671458079340033, 0.3963039035669832, 0.4045174559283795, 0.4016427109985626, 0.3958932221914953, 0.395893225165608, 0.4036961001047608, 0.4065708398696578, 0.3975359338387327, 0.4119096506670026, 0.4053388086187766, 0.40082135619079307, 0.420123204044248, 0.4020533894366552, 0.41971252638946077, 0.41806981474222343, 0.4114989736731292, 0.4151950726274103, 0.424229978621618, 0.42381930041606913, 0.4082135531202234, 0.4271047223397594, 0.4320328539524235, 0.42751539858704474, 0.43531827509280835, 0.43901437604207033, 0.43203285375659717, 0.4320328551273816, 0.4353182752886347, 0.44271047143475967, 0.4451745369473522, 0.4349075957123014, 0.4492813160654456, 0.4550308019351176, 0.4562628355726324, 0.4591375792907738, 0.45913757651248754, 0.464476387542376, 0.4616016404584693, 0.46036961077419886, 0.46570842117989086, 0.46570842020075914, 0.466940451843293, 0.4739219721337853, 0.4821355233202235, 0.4743326503760516, 0.4772073931150613, 0.47556468362191373, 0.4940451750275536, 0.47761806834649745, 0.45831622068886885, 0.46611909425723724, 0.4866529769231651, 0.4907597519289052, 0.4866529779022969, 0.49609856233459726, 0.500205342235996, 0.4903490743108354, 0.49650924386919404, 0.4969199189415213, 0.5014373748943791, 0.5014373705861994, 0.5100616031114081, 0.5030800828209159, 0.4997946608972256, 0.5006160173083233, 0.5121149923767152, 0.4924024625970108, 0.5260780294328254, 0.5170431220311159, 0.515811092310128, 0.5301848062010027, 0.5104722797503461, 0.5207392194187861, 0.5219712559936962, 0.5084188912683444, 0.5182751548853254, 0.5215605781798001, 0.5096509260808173, 0.5141683783129745, 0.514989732374156, 0.5318275176524137, 0.5375770043053911, 0.5342915839483117, 0.5363449720386607, 0.5363449694929182, 0.5375770023471276, 0.5412731028680194, 0.5449691996682107, 0.5519507140104776, 0.5490759789821303, 0.5498973350015753, 0.5486653003849288, 0.5441478485444243, 0.5457905574500928, 0.5441478473694662, 0.550308010269729, 0.5494866552294158, 0.5601642748658417, 0.552772076565627, 0.554004108049052, 0.5552361418823931, 0.5556468141397167, 0.5585215651768678, 0.5429158133402987, 0.5609856281437179, 0.5552361414907404, 0.5544147868420799, 0.5634496963978793, 0.5523614007099943, 0.5671457935897232, 0.5638603732326437, 0.5618069776274585, 0.5589322344967962, 0.5548254658309342, 0.5659137530248513, 0.5535934316059402, 0.5655030797149612, 0.5613963000093887, 0.5671457872498451, 0.5774127356080794, 0.5679671434651166, 0.5630390185839831, 0.5749486633639561, 0.5864476352991265, 0.5819301815003585, 0.5864476351033001, 0.5790554387613488, 0.5889117087182079, 0.588090345379753, 0.5868583129171963, 0.5880903498837591, 0.5852156049906595, 0.5872689901436134, 0.59219712946694, 0.5872689922977032, 0.599589321035624, 0.597125259904646, 0.591375767695096, 0.5958932222771693, 0.6069815191400124, 0.6102669369513494, 0.6004106778383744, 0.6036960989787593, 0.6020533869398693, 0.5926078028992217, 0.6069815162026172, 0.6180698181569454, 0.6197125219711288, 0.613963037863894, 0.620533879753011, 0.6168377796237718, 0.6246406551504037, 0.6123203271957883, 0.6094455830859942, 0.6213552340100188, 0.6287474362267605, 0.6197125258876558, 0.6398357251831148, 0.6332648896338758, 0.610677620052557, 0.6213552353808033, 0.6184804912710092, 0.6221765919877273, 0.6295687871547205, 0.636550310382608, 0.6443531849301082, 0.6369609864340671, 0.6340862436950574, 0.6377823391244641, 0.6451745409495532, 0.6410677655521604, 0.6414784400370087, 0.6398357272148132, 0.6381930163508813, 0.6447638648980941, 0.6406570849966954, 0.6476386078329301, 0.6550308029999233, 0.6574948683166896, 0.6542094497220472, 0.6505133486136764, 0.6492813171302514, 0.6529774137346162, 0.654620126165159, 0.6488706387288762, 0.6509240270150516, 0.6451745415370322, 0.6599589353958929, 0.6661190971211975, 0.6611909676626233, 0.635728949859157, 0.6620123219196312, 0.6689938398602072, 0.6640657119682437, 0.6714579015786643, 0.6488706387288762, 0.6476386062663194, 0.6328542112325006, 0.6533880867752451, 0.6537987709290193, 0.6422997968397591, 0.649691995727453, 0.6558521598026738, 0.6755646787384941, 0.6804928089803739, 0.6755646785426679, 0.6804928178659945, 0.6829568822036289, 0.6644763884113555, 0.6731006120509435, 0.6747433225232228, 0.6817248481744613, 0.6841889081304813, 0.6837782384189003, 0.6870636512611437, 0.682135521606743, 0.68665298115791, 0.6780287513742701, 0.6735112977713286, 0.6755646848825458, 0.6739219666996041, 0.6878850122496822, 0.6809034935258008, 0.6694045198281933, 0.679671461063244, 0.6841889148620119, 0.6948665256128174, 0.6973305994235515, 0.6956878804573043, 0.7002053417709084, 0.6989733091125253, 0.7014373676977608, 0.7018480534181458, 0.7084188875966003, 0.7034907642820777, 0.7030800799324772, 0.6960985671568211, 0.6960985604252904, 0.6956878883637931, 0.6895277250718777, 0.7006160120699685, 0.7047227971362872, 0.7030800856848762, 0.7014373748209443, 0.7055441452492434, 0.7129363406120629, 0.7055441466200278, 0.7051334670436945, 0.7088295663896282, 0.6948665275710809, 0.6989733014018629, 0.7030800841182654, 0.7063655000937303, 0.7108829612115081, 0.7051334686103053, 0.7059548216923551, 0.7059548280322332, 0.7154004154019287, 0.7145790548050428]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
