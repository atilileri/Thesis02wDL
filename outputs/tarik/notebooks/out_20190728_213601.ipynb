{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf70.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 21:36:01 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'AllShfUni', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eb', 'mb', 'eo', 'yd', 'by', 'ib', 'ce', 'sk', 'ds', 'ek', 'sg', 'eg', 'aa', 'ck', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000022A8059D278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000022ADF926EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6924, Accuracy:0.1248, Validation Loss:2.6848, Validation Accuracy:0.1264\n",
    "Epoch #2: Loss:2.6807, Accuracy:0.1269, Validation Loss:2.6768, Validation Accuracy:0.1396\n",
    "Epoch #3: Loss:2.6742, Accuracy:0.1265, Validation Loss:2.6712, Validation Accuracy:0.1051\n",
    "Epoch #4: Loss:2.6697, Accuracy:0.1039, Validation Loss:2.6677, Validation Accuracy:0.1067\n",
    "Epoch #5: Loss:2.6666, Accuracy:0.1018, Validation Loss:2.6659, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6647, Accuracy:0.1023, Validation Loss:2.6640, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6625, Accuracy:0.1023, Validation Loss:2.6621, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6607, Accuracy:0.1023, Validation Loss:2.6601, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6613, Accuracy:0.1023, Validation Loss:2.6610, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6602, Accuracy:0.1023, Validation Loss:2.6594, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6586, Accuracy:0.1023, Validation Loss:2.6583, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6573, Accuracy:0.1023, Validation Loss:2.6567, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6558, Accuracy:0.1023, Validation Loss:2.6555, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6539, Accuracy:0.1023, Validation Loss:2.6532, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6509, Accuracy:0.1023, Validation Loss:2.6500, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6472, Accuracy:0.1027, Validation Loss:2.6456, Validation Accuracy:0.1034\n",
    "Epoch #17: Loss:2.6410, Accuracy:0.1055, Validation Loss:2.6402, Validation Accuracy:0.1067\n",
    "Epoch #18: Loss:2.6353, Accuracy:0.1097, Validation Loss:2.6332, Validation Accuracy:0.1067\n",
    "Epoch #19: Loss:2.6335, Accuracy:0.1060, Validation Loss:2.6465, Validation Accuracy:0.1018\n",
    "Epoch #20: Loss:2.6348, Accuracy:0.1039, Validation Loss:2.6274, Validation Accuracy:0.1166\n",
    "Epoch #21: Loss:2.6196, Accuracy:0.1220, Validation Loss:2.6134, Validation Accuracy:0.1215\n",
    "Epoch #22: Loss:2.6040, Accuracy:0.1191, Validation Loss:2.6030, Validation Accuracy:0.1149\n",
    "Epoch #23: Loss:2.5905, Accuracy:0.1232, Validation Loss:2.5903, Validation Accuracy:0.1445\n",
    "Epoch #24: Loss:2.5782, Accuracy:0.1405, Validation Loss:2.5781, Validation Accuracy:0.1626\n",
    "Epoch #25: Loss:2.5648, Accuracy:0.1413, Validation Loss:2.5676, Validation Accuracy:0.1544\n",
    "Epoch #26: Loss:2.5509, Accuracy:0.1565, Validation Loss:2.5555, Validation Accuracy:0.1527\n",
    "Epoch #27: Loss:2.5376, Accuracy:0.1585, Validation Loss:2.5436, Validation Accuracy:0.1691\n",
    "Epoch #28: Loss:2.5298, Accuracy:0.1610, Validation Loss:2.5680, Validation Accuracy:0.1675\n",
    "Epoch #29: Loss:2.5299, Accuracy:0.1630, Validation Loss:2.5244, Validation Accuracy:0.1642\n",
    "Epoch #30: Loss:2.5241, Accuracy:0.1626, Validation Loss:2.5267, Validation Accuracy:0.1708\n",
    "Epoch #31: Loss:2.5160, Accuracy:0.1598, Validation Loss:2.5150, Validation Accuracy:0.1642\n",
    "Epoch #32: Loss:2.5086, Accuracy:0.1671, Validation Loss:2.5044, Validation Accuracy:0.1658\n",
    "Epoch #33: Loss:2.5049, Accuracy:0.1634, Validation Loss:2.4968, Validation Accuracy:0.1691\n",
    "Epoch #34: Loss:2.4950, Accuracy:0.1663, Validation Loss:2.4955, Validation Accuracy:0.1708\n",
    "Epoch #35: Loss:2.4977, Accuracy:0.1688, Validation Loss:2.4880, Validation Accuracy:0.1691\n",
    "Epoch #36: Loss:2.4916, Accuracy:0.1700, Validation Loss:2.4843, Validation Accuracy:0.1675\n",
    "Epoch #37: Loss:2.4907, Accuracy:0.1729, Validation Loss:2.4771, Validation Accuracy:0.1708\n",
    "Epoch #38: Loss:2.4876, Accuracy:0.1643, Validation Loss:2.4739, Validation Accuracy:0.1790\n",
    "Epoch #39: Loss:2.4847, Accuracy:0.1692, Validation Loss:2.4692, Validation Accuracy:0.1675\n",
    "Epoch #40: Loss:2.4805, Accuracy:0.1708, Validation Loss:2.4668, Validation Accuracy:0.1773\n",
    "Epoch #41: Loss:2.4862, Accuracy:0.1680, Validation Loss:2.4754, Validation Accuracy:0.1757\n",
    "Epoch #42: Loss:2.4792, Accuracy:0.1671, Validation Loss:2.4706, Validation Accuracy:0.1675\n",
    "Epoch #43: Loss:2.4789, Accuracy:0.1688, Validation Loss:2.4958, Validation Accuracy:0.1724\n",
    "Epoch #44: Loss:2.4831, Accuracy:0.1700, Validation Loss:2.4833, Validation Accuracy:0.1823\n",
    "Epoch #45: Loss:2.4875, Accuracy:0.1778, Validation Loss:2.4663, Validation Accuracy:0.1593\n",
    "Epoch #46: Loss:2.4794, Accuracy:0.1700, Validation Loss:2.4601, Validation Accuracy:0.1691\n",
    "Epoch #47: Loss:2.4756, Accuracy:0.1708, Validation Loss:2.4636, Validation Accuracy:0.1806\n",
    "Epoch #48: Loss:2.4700, Accuracy:0.1758, Validation Loss:2.4611, Validation Accuracy:0.1675\n",
    "Epoch #49: Loss:2.4689, Accuracy:0.1749, Validation Loss:2.4562, Validation Accuracy:0.1708\n",
    "Epoch #50: Loss:2.4660, Accuracy:0.1749, Validation Loss:2.4556, Validation Accuracy:0.1691\n",
    "Epoch #51: Loss:2.4656, Accuracy:0.1754, Validation Loss:2.4548, Validation Accuracy:0.1658\n",
    "Epoch #52: Loss:2.4662, Accuracy:0.1770, Validation Loss:2.4537, Validation Accuracy:0.1708\n",
    "Epoch #53: Loss:2.4648, Accuracy:0.1770, Validation Loss:2.4513, Validation Accuracy:0.1773\n",
    "Epoch #54: Loss:2.4661, Accuracy:0.1774, Validation Loss:2.4510, Validation Accuracy:0.1773\n",
    "Epoch #55: Loss:2.4644, Accuracy:0.1782, Validation Loss:2.4510, Validation Accuracy:0.1724\n",
    "Epoch #56: Loss:2.4647, Accuracy:0.1778, Validation Loss:2.4494, Validation Accuracy:0.1773\n",
    "Epoch #57: Loss:2.4646, Accuracy:0.1782, Validation Loss:2.4489, Validation Accuracy:0.1773\n",
    "Epoch #58: Loss:2.4634, Accuracy:0.1791, Validation Loss:2.4486, Validation Accuracy:0.1741\n",
    "Epoch #59: Loss:2.4628, Accuracy:0.1791, Validation Loss:2.4474, Validation Accuracy:0.1773\n",
    "Epoch #60: Loss:2.4623, Accuracy:0.1778, Validation Loss:2.4470, Validation Accuracy:0.1741\n",
    "Epoch #61: Loss:2.4612, Accuracy:0.1791, Validation Loss:2.4467, Validation Accuracy:0.1741\n",
    "Epoch #62: Loss:2.4609, Accuracy:0.1795, Validation Loss:2.4462, Validation Accuracy:0.1773\n",
    "Epoch #63: Loss:2.4607, Accuracy:0.1782, Validation Loss:2.4458, Validation Accuracy:0.1741\n",
    "Epoch #64: Loss:2.4596, Accuracy:0.1782, Validation Loss:2.4453, Validation Accuracy:0.1741\n",
    "Epoch #65: Loss:2.4597, Accuracy:0.1782, Validation Loss:2.4446, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4587, Accuracy:0.1774, Validation Loss:2.4443, Validation Accuracy:0.1773\n",
    "Epoch #67: Loss:2.4584, Accuracy:0.1778, Validation Loss:2.4443, Validation Accuracy:0.1741\n",
    "Epoch #68: Loss:2.4607, Accuracy:0.1778, Validation Loss:2.4444, Validation Accuracy:0.1708\n",
    "Epoch #69: Loss:2.4571, Accuracy:0.1799, Validation Loss:2.4453, Validation Accuracy:0.1823\n",
    "Epoch #70: Loss:2.4575, Accuracy:0.1778, Validation Loss:2.4438, Validation Accuracy:0.1708\n",
    "Epoch #71: Loss:2.4574, Accuracy:0.1807, Validation Loss:2.4430, Validation Accuracy:0.1741\n",
    "Epoch #72: Loss:2.4563, Accuracy:0.1782, Validation Loss:2.4429, Validation Accuracy:0.1773\n",
    "Epoch #73: Loss:2.4561, Accuracy:0.1778, Validation Loss:2.4428, Validation Accuracy:0.1773\n",
    "Epoch #74: Loss:2.4553, Accuracy:0.1799, Validation Loss:2.4426, Validation Accuracy:0.1773\n",
    "Epoch #75: Loss:2.4555, Accuracy:0.1762, Validation Loss:2.4428, Validation Accuracy:0.1708\n",
    "Epoch #76: Loss:2.4547, Accuracy:0.1811, Validation Loss:2.4437, Validation Accuracy:0.1708\n",
    "Epoch #77: Loss:2.4546, Accuracy:0.1828, Validation Loss:2.4436, Validation Accuracy:0.1823\n",
    "Epoch #78: Loss:2.4541, Accuracy:0.1791, Validation Loss:2.4421, Validation Accuracy:0.1708\n",
    "Epoch #79: Loss:2.4536, Accuracy:0.1803, Validation Loss:2.4422, Validation Accuracy:0.1741\n",
    "Epoch #80: Loss:2.4531, Accuracy:0.1803, Validation Loss:2.4418, Validation Accuracy:0.1708\n",
    "Epoch #81: Loss:2.4532, Accuracy:0.1803, Validation Loss:2.4417, Validation Accuracy:0.1741\n",
    "Epoch #82: Loss:2.4526, Accuracy:0.1807, Validation Loss:2.4424, Validation Accuracy:0.1708\n",
    "Epoch #83: Loss:2.4526, Accuracy:0.1782, Validation Loss:2.4419, Validation Accuracy:0.1823\n",
    "Epoch #84: Loss:2.4520, Accuracy:0.1811, Validation Loss:2.4418, Validation Accuracy:0.1708\n",
    "Epoch #85: Loss:2.4510, Accuracy:0.1807, Validation Loss:2.4418, Validation Accuracy:0.1790\n",
    "Epoch #86: Loss:2.4509, Accuracy:0.1799, Validation Loss:2.4415, Validation Accuracy:0.1708\n",
    "Epoch #87: Loss:2.4505, Accuracy:0.1815, Validation Loss:2.4414, Validation Accuracy:0.1708\n",
    "Epoch #88: Loss:2.4509, Accuracy:0.1754, Validation Loss:2.4415, Validation Accuracy:0.1708\n",
    "Epoch #89: Loss:2.4500, Accuracy:0.1815, Validation Loss:2.4414, Validation Accuracy:0.1708\n",
    "Epoch #90: Loss:2.4494, Accuracy:0.1782, Validation Loss:2.4416, Validation Accuracy:0.1823\n",
    "Epoch #91: Loss:2.4498, Accuracy:0.1811, Validation Loss:2.4417, Validation Accuracy:0.1691\n",
    "Epoch #92: Loss:2.4493, Accuracy:0.1815, Validation Loss:2.4411, Validation Accuracy:0.1806\n",
    "Epoch #93: Loss:2.4487, Accuracy:0.1791, Validation Loss:2.4413, Validation Accuracy:0.1708\n",
    "Epoch #94: Loss:2.4486, Accuracy:0.1815, Validation Loss:2.4413, Validation Accuracy:0.1790\n",
    "Epoch #95: Loss:2.4481, Accuracy:0.1819, Validation Loss:2.4414, Validation Accuracy:0.1773\n",
    "Epoch #96: Loss:2.4483, Accuracy:0.1811, Validation Loss:2.4421, Validation Accuracy:0.1708\n",
    "Epoch #97: Loss:2.4482, Accuracy:0.1803, Validation Loss:2.4418, Validation Accuracy:0.1708\n",
    "Epoch #98: Loss:2.4474, Accuracy:0.1786, Validation Loss:2.4426, Validation Accuracy:0.1806\n",
    "Epoch #99: Loss:2.4496, Accuracy:0.1795, Validation Loss:2.4421, Validation Accuracy:0.1708\n",
    "Epoch #100: Loss:2.4484, Accuracy:0.1828, Validation Loss:2.4428, Validation Accuracy:0.1773\n",
    "Epoch #101: Loss:2.4459, Accuracy:0.1860, Validation Loss:2.4426, Validation Accuracy:0.1708\n",
    "Epoch #102: Loss:2.4474, Accuracy:0.1807, Validation Loss:2.4418, Validation Accuracy:0.1708\n",
    "Epoch #103: Loss:2.4462, Accuracy:0.1832, Validation Loss:2.4422, Validation Accuracy:0.1790\n",
    "Epoch #104: Loss:2.4457, Accuracy:0.1848, Validation Loss:2.4423, Validation Accuracy:0.1708\n",
    "Epoch #105: Loss:2.4450, Accuracy:0.1815, Validation Loss:2.4420, Validation Accuracy:0.1708\n",
    "Epoch #106: Loss:2.4444, Accuracy:0.1815, Validation Loss:2.4420, Validation Accuracy:0.1773\n",
    "Epoch #107: Loss:2.4448, Accuracy:0.1828, Validation Loss:2.4419, Validation Accuracy:0.1708\n",
    "Epoch #108: Loss:2.4445, Accuracy:0.1782, Validation Loss:2.4421, Validation Accuracy:0.1691\n",
    "Epoch #109: Loss:2.4439, Accuracy:0.1815, Validation Loss:2.4430, Validation Accuracy:0.1708\n",
    "Epoch #110: Loss:2.4438, Accuracy:0.1799, Validation Loss:2.4423, Validation Accuracy:0.1773\n",
    "Epoch #111: Loss:2.4445, Accuracy:0.1799, Validation Loss:2.4424, Validation Accuracy:0.1708\n",
    "Epoch #112: Loss:2.4449, Accuracy:0.1811, Validation Loss:2.4428, Validation Accuracy:0.1773\n",
    "Epoch #113: Loss:2.4472, Accuracy:0.1778, Validation Loss:2.4438, Validation Accuracy:0.1708\n",
    "Epoch #114: Loss:2.4444, Accuracy:0.1786, Validation Loss:2.4457, Validation Accuracy:0.1790\n",
    "Epoch #115: Loss:2.4439, Accuracy:0.1770, Validation Loss:2.4451, Validation Accuracy:0.1691\n",
    "Epoch #116: Loss:2.4430, Accuracy:0.1807, Validation Loss:2.4435, Validation Accuracy:0.1773\n",
    "Epoch #117: Loss:2.4429, Accuracy:0.1807, Validation Loss:2.4435, Validation Accuracy:0.1708\n",
    "Epoch #118: Loss:2.4427, Accuracy:0.1815, Validation Loss:2.4432, Validation Accuracy:0.1708\n",
    "Epoch #119: Loss:2.4415, Accuracy:0.1803, Validation Loss:2.4438, Validation Accuracy:0.1773\n",
    "Epoch #120: Loss:2.4415, Accuracy:0.1832, Validation Loss:2.4440, Validation Accuracy:0.1708\n",
    "Epoch #121: Loss:2.4412, Accuracy:0.1828, Validation Loss:2.4437, Validation Accuracy:0.1724\n",
    "Epoch #122: Loss:2.4416, Accuracy:0.1815, Validation Loss:2.4440, Validation Accuracy:0.1708\n",
    "Epoch #123: Loss:2.4406, Accuracy:0.1844, Validation Loss:2.4445, Validation Accuracy:0.1708\n",
    "Epoch #124: Loss:2.4404, Accuracy:0.1815, Validation Loss:2.4442, Validation Accuracy:0.1691\n",
    "Epoch #125: Loss:2.4402, Accuracy:0.1828, Validation Loss:2.4446, Validation Accuracy:0.1790\n",
    "Epoch #126: Loss:2.4408, Accuracy:0.1819, Validation Loss:2.4448, Validation Accuracy:0.1708\n",
    "Epoch #127: Loss:2.4404, Accuracy:0.1811, Validation Loss:2.4446, Validation Accuracy:0.1741\n",
    "Epoch #128: Loss:2.4402, Accuracy:0.1860, Validation Loss:2.4450, Validation Accuracy:0.1675\n",
    "Epoch #129: Loss:2.4411, Accuracy:0.1856, Validation Loss:2.4454, Validation Accuracy:0.1741\n",
    "Epoch #130: Loss:2.4405, Accuracy:0.1860, Validation Loss:2.4448, Validation Accuracy:0.1675\n",
    "Epoch #131: Loss:2.4422, Accuracy:0.1844, Validation Loss:2.4452, Validation Accuracy:0.1741\n",
    "Epoch #132: Loss:2.4409, Accuracy:0.1848, Validation Loss:2.4469, Validation Accuracy:0.1609\n",
    "Epoch #133: Loss:2.4385, Accuracy:0.1844, Validation Loss:2.4472, Validation Accuracy:0.1741\n",
    "Epoch #134: Loss:2.4401, Accuracy:0.1811, Validation Loss:2.4456, Validation Accuracy:0.1658\n",
    "Epoch #135: Loss:2.4396, Accuracy:0.1856, Validation Loss:2.4453, Validation Accuracy:0.1675\n",
    "Epoch #136: Loss:2.4384, Accuracy:0.1873, Validation Loss:2.4456, Validation Accuracy:0.1642\n",
    "Epoch #137: Loss:2.4385, Accuracy:0.1836, Validation Loss:2.4458, Validation Accuracy:0.1741\n",
    "Epoch #138: Loss:2.4384, Accuracy:0.1869, Validation Loss:2.4465, Validation Accuracy:0.1658\n",
    "Epoch #139: Loss:2.4384, Accuracy:0.1844, Validation Loss:2.4462, Validation Accuracy:0.1642\n",
    "Epoch #140: Loss:2.4384, Accuracy:0.1848, Validation Loss:2.4460, Validation Accuracy:0.1675\n",
    "Epoch #141: Loss:2.4387, Accuracy:0.1873, Validation Loss:2.4468, Validation Accuracy:0.1757\n",
    "Epoch #142: Loss:2.4378, Accuracy:0.1864, Validation Loss:2.4458, Validation Accuracy:0.1658\n",
    "Epoch #143: Loss:2.4379, Accuracy:0.1873, Validation Loss:2.4458, Validation Accuracy:0.1658\n",
    "Epoch #144: Loss:2.4376, Accuracy:0.1864, Validation Loss:2.4461, Validation Accuracy:0.1609\n",
    "Epoch #145: Loss:2.4382, Accuracy:0.1897, Validation Loss:2.4470, Validation Accuracy:0.1708\n",
    "Epoch #146: Loss:2.4380, Accuracy:0.1885, Validation Loss:2.4464, Validation Accuracy:0.1609\n",
    "Epoch #147: Loss:2.4382, Accuracy:0.1901, Validation Loss:2.4464, Validation Accuracy:0.1626\n",
    "Epoch #148: Loss:2.4367, Accuracy:0.1893, Validation Loss:2.4466, Validation Accuracy:0.1626\n",
    "Epoch #149: Loss:2.4364, Accuracy:0.1869, Validation Loss:2.4469, Validation Accuracy:0.1593\n",
    "Epoch #150: Loss:2.4367, Accuracy:0.1881, Validation Loss:2.4476, Validation Accuracy:0.1609\n",
    "Epoch #151: Loss:2.4364, Accuracy:0.1873, Validation Loss:2.4473, Validation Accuracy:0.1609\n",
    "Epoch #152: Loss:2.4366, Accuracy:0.1844, Validation Loss:2.4479, Validation Accuracy:0.1576\n",
    "Epoch #153: Loss:2.4363, Accuracy:0.1844, Validation Loss:2.4475, Validation Accuracy:0.1609\n",
    "Epoch #154: Loss:2.4368, Accuracy:0.1881, Validation Loss:2.4480, Validation Accuracy:0.1691\n",
    "Epoch #155: Loss:2.4370, Accuracy:0.1877, Validation Loss:2.4473, Validation Accuracy:0.1609\n",
    "Epoch #156: Loss:2.4362, Accuracy:0.1864, Validation Loss:2.4474, Validation Accuracy:0.1609\n",
    "Epoch #157: Loss:2.4356, Accuracy:0.1893, Validation Loss:2.4478, Validation Accuracy:0.1609\n",
    "Epoch #158: Loss:2.4355, Accuracy:0.1881, Validation Loss:2.4482, Validation Accuracy:0.1609\n",
    "Epoch #159: Loss:2.4353, Accuracy:0.1881, Validation Loss:2.4484, Validation Accuracy:0.1675\n",
    "Epoch #160: Loss:2.4355, Accuracy:0.1885, Validation Loss:2.4480, Validation Accuracy:0.1609\n",
    "Epoch #161: Loss:2.4353, Accuracy:0.1869, Validation Loss:2.4481, Validation Accuracy:0.1609\n",
    "Epoch #162: Loss:2.4352, Accuracy:0.1881, Validation Loss:2.4481, Validation Accuracy:0.1593\n",
    "Epoch #163: Loss:2.4350, Accuracy:0.1881, Validation Loss:2.4482, Validation Accuracy:0.1609\n",
    "Epoch #164: Loss:2.4343, Accuracy:0.1881, Validation Loss:2.4490, Validation Accuracy:0.1593\n",
    "Epoch #165: Loss:2.4344, Accuracy:0.1877, Validation Loss:2.4501, Validation Accuracy:0.1691\n",
    "Epoch #166: Loss:2.4350, Accuracy:0.1877, Validation Loss:2.4498, Validation Accuracy:0.1593\n",
    "Epoch #167: Loss:2.4349, Accuracy:0.1877, Validation Loss:2.4487, Validation Accuracy:0.1609\n",
    "Epoch #168: Loss:2.4341, Accuracy:0.1877, Validation Loss:2.4490, Validation Accuracy:0.1593\n",
    "Epoch #169: Loss:2.4342, Accuracy:0.1869, Validation Loss:2.4494, Validation Accuracy:0.1609\n",
    "Epoch #170: Loss:2.4342, Accuracy:0.1873, Validation Loss:2.4502, Validation Accuracy:0.1593\n",
    "Epoch #171: Loss:2.4343, Accuracy:0.1864, Validation Loss:2.4498, Validation Accuracy:0.1593\n",
    "Epoch #172: Loss:2.4343, Accuracy:0.1860, Validation Loss:2.4501, Validation Accuracy:0.1658\n",
    "Epoch #173: Loss:2.4359, Accuracy:0.1856, Validation Loss:2.4498, Validation Accuracy:0.1593\n",
    "Epoch #174: Loss:2.4339, Accuracy:0.1864, Validation Loss:2.4502, Validation Accuracy:0.1544\n",
    "Epoch #175: Loss:2.4336, Accuracy:0.1934, Validation Loss:2.4509, Validation Accuracy:0.1691\n",
    "Epoch #176: Loss:2.4338, Accuracy:0.1893, Validation Loss:2.4505, Validation Accuracy:0.1609\n",
    "Epoch #177: Loss:2.4332, Accuracy:0.1873, Validation Loss:2.4501, Validation Accuracy:0.1593\n",
    "Epoch #178: Loss:2.4337, Accuracy:0.1869, Validation Loss:2.4500, Validation Accuracy:0.1593\n",
    "Epoch #179: Loss:2.4333, Accuracy:0.1901, Validation Loss:2.4503, Validation Accuracy:0.1691\n",
    "Epoch #180: Loss:2.4328, Accuracy:0.1860, Validation Loss:2.4506, Validation Accuracy:0.1593\n",
    "Epoch #181: Loss:2.4342, Accuracy:0.1873, Validation Loss:2.4508, Validation Accuracy:0.1675\n",
    "Epoch #182: Loss:2.4366, Accuracy:0.1815, Validation Loss:2.4511, Validation Accuracy:0.1675\n",
    "Epoch #183: Loss:2.4349, Accuracy:0.1869, Validation Loss:2.4508, Validation Accuracy:0.1675\n",
    "Epoch #184: Loss:2.4346, Accuracy:0.1852, Validation Loss:2.4509, Validation Accuracy:0.1576\n",
    "Epoch #185: Loss:2.4333, Accuracy:0.1864, Validation Loss:2.4534, Validation Accuracy:0.1741\n",
    "Epoch #186: Loss:2.4330, Accuracy:0.1844, Validation Loss:2.4514, Validation Accuracy:0.1544\n",
    "Epoch #187: Loss:2.4345, Accuracy:0.1836, Validation Loss:2.4513, Validation Accuracy:0.1675\n",
    "Epoch #188: Loss:2.4339, Accuracy:0.1864, Validation Loss:2.4515, Validation Accuracy:0.1675\n",
    "Epoch #189: Loss:2.4366, Accuracy:0.1815, Validation Loss:2.4516, Validation Accuracy:0.1560\n",
    "Epoch #190: Loss:2.4332, Accuracy:0.1877, Validation Loss:2.4527, Validation Accuracy:0.1675\n",
    "Epoch #191: Loss:2.4342, Accuracy:0.1823, Validation Loss:2.4523, Validation Accuracy:0.1527\n",
    "Epoch #192: Loss:2.4327, Accuracy:0.1856, Validation Loss:2.4531, Validation Accuracy:0.1691\n",
    "Epoch #193: Loss:2.4340, Accuracy:0.1860, Validation Loss:2.4517, Validation Accuracy:0.1593\n",
    "Epoch #194: Loss:2.4374, Accuracy:0.1823, Validation Loss:2.4510, Validation Accuracy:0.1576\n",
    "Epoch #195: Loss:2.4371, Accuracy:0.1864, Validation Loss:2.4536, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4354, Accuracy:0.1844, Validation Loss:2.4557, Validation Accuracy:0.1527\n",
    "Epoch #197: Loss:2.4330, Accuracy:0.1844, Validation Loss:2.4535, Validation Accuracy:0.1675\n",
    "Epoch #198: Loss:2.4336, Accuracy:0.1852, Validation Loss:2.4516, Validation Accuracy:0.1593\n",
    "Epoch #199: Loss:2.4328, Accuracy:0.1864, Validation Loss:2.4526, Validation Accuracy:0.1576\n",
    "Epoch #200: Loss:2.4308, Accuracy:0.1869, Validation Loss:2.4536, Validation Accuracy:0.1691\n",
    "Epoch #201: Loss:2.4335, Accuracy:0.1860, Validation Loss:2.4525, Validation Accuracy:0.1576\n",
    "Epoch #202: Loss:2.4325, Accuracy:0.1848, Validation Loss:2.4523, Validation Accuracy:0.1593\n",
    "Epoch #203: Loss:2.4331, Accuracy:0.1840, Validation Loss:2.4525, Validation Accuracy:0.1675\n",
    "Epoch #204: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4531, Validation Accuracy:0.1560\n",
    "Epoch #205: Loss:2.4314, Accuracy:0.1864, Validation Loss:2.4522, Validation Accuracy:0.1675\n",
    "Epoch #206: Loss:2.4318, Accuracy:0.1889, Validation Loss:2.4528, Validation Accuracy:0.1708\n",
    "Epoch #207: Loss:2.4315, Accuracy:0.1836, Validation Loss:2.4537, Validation Accuracy:0.1511\n",
    "Epoch #208: Loss:2.4319, Accuracy:0.1836, Validation Loss:2.4531, Validation Accuracy:0.1708\n",
    "Epoch #209: Loss:2.4313, Accuracy:0.1844, Validation Loss:2.4528, Validation Accuracy:0.1576\n",
    "Epoch #210: Loss:2.4307, Accuracy:0.1819, Validation Loss:2.4531, Validation Accuracy:0.1708\n",
    "Epoch #211: Loss:2.4310, Accuracy:0.1877, Validation Loss:2.4535, Validation Accuracy:0.1658\n",
    "Epoch #212: Loss:2.4302, Accuracy:0.1860, Validation Loss:2.4538, Validation Accuracy:0.1560\n",
    "Epoch #213: Loss:2.4306, Accuracy:0.1860, Validation Loss:2.4533, Validation Accuracy:0.1593\n",
    "Epoch #214: Loss:2.4299, Accuracy:0.1869, Validation Loss:2.4534, Validation Accuracy:0.1626\n",
    "Epoch #215: Loss:2.4304, Accuracy:0.1901, Validation Loss:2.4538, Validation Accuracy:0.1626\n",
    "Epoch #216: Loss:2.4300, Accuracy:0.1864, Validation Loss:2.4537, Validation Accuracy:0.1691\n",
    "Epoch #217: Loss:2.4299, Accuracy:0.1869, Validation Loss:2.4536, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.4310, Accuracy:0.1873, Validation Loss:2.4535, Validation Accuracy:0.1593\n",
    "Epoch #219: Loss:2.4322, Accuracy:0.1864, Validation Loss:2.4545, Validation Accuracy:0.1708\n",
    "Epoch #220: Loss:2.4324, Accuracy:0.1864, Validation Loss:2.4544, Validation Accuracy:0.1544\n",
    "Epoch #221: Loss:2.4291, Accuracy:0.1844, Validation Loss:2.4554, Validation Accuracy:0.1757\n",
    "Epoch #222: Loss:2.4301, Accuracy:0.1889, Validation Loss:2.4546, Validation Accuracy:0.1560\n",
    "Epoch #223: Loss:2.4302, Accuracy:0.1856, Validation Loss:2.4540, Validation Accuracy:0.1626\n",
    "Epoch #224: Loss:2.4295, Accuracy:0.1881, Validation Loss:2.4544, Validation Accuracy:0.1708\n",
    "Epoch #225: Loss:2.4300, Accuracy:0.1893, Validation Loss:2.4553, Validation Accuracy:0.1593\n",
    "Epoch #226: Loss:2.4309, Accuracy:0.1873, Validation Loss:2.4540, Validation Accuracy:0.1626\n",
    "Epoch #227: Loss:2.4304, Accuracy:0.1873, Validation Loss:2.4541, Validation Accuracy:0.1609\n",
    "Epoch #228: Loss:2.4296, Accuracy:0.1906, Validation Loss:2.4547, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4289, Accuracy:0.1877, Validation Loss:2.4552, Validation Accuracy:0.1593\n",
    "Epoch #230: Loss:2.4293, Accuracy:0.1864, Validation Loss:2.4548, Validation Accuracy:0.1708\n",
    "Epoch #231: Loss:2.4299, Accuracy:0.1889, Validation Loss:2.4544, Validation Accuracy:0.1626\n",
    "Epoch #232: Loss:2.4295, Accuracy:0.1864, Validation Loss:2.4550, Validation Accuracy:0.1593\n",
    "Epoch #233: Loss:2.4285, Accuracy:0.1852, Validation Loss:2.4553, Validation Accuracy:0.1691\n",
    "Epoch #234: Loss:2.4294, Accuracy:0.1885, Validation Loss:2.4551, Validation Accuracy:0.1560\n",
    "Epoch #235: Loss:2.4300, Accuracy:0.1840, Validation Loss:2.4553, Validation Accuracy:0.1593\n",
    "Epoch #236: Loss:2.4298, Accuracy:0.1906, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #237: Loss:2.4289, Accuracy:0.1885, Validation Loss:2.4559, Validation Accuracy:0.1544\n",
    "Epoch #238: Loss:2.4287, Accuracy:0.1889, Validation Loss:2.4560, Validation Accuracy:0.1757\n",
    "Epoch #239: Loss:2.4291, Accuracy:0.1823, Validation Loss:2.4564, Validation Accuracy:0.1593\n",
    "Epoch #240: Loss:2.4288, Accuracy:0.1873, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #241: Loss:2.4283, Accuracy:0.1860, Validation Loss:2.4559, Validation Accuracy:0.1708\n",
    "Epoch #242: Loss:2.4278, Accuracy:0.1893, Validation Loss:2.4561, Validation Accuracy:0.1576\n",
    "Epoch #243: Loss:2.4288, Accuracy:0.1848, Validation Loss:2.4556, Validation Accuracy:0.1708\n",
    "Epoch #244: Loss:2.4287, Accuracy:0.1832, Validation Loss:2.4560, Validation Accuracy:0.1560\n",
    "Epoch #245: Loss:2.4275, Accuracy:0.1828, Validation Loss:2.4561, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4282, Accuracy:0.1877, Validation Loss:2.4557, Validation Accuracy:0.1593\n",
    "Epoch #247: Loss:2.4287, Accuracy:0.1893, Validation Loss:2.4553, Validation Accuracy:0.1724\n",
    "Epoch #248: Loss:2.4274, Accuracy:0.1864, Validation Loss:2.4552, Validation Accuracy:0.1609\n",
    "Epoch #249: Loss:2.4278, Accuracy:0.1869, Validation Loss:2.4562, Validation Accuracy:0.1576\n",
    "Epoch #250: Loss:2.4275, Accuracy:0.1848, Validation Loss:2.4563, Validation Accuracy:0.1609\n",
    "Epoch #251: Loss:2.4284, Accuracy:0.1869, Validation Loss:2.4572, Validation Accuracy:0.1773\n",
    "Epoch #252: Loss:2.4277, Accuracy:0.1844, Validation Loss:2.4558, Validation Accuracy:0.1609\n",
    "Epoch #253: Loss:2.4278, Accuracy:0.1856, Validation Loss:2.4558, Validation Accuracy:0.1691\n",
    "Epoch #254: Loss:2.4281, Accuracy:0.1885, Validation Loss:2.4566, Validation Accuracy:0.1593\n",
    "Epoch #255: Loss:2.4282, Accuracy:0.1860, Validation Loss:2.4571, Validation Accuracy:0.1741\n",
    "Epoch #256: Loss:2.4273, Accuracy:0.1881, Validation Loss:2.4579, Validation Accuracy:0.1544\n",
    "Epoch #257: Loss:2.4269, Accuracy:0.1852, Validation Loss:2.4574, Validation Accuracy:0.1757\n",
    "Epoch #258: Loss:2.4276, Accuracy:0.1889, Validation Loss:2.4569, Validation Accuracy:0.1544\n",
    "Epoch #259: Loss:2.4282, Accuracy:0.1832, Validation Loss:2.4571, Validation Accuracy:0.1609\n",
    "Epoch #260: Loss:2.4277, Accuracy:0.1889, Validation Loss:2.4566, Validation Accuracy:0.1757\n",
    "Epoch #261: Loss:2.4273, Accuracy:0.1844, Validation Loss:2.4567, Validation Accuracy:0.1544\n",
    "Epoch #262: Loss:2.4260, Accuracy:0.1910, Validation Loss:2.4572, Validation Accuracy:0.1741\n",
    "Epoch #263: Loss:2.4273, Accuracy:0.1885, Validation Loss:2.4565, Validation Accuracy:0.1658\n",
    "Epoch #264: Loss:2.4264, Accuracy:0.1840, Validation Loss:2.4571, Validation Accuracy:0.1560\n",
    "Epoch #265: Loss:2.4261, Accuracy:0.1840, Validation Loss:2.4578, Validation Accuracy:0.1757\n",
    "Epoch #266: Loss:2.4269, Accuracy:0.1881, Validation Loss:2.4571, Validation Accuracy:0.1675\n",
    "Epoch #267: Loss:2.4288, Accuracy:0.1881, Validation Loss:2.4575, Validation Accuracy:0.1544\n",
    "Epoch #268: Loss:2.4278, Accuracy:0.1910, Validation Loss:2.4584, Validation Accuracy:0.1757\n",
    "Epoch #269: Loss:2.4265, Accuracy:0.1864, Validation Loss:2.4582, Validation Accuracy:0.1544\n",
    "Epoch #270: Loss:2.4273, Accuracy:0.1873, Validation Loss:2.4574, Validation Accuracy:0.1741\n",
    "Epoch #271: Loss:2.4264, Accuracy:0.1889, Validation Loss:2.4575, Validation Accuracy:0.1544\n",
    "Epoch #272: Loss:2.4276, Accuracy:0.1848, Validation Loss:2.4571, Validation Accuracy:0.1560\n",
    "Epoch #273: Loss:2.4265, Accuracy:0.1877, Validation Loss:2.4587, Validation Accuracy:0.1790\n",
    "Epoch #274: Loss:2.4269, Accuracy:0.1881, Validation Loss:2.4583, Validation Accuracy:0.1544\n",
    "Epoch #275: Loss:2.4263, Accuracy:0.1832, Validation Loss:2.4577, Validation Accuracy:0.1757\n",
    "Epoch #276: Loss:2.4273, Accuracy:0.1877, Validation Loss:2.4581, Validation Accuracy:0.1724\n",
    "Epoch #277: Loss:2.4263, Accuracy:0.1889, Validation Loss:2.4573, Validation Accuracy:0.1658\n",
    "Epoch #278: Loss:2.4259, Accuracy:0.1910, Validation Loss:2.4570, Validation Accuracy:0.1658\n",
    "Epoch #279: Loss:2.4260, Accuracy:0.1864, Validation Loss:2.4584, Validation Accuracy:0.1691\n",
    "Epoch #280: Loss:2.4251, Accuracy:0.1914, Validation Loss:2.4586, Validation Accuracy:0.1773\n",
    "Epoch #281: Loss:2.4273, Accuracy:0.1832, Validation Loss:2.4578, Validation Accuracy:0.1560\n",
    "Epoch #282: Loss:2.4251, Accuracy:0.1893, Validation Loss:2.4577, Validation Accuracy:0.1741\n",
    "Epoch #283: Loss:2.4258, Accuracy:0.1889, Validation Loss:2.4573, Validation Accuracy:0.1658\n",
    "Epoch #284: Loss:2.4248, Accuracy:0.1881, Validation Loss:2.4580, Validation Accuracy:0.1642\n",
    "Epoch #285: Loss:2.4254, Accuracy:0.1893, Validation Loss:2.4581, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.4250, Accuracy:0.1836, Validation Loss:2.4575, Validation Accuracy:0.1560\n",
    "Epoch #287: Loss:2.4257, Accuracy:0.1836, Validation Loss:2.4574, Validation Accuracy:0.1658\n",
    "Epoch #288: Loss:2.4249, Accuracy:0.1885, Validation Loss:2.4587, Validation Accuracy:0.1724\n",
    "Epoch #289: Loss:2.4251, Accuracy:0.1910, Validation Loss:2.4583, Validation Accuracy:0.1642\n",
    "Epoch #290: Loss:2.4259, Accuracy:0.1897, Validation Loss:2.4578, Validation Accuracy:0.1609\n",
    "Epoch #291: Loss:2.4266, Accuracy:0.1852, Validation Loss:2.4582, Validation Accuracy:0.1544\n",
    "Epoch #292: Loss:2.4253, Accuracy:0.1881, Validation Loss:2.4589, Validation Accuracy:0.1741\n",
    "Epoch #293: Loss:2.4239, Accuracy:0.1832, Validation Loss:2.4595, Validation Accuracy:0.1593\n",
    "Epoch #294: Loss:2.4261, Accuracy:0.1840, Validation Loss:2.4583, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4247, Accuracy:0.1852, Validation Loss:2.4589, Validation Accuracy:0.1658\n",
    "Epoch #296: Loss:2.4253, Accuracy:0.1852, Validation Loss:2.4584, Validation Accuracy:0.1560\n",
    "Epoch #297: Loss:2.4246, Accuracy:0.1897, Validation Loss:2.4582, Validation Accuracy:0.1741\n",
    "Epoch #298: Loss:2.4250, Accuracy:0.1926, Validation Loss:2.4594, Validation Accuracy:0.1642\n",
    "Epoch #299: Loss:2.4242, Accuracy:0.1856, Validation Loss:2.4593, Validation Accuracy:0.1757\n",
    "Epoch #300: Loss:2.4252, Accuracy:0.1848, Validation Loss:2.4585, Validation Accuracy:0.1560\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45850301, Accuracy:0.1560\n",
    "Labels: ['eb', 'mb', 'eo', 'yd', 'by', 'ib', 'ce', 'sk', 'ds', 'ek', 'sg', 'eg', 'aa', 'ck', 'my']\n",
    "Confusion Matrix:\n",
    "      eb  mb  eo  yd  by  ib  ce  sk  ds  ek  sg  eg  aa  ck  my\n",
    "t:eb   3   0   0   8   2   0   0   0   3   3  16  15   0   0   0\n",
    "t:mb   3   0   0  15   1   0   0   0   2   1  22   8   0   0   0\n",
    "t:eo   4   0   0   5   1   0   0   0   1   0  21   2   0   0   0\n",
    "t:yd   0   0   0  41   0   0   0   0   0   0  19   2   0   0   0\n",
    "t:by   2   0   0   6   0   0   0   0   1   5  18   8   0   0   0\n",
    "t:ib   0   0   0  37   0   0   0   0   0   1  10   6   0   0   0\n",
    "t:ce   1   0   0   9   1   0   0   0   0   1   6   9   0   0   0\n",
    "t:sk   1   0   0   3   0   0   0   0   6   1  10  11   1   0   0\n",
    "t:ds   2   0   0   2   0   0   0   0   5   1   5  15   1   0   0\n",
    "t:ek   3   0   0   8   0   0   0   0   4   1  19  13   0   0   0\n",
    "t:sg   4   0   0  25   1   0   0   0   0   1  19   1   0   0   0\n",
    "t:eg   4   0   0   3   0   0   0   0  10   2   7  24   0   0   0\n",
    "t:aa   1   0   0   3   0   0   0   0   6   2   6  14   2   0   0\n",
    "t:ck   2   0   0   3   0   0   0   0   4   1   8   5   0   0   0\n",
    "t:my   0   0   0  10   0   0   0   0   1   0   7   2   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eb       0.10      0.06      0.07        50\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          yd       0.23      0.66      0.34        62\n",
    "          by       0.00      0.00      0.00        40\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ds       0.12      0.16      0.14        31\n",
    "          ek       0.05      0.02      0.03        48\n",
    "          sg       0.10      0.37      0.16        51\n",
    "          eg       0.18      0.48      0.26        50\n",
    "          aa       0.50      0.06      0.11        34\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.08      0.12      0.07       609\n",
    "weighted avg       0.09      0.16      0.09       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 22:16:47 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 46 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.684756968017478, 2.6768289320966097, 2.671174793603581, 2.6676684956636727, 2.6659462158315876, 2.6639731157393682, 2.662091266541254, 2.660093064770127, 2.6610048122593923, 2.6594294922300943, 2.658319796247435, 2.6566959410073916, 2.655478946485347, 2.653204458687693, 2.6500354553091117, 2.6455602759406682, 2.6402404151722325, 2.6331820495805913, 2.6465110195485635, 2.6274390627793687, 2.613431478760317, 2.6029930932964205, 2.5903220669976594, 2.578142256572329, 2.5676364679446166, 2.5554614615166322, 2.5436233291876533, 2.5680007143756636, 2.5244302659590647, 2.526683639814505, 2.5150389150641432, 2.504356338864281, 2.496843774526186, 2.495507612604226, 2.4880335914286094, 2.4843222362850295, 2.4771471258454723, 2.4739256472814652, 2.4692201320760945, 2.4668328609372594, 2.4753681431067207, 2.470625949414884, 2.4958042953597697, 2.4832584337256423, 2.4662795466155254, 2.4600869008081494, 2.463582319579101, 2.4611100179612735, 2.4561920502698675, 2.455633639897815, 2.454834521110422, 2.453705092369042, 2.4513187020870264, 2.450954429034529, 2.4509542947528007, 2.44937607220241, 2.448900741896606, 2.448576045545255, 2.447395325490015, 2.4470121014881605, 2.4467461935209327, 2.4462229168082303, 2.445757850246085, 2.4453298654070825, 2.4445800174437524, 2.4443433480505483, 2.4442711518511593, 2.444447891661295, 2.4453301566770707, 2.443764980594904, 2.4430326013925234, 2.442939428273093, 2.4428196223498566, 2.442591222831964, 2.4428319578687545, 2.4437046524730612, 2.443558906686717, 2.442140828995478, 2.4421870450080907, 2.441842062719937, 2.441713615787049, 2.44239532379877, 2.441911395938917, 2.441803576323786, 2.441807616128906, 2.4414831722898436, 2.4414133242589893, 2.4415482754386315, 2.4413522436896766, 2.4415566463188583, 2.441711186579687, 2.441096574019133, 2.441291635651111, 2.441305771249856, 2.441432170288512, 2.442112791910156, 2.4417640822274342, 2.4425527865467793, 2.4421014327721053, 2.4427795437560684, 2.4426356642117053, 2.4418155262231434, 2.442248849445963, 2.4422676770753657, 2.4420366784426184, 2.441969608829918, 2.4419163862864175, 2.442091619244154, 2.4429539698489586, 2.4423471738160734, 2.442379520835939, 2.4427762039385015, 2.443818289266627, 2.4456965664924657, 2.4450556852155914, 2.443471490455966, 2.4434506313749917, 2.4432011922983503, 2.443793525836738, 2.444011657696052, 2.443717325849486, 2.444007440936585, 2.4445381685235033, 2.4442459660014886, 2.4446283686532957, 2.444791511166076, 2.444591156563344, 2.445013857435906, 2.4453668680488576, 2.444750121857341, 2.4452290628931204, 2.4469116708915224, 2.447168361964484, 2.4456207932314067, 2.445313742203861, 2.4455916619261693, 2.4458135869506936, 2.4464791635378633, 2.4462373863495825, 2.4459859573195133, 2.44676421663444, 2.445810064697892, 2.4458112947655035, 2.446092342116758, 2.4470282297807766, 2.4464110756546797, 2.4463551710000373, 2.446592852399854, 2.4468788607367156, 2.4475980012483394, 2.447287694182498, 2.4478553010912365, 2.4474809263727346, 2.4480452173449136, 2.4472772597483616, 2.447440276200744, 2.4478000149938275, 2.4482030574911335, 2.44835666130329, 2.4479887508993667, 2.4481433855097476, 2.4481078259071887, 2.448233395374467, 2.449032164168084, 2.4501015102530546, 2.4497832515948317, 2.448713406357663, 2.4490369098331346, 2.449430495451628, 2.4501768395622765, 2.4498422596059215, 2.450068453066846, 2.449761118599151, 2.450220783355788, 2.450906727701572, 2.4505401080465083, 2.450117228653631, 2.4500108328946117, 2.450285534161848, 2.4506047069537034, 2.4508310781519596, 2.4510640534274097, 2.4508327118477404, 2.450871097238976, 2.4534171911687492, 2.451386835774765, 2.4513256549835205, 2.4514786187063886, 2.451581343837168, 2.452706360464613, 2.4523191957050945, 2.453098158531001, 2.4516517227310657, 2.4509515887606517, 2.453642287277823, 2.455670260834968, 2.453467557778695, 2.451581260840881, 2.4526343267343704, 2.453593997532511, 2.452517192351994, 2.4523471548835243, 2.4525064106645256, 2.453112269465755, 2.4521900730571526, 2.452765990556363, 2.453749334870888, 2.4531074631194567, 2.4527629306555188, 2.453072760101218, 2.4534557272843736, 2.453761491477979, 2.4533041008979035, 2.4533715565216365, 2.453832632215152, 2.4537122680244385, 2.4536103677671335, 2.453542900398643, 2.454485342028889, 2.454385450125132, 2.4554164104273752, 2.4546374245034452, 2.454036125800097, 2.454414699660929, 2.455250143026092, 2.453998488158428, 2.4541078451623273, 2.454735563306386, 2.4551938699775535, 2.454773994110684, 2.454390501936864, 2.454970160141367, 2.4552982104039938, 2.4551360415316177, 2.4553089920914624, 2.455333089593596, 2.455852049716392, 2.455971116111392, 2.456412394841512, 2.4553143500498753, 2.455873104151834, 2.456060963898457, 2.455552609291766, 2.4559747992673726, 2.456077534968434, 2.455675205769406, 2.455341473001565, 2.455237947269809, 2.4562061228384136, 2.456283415479613, 2.45721489181268, 2.455847208918805, 2.4557919870260707, 2.4566040117360886, 2.4570842431292355, 2.4579138544392705, 2.4574169223923206, 2.4568613859624504, 2.457090510131886, 2.4566436268034435, 2.456748883320976, 2.4572441581826294, 2.456455286304743, 2.4570777040397007, 2.457820756486288, 2.4570792876245156, 2.4574888053981736, 2.4584325201601427, 2.45821962763719, 2.457372407412098, 2.4574539504810704, 2.4571132538549616, 2.458714826940903, 2.4582809994764907, 2.457671908909464, 2.458050096759264, 2.457294711925713, 2.4570300097535984, 2.458426792241865, 2.4586282516347953, 2.4578205067144436, 2.4577221639442133, 2.4572555047929385, 2.458049597607066, 2.4580699320888675, 2.4574759617227637, 2.4573940655280806, 2.458711662903208, 2.4582686580852138, 2.457774392098237, 2.458242060907173, 2.458935592757853, 2.459491271690782, 2.4583494287406285, 2.458856824583608, 2.458372760093075, 2.458174973285844, 2.4593813736450496, 2.459338373738557, 2.4585028779134768], 'val_acc': [0.1264367814012153, 0.13957307059531923, 0.10509031168101064, 0.1067323478058054, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10344827545834293, 0.1067323478058054, 0.1067323478058054, 0.10180623933354818, 0.11658456455457386, 0.1215106729289581, 0.11494252862552508, 0.14449917797873957, 0.16256157554722772, 0.15435139482538102, 0.15270935870058627, 0.16912971985066075, 0.16748768401948494, 0.16420361137840353, 0.1707717559754555, 0.16420361157414948, 0.1658456477968172, 0.16912971975278776, 0.1707717559754555, 0.16912971985066075, 0.16748768362799302, 0.17077175607332848, 0.17898193679517518, 0.16748768392161195, 0.17733990067038044, 0.17569786454558567, 0.16748768392161195, 0.17241379239386917, 0.18226600904476467, 0.15927750329763823, 0.1691297200464067, 0.1806239730178429, 0.16748768392161195, 0.1707717563669474, 0.16912972024215267, 0.16584564809043614, 0.1707717563669474, 0.1773399008661264, 0.1773399008661264, 0.17241379249174216, 0.1773399008661264, 0.1773399008661264, 0.1740558286165369, 0.1773399008661264, 0.1740558286165369, 0.1740558286165369, 0.1773399008661264, 0.1740558286165369, 0.1740558285186639, 0.17077175626907443, 0.1773399008661264, 0.1740558286165369, 0.1707717563669474, 0.18226600914263763, 0.17077175626907443, 0.1740558285186639, 0.1773399008661264, 0.1773399008661264, 0.1773399008661264, 0.1707717563669474, 0.1707717563669474, 0.18226600914263763, 0.1707717563669474, 0.1740558286165369, 0.1707717563669474, 0.1740558286165369, 0.1707717563669474, 0.18226600914263763, 0.1707717563669474, 0.17898193689304814, 0.1707717563669474, 0.1707717563669474, 0.1707717563669474, 0.1707717563669474, 0.18226600914263763, 0.16912972024215267, 0.1806239730178429, 0.1707717563669474, 0.17898193689304814, 0.1773399007682534, 0.1707717563669474, 0.1707717563669474, 0.1806239730178429, 0.1707717563669474, 0.1773399007682534, 0.1707717563669474, 0.1707717563669474, 0.17898193689304814, 0.1707717563669474, 0.1707717563669474, 0.1773399007682534, 0.1707717563669474, 0.16912972024215267, 0.1707717563669474, 0.1773399007682534, 0.1707717563669474, 0.1773399007682534, 0.1707717563669474, 0.17898193689304814, 0.16912972024215267, 0.1773399007682534, 0.1707717563669474, 0.1707717563669474, 0.1773399007682534, 0.1707717563669474, 0.17241379249174216, 0.1707717563669474, 0.1707717563669474, 0.16912972024215267, 0.17898193689304814, 0.1707717563669474, 0.1740558285186639, 0.16748768411735793, 0.1740558285186639, 0.16748768411735793, 0.1740558285186639, 0.16091953942243298, 0.1740558285186639, 0.16584564799256318, 0.16748768411735793, 0.1642036118677684, 0.1740558285186639, 0.16584564799256318, 0.1642036119656414, 0.1674876842152309, 0.17569786474133164, 0.16584564809043614, 0.16584564809043614, 0.16091953952030597, 0.17077175617120144, 0.16091953952030597, 0.1625615756451007, 0.1625615756451007, 0.1592775033955112, 0.16091953952030597, 0.16091953952030597, 0.1576354670749705, 0.16091953952030597, 0.1691297200464067, 0.16091953952030597, 0.16091953952030597, 0.16091953952030597, 0.16091953952030597, 0.16748768392161195, 0.16091953952030597, 0.16091953952030597, 0.1592775033955112, 0.16091953952030597, 0.1592775033955112, 0.1691297200464067, 0.1592775033955112, 0.16091953952030597, 0.1592775033955112, 0.16091953952030597, 0.1592775033955112, 0.1592775033955112, 0.1658456477968172, 0.1592775033955112, 0.15435139482538102, 0.1691297200464067, 0.16091953952030597, 0.1592775033955112, 0.1592775033955112, 0.1691297200464067, 0.1592775033955112, 0.16748768392161195, 0.16748768392161195, 0.16748768392161195, 0.15763546727071645, 0.17405582842079093, 0.15435139482538102, 0.16748768392161195, 0.16748768392161195, 0.1559934311459217, 0.16748768392161195, 0.15270935870058627, 0.1691297200464067, 0.1592775033955112, 0.15763546727071645, 0.17241379229599618, 0.15270935860271329, 0.16748768392161195, 0.1592775033955112, 0.15763546727071645, 0.1691297200464067, 0.15763546727071645, 0.1592775033955112, 0.16748768392161195, 0.1559934311459217, 0.16748768392161195, 0.17077175617120144, 0.15106732257579153, 0.17077175617120144, 0.15763546727071645, 0.17077175617120144, 0.1658456477968172, 0.1559934311459217, 0.1592775033955112, 0.1625615756451007, 0.1625615756451007, 0.1691297200464067, 0.1691297200464067, 0.1592775033955112, 0.17077175617120144, 0.15435139482538102, 0.17569786454558567, 0.15599343095017576, 0.1625615756451007, 0.17077175617120144, 0.1592775033955112, 0.1625615756451007, 0.16091953952030597, 0.1691297200464067, 0.1592775033955112, 0.17077175617120144, 0.1625615756451007, 0.1592775033955112, 0.1691297200464067, 0.15599343095017576, 0.1592775033955112, 0.17405582842079093, 0.15435139482538102, 0.17569786454558567, 0.1592775033955112, 0.17405582842079093, 0.17077175617120144, 0.1576354670749705, 0.17077175617120144, 0.15599343095017576, 0.1691297200464067, 0.1592775033955112, 0.17241379229599618, 0.16091953952030597, 0.1576354670749705, 0.16091953952030597, 0.17733990067038044, 0.16091953952030597, 0.1691297200464067, 0.15927750319976525, 0.17405582842079093, 0.15435139482538102, 0.17569786454558567, 0.15435139482538102, 0.16091953952030597, 0.17569786454558567, 0.15435139482538102, 0.17405582842079093, 0.1658456478946902, 0.15599343095017576, 0.17569786454558567, 0.16748768392161195, 0.15435139482538102, 0.17569786454558567, 0.15435139482538102, 0.17405582842079093, 0.15435139482538102, 0.15599343095017576, 0.17898193679517518, 0.15435139482538102, 0.17569786454558567, 0.17241379210025023, 0.1658456478946902, 0.1658456478946902, 0.16912972014427968, 0.17733990067038044, 0.15599343095017576, 0.17405582842079093, 0.1658456478946902, 0.16420361176989545, 0.16912972014427968, 0.15599343095017576, 0.1658456478946902, 0.17241379210025023, 0.16420361176989545, 0.16091953932456, 0.15435139482538102, 0.17405582842079093, 0.15927750319976525, 0.16912972014427968, 0.1658456478946902, 0.15599343095017576, 0.17405582842079093, 0.16420361157414948, 0.17569786454558567, 0.15599343095017576], 'loss': [2.692415510114948, 2.680712937133758, 2.6742264621311636, 2.6696532868261946, 2.6665985613632985, 2.6647315584413813, 2.662450154361294, 2.660748910512278, 2.6612672171308764, 2.660242759618426, 2.658625871493831, 2.6572727514733034, 2.6558175085016833, 2.65387987346375, 2.6509352019190544, 2.6471614384308486, 2.6410113622520495, 2.6353458960687846, 2.633458070147943, 2.6347825990320475, 2.6195732051097393, 2.604014294739866, 2.590523686790858, 2.5782109168520697, 2.564828645130447, 2.5509064903494267, 2.5375840132241376, 2.52983068714886, 2.5298805937874733, 2.5240932934582845, 2.515987151946865, 2.5086431929218205, 2.50491139521589, 2.4949903621321097, 2.4976870320416085, 2.491561015381705, 2.49065513708753, 2.487579879623664, 2.484672845609379, 2.4805247619166755, 2.486156321648944, 2.4792213191241945, 2.4788700097884977, 2.4831270363786135, 2.4875479672723726, 2.479442813411141, 2.4755675530286787, 2.469979213836012, 2.4688510227986673, 2.4660443680976694, 2.4656294434956703, 2.4661568813010653, 2.4647766178883077, 2.4661230925417046, 2.4643923297310266, 2.464713909885477, 2.4645956622991227, 2.463387679955798, 2.46278822789202, 2.4622553109633114, 2.461154360056413, 2.46088734701184, 2.460712949549148, 2.4596196322470476, 2.4597286387880235, 2.458654952196125, 2.458419858552592, 2.4606798344324257, 2.4570707374040106, 2.4575026271279587, 2.457373757039252, 2.4562643216620725, 2.4560736020732463, 2.455283185050228, 2.455544866037075, 2.454714963862049, 2.4546180242385707, 2.45407044725986, 2.4536265545557168, 2.4530895132303727, 2.453193365817687, 2.45256545940219, 2.452590317990501, 2.4520131339282716, 2.450967550375623, 2.4508512888600937, 2.4505197127496925, 2.4509442423403387, 2.4500436115069064, 2.449397229462923, 2.4498242555457708, 2.4493111389128823, 2.4486872698492093, 2.4485940254689242, 2.448118449138665, 2.448279428677882, 2.4482346102931904, 2.447377298597927, 2.4495597427141007, 2.4483744974743415, 2.4459079685642, 2.4474052927577277, 2.446195479829698, 2.4456639805858384, 2.445012977235861, 2.4444044202260167, 2.444849295586776, 2.4445387357559047, 2.443855141614252, 2.4437590568706975, 2.4444984705296386, 2.44485206143812, 2.447242900820973, 2.444446512610026, 2.4438953670877694, 2.4429687792760393, 2.442916662443345, 2.4426735001667814, 2.4414868450752274, 2.44154003533005, 2.4412371207556443, 2.44157725516286, 2.440579958667011, 2.4404337871246025, 2.4402330412267417, 2.44082097879915, 2.4403791073166614, 2.440228094849009, 2.4411075108349936, 2.440541819233669, 2.4422115781223996, 2.4409399657278823, 2.4384820927339903, 2.4400938379691124, 2.4395765405416, 2.4384280612091755, 2.4385149788317984, 2.438387062809061, 2.4384310032063197, 2.438369289707599, 2.4386504035221233, 2.43784732740273, 2.4378813938438526, 2.437596416179649, 2.4381872869370165, 2.4379599001373355, 2.4382303952681212, 2.4367071222230883, 2.4363659399490825, 2.4367311417934094, 2.43638736393907, 2.436600177439821, 2.436304198838847, 2.4368353907332527, 2.4369659417953335, 2.436151679487444, 2.4356091044032353, 2.435544089663934, 2.4353223477545707, 2.4354991697432813, 2.4352549427588617, 2.435223781695356, 2.434998076260702, 2.4343339104427204, 2.4344074989491173, 2.4349871204619045, 2.4349148745409517, 2.4341056665111127, 2.434193447923758, 2.434241708544001, 2.434299006452306, 2.434280483727582, 2.4359424532316547, 2.433901143122992, 2.433603913093739, 2.4337731791227997, 2.4332217841177752, 2.433684219912582, 2.4333303978310963, 2.432783971332182, 2.434167356902324, 2.4366376803396173, 2.4349150949435066, 2.4345643864030464, 2.4333480692007705, 2.4329970267763863, 2.4345114606117075, 2.4339436266701324, 2.4365526618409206, 2.433197513987641, 2.4342195925036982, 2.4327378875176273, 2.433969420720909, 2.4374096618785503, 2.4370625838606754, 2.4354389599950896, 2.4330296632935133, 2.4336178367387586, 2.432764705740206, 2.4307860729875506, 2.433458329422028, 2.4324549968727314, 2.4330896977771235, 2.4308195228694154, 2.431411238617476, 2.4318145022499977, 2.4315201657508676, 2.43190231842182, 2.4312579862880512, 2.4307419061171203, 2.4309914852314662, 2.430238691933101, 2.4306128813255983, 2.4298628622991103, 2.430444946621967, 2.430025337266237, 2.4299002704189543, 2.431035555559507, 2.43220224067171, 2.4324301764705587, 2.4291200356806573, 2.4300697810351237, 2.430199165461734, 2.429467148261883, 2.429991446187609, 2.430898492194299, 2.4304318381041226, 2.429555818334497, 2.428906294452581, 2.4292957789599283, 2.429861458909585, 2.4294596059121636, 2.4285200851409097, 2.4293964517189983, 2.4299893035536186, 2.429759563900362, 2.428925788965558, 2.4287204520168735, 2.429099074526244, 2.428822288376105, 2.428325968650332, 2.4277719841355907, 2.428835223783458, 2.428680217633257, 2.42753531614613, 2.428244595067457, 2.4286872612132675, 2.427407826143613, 2.4278342874632726, 2.427546688173831, 2.428417856345676, 2.4276851326043603, 2.4278349721700994, 2.428090550130887, 2.428237127817142, 2.427321505791353, 2.4268951594217603, 2.4276083173693084, 2.4281931299448503, 2.427739313740505, 2.4272800459264485, 2.426030116893917, 2.427324553045159, 2.4263716092589456, 2.42610831965656, 2.42689943078607, 2.428849214890654, 2.427757227249459, 2.4265323502816702, 2.4272936785735144, 2.4263928686568867, 2.427557726611347, 2.4265002014700636, 2.4269488633290943, 2.426336248797313, 2.427327825890919, 2.426315240350837, 2.425935663188018, 2.4260275874049757, 2.4250781571351037, 2.427276254336692, 2.4250992844481734, 2.425814975311624, 2.424799494772721, 2.4253552179316964, 2.4249538380507327, 2.425699768125154, 2.42491432499347, 2.425149206361242, 2.425919102055826, 2.426596667585432, 2.425283759820143, 2.423947499077423, 2.4261095199741622, 2.4246906526279646, 2.425327306802268, 2.424560296119361, 2.4250192185936523, 2.424153940878365, 2.425245122106658], 'acc': [0.12484599640726798, 0.12689938488926975, 0.12648870649707392, 0.10390143711158138, 0.10184804883458531, 0.10225872683512846, 0.10225872742260751, 0.10225872743178686, 0.10225872644347576, 0.10225872703095482, 0.10225872684430783, 0.10225872704931353, 0.10225872742260751, 0.10225872683512846, 0.10225872664848147, 0.10266940463984527, 0.10554414818051903, 0.10965092434285847, 0.10595482560276251, 0.10390143750323408, 0.12197125249330024, 0.11909650899852325, 0.12320328474167191, 0.1404517445047778, 0.14127310132588694, 0.15646817337193772, 0.1585215614622868, 0.16098562540826855, 0.1630390140860967, 0.1626283356847215, 0.15975359433485498, 0.1671457906400888, 0.1634496913125138, 0.1663244350306552, 0.16878850054324776, 0.17002053476824164, 0.17289527788054526, 0.1642710465302947, 0.16919917776966487, 0.17084188882942317, 0.16796714509292304, 0.1671457906400888, 0.16878850209149981, 0.17002053298744577, 0.1778234091015567, 0.17002053318327212, 0.17084188802775907, 0.17577002022790222, 0.17494866639926449, 0.1749486653834153, 0.17535934241400608, 0.17700205308211167, 0.17700205327793803, 0.17741273208932465, 0.17823408712963792, 0.17782340892408907, 0.17823408613214747, 0.1790554417599398, 0.1790554417599398, 0.177823408709904, 0.1790554425616039, 0.1794661188088893, 0.17823408534884208, 0.17823408711127922, 0.17823408515301573, 0.17741273130601926, 0.17782340931574178, 0.17782340812242498, 0.17987679762027592, 0.17782340911991543, 0.18069815105726098, 0.17823408732546428, 0.17782340968903576, 0.179876795625295, 0.1761806976501457, 0.1811088294769949, 0.18275154071422084, 0.1790554425616039, 0.1802874744366816, 0.18028747365337622, 0.18028747343919116, 0.18069815103890224, 0.1782340869338116, 0.181108830651953, 0.18069815303388317, 0.17987679639024168, 0.1815195074867174, 0.17535934341149653, 0.18151950648922696, 0.17823408613214747, 0.1811088286936895, 0.18151950705834727, 0.1790554425616039, 0.1815195062934006, 0.18193018391147042, 0.18110883006447395, 0.1802874744183229, 0.17864476433769633, 0.17946611800722517, 0.1827515397350891, 0.18603696048382126, 0.1806981530522419, 0.1831622181364643, 0.18480492704213278, 0.18151950766418504, 0.181519507076706, 0.18275153953926274, 0.17823408611378874, 0.1815195066850533, 0.1798767976019172, 0.1798767976019172, 0.181108830651953, 0.17782340833661003, 0.1786447633769233, 0.177002052690459, 0.1806981530522419, 0.18069815105726098, 0.18151950609757425, 0.1802874744183229, 0.18316221815482303, 0.18275154112423225, 0.18151950650758567, 0.18439424961988932, 0.18151950727253235, 0.18275154110587352, 0.18193018490896087, 0.18110882965446254, 0.18603696126712665, 0.18562628445072096, 0.18603696206879078, 0.18439424961988932, 0.18480492782543817, 0.18439425136396773, 0.18110883024194158, 0.18562628423653588, 0.18726899275055167, 0.18357289575453412, 0.18685831671745134, 0.184394250011542, 0.1848049284129172, 0.18726899372968342, 0.18644763888519647, 0.18726899374804212, 0.18644763810189108, 0.1897330586547969, 0.18850102775885094, 0.1901437366461607, 0.18932238125091216, 0.1868583157199609, 0.18809035014078113, 0.18726899353385706, 0.18439425138232643, 0.18439424963824802, 0.18809034994495477, 0.18767967095610052, 0.18644763769187966, 0.18932238240751154, 0.18809034818251766, 0.1880903495533021, 0.18850102578222874, 0.18685831630743993, 0.18809034935747573, 0.18809034894746432, 0.18809034875163796, 0.18767967074191547, 0.18767967115192688, 0.1876796725227113, 0.18767967213105863, 0.1868583153283082, 0.1872689929280193, 0.18644763788770602, 0.18603696087547397, 0.18562628304321907, 0.18644763949103424, 0.1934291585882097, 0.1893223823891528, 0.18726899312384565, 0.1868583153283082, 0.19014373646869306, 0.18603696187296442, 0.18726899433552116, 0.1815195062934006, 0.18685831609325487, 0.18521560603098702, 0.18644763888519647, 0.184394250011542, 0.18357289616454553, 0.18644763888519647, 0.18151950688087964, 0.1876796703686215, 0.18234086290032467, 0.18562628325740416, 0.18603696167713807, 0.18234086250867199, 0.18644763790606472, 0.18439425159651152, 0.1843942511865001, 0.18521560464184386, 0.1864476394726755, 0.18685831513248186, 0.18603696067964762, 0.18480492704213278, 0.18398357317677758, 0.1823408617253666, 0.18644763888519647, 0.18891170459361536, 0.18357289459793474, 0.18357289418792333, 0.18439425140068516, 0.18193018588809262, 0.18767967193523227, 0.18603696144459428, 0.18603696206879078, 0.18685831652162502, 0.19014373746618354, 0.18644763829771743, 0.1868583153283082, 0.1872689929280193, 0.18644763966850186, 0.18644763929520788, 0.18439424961988932, 0.18891170379195124, 0.18562628423653588, 0.18809034816415893, 0.1893223830133493, 0.18726899312384565, 0.18726899470881514, 0.1905544154575473, 0.18767967056444784, 0.18644763831607614, 0.18891170457525663, 0.18644763790606472, 0.1852156067959337, 0.1885010263880665, 0.18398357178763441, 0.19055441524336225, 0.18850102636970778, 0.18891170537692076, 0.18234086211701928, 0.18726899472717387, 0.1860369602879949, 0.18932238258497916, 0.18480492823544958, 0.18316221753062653, 0.18275154112423225, 0.18767967117028558, 0.18932238080418332, 0.18644763966850186, 0.1868583165032663, 0.18480492823544958, 0.18685831630743993, 0.18439425159651152, 0.18562628327576286, 0.1885010259964138, 0.18603696165877937, 0.18809034898418175, 0.18521560681429242, 0.18891170379195124, 0.18316221794063794, 0.1889117035961249, 0.18439424960153059, 0.19096509227395303, 0.1885010262105989, 0.1839835735684303, 0.18398357298095122, 0.1880903495533021, 0.18809034935747573, 0.1909650924881381, 0.18644763927684918, 0.1872689929280193, 0.18891170340029856, 0.18480492723795913, 0.18767967173940592, 0.18809034837834399, 0.18316221717569128, 0.18767967254107004, 0.1889117043794303, 0.19096509131318, 0.18644763929520788, 0.19137577012456664, 0.18316221874230207, 0.18932238103672708, 0.18891170500362678, 0.18809034796833257, 0.18932238221168518, 0.18357289514869635, 0.18357289575453412, 0.18850102738555696, 0.19096509327144348, 0.1897330594381023, 0.1852156062451721, 0.18809034916164938, 0.1831622181364643, 0.18398357198346077, 0.18521560526604036, 0.1852156058535194, 0.18973305902809087, 0.19260780215875323, 0.18562628345323048, 0.18480492780707944]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
