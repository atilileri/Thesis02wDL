{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf8.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 14:48:23 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'Split', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 5 Label(s): ['02', '05', '01', '04', '03'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000017780321E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000177C7446EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6067, Accuracy:0.2253, Validation Loss:1.6042, Validation Accuracy:0.2328\n",
    "Epoch #2: Loss:1.6047, Accuracy:0.2331, Validation Loss:1.6046, Validation Accuracy:0.2328\n",
    "Epoch #3: Loss:1.6038, Accuracy:0.2346, Validation Loss:1.6039, Validation Accuracy:0.2356\n",
    "Epoch #4: Loss:1.6030, Accuracy:0.2402, Validation Loss:1.6027, Validation Accuracy:0.2426\n",
    "Epoch #5: Loss:1.6028, Accuracy:0.2444, Validation Loss:1.6021, Validation Accuracy:0.2414\n",
    "Epoch #6: Loss:1.6025, Accuracy:0.2400, Validation Loss:1.6016, Validation Accuracy:0.2352\n",
    "Epoch #7: Loss:1.6027, Accuracy:0.2405, Validation Loss:1.6024, Validation Accuracy:0.2328\n",
    "Epoch #8: Loss:1.6026, Accuracy:0.2383, Validation Loss:1.6017, Validation Accuracy:0.2377\n",
    "Epoch #9: Loss:1.6022, Accuracy:0.2420, Validation Loss:1.6015, Validation Accuracy:0.2426\n",
    "Epoch #10: Loss:1.6015, Accuracy:0.2440, Validation Loss:1.6009, Validation Accuracy:0.2410\n",
    "Epoch #11: Loss:1.6026, Accuracy:0.2413, Validation Loss:1.6014, Validation Accuracy:0.2352\n",
    "Epoch #12: Loss:1.6018, Accuracy:0.2421, Validation Loss:1.6008, Validation Accuracy:0.2336\n",
    "Epoch #13: Loss:1.6014, Accuracy:0.2428, Validation Loss:1.6010, Validation Accuracy:0.2418\n",
    "Epoch #14: Loss:1.6026, Accuracy:0.2417, Validation Loss:1.6009, Validation Accuracy:0.2381\n",
    "Epoch #15: Loss:1.6013, Accuracy:0.2421, Validation Loss:1.6009, Validation Accuracy:0.2418\n",
    "Epoch #16: Loss:1.6014, Accuracy:0.2437, Validation Loss:1.6004, Validation Accuracy:0.2434\n",
    "Epoch #17: Loss:1.6010, Accuracy:0.2447, Validation Loss:1.5998, Validation Accuracy:0.2340\n",
    "Epoch #18: Loss:1.6011, Accuracy:0.2420, Validation Loss:1.5997, Validation Accuracy:0.2418\n",
    "Epoch #19: Loss:1.6009, Accuracy:0.2443, Validation Loss:1.5997, Validation Accuracy:0.2414\n",
    "Epoch #20: Loss:1.6005, Accuracy:0.2449, Validation Loss:1.5993, Validation Accuracy:0.2381\n",
    "Epoch #21: Loss:1.6002, Accuracy:0.2434, Validation Loss:1.5992, Validation Accuracy:0.2459\n",
    "Epoch #22: Loss:1.6003, Accuracy:0.2408, Validation Loss:1.5986, Validation Accuracy:0.2365\n",
    "Epoch #23: Loss:1.6004, Accuracy:0.2425, Validation Loss:1.5995, Validation Accuracy:0.2414\n",
    "Epoch #24: Loss:1.6009, Accuracy:0.2436, Validation Loss:1.5994, Validation Accuracy:0.2381\n",
    "Epoch #25: Loss:1.6006, Accuracy:0.2443, Validation Loss:1.5988, Validation Accuracy:0.2504\n",
    "Epoch #26: Loss:1.6013, Accuracy:0.2378, Validation Loss:1.5980, Validation Accuracy:0.2455\n",
    "Epoch #27: Loss:1.5997, Accuracy:0.2451, Validation Loss:1.5998, Validation Accuracy:0.2385\n",
    "Epoch #28: Loss:1.6020, Accuracy:0.2347, Validation Loss:1.5995, Validation Accuracy:0.2434\n",
    "Epoch #29: Loss:1.6020, Accuracy:0.2398, Validation Loss:1.6021, Validation Accuracy:0.2328\n",
    "Epoch #30: Loss:1.6026, Accuracy:0.2356, Validation Loss:1.6022, Validation Accuracy:0.2328\n",
    "Epoch #31: Loss:1.6020, Accuracy:0.2391, Validation Loss:1.6032, Validation Accuracy:0.2389\n",
    "Epoch #32: Loss:1.6026, Accuracy:0.2364, Validation Loss:1.6016, Validation Accuracy:0.2389\n",
    "Epoch #33: Loss:1.6019, Accuracy:0.2391, Validation Loss:1.6010, Validation Accuracy:0.2344\n",
    "Epoch #34: Loss:1.6014, Accuracy:0.2398, Validation Loss:1.6029, Validation Accuracy:0.2328\n",
    "Epoch #35: Loss:1.6041, Accuracy:0.2341, Validation Loss:1.6022, Validation Accuracy:0.2389\n",
    "Epoch #36: Loss:1.6012, Accuracy:0.2437, Validation Loss:1.6008, Validation Accuracy:0.2410\n",
    "Epoch #37: Loss:1.6007, Accuracy:0.2437, Validation Loss:1.6004, Validation Accuracy:0.2385\n",
    "Epoch #38: Loss:1.6009, Accuracy:0.2439, Validation Loss:1.5999, Validation Accuracy:0.2389\n",
    "Epoch #39: Loss:1.6005, Accuracy:0.2441, Validation Loss:1.6004, Validation Accuracy:0.2401\n",
    "Epoch #40: Loss:1.6000, Accuracy:0.2444, Validation Loss:1.6001, Validation Accuracy:0.2389\n",
    "Epoch #41: Loss:1.6007, Accuracy:0.2421, Validation Loss:1.5998, Validation Accuracy:0.2410\n",
    "Epoch #42: Loss:1.6001, Accuracy:0.2424, Validation Loss:1.5994, Validation Accuracy:0.2422\n",
    "Epoch #43: Loss:1.6010, Accuracy:0.2388, Validation Loss:1.5994, Validation Accuracy:0.2418\n",
    "Epoch #44: Loss:1.5999, Accuracy:0.2446, Validation Loss:1.5994, Validation Accuracy:0.2418\n",
    "Epoch #45: Loss:1.5994, Accuracy:0.2441, Validation Loss:1.5997, Validation Accuracy:0.2418\n",
    "Epoch #46: Loss:1.5997, Accuracy:0.2444, Validation Loss:1.5995, Validation Accuracy:0.2418\n",
    "Epoch #47: Loss:1.6017, Accuracy:0.2351, Validation Loss:1.6000, Validation Accuracy:0.2418\n",
    "Epoch #48: Loss:1.5998, Accuracy:0.2397, Validation Loss:1.6001, Validation Accuracy:0.2360\n",
    "Epoch #49: Loss:1.5995, Accuracy:0.2428, Validation Loss:1.5999, Validation Accuracy:0.2430\n",
    "Epoch #50: Loss:1.5990, Accuracy:0.2435, Validation Loss:1.5999, Validation Accuracy:0.2422\n",
    "Epoch #51: Loss:1.5996, Accuracy:0.2416, Validation Loss:1.5986, Validation Accuracy:0.2426\n",
    "Epoch #52: Loss:1.5991, Accuracy:0.2449, Validation Loss:1.5985, Validation Accuracy:0.2443\n",
    "Epoch #53: Loss:1.5992, Accuracy:0.2436, Validation Loss:1.5990, Validation Accuracy:0.2484\n",
    "Epoch #54: Loss:1.5996, Accuracy:0.2383, Validation Loss:1.5996, Validation Accuracy:0.2471\n",
    "Epoch #55: Loss:1.6015, Accuracy:0.2399, Validation Loss:1.5977, Validation Accuracy:0.2504\n",
    "Epoch #56: Loss:1.6025, Accuracy:0.2343, Validation Loss:1.5992, Validation Accuracy:0.2340\n",
    "Epoch #57: Loss:1.6002, Accuracy:0.2407, Validation Loss:1.5984, Validation Accuracy:0.2410\n",
    "Epoch #58: Loss:1.6008, Accuracy:0.2406, Validation Loss:1.5988, Validation Accuracy:0.2406\n",
    "Epoch #59: Loss:1.6026, Accuracy:0.2357, Validation Loss:1.6005, Validation Accuracy:0.2332\n",
    "Epoch #60: Loss:1.6016, Accuracy:0.2416, Validation Loss:1.5999, Validation Accuracy:0.2344\n",
    "Epoch #61: Loss:1.6006, Accuracy:0.2397, Validation Loss:1.5989, Validation Accuracy:0.2348\n",
    "Epoch #62: Loss:1.6005, Accuracy:0.2424, Validation Loss:1.5988, Validation Accuracy:0.2426\n",
    "Epoch #63: Loss:1.5992, Accuracy:0.2420, Validation Loss:1.5987, Validation Accuracy:0.2340\n",
    "Epoch #64: Loss:1.6003, Accuracy:0.2412, Validation Loss:1.5981, Validation Accuracy:0.2426\n",
    "Epoch #65: Loss:1.5998, Accuracy:0.2434, Validation Loss:1.5974, Validation Accuracy:0.2516\n",
    "Epoch #66: Loss:1.5992, Accuracy:0.2423, Validation Loss:1.5992, Validation Accuracy:0.2332\n",
    "Epoch #67: Loss:1.5994, Accuracy:0.2425, Validation Loss:1.5977, Validation Accuracy:0.2516\n",
    "Epoch #68: Loss:1.6004, Accuracy:0.2383, Validation Loss:1.6007, Validation Accuracy:0.2410\n",
    "Epoch #69: Loss:1.6004, Accuracy:0.2438, Validation Loss:1.5985, Validation Accuracy:0.2406\n",
    "Epoch #70: Loss:1.6011, Accuracy:0.2400, Validation Loss:1.6040, Validation Accuracy:0.2348\n",
    "Epoch #71: Loss:1.6006, Accuracy:0.2438, Validation Loss:1.5981, Validation Accuracy:0.2443\n",
    "Epoch #72: Loss:1.6002, Accuracy:0.2425, Validation Loss:1.5993, Validation Accuracy:0.2344\n",
    "Epoch #73: Loss:1.6008, Accuracy:0.2398, Validation Loss:1.5987, Validation Accuracy:0.2426\n",
    "Epoch #74: Loss:1.6005, Accuracy:0.2427, Validation Loss:1.5982, Validation Accuracy:0.2373\n",
    "Epoch #75: Loss:1.6007, Accuracy:0.2370, Validation Loss:1.5983, Validation Accuracy:0.2434\n",
    "Epoch #76: Loss:1.6000, Accuracy:0.2440, Validation Loss:1.5989, Validation Accuracy:0.2369\n",
    "Epoch #77: Loss:1.5996, Accuracy:0.2413, Validation Loss:1.5984, Validation Accuracy:0.2410\n",
    "Epoch #78: Loss:1.5996, Accuracy:0.2372, Validation Loss:1.5992, Validation Accuracy:0.2360\n",
    "Epoch #79: Loss:1.5998, Accuracy:0.2452, Validation Loss:1.5987, Validation Accuracy:0.2365\n",
    "Epoch #80: Loss:1.5991, Accuracy:0.2381, Validation Loss:1.5976, Validation Accuracy:0.2422\n",
    "Epoch #81: Loss:1.5989, Accuracy:0.2378, Validation Loss:1.5978, Validation Accuracy:0.2422\n",
    "Epoch #82: Loss:1.5990, Accuracy:0.2432, Validation Loss:1.5996, Validation Accuracy:0.2328\n",
    "Epoch #83: Loss:1.5999, Accuracy:0.2427, Validation Loss:1.5989, Validation Accuracy:0.2393\n",
    "Epoch #84: Loss:1.5997, Accuracy:0.2420, Validation Loss:1.5994, Validation Accuracy:0.2340\n",
    "Epoch #85: Loss:1.6006, Accuracy:0.2433, Validation Loss:1.5981, Validation Accuracy:0.2418\n",
    "Epoch #86: Loss:1.5986, Accuracy:0.2468, Validation Loss:1.5986, Validation Accuracy:0.2356\n",
    "Epoch #87: Loss:1.5997, Accuracy:0.2434, Validation Loss:1.5983, Validation Accuracy:0.2455\n",
    "Epoch #88: Loss:1.5991, Accuracy:0.2462, Validation Loss:1.5987, Validation Accuracy:0.2369\n",
    "Epoch #89: Loss:1.5993, Accuracy:0.2447, Validation Loss:1.5985, Validation Accuracy:0.2426\n",
    "Epoch #90: Loss:1.6019, Accuracy:0.2409, Validation Loss:1.5985, Validation Accuracy:0.2410\n",
    "Epoch #91: Loss:1.5999, Accuracy:0.2415, Validation Loss:1.5984, Validation Accuracy:0.2426\n",
    "Epoch #92: Loss:1.5998, Accuracy:0.2391, Validation Loss:1.5990, Validation Accuracy:0.2397\n",
    "Epoch #93: Loss:1.6003, Accuracy:0.2439, Validation Loss:1.5992, Validation Accuracy:0.2348\n",
    "Epoch #94: Loss:1.5991, Accuracy:0.2452, Validation Loss:1.5970, Validation Accuracy:0.2438\n",
    "Epoch #95: Loss:1.5987, Accuracy:0.2397, Validation Loss:1.5978, Validation Accuracy:0.2360\n",
    "Epoch #96: Loss:1.5991, Accuracy:0.2424, Validation Loss:1.5976, Validation Accuracy:0.2422\n",
    "Epoch #97: Loss:1.5982, Accuracy:0.2410, Validation Loss:1.5968, Validation Accuracy:0.2533\n",
    "Epoch #98: Loss:1.5980, Accuracy:0.2458, Validation Loss:1.5971, Validation Accuracy:0.2459\n",
    "Epoch #99: Loss:1.5988, Accuracy:0.2437, Validation Loss:1.5962, Validation Accuracy:0.2504\n",
    "Epoch #100: Loss:1.5990, Accuracy:0.2386, Validation Loss:1.5982, Validation Accuracy:0.2319\n",
    "Epoch #101: Loss:1.5981, Accuracy:0.2446, Validation Loss:1.5975, Validation Accuracy:0.2414\n",
    "Epoch #102: Loss:1.5983, Accuracy:0.2429, Validation Loss:1.5963, Validation Accuracy:0.2443\n",
    "Epoch #103: Loss:1.5984, Accuracy:0.2431, Validation Loss:1.5966, Validation Accuracy:0.2451\n",
    "Epoch #104: Loss:1.5989, Accuracy:0.2411, Validation Loss:1.5982, Validation Accuracy:0.2360\n",
    "Epoch #105: Loss:1.5982, Accuracy:0.2454, Validation Loss:1.5979, Validation Accuracy:0.2369\n",
    "Epoch #106: Loss:1.5975, Accuracy:0.2416, Validation Loss:1.5968, Validation Accuracy:0.2512\n",
    "Epoch #107: Loss:1.5982, Accuracy:0.2356, Validation Loss:1.5964, Validation Accuracy:0.2484\n",
    "Epoch #108: Loss:1.5977, Accuracy:0.2394, Validation Loss:1.5964, Validation Accuracy:0.2492\n",
    "Epoch #109: Loss:1.5971, Accuracy:0.2427, Validation Loss:1.5971, Validation Accuracy:0.2516\n",
    "Epoch #110: Loss:1.5974, Accuracy:0.2445, Validation Loss:1.5967, Validation Accuracy:0.2529\n",
    "Epoch #111: Loss:1.5970, Accuracy:0.2375, Validation Loss:1.5961, Validation Accuracy:0.2492\n",
    "Epoch #112: Loss:1.5967, Accuracy:0.2393, Validation Loss:1.5958, Validation Accuracy:0.2479\n",
    "Epoch #113: Loss:1.5965, Accuracy:0.2387, Validation Loss:1.5959, Validation Accuracy:0.2512\n",
    "Epoch #114: Loss:1.5964, Accuracy:0.2398, Validation Loss:1.5954, Validation Accuracy:0.2463\n",
    "Epoch #115: Loss:1.5963, Accuracy:0.2391, Validation Loss:1.5955, Validation Accuracy:0.2488\n",
    "Epoch #116: Loss:1.5975, Accuracy:0.2395, Validation Loss:1.5958, Validation Accuracy:0.2397\n",
    "Epoch #117: Loss:1.5963, Accuracy:0.2445, Validation Loss:1.5944, Validation Accuracy:0.2521\n",
    "Epoch #118: Loss:1.5958, Accuracy:0.2408, Validation Loss:1.5942, Validation Accuracy:0.2533\n",
    "Epoch #119: Loss:1.5965, Accuracy:0.2415, Validation Loss:1.5945, Validation Accuracy:0.2508\n",
    "Epoch #120: Loss:1.5971, Accuracy:0.2427, Validation Loss:1.5961, Validation Accuracy:0.2500\n",
    "Epoch #121: Loss:1.5954, Accuracy:0.2425, Validation Loss:1.5963, Validation Accuracy:0.2377\n",
    "Epoch #122: Loss:1.5990, Accuracy:0.2341, Validation Loss:1.5949, Validation Accuracy:0.2504\n",
    "Epoch #123: Loss:1.5967, Accuracy:0.2468, Validation Loss:1.5953, Validation Accuracy:0.2451\n",
    "Epoch #124: Loss:1.5965, Accuracy:0.2461, Validation Loss:1.5959, Validation Accuracy:0.2451\n",
    "Epoch #125: Loss:1.5957, Accuracy:0.2493, Validation Loss:1.5954, Validation Accuracy:0.2455\n",
    "Epoch #126: Loss:1.5956, Accuracy:0.2479, Validation Loss:1.5948, Validation Accuracy:0.2479\n",
    "Epoch #127: Loss:1.5954, Accuracy:0.2431, Validation Loss:1.5944, Validation Accuracy:0.2582\n",
    "Epoch #128: Loss:1.5954, Accuracy:0.2416, Validation Loss:1.5930, Validation Accuracy:0.2590\n",
    "Epoch #129: Loss:1.5947, Accuracy:0.2431, Validation Loss:1.5931, Validation Accuracy:0.2582\n",
    "Epoch #130: Loss:1.5944, Accuracy:0.2465, Validation Loss:1.5944, Validation Accuracy:0.2562\n",
    "Epoch #131: Loss:1.5936, Accuracy:0.2424, Validation Loss:1.5956, Validation Accuracy:0.2430\n",
    "Epoch #132: Loss:1.5971, Accuracy:0.2449, Validation Loss:1.5948, Validation Accuracy:0.2492\n",
    "Epoch #133: Loss:1.5946, Accuracy:0.2398, Validation Loss:1.5980, Validation Accuracy:0.2438\n",
    "Epoch #134: Loss:1.5953, Accuracy:0.2469, Validation Loss:1.5941, Validation Accuracy:0.2549\n",
    "Epoch #135: Loss:1.5945, Accuracy:0.2388, Validation Loss:1.5971, Validation Accuracy:0.2365\n",
    "Epoch #136: Loss:1.5993, Accuracy:0.2444, Validation Loss:1.5976, Validation Accuracy:0.2410\n",
    "Epoch #137: Loss:1.5969, Accuracy:0.2416, Validation Loss:1.5961, Validation Accuracy:0.2471\n",
    "Epoch #138: Loss:1.5965, Accuracy:0.2406, Validation Loss:1.5954, Validation Accuracy:0.2430\n",
    "Epoch #139: Loss:1.5948, Accuracy:0.2412, Validation Loss:1.5940, Validation Accuracy:0.2516\n",
    "Epoch #140: Loss:1.5955, Accuracy:0.2440, Validation Loss:1.5946, Validation Accuracy:0.2455\n",
    "Epoch #141: Loss:1.5950, Accuracy:0.2479, Validation Loss:1.5976, Validation Accuracy:0.2414\n",
    "Epoch #142: Loss:1.5958, Accuracy:0.2458, Validation Loss:1.5960, Validation Accuracy:0.2443\n",
    "Epoch #143: Loss:1.5948, Accuracy:0.2463, Validation Loss:1.5950, Validation Accuracy:0.2521\n",
    "Epoch #144: Loss:1.5938, Accuracy:0.2429, Validation Loss:1.5951, Validation Accuracy:0.2521\n",
    "Epoch #145: Loss:1.5953, Accuracy:0.2443, Validation Loss:1.5953, Validation Accuracy:0.2459\n",
    "Epoch #146: Loss:1.5945, Accuracy:0.2373, Validation Loss:1.5944, Validation Accuracy:0.2508\n",
    "Epoch #147: Loss:1.5958, Accuracy:0.2436, Validation Loss:1.5968, Validation Accuracy:0.2365\n",
    "Epoch #148: Loss:1.5948, Accuracy:0.2464, Validation Loss:1.5949, Validation Accuracy:0.2393\n",
    "Epoch #149: Loss:1.5951, Accuracy:0.2449, Validation Loss:1.5945, Validation Accuracy:0.2566\n",
    "Epoch #150: Loss:1.5946, Accuracy:0.2476, Validation Loss:1.5960, Validation Accuracy:0.2410\n",
    "Epoch #151: Loss:1.5959, Accuracy:0.2461, Validation Loss:1.5982, Validation Accuracy:0.2377\n",
    "Epoch #152: Loss:1.5962, Accuracy:0.2403, Validation Loss:1.5969, Validation Accuracy:0.2389\n",
    "Epoch #153: Loss:1.5945, Accuracy:0.2457, Validation Loss:1.5969, Validation Accuracy:0.2418\n",
    "Epoch #154: Loss:1.5944, Accuracy:0.2502, Validation Loss:1.5953, Validation Accuracy:0.2504\n",
    "Epoch #155: Loss:1.5943, Accuracy:0.2426, Validation Loss:1.5961, Validation Accuracy:0.2426\n",
    "Epoch #156: Loss:1.5939, Accuracy:0.2512, Validation Loss:1.5925, Validation Accuracy:0.2525\n",
    "Epoch #157: Loss:1.5942, Accuracy:0.2512, Validation Loss:1.5949, Validation Accuracy:0.2508\n",
    "Epoch #158: Loss:1.5973, Accuracy:0.2472, Validation Loss:1.5959, Validation Accuracy:0.2340\n",
    "Epoch #159: Loss:1.5947, Accuracy:0.2482, Validation Loss:1.5945, Validation Accuracy:0.2397\n",
    "Epoch #160: Loss:1.5937, Accuracy:0.2484, Validation Loss:1.5951, Validation Accuracy:0.2430\n",
    "Epoch #161: Loss:1.5934, Accuracy:0.2507, Validation Loss:1.5925, Validation Accuracy:0.2471\n",
    "Epoch #162: Loss:1.5921, Accuracy:0.2480, Validation Loss:1.5939, Validation Accuracy:0.2426\n",
    "Epoch #163: Loss:1.5925, Accuracy:0.2495, Validation Loss:1.5935, Validation Accuracy:0.2447\n",
    "Epoch #164: Loss:1.5930, Accuracy:0.2493, Validation Loss:1.5907, Validation Accuracy:0.2582\n",
    "Epoch #165: Loss:1.5922, Accuracy:0.2465, Validation Loss:1.5933, Validation Accuracy:0.2401\n",
    "Epoch #166: Loss:1.5916, Accuracy:0.2423, Validation Loss:1.5924, Validation Accuracy:0.2397\n",
    "Epoch #167: Loss:1.5916, Accuracy:0.2491, Validation Loss:1.5921, Validation Accuracy:0.2406\n",
    "Epoch #168: Loss:1.5908, Accuracy:0.2490, Validation Loss:1.5912, Validation Accuracy:0.2463\n",
    "Epoch #169: Loss:1.5900, Accuracy:0.2493, Validation Loss:1.5914, Validation Accuracy:0.2467\n",
    "Epoch #170: Loss:1.5899, Accuracy:0.2458, Validation Loss:1.5915, Validation Accuracy:0.2500\n",
    "Epoch #171: Loss:1.5904, Accuracy:0.2452, Validation Loss:1.5913, Validation Accuracy:0.2447\n",
    "Epoch #172: Loss:1.5898, Accuracy:0.2524, Validation Loss:1.5906, Validation Accuracy:0.2488\n",
    "Epoch #173: Loss:1.5894, Accuracy:0.2504, Validation Loss:1.5918, Validation Accuracy:0.2549\n",
    "Epoch #174: Loss:1.5909, Accuracy:0.2474, Validation Loss:1.5919, Validation Accuracy:0.2492\n",
    "Epoch #175: Loss:1.5907, Accuracy:0.2518, Validation Loss:1.5936, Validation Accuracy:0.2562\n",
    "Epoch #176: Loss:1.5902, Accuracy:0.2495, Validation Loss:1.5940, Validation Accuracy:0.2512\n",
    "Epoch #177: Loss:1.5903, Accuracy:0.2527, Validation Loss:1.5933, Validation Accuracy:0.2529\n",
    "Epoch #178: Loss:1.5900, Accuracy:0.2479, Validation Loss:1.5930, Validation Accuracy:0.2537\n",
    "Epoch #179: Loss:1.5896, Accuracy:0.2524, Validation Loss:1.5928, Validation Accuracy:0.2471\n",
    "Epoch #180: Loss:1.5891, Accuracy:0.2507, Validation Loss:1.5938, Validation Accuracy:0.2521\n",
    "Epoch #181: Loss:1.5885, Accuracy:0.2497, Validation Loss:1.5921, Validation Accuracy:0.2553\n",
    "Epoch #182: Loss:1.5888, Accuracy:0.2515, Validation Loss:1.5928, Validation Accuracy:0.2479\n",
    "Epoch #183: Loss:1.5885, Accuracy:0.2526, Validation Loss:1.5933, Validation Accuracy:0.2479\n",
    "Epoch #184: Loss:1.5901, Accuracy:0.2537, Validation Loss:1.5938, Validation Accuracy:0.2451\n",
    "Epoch #185: Loss:1.5913, Accuracy:0.2510, Validation Loss:1.5930, Validation Accuracy:0.2475\n",
    "Epoch #186: Loss:1.5899, Accuracy:0.2518, Validation Loss:1.5949, Validation Accuracy:0.2492\n",
    "Epoch #187: Loss:1.5934, Accuracy:0.2454, Validation Loss:1.5942, Validation Accuracy:0.2463\n",
    "Epoch #188: Loss:1.5911, Accuracy:0.2510, Validation Loss:1.5935, Validation Accuracy:0.2475\n",
    "Epoch #189: Loss:1.5891, Accuracy:0.2504, Validation Loss:1.5934, Validation Accuracy:0.2578\n",
    "Epoch #190: Loss:1.5887, Accuracy:0.2458, Validation Loss:1.5944, Validation Accuracy:0.2599\n",
    "Epoch #191: Loss:1.5883, Accuracy:0.2468, Validation Loss:1.5934, Validation Accuracy:0.2578\n",
    "Epoch #192: Loss:1.5909, Accuracy:0.2487, Validation Loss:1.5951, Validation Accuracy:0.2471\n",
    "Epoch #193: Loss:1.5890, Accuracy:0.2508, Validation Loss:1.5946, Validation Accuracy:0.2467\n",
    "Epoch #194: Loss:1.5875, Accuracy:0.2541, Validation Loss:1.5930, Validation Accuracy:0.2467\n",
    "Epoch #195: Loss:1.5862, Accuracy:0.2548, Validation Loss:1.5926, Validation Accuracy:0.2459\n",
    "Epoch #196: Loss:1.5856, Accuracy:0.2545, Validation Loss:1.5932, Validation Accuracy:0.2447\n",
    "Epoch #197: Loss:1.5868, Accuracy:0.2551, Validation Loss:1.5943, Validation Accuracy:0.2459\n",
    "Epoch #198: Loss:1.5853, Accuracy:0.2523, Validation Loss:1.5950, Validation Accuracy:0.2447\n",
    "Epoch #199: Loss:1.5873, Accuracy:0.2545, Validation Loss:1.5926, Validation Accuracy:0.2479\n",
    "Epoch #200: Loss:1.5866, Accuracy:0.2563, Validation Loss:1.5931, Validation Accuracy:0.2434\n",
    "Epoch #201: Loss:1.5878, Accuracy:0.2523, Validation Loss:1.5944, Validation Accuracy:0.2463\n",
    "Epoch #202: Loss:1.5869, Accuracy:0.2524, Validation Loss:1.5936, Validation Accuracy:0.2434\n",
    "Epoch #203: Loss:1.5872, Accuracy:0.2518, Validation Loss:1.5948, Validation Accuracy:0.2488\n",
    "Epoch #204: Loss:1.5929, Accuracy:0.2507, Validation Loss:1.5948, Validation Accuracy:0.2479\n",
    "Epoch #205: Loss:1.5914, Accuracy:0.2486, Validation Loss:1.5944, Validation Accuracy:0.2488\n",
    "Epoch #206: Loss:1.5891, Accuracy:0.2487, Validation Loss:1.5930, Validation Accuracy:0.2463\n",
    "Epoch #207: Loss:1.5873, Accuracy:0.2531, Validation Loss:1.5934, Validation Accuracy:0.2500\n",
    "Epoch #208: Loss:1.5876, Accuracy:0.2507, Validation Loss:1.5938, Validation Accuracy:0.2521\n",
    "Epoch #209: Loss:1.5877, Accuracy:0.2526, Validation Loss:1.5950, Validation Accuracy:0.2455\n",
    "Epoch #210: Loss:1.5872, Accuracy:0.2523, Validation Loss:1.5927, Validation Accuracy:0.2521\n",
    "Epoch #211: Loss:1.5853, Accuracy:0.2517, Validation Loss:1.5927, Validation Accuracy:0.2463\n",
    "Epoch #212: Loss:1.5853, Accuracy:0.2543, Validation Loss:1.5943, Validation Accuracy:0.2471\n",
    "Epoch #213: Loss:1.5860, Accuracy:0.2477, Validation Loss:1.5966, Validation Accuracy:0.2496\n",
    "Epoch #214: Loss:1.5843, Accuracy:0.2550, Validation Loss:1.5963, Validation Accuracy:0.2443\n",
    "Epoch #215: Loss:1.5837, Accuracy:0.2572, Validation Loss:1.5931, Validation Accuracy:0.2537\n",
    "Epoch #216: Loss:1.5846, Accuracy:0.2560, Validation Loss:1.5930, Validation Accuracy:0.2500\n",
    "Epoch #217: Loss:1.5834, Accuracy:0.2493, Validation Loss:1.5952, Validation Accuracy:0.2586\n",
    "Epoch #218: Loss:1.5834, Accuracy:0.2551, Validation Loss:1.5952, Validation Accuracy:0.2529\n",
    "Epoch #219: Loss:1.5862, Accuracy:0.2484, Validation Loss:1.5938, Validation Accuracy:0.2512\n",
    "Epoch #220: Loss:1.5844, Accuracy:0.2549, Validation Loss:1.5940, Validation Accuracy:0.2488\n",
    "Epoch #221: Loss:1.5845, Accuracy:0.2449, Validation Loss:1.5951, Validation Accuracy:0.2479\n",
    "Epoch #222: Loss:1.5830, Accuracy:0.2547, Validation Loss:1.5978, Validation Accuracy:0.2492\n",
    "Epoch #223: Loss:1.5836, Accuracy:0.2532, Validation Loss:1.5959, Validation Accuracy:0.2479\n",
    "Epoch #224: Loss:1.5836, Accuracy:0.2561, Validation Loss:1.5948, Validation Accuracy:0.2463\n",
    "Epoch #225: Loss:1.5827, Accuracy:0.2550, Validation Loss:1.5981, Validation Accuracy:0.2447\n",
    "Epoch #226: Loss:1.5827, Accuracy:0.2559, Validation Loss:1.5986, Validation Accuracy:0.2492\n",
    "Epoch #227: Loss:1.5843, Accuracy:0.2557, Validation Loss:1.5956, Validation Accuracy:0.2463\n",
    "Epoch #228: Loss:1.5881, Accuracy:0.2531, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #229: Loss:1.5888, Accuracy:0.2508, Validation Loss:1.5987, Validation Accuracy:0.2451\n",
    "Epoch #230: Loss:1.5868, Accuracy:0.2578, Validation Loss:1.5992, Validation Accuracy:0.2401\n",
    "Epoch #231: Loss:1.5867, Accuracy:0.2506, Validation Loss:1.5957, Validation Accuracy:0.2434\n",
    "Epoch #232: Loss:1.5859, Accuracy:0.2550, Validation Loss:1.5961, Validation Accuracy:0.2430\n",
    "Epoch #233: Loss:1.5854, Accuracy:0.2514, Validation Loss:1.5932, Validation Accuracy:0.2471\n",
    "Epoch #234: Loss:1.5848, Accuracy:0.2535, Validation Loss:1.5953, Validation Accuracy:0.2467\n",
    "Epoch #235: Loss:1.5833, Accuracy:0.2536, Validation Loss:1.5937, Validation Accuracy:0.2640\n",
    "Epoch #236: Loss:1.5858, Accuracy:0.2571, Validation Loss:1.5921, Validation Accuracy:0.2607\n",
    "Epoch #237: Loss:1.5837, Accuracy:0.2574, Validation Loss:1.5936, Validation Accuracy:0.2545\n",
    "Epoch #238: Loss:1.5836, Accuracy:0.2508, Validation Loss:1.5940, Validation Accuracy:0.2525\n",
    "Epoch #239: Loss:1.5808, Accuracy:0.2564, Validation Loss:1.5941, Validation Accuracy:0.2504\n",
    "Epoch #240: Loss:1.5798, Accuracy:0.2554, Validation Loss:1.5947, Validation Accuracy:0.2570\n",
    "Epoch #241: Loss:1.5843, Accuracy:0.2598, Validation Loss:1.5991, Validation Accuracy:0.2471\n",
    "Epoch #242: Loss:1.5812, Accuracy:0.2605, Validation Loss:1.5969, Validation Accuracy:0.2500\n",
    "Epoch #243: Loss:1.5821, Accuracy:0.2565, Validation Loss:1.5954, Validation Accuracy:0.2488\n",
    "Epoch #244: Loss:1.5833, Accuracy:0.2553, Validation Loss:1.5973, Validation Accuracy:0.2484\n",
    "Epoch #245: Loss:1.5829, Accuracy:0.2548, Validation Loss:1.5967, Validation Accuracy:0.2443\n",
    "Epoch #246: Loss:1.5809, Accuracy:0.2548, Validation Loss:1.5968, Validation Accuracy:0.2471\n",
    "Epoch #247: Loss:1.5815, Accuracy:0.2545, Validation Loss:1.5944, Validation Accuracy:0.2496\n",
    "Epoch #248: Loss:1.5831, Accuracy:0.2612, Validation Loss:1.5947, Validation Accuracy:0.2500\n",
    "Epoch #249: Loss:1.5817, Accuracy:0.2575, Validation Loss:1.5982, Validation Accuracy:0.2529\n",
    "Epoch #250: Loss:1.5840, Accuracy:0.2585, Validation Loss:1.5963, Validation Accuracy:0.2672\n",
    "Epoch #251: Loss:1.5822, Accuracy:0.2555, Validation Loss:1.5969, Validation Accuracy:0.2574\n",
    "Epoch #252: Loss:1.5812, Accuracy:0.2542, Validation Loss:1.5968, Validation Accuracy:0.2455\n",
    "Epoch #253: Loss:1.5814, Accuracy:0.2571, Validation Loss:1.5968, Validation Accuracy:0.2484\n",
    "Epoch #254: Loss:1.5813, Accuracy:0.2579, Validation Loss:1.5956, Validation Accuracy:0.2508\n",
    "Epoch #255: Loss:1.5810, Accuracy:0.2590, Validation Loss:1.5949, Validation Accuracy:0.2504\n",
    "Epoch #256: Loss:1.5811, Accuracy:0.2539, Validation Loss:1.5948, Validation Accuracy:0.2553\n",
    "Epoch #257: Loss:1.5807, Accuracy:0.2575, Validation Loss:1.5944, Validation Accuracy:0.2607\n",
    "Epoch #258: Loss:1.5795, Accuracy:0.2580, Validation Loss:1.5965, Validation Accuracy:0.2529\n",
    "Epoch #259: Loss:1.5796, Accuracy:0.2578, Validation Loss:1.5983, Validation Accuracy:0.2496\n",
    "Epoch #260: Loss:1.5802, Accuracy:0.2586, Validation Loss:1.5987, Validation Accuracy:0.2566\n",
    "Epoch #261: Loss:1.5814, Accuracy:0.2544, Validation Loss:1.5980, Validation Accuracy:0.2451\n",
    "Epoch #262: Loss:1.5835, Accuracy:0.2523, Validation Loss:1.5971, Validation Accuracy:0.2590\n",
    "Epoch #263: Loss:1.5865, Accuracy:0.2540, Validation Loss:1.5962, Validation Accuracy:0.2541\n",
    "Epoch #264: Loss:1.5851, Accuracy:0.2527, Validation Loss:1.5943, Validation Accuracy:0.2549\n",
    "Epoch #265: Loss:1.5841, Accuracy:0.2520, Validation Loss:1.5949, Validation Accuracy:0.2586\n",
    "Epoch #266: Loss:1.5835, Accuracy:0.2544, Validation Loss:1.5957, Validation Accuracy:0.2533\n",
    "Epoch #267: Loss:1.5831, Accuracy:0.2551, Validation Loss:1.5963, Validation Accuracy:0.2521\n",
    "Epoch #268: Loss:1.5829, Accuracy:0.2571, Validation Loss:1.5953, Validation Accuracy:0.2578\n",
    "Epoch #269: Loss:1.5827, Accuracy:0.2560, Validation Loss:1.5962, Validation Accuracy:0.2615\n",
    "Epoch #270: Loss:1.5832, Accuracy:0.2516, Validation Loss:1.5934, Validation Accuracy:0.2578\n",
    "Epoch #271: Loss:1.5841, Accuracy:0.2561, Validation Loss:1.5953, Validation Accuracy:0.2549\n",
    "Epoch #272: Loss:1.5815, Accuracy:0.2594, Validation Loss:1.5934, Validation Accuracy:0.2566\n",
    "Epoch #273: Loss:1.5821, Accuracy:0.2572, Validation Loss:1.5947, Validation Accuracy:0.2553\n",
    "Epoch #274: Loss:1.5809, Accuracy:0.2587, Validation Loss:1.5936, Validation Accuracy:0.2488\n",
    "Epoch #275: Loss:1.5818, Accuracy:0.2571, Validation Loss:1.5935, Validation Accuracy:0.2578\n",
    "Epoch #276: Loss:1.5795, Accuracy:0.2590, Validation Loss:1.5950, Validation Accuracy:0.2475\n",
    "Epoch #277: Loss:1.5793, Accuracy:0.2556, Validation Loss:1.5953, Validation Accuracy:0.2557\n",
    "Epoch #278: Loss:1.5826, Accuracy:0.2545, Validation Loss:1.5941, Validation Accuracy:0.2475\n",
    "Epoch #279: Loss:1.5803, Accuracy:0.2626, Validation Loss:1.5940, Validation Accuracy:0.2574\n",
    "Epoch #280: Loss:1.5783, Accuracy:0.2656, Validation Loss:1.5966, Validation Accuracy:0.2557\n",
    "Epoch #281: Loss:1.5791, Accuracy:0.2653, Validation Loss:1.5986, Validation Accuracy:0.2529\n",
    "Epoch #282: Loss:1.5793, Accuracy:0.2633, Validation Loss:1.5961, Validation Accuracy:0.2570\n",
    "Epoch #283: Loss:1.5791, Accuracy:0.2643, Validation Loss:1.5959, Validation Accuracy:0.2529\n",
    "Epoch #284: Loss:1.5789, Accuracy:0.2592, Validation Loss:1.5948, Validation Accuracy:0.2553\n",
    "Epoch #285: Loss:1.5781, Accuracy:0.2611, Validation Loss:1.5918, Validation Accuracy:0.2623\n",
    "Epoch #286: Loss:1.5782, Accuracy:0.2607, Validation Loss:1.5942, Validation Accuracy:0.2521\n",
    "Epoch #287: Loss:1.5772, Accuracy:0.2643, Validation Loss:1.5962, Validation Accuracy:0.2611\n",
    "Epoch #288: Loss:1.5781, Accuracy:0.2612, Validation Loss:1.5982, Validation Accuracy:0.2549\n",
    "Epoch #289: Loss:1.5774, Accuracy:0.2645, Validation Loss:1.5965, Validation Accuracy:0.2553\n",
    "Epoch #290: Loss:1.5771, Accuracy:0.2647, Validation Loss:1.5995, Validation Accuracy:0.2430\n",
    "Epoch #291: Loss:1.5782, Accuracy:0.2602, Validation Loss:1.6018, Validation Accuracy:0.2369\n",
    "Epoch #292: Loss:1.5802, Accuracy:0.2633, Validation Loss:1.6027, Validation Accuracy:0.2545\n",
    "Epoch #293: Loss:1.5785, Accuracy:0.2621, Validation Loss:1.6004, Validation Accuracy:0.2611\n",
    "Epoch #294: Loss:1.5755, Accuracy:0.2658, Validation Loss:1.5988, Validation Accuracy:0.2611\n",
    "Epoch #295: Loss:1.5751, Accuracy:0.2627, Validation Loss:1.5977, Validation Accuracy:0.2488\n",
    "Epoch #296: Loss:1.5767, Accuracy:0.2598, Validation Loss:1.5997, Validation Accuracy:0.2508\n",
    "Epoch #297: Loss:1.5765, Accuracy:0.2665, Validation Loss:1.5969, Validation Accuracy:0.2582\n",
    "Epoch #298: Loss:1.5755, Accuracy:0.2628, Validation Loss:1.5970, Validation Accuracy:0.2541\n",
    "Epoch #299: Loss:1.5743, Accuracy:0.2678, Validation Loss:1.5995, Validation Accuracy:0.2582\n",
    "Epoch #300: Loss:1.5744, Accuracy:0.2668, Validation Loss:1.5990, Validation Accuracy:0.2496\n",
    "\n",
    "Test:\n",
    "Test Loss:1.59903634, Accuracy:0.2496\n",
    "Labels: ['02', '05', '01', '04', '03']\n",
    "Confusion Matrix:\n",
    "      02   05  01   04  03\n",
    "t:02   8  242  52  140  15\n",
    "t:05   4  351  49  161   2\n",
    "t:01   5  261  69  166   2\n",
    "t:04   4  230  42  169   5\n",
    "t:03   7  270  49  122  11\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.29      0.02      0.03       457\n",
    "          05       0.26      0.62      0.37       567\n",
    "          01       0.26      0.14      0.18       503\n",
    "          04       0.22      0.38      0.28       450\n",
    "          03       0.31      0.02      0.04       459\n",
    "\n",
    "    accuracy                           0.25      2436\n",
    "   macro avg       0.27      0.23      0.18      2436\n",
    "weighted avg       0.27      0.25      0.19      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 15:50:43 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 20 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6041512735958756, 1.604610181598632, 1.6038992782727446, 1.6026536627551802, 1.6021452376799434, 1.6016148203503713, 1.602437635947918, 1.6017245805909481, 1.6015215256726996, 1.6008580723419565, 1.601385850233006, 1.6007816401999964, 1.6010184370238205, 1.6008501230985268, 1.6008544870589558, 1.6004276526189594, 1.5998455035471173, 1.5997424029755867, 1.5997133959690337, 1.5992765354209737, 1.5991542846306988, 1.598553039757489, 1.599523536677431, 1.5994474098991682, 1.5987904277341118, 1.598044583362899, 1.599810243435877, 1.5994820618277112, 1.602088222949963, 1.6022314259962886, 1.603198254636943, 1.6015529840059077, 1.6009785545674842, 1.6029040778211772, 1.6021665884747684, 1.6007761383683028, 1.600382616367246, 1.5998996433561854, 1.6003704329429589, 1.6001425789690567, 1.5997818992251442, 1.5993948168746748, 1.5993889591768262, 1.5994116678613748, 1.5996675062649355, 1.5995310817054536, 1.5999871423874779, 1.6001494621799888, 1.5999464466066784, 1.5998594598425628, 1.5986218440708855, 1.5984501586171793, 1.5990283818080508, 1.5995948426241946, 1.5976892942669747, 1.5991720295891974, 1.5984324874549076, 1.5988128157867783, 1.600496924569454, 1.5998532464743052, 1.5988686607389027, 1.598754552197574, 1.5987060432167868, 1.598144702527715, 1.5973873713920856, 1.5991810767717158, 1.5977036723949638, 1.60070597968861, 1.5985174478568467, 1.603958888594153, 1.5980949681772192, 1.5992768104440473, 1.5987059561098347, 1.5982305709951616, 1.5982808352299707, 1.5989456752250935, 1.5984493537097926, 1.5991890348237137, 1.5987090860877327, 1.5976261274372219, 1.5978014498508621, 1.5995914887129183, 1.5989367687839202, 1.599433559306541, 1.5981470548069143, 1.5985666153270428, 1.598297421568133, 1.5986591994468802, 1.5984578467354986, 1.5984561511839943, 1.5984266270166156, 1.5989978409361565, 1.5992359069767843, 1.5969813101005867, 1.5978482773738543, 1.5976415267718838, 1.5967583364649556, 1.59711710474957, 1.5961825105748544, 1.5982028012988212, 1.597534398335737, 1.596279248228214, 1.5966157208522553, 1.5982097678975322, 1.5978761439644449, 1.5967518304564878, 1.596372585774251, 1.5963608073483546, 1.5970626842407953, 1.5966957658773964, 1.5960690791188006, 1.5958298824495087, 1.595898059397104, 1.595360347398592, 1.5954901762979568, 1.595846511656036, 1.5944441431653127, 1.5941884153582193, 1.5944628073468388, 1.5960760977859372, 1.5962529507372376, 1.5948720894423611, 1.5952778641617749, 1.5958726069414362, 1.5954214076103248, 1.594772761091223, 1.59439610005991, 1.5930357623374325, 1.5931055258060325, 1.5943780867337005, 1.5956234957392776, 1.5948134333825073, 1.5980304090064539, 1.5941185820083117, 1.597101589141808, 1.597586623945064, 1.5961199241318726, 1.5953749960474977, 1.5940204768736765, 1.5946088436201875, 1.5976478064980217, 1.5960106728307915, 1.5950129783799496, 1.595059354121266, 1.5952784075525595, 1.5943989542317507, 1.5968151871598217, 1.5949385412808121, 1.594492051597495, 1.5960109038110242, 1.5982147306448524, 1.5968873203290115, 1.5969316773422442, 1.5953438835974005, 1.5961142114817803, 1.5924985119078938, 1.5949051806883663, 1.595925869808604, 1.5944914490914306, 1.595052085290793, 1.5925157117138942, 1.5938914088388578, 1.593482831820283, 1.5907372572934881, 1.593282405966021, 1.5923846560745991, 1.5921130998577, 1.5912341948213249, 1.5913596544751196, 1.5915426714666958, 1.5912917789763026, 1.5906181221916562, 1.5918226099170878, 1.5919150184527995, 1.5935711976146854, 1.5939613960450898, 1.5932666846292556, 1.593001112757841, 1.5927531437333582, 1.5937970827757235, 1.592105755860778, 1.5927662271975689, 1.5933411276007716, 1.5938381909932604, 1.592998616213869, 1.5948917716986244, 1.5942459249339864, 1.593507239384017, 1.5933603886117294, 1.5944047766953267, 1.5934168116016731, 1.5951374381633814, 1.5946337736298886, 1.5929937466416257, 1.59262410134126, 1.5932442987297948, 1.594303021485778, 1.5950154952815014, 1.5926237531091976, 1.5930590120638142, 1.5943661329194243, 1.5936101741587196, 1.5947663163512407, 1.594844205234634, 1.594425979701952, 1.592975564190907, 1.5933567555667145, 1.5937933951175858, 1.595049193731474, 1.5926999027897375, 1.5926513994855833, 1.5943259674144301, 1.5966276390407668, 1.596349434899579, 1.5931154133259564, 1.5930266151287285, 1.5952363157115743, 1.595156816817661, 1.5938202035055176, 1.5940011612495961, 1.5950730285425296, 1.597764141258152, 1.5959410786824468, 1.5947594118039987, 1.5981105493598775, 1.5985847607817751, 1.595645263668743, 1.5994401283452075, 1.598749792438814, 1.5991803758054335, 1.5957205931737113, 1.5960737218214764, 1.5932448370311842, 1.5953286377275715, 1.5937342896250082, 1.5921461157415104, 1.5936171750129737, 1.5940396660458669, 1.594126912760617, 1.5947273611435162, 1.5990777561817262, 1.5969381363716815, 1.5954263674214555, 1.5972627236925323, 1.5967172785541304, 1.5968437504102835, 1.594440903961169, 1.5946613900571425, 1.5982017773517052, 1.596329937427502, 1.5968513514216507, 1.5968374963268661, 1.5967613192418917, 1.5956152505279566, 1.5948547921548728, 1.5948318957499488, 1.5944074566532629, 1.5964737850652735, 1.5982767695863846, 1.5986729421834835, 1.5979833906311511, 1.5970809193471773, 1.5961743229128458, 1.594268677269884, 1.5948857442890285, 1.5957314126401503, 1.5963140532300977, 1.595337538296366, 1.5962083521734904, 1.5933514610299924, 1.5952590486686218, 1.593403954615538, 1.5946685077717346, 1.5935934343557248, 1.5934526583635553, 1.5950267283591535, 1.5953355661558204, 1.5940575096603293, 1.593966032092403, 1.596618296086103, 1.5986121622799652, 1.5961051564694233, 1.5959499397105577, 1.5948317491362247, 1.5917849896968097, 1.5941782917686675, 1.5962149100546374, 1.5981948506851698, 1.5964948097473295, 1.5994911058782944, 1.6018203207228963, 1.6026726086151424, 1.600437640556561, 1.5987544114562287, 1.5977283657477994, 1.5997258755569583, 1.5968509507296709, 1.5969807109222036, 1.5995335780536795, 1.599036464550225], 'val_acc': [0.23275861838964015, 0.23275861838964015, 0.23563218395698246, 0.24261083738948716, 0.24137931027142284, 0.23522167492578377, 0.23275862064071867, 0.23768472901510293, 0.24261083738948716, 0.24096880126469242, 0.2352216748279108, 0.2335796387275843, 0.24178981935155802, 0.23809523802183336, 0.2417898193270898, 0.24343185547635277, 0.233990147758783, 0.24178981930262153, 0.24137931027142284, 0.23809523802183336, 0.2458949073879981, 0.23645320201937983, 0.24137931032035934, 0.23809523799736512, 0.25041050912907165, 0.24548440070575095, 0.23850574710196853, 0.24343185550082103, 0.23275861838964015, 0.23275862064071867, 0.23891625615763548, 0.23891625608423073, 0.2344006568633864, 0.23275861838964015, 0.23891625608423073, 0.24096880126469242, 0.2385057470285638, 0.2389162560597625, 0.24014778320229505, 0.2389162560597625, 0.24096880126469242, 0.24220032833382021, 0.2417898193270898, 0.2417898193270898, 0.2417898193270898, 0.2417898193270898, 0.2417898193270898, 0.23604269298818115, 0.24302134642068585, 0.24220032833382021, 0.24261083738948716, 0.2442528735142819, 0.24835796152625372, 0.2471264368305457, 0.25041050912907165, 0.23399014773431473, 0.24096879896467738, 0.2405582899334787, 0.23316912742083884, 0.23440065678998168, 0.23481116582118036, 0.2426108374139554, 0.23399014773431473, 0.2426108374139554, 0.2516420360758583, 0.23316912967191736, 0.25164203622266773, 0.24096880128916065, 0.2405582923313667, 0.2348111657967121, 0.2442528735632184, 0.23440065676551342, 0.24261083748736015, 0.23727421998390424, 0.24343185554975752, 0.23686371095270556, 0.24096880119128766, 0.2360426928903082, 0.23645320192150687, 0.24220032833382021, 0.24220032833382021, 0.23275862064071867, 0.23932676523777063, 0.23399014773431473, 0.24178981702707475, 0.2356321838591095, 0.24548439830786292, 0.23686371095270556, 0.24261083743842365, 0.24096879901361387, 0.24261083743842365, 0.2397372719200178, 0.23481116582118036, 0.24384236223200467, 0.2360426928903082, 0.24220032830935198, 0.25328406990063795, 0.24589490973694964, 0.2504105090801352, 0.2319376025293848, 0.2413793103448276, 0.24425287348981364, 0.2450738916011475, 0.2360426928903082, 0.23686371095270556, 0.2512315270201913, 0.24835796392414175, 0.24917898198653912, 0.2516420360758583, 0.25287356321839083, 0.24917897958865112, 0.24794745484400657, 0.25123152704465956, 0.24630541871921183, 0.24876847290640394, 0.23973727412215987, 0.2520525428070419, 0.2532840699495745, 0.25082101816027036, 0.25000000024468244, 0.23768472894169818, 0.25041050912907165, 0.24507389162561577, 0.24507389162561577, 0.24548439830786292, 0.24794745494187956, 0.25821017856864115, 0.2590311966310385, 0.2582101807218467, 0.25615763321690177, 0.24302134642068585, 0.24917898198653912, 0.24384236218306818, 0.2549261061722422, 0.23645320211725282, 0.24096880146043836, 0.24712643675714094, 0.24302134651855883, 0.25164203382477973, 0.2454843984057359, 0.24137930804481256, 0.2442528735632184, 0.25205254305172436, 0.2520525429538514, 0.2458949098837591, 0.2508210159581283, 0.23645320197044334, 0.23932676523777063, 0.25656814229703695, 0.24096879906255037, 0.23768472899063467, 0.238916256010826, 0.24178981937602628, 0.25041050917800817, 0.2426108373650189, 0.2524630518871771, 0.2508210182092068, 0.2339901477098465, 0.23973727422003285, 0.24302134644515408, 0.24712643467734013, 0.24261083508947212, 0.24466338039227503, 0.25821017847076816, 0.24014778090228, 0.23973727187108132, 0.24055829228243022, 0.24630541646813328, 0.24671592554826846, 0.24999999799360392, 0.24466338039227503, 0.24876847305321342, 0.25492610622117867, 0.24917897973546058, 0.2561576334126477, 0.2512315273382785, 0.2528735612119947, 0.25369458162334363, 0.24712643692841865, 0.2520525454985489, 0.25533661769920185, 0.24794745499081605, 0.24794745499081605, 0.24507388937453722, 0.24753694356172934, 0.24917897968652408, 0.24630541881708481, 0.24753694595961734, 0.2577996696353154, 0.25985221488918187, 0.2577996696353154, 0.24712643692841865, 0.24671592789721997, 0.2467159278482835, 0.24589490978588613, 0.24466338034333854, 0.2458949098348226, 0.24466338034333854, 0.247947452690801, 0.24343185564763048, 0.24630541651706978, 0.24343185324974248, 0.2487684731021499, 0.24794745503975252, 0.2487684731021499, 0.24630541876814832, 0.24999999769998496, 0.2520525428559784, 0.24548439835679942, 0.2520525428559784, 0.24630541646813328, 0.24712643453053065, 0.24958948866878627, 0.24425287126320336, 0.25369458137866113, 0.25000000014680945, 0.2586206900466643, 0.2528735632673273, 0.25123152724040554, 0.24876847300427693, 0.24794745259292802, 0.2491789796375876, 0.24794745494187956, 0.24630541881708481, 0.24466338269229004, 0.24917898203547562, 0.24630541896389427, 0.24794745499081605, 0.24507389167455226, 0.240147781000153, 0.24343185554975752, 0.2430213442185438, 0.24712643448159416, 0.24671592540145898, 0.26395730745224727, 0.2606732350558483, 0.2545155995389315, 0.25246305193611357, 0.25041050678012017, 0.25697865137717213, 0.24712643457946715, 0.24999999779785795, 0.2487684706063889, 0.24835796392414175, 0.24425287126320336, 0.2471264368794822, 0.2495894910666743, 0.25000000014680945, 0.2528735609673123, 0.2672413795550273, 0.2573891629530683, 0.24548439835679942, 0.24835796402201474, 0.2508210182092068, 0.25041050687799316, 0.25533661784601136, 0.26067323285370625, 0.25287356346307327, 0.2495894911156108, 0.25656814239490994, 0.24507388942347372, 0.25903119653316553, 0.2541050882566543, 0.2549261063679881, 0.2586206877466493, 0.25328407254320845, 0.25205254544961236, 0.25779966968425194, 0.2614942509161036, 0.25779967208213994, 0.2549261061233057, 0.2565681446459884, 0.2553366152523774, 0.24876847305321342, 0.257799669390633, 0.24753694586174438, 0.2557471264857181, 0.24753694581280788, 0.2573891628551953, 0.2557471266325276, 0.2528735608694393, 0.25697865137717213, 0.25287356101624875, 0.2553366176502654, 0.2623152689295645, 0.2520525452538664, 0.26108374183596844, 0.2549261085701302, 0.25533661520344086, 0.2430213441206708, 0.2368637109771738, 0.25451559718998, 0.261083741787032, 0.26108374413598345, 0.24876847295534044, 0.2508210158113188, 0.25821017837289517, 0.25410509040985985, 0.25821018077078317, 0.24958948866878627], 'loss': [1.6067129628124668, 1.6047405314886105, 1.603843798382816, 1.6030114515116571, 1.6028482928657923, 1.6025321291212673, 1.6027008440949833, 1.602611270577511, 1.6021959387056637, 1.6014721047462135, 1.6026368920073617, 1.6018322715524285, 1.6013695280165154, 1.6026375428362305, 1.601308728243536, 1.6014130837129126, 1.6009578837506335, 1.601097773477527, 1.6009118450250959, 1.6005368461354312, 1.6002144459091907, 1.6002998402965631, 1.6004391601198262, 1.6008843531109225, 1.6006323986719277, 1.6012678796995348, 1.5997266377756483, 1.6020138034095999, 1.601984821207959, 1.6025854211078776, 1.6020028381132247, 1.6025588117830563, 1.6018695106251772, 1.6014319049259476, 1.6041351740854721, 1.601194748937227, 1.6006998724516412, 1.600917036430547, 1.6004534335596605, 1.6000191672381925, 1.600656319692639, 1.6001469437346567, 1.6010475288426362, 1.599913841547173, 1.5994470839627715, 1.5997132241113965, 1.6017031933982269, 1.5998438559028891, 1.5994910272974254, 1.5990289292541129, 1.5996488939810094, 1.5990839177333354, 1.5991729051425472, 1.5996178836058788, 1.6015464558983241, 1.6025234087291929, 1.6002288121217574, 1.600785414296254, 1.6025648551310356, 1.6015735524391002, 1.6005957899152377, 1.6004524033661984, 1.5992443939009242, 1.6002748351811873, 1.599839054583524, 1.5991608556535943, 1.599354547200996, 1.6003503051871393, 1.6003574930911681, 1.6010762138777934, 1.6005875343170008, 1.6002226572996292, 1.6007616446982664, 1.6004848734798862, 1.600662023526687, 1.6000084788402738, 1.5996090747492515, 1.5995519511263963, 1.5998234034563728, 1.5990866781994548, 1.5989255433699434, 1.598987174327858, 1.5998765658059404, 1.5997467452250957, 1.600599338337626, 1.598631616539534, 1.5996714617437406, 1.5991093324684753, 1.5993344622715788, 1.6018943109551493, 1.5999191789411666, 1.59975867276319, 1.6003440543611436, 1.5991344361334612, 1.5987317422576999, 1.599113391067458, 1.598156828753023, 1.5980011127323095, 1.5987578621635201, 1.598988945675092, 1.598137696910443, 1.5983081167483477, 1.5983871444784397, 1.5988717028247748, 1.598207625373433, 1.597466643830834, 1.598207171643784, 1.5977260848579955, 1.5970546814940059, 1.5973795914307267, 1.5969661575568042, 1.5967014762655176, 1.596531465704681, 1.5963778928564805, 1.5962637483706466, 1.5975181389638287, 1.596323773650418, 1.5957742043344392, 1.5965446080515273, 1.5970548775161806, 1.5954056882760363, 1.5990039576250425, 1.5967219087867033, 1.5964853838483901, 1.5957175671442334, 1.5955672845703375, 1.5953831469009054, 1.5954429845790354, 1.5946937278555648, 1.594437527901338, 1.5936309011565097, 1.5970522921188166, 1.5946452499659889, 1.595319960347436, 1.5945444784125264, 1.5993447642551555, 1.5969081960419609, 1.596478442195994, 1.5948460860908398, 1.59551685541807, 1.594950568112994, 1.5957708335755052, 1.5947899534961771, 1.5938464542678739, 1.5952962711361645, 1.5945184637144116, 1.5957853085696085, 1.5948267449098936, 1.595053991155213, 1.5945771983027213, 1.5958693048058104, 1.596241892240865, 1.5944744253550223, 1.5943867504719103, 1.5942974056796126, 1.5938606195626073, 1.594154065198722, 1.5973368623663022, 1.5946671515764397, 1.5937093873288353, 1.5934458303745278, 1.5920769173996159, 1.5925412363585016, 1.5929676772143073, 1.5922136405165435, 1.5916232263772638, 1.591627076127446, 1.5907500174990425, 1.5899824525786133, 1.589929556454966, 1.5903580310653123, 1.5898129179247595, 1.5893542048377913, 1.5908966956931707, 1.590663104918948, 1.5901936152632477, 1.5902847155408448, 1.5899935986227078, 1.5895841745380503, 1.5890754646833918, 1.5884985921808827, 1.5887823246342936, 1.5885290071949576, 1.5901212950751522, 1.5912996794898406, 1.589943135984135, 1.5933656413941901, 1.5910974839384795, 1.589127644914866, 1.588723558713768, 1.58834856604159, 1.590852693170003, 1.5890238269888155, 1.5874971372635702, 1.586198060223699, 1.5855618591915654, 1.586843775723749, 1.5852880396147773, 1.5872857110945842, 1.586620278867608, 1.5877939625197612, 1.5869126780566738, 1.5871691384109874, 1.5929313814370785, 1.5913734544229214, 1.5891162486536548, 1.587258224125026, 1.5875578451939922, 1.5876736083314649, 1.5872260337003203, 1.58531561000636, 1.5852590998095408, 1.5859631996135202, 1.5843009476299403, 1.5836648071081487, 1.5846170954635745, 1.583434021154713, 1.5834065749170354, 1.586172446186293, 1.5843594846784212, 1.5845010796121992, 1.5829562294899315, 1.583633367187923, 1.5836188790244978, 1.582717019968209, 1.5827497938575197, 1.5842513156377804, 1.5881470819273524, 1.5887821659659949, 1.5868409260587282, 1.5867182402150586, 1.5859329199644085, 1.5854225125890493, 1.5848276126556082, 1.5832852629420693, 1.5857551625132316, 1.58372167674423, 1.5836386012345613, 1.5808083928341248, 1.5798224442793358, 1.5843113440507737, 1.5811549515205243, 1.5821280336967485, 1.5833346315967474, 1.58290036067336, 1.5808704464832126, 1.5814957863496315, 1.583132548351797, 1.5817017494041083, 1.584017860571217, 1.5822192984684782, 1.581199337226899, 1.5813621517079566, 1.58129504490682, 1.5809549225429245, 1.581149598998945, 1.5806771683252323, 1.5795047889255156, 1.579617212881053, 1.5801904755206568, 1.5814433900727383, 1.583522135080498, 1.5865349778894036, 1.5850906458723473, 1.5841231841571033, 1.5834913187692787, 1.5831391074084649, 1.5829462811687398, 1.5827377522505774, 1.5831756132094523, 1.5840588277859855, 1.5815240666606833, 1.582146188855416, 1.5809338661189931, 1.5818496065218102, 1.5795408990593662, 1.5793211290723734, 1.5825531632503689, 1.5803401559285315, 1.5782661183414028, 1.5791197977026874, 1.5793126339785128, 1.5790786810234587, 1.578941176804184, 1.5780794750248872, 1.578183579346972, 1.577246557809489, 1.5780636879452934, 1.5774027994281212, 1.5770804747418945, 1.5782064308131256, 1.5801719474107088, 1.5785143869858258, 1.575534342348698, 1.5751207317904525, 1.5767181747503105, 1.5764949803969208, 1.5755442530712307, 1.574340148774995, 1.5743800066824567], 'acc': [0.22525667351741321, 0.23305954826073969, 0.23459958932238192, 0.24024640657390167, 0.24435318276377918, 0.24004106776486678, 0.2404517453798768, 0.23829568788501027, 0.2419917864476386, 0.24404517454104746, 0.241273100622136, 0.2420944558399169, 0.24281314168989782, 0.24168377824020582, 0.2420944558643952, 0.24373716632443532, 0.2446611909681522, 0.24199178645375818, 0.24425051334702258, 0.24486652978636644, 0.24342915811394275, 0.24075975360566831, 0.24250513347328567, 0.24363449691991787, 0.24425051335314216, 0.237782340862423, 0.245071868586222, 0.23470225871466024, 0.23983572895889166, 0.2356262833797956, 0.23911704312726947, 0.23644763860981569, 0.23911704312726947, 0.23983572895583186, 0.23408624231203382, 0.24373716632443532, 0.2437371663305549, 0.2439425051395898, 0.2441478439486247, 0.24435318275154005, 0.24209445585215605, 0.2424024640687682, 0.2388090349137171, 0.24455852156669453, 0.2441478439486247, 0.24435318275154005, 0.235112936348029, 0.23973305955437418, 0.2428131416960174, 0.2435318275184602, 0.24158110883262857, 0.24486652978636644, 0.24363449692297764, 0.23829568789724942, 0.23993839835728953, 0.23429158110882956, 0.24065708420115084, 0.2405544147966334, 0.23572895278431308, 0.24158110884180795, 0.2397330595482546, 0.24240246407794758, 0.2419917864476386, 0.24117043121149898, 0.24342915809864382, 0.24229979464895182, 0.24250513347634545, 0.23829568788807004, 0.24383983572895276, 0.24004106776180698, 0.24383983571671364, 0.24250513345798672, 0.23983572895583186, 0.2427104722792608, 0.23696098562628337, 0.24404517454410726, 0.24127310062825558, 0.23716632443531827, 0.24517453799379924, 0.23809034907903515, 0.237782340862423, 0.2432238192896089, 0.24271047225478248, 0.2419917864476386, 0.24332648871248508, 0.24681724847219808, 0.24342915811088295, 0.2462012320389738, 0.24466119095285327, 0.2408624229979466, 0.2414784394128122, 0.2391170431211499, 0.24394250514570937, 0.2451745379999188, 0.2397330595482546, 0.24240246406570842, 0.24096509240246405, 0.24579055442702355, 0.24373716632443532, 0.23860369609856263, 0.24455852156363472, 0.2429158110913555, 0.2431211498973306, 0.2410677618100413, 0.24537987679671458, 0.24158110881732964, 0.23562628336755648, 0.23942505134082184, 0.24271047226702164, 0.24445585215911728, 0.23747433264887063, 0.23932238193630437, 0.23870636551531924, 0.23983572894053293, 0.23911704310891074, 0.2395277207392197, 0.24445585216217708, 0.24075975359342916, 0.24147843942811112, 0.24271047226702164, 0.24250513347022587, 0.2340862422875555, 0.24681724846301872, 0.2460985626405759, 0.2492813141561387, 0.24794661190965092, 0.24312114990345016, 0.24158110883262857, 0.24312114990956973, 0.24650924025252616, 0.24240246407794758, 0.24486652977412732, 0.23983572894053293, 0.24691991787059597, 0.2388090349137171, 0.2443531827576596, 0.24158110881732964, 0.24055441479051382, 0.24117043121149898, 0.24404517455022684, 0.2479466119157705, 0.2457905544147844, 0.24630390141289338, 0.24291581109441526, 0.24425051335314216, 0.23726899385207487, 0.24363449690767872, 0.2464065708418891, 0.24486652978636644, 0.24763860370221813, 0.24609856262833676, 0.2403490759875985, 0.2456878849980278, 0.2502053388151545, 0.24260780287474332, 0.25123203285726925, 0.2512320328542094, 0.24722792608414831, 0.2481519507064467, 0.2483572895399599, 0.250718685834682, 0.24804928131416837, 0.24948665297741274, 0.2492813141561387, 0.24650924025252616, 0.24229979464895182, 0.2490759753654625, 0.2489733059670646, 0.2492813141744974, 0.2457905544147844, 0.2451745379999188, 0.25236139630696125, 0.2504106776241894, 0.2474332648993028, 0.25184804928437393, 0.24948665298353231, 0.2526694045174538, 0.24794661192189008, 0.252361396310021, 0.25071868583774176, 0.2496919917742085, 0.2515400410677618, 0.2525667351190559, 0.2536960985626283, 0.2510266940574137, 0.25184804928131416, 0.24537987678447543, 0.25102669405129413, 0.2504106776241894, 0.24579055442702355, 0.24681724847219808, 0.2486652977657514, 0.25082135523613963, 0.25410677618069816, 0.25482546202455947, 0.2545174538018278, 0.2551334702258727, 0.25225872690550355, 0.2545174538110071, 0.25626283368168423, 0.25225872688714485, 0.2523613963039014, 0.25184804928743376, 0.25071868583162216, 0.24856262834899479, 0.24866529772903395, 0.25308008213552363, 0.25071868583774176, 0.25256673511599614, 0.252258726899384, 0.25174537988903584, 0.25431211499585266, 0.24774127310061603, 0.25503080082441504, 0.2571868583162218, 0.2559548254681319, 0.2492813141744974, 0.2551334702258727, 0.24835728953384031, 0.2549281314168378, 0.24486652978636644, 0.25472279262004205, 0.25318275154004105, 0.2560574948665298, 0.25503080083359436, 0.25585215605749484, 0.25574948665909697, 0.25308008213552363, 0.25082135523613963, 0.2578028747494461, 0.25061601642710474, 0.2550308008091161, 0.2514373716510052, 0.2534907597566532, 0.25359342914587174, 0.2570841889147641, 0.25739219713749584, 0.2508213552422592, 0.2563655030862017, 0.25544147843942505, 0.2597535934352777, 0.26047227926689986, 0.2564681724723604, 0.2553388090349076, 0.25482546200008116, 0.2548254620123203, 0.2545174538110071, 0.2611909650801633, 0.25749486653283393, 0.2585215605749487, 0.2555441478439425, 0.2542094455913352, 0.2570841889117043, 0.2579055441600831, 0.2590349075852968, 0.25390143737472304, 0.25749486654201326, 0.2580082135646005, 0.25780287474332647, 0.25862422999170526, 0.2544147844003701, 0.2522587268749057, 0.25400410678841984, 0.2526694045235734, 0.2519507186980708, 0.2544147844003701, 0.2551334702258727, 0.25708418891782386, 0.25595482547425147, 0.25164271047227926, 0.25605749486958956, 0.25944558522172534, 0.2571868583284609, 0.2587268993839836, 0.25708418891782386, 0.2590349076097751, 0.2556468172362208, 0.25451745380488755, 0.26262833675564684, 0.265605749486653, 0.2652977412853398, 0.26334702259338855, 0.2642710472309859, 0.2592402464096306, 0.2610882956940046, 0.26067761805757605, 0.2642710472401652, 0.2611909651046416, 0.264476386036961, 0.26468172485823505, 0.2601642710594671, 0.26334702259338855, 0.2621149897330595, 0.26581108828344874, 0.26273100616628386, 0.2597535934291581, 0.2665297741395492, 0.2628336755677415, 0.2677618069937587, 0.2668377823408624]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
