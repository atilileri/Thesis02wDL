{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf56.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 01:44:35 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '3Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '03', '01', '04', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002190352BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002197E476EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6087, Accuracy:0.2066, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6066, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6006, Accuracy:0.2329, Validation Loss:1.5978, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.5980, Accuracy:0.2452, Validation Loss:1.5931, Validation Accuracy:0.2578\n",
    "Epoch #27: Loss:1.5935, Accuracy:0.2386, Validation Loss:1.5860, Validation Accuracy:0.2644\n",
    "Epoch #28: Loss:1.5853, Accuracy:0.2793, Validation Loss:1.5742, Validation Accuracy:0.3169\n",
    "Epoch #29: Loss:1.5741, Accuracy:0.2891, Validation Loss:1.5623, Validation Accuracy:0.3153\n",
    "Epoch #30: Loss:1.5630, Accuracy:0.2924, Validation Loss:1.5559, Validation Accuracy:0.3120\n",
    "Epoch #31: Loss:1.5593, Accuracy:0.2903, Validation Loss:1.5541, Validation Accuracy:0.3038\n",
    "Epoch #32: Loss:1.5567, Accuracy:0.2924, Validation Loss:1.5553, Validation Accuracy:0.3136\n",
    "Epoch #33: Loss:1.5553, Accuracy:0.2936, Validation Loss:1.5513, Validation Accuracy:0.3087\n",
    "Epoch #34: Loss:1.5533, Accuracy:0.2973, Validation Loss:1.5514, Validation Accuracy:0.3103\n",
    "Epoch #35: Loss:1.5515, Accuracy:0.2986, Validation Loss:1.5500, Validation Accuracy:0.3087\n",
    "Epoch #36: Loss:1.5505, Accuracy:0.2969, Validation Loss:1.5491, Validation Accuracy:0.3038\n",
    "Epoch #37: Loss:1.5470, Accuracy:0.2973, Validation Loss:1.5509, Validation Accuracy:0.3054\n",
    "Epoch #38: Loss:1.5487, Accuracy:0.2961, Validation Loss:1.5473, Validation Accuracy:0.3054\n",
    "Epoch #39: Loss:1.5442, Accuracy:0.2982, Validation Loss:1.5488, Validation Accuracy:0.3087\n",
    "Epoch #40: Loss:1.5451, Accuracy:0.2969, Validation Loss:1.5460, Validation Accuracy:0.3054\n",
    "Epoch #41: Loss:1.5420, Accuracy:0.3006, Validation Loss:1.5490, Validation Accuracy:0.3087\n",
    "Epoch #42: Loss:1.5420, Accuracy:0.2998, Validation Loss:1.5453, Validation Accuracy:0.3054\n",
    "Epoch #43: Loss:1.5413, Accuracy:0.3014, Validation Loss:1.5453, Validation Accuracy:0.3054\n",
    "Epoch #44: Loss:1.5395, Accuracy:0.3010, Validation Loss:1.5441, Validation Accuracy:0.3087\n",
    "Epoch #45: Loss:1.5384, Accuracy:0.2965, Validation Loss:1.5444, Validation Accuracy:0.3120\n",
    "Epoch #46: Loss:1.5373, Accuracy:0.2994, Validation Loss:1.5433, Validation Accuracy:0.3021\n",
    "Epoch #47: Loss:1.5369, Accuracy:0.3010, Validation Loss:1.5435, Validation Accuracy:0.3136\n",
    "Epoch #48: Loss:1.5357, Accuracy:0.3006, Validation Loss:1.5415, Validation Accuracy:0.3038\n",
    "Epoch #49: Loss:1.5350, Accuracy:0.3006, Validation Loss:1.5407, Validation Accuracy:0.3005\n",
    "Epoch #50: Loss:1.5329, Accuracy:0.3051, Validation Loss:1.5427, Validation Accuracy:0.3136\n",
    "Epoch #51: Loss:1.5318, Accuracy:0.2977, Validation Loss:1.5407, Validation Accuracy:0.3120\n",
    "Epoch #52: Loss:1.5305, Accuracy:0.3060, Validation Loss:1.5438, Validation Accuracy:0.3120\n",
    "Epoch #53: Loss:1.5310, Accuracy:0.3055, Validation Loss:1.5399, Validation Accuracy:0.3153\n",
    "Epoch #54: Loss:1.5295, Accuracy:0.3047, Validation Loss:1.5374, Validation Accuracy:0.3169\n",
    "Epoch #55: Loss:1.5274, Accuracy:0.3117, Validation Loss:1.5480, Validation Accuracy:0.2923\n",
    "Epoch #56: Loss:1.5338, Accuracy:0.3043, Validation Loss:1.5498, Validation Accuracy:0.3153\n",
    "Epoch #57: Loss:1.5294, Accuracy:0.3101, Validation Loss:1.5370, Validation Accuracy:0.3038\n",
    "Epoch #58: Loss:1.5217, Accuracy:0.3142, Validation Loss:1.5371, Validation Accuracy:0.3169\n",
    "Epoch #59: Loss:1.5219, Accuracy:0.3125, Validation Loss:1.5338, Validation Accuracy:0.3103\n",
    "Epoch #60: Loss:1.5202, Accuracy:0.3117, Validation Loss:1.5343, Validation Accuracy:0.3153\n",
    "Epoch #61: Loss:1.5184, Accuracy:0.3125, Validation Loss:1.5305, Validation Accuracy:0.3120\n",
    "Epoch #62: Loss:1.5151, Accuracy:0.3129, Validation Loss:1.5285, Validation Accuracy:0.3202\n",
    "Epoch #63: Loss:1.5137, Accuracy:0.3195, Validation Loss:1.5268, Validation Accuracy:0.3169\n",
    "Epoch #64: Loss:1.5100, Accuracy:0.3261, Validation Loss:1.5339, Validation Accuracy:0.3136\n",
    "Epoch #65: Loss:1.5093, Accuracy:0.3199, Validation Loss:1.5284, Validation Accuracy:0.3202\n",
    "Epoch #66: Loss:1.5084, Accuracy:0.3248, Validation Loss:1.5215, Validation Accuracy:0.3300\n",
    "Epoch #67: Loss:1.5011, Accuracy:0.3273, Validation Loss:1.5264, Validation Accuracy:0.3153\n",
    "Epoch #68: Loss:1.4964, Accuracy:0.3372, Validation Loss:1.5169, Validation Accuracy:0.3300\n",
    "Epoch #69: Loss:1.4950, Accuracy:0.3466, Validation Loss:1.5133, Validation Accuracy:0.3350\n",
    "Epoch #70: Loss:1.4883, Accuracy:0.3462, Validation Loss:1.5124, Validation Accuracy:0.3383\n",
    "Epoch #71: Loss:1.4857, Accuracy:0.3495, Validation Loss:1.5096, Validation Accuracy:0.3333\n",
    "Epoch #72: Loss:1.4808, Accuracy:0.3548, Validation Loss:1.5072, Validation Accuracy:0.3333\n",
    "Epoch #73: Loss:1.4742, Accuracy:0.3536, Validation Loss:1.5079, Validation Accuracy:0.3383\n",
    "Epoch #74: Loss:1.4960, Accuracy:0.3388, Validation Loss:1.5240, Validation Accuracy:0.3235\n",
    "Epoch #75: Loss:1.4888, Accuracy:0.3495, Validation Loss:1.5590, Validation Accuracy:0.2923\n",
    "Epoch #76: Loss:1.5030, Accuracy:0.3384, Validation Loss:1.5336, Validation Accuracy:0.2972\n",
    "Epoch #77: Loss:1.4831, Accuracy:0.3413, Validation Loss:1.4977, Validation Accuracy:0.3350\n",
    "Epoch #78: Loss:1.4774, Accuracy:0.3454, Validation Loss:1.4978, Validation Accuracy:0.3399\n",
    "Epoch #79: Loss:1.4749, Accuracy:0.3507, Validation Loss:1.5091, Validation Accuracy:0.3235\n",
    "Epoch #80: Loss:1.4831, Accuracy:0.3421, Validation Loss:1.4967, Validation Accuracy:0.3350\n",
    "Epoch #81: Loss:1.4680, Accuracy:0.3569, Validation Loss:1.4967, Validation Accuracy:0.3399\n",
    "Epoch #82: Loss:1.4666, Accuracy:0.3544, Validation Loss:1.5128, Validation Accuracy:0.3235\n",
    "Epoch #83: Loss:1.4660, Accuracy:0.3536, Validation Loss:1.4996, Validation Accuracy:0.3317\n",
    "Epoch #84: Loss:1.4688, Accuracy:0.3536, Validation Loss:1.4999, Validation Accuracy:0.3399\n",
    "Epoch #85: Loss:1.4647, Accuracy:0.3577, Validation Loss:1.5073, Validation Accuracy:0.3366\n",
    "Epoch #86: Loss:1.4622, Accuracy:0.3573, Validation Loss:1.4978, Validation Accuracy:0.3415\n",
    "Epoch #87: Loss:1.4642, Accuracy:0.3528, Validation Loss:1.4972, Validation Accuracy:0.3448\n",
    "Epoch #88: Loss:1.4678, Accuracy:0.3598, Validation Loss:1.5031, Validation Accuracy:0.3366\n",
    "Epoch #89: Loss:1.4657, Accuracy:0.3540, Validation Loss:1.4985, Validation Accuracy:0.3350\n",
    "Epoch #90: Loss:1.4597, Accuracy:0.3630, Validation Loss:1.5009, Validation Accuracy:0.3383\n",
    "Epoch #91: Loss:1.4600, Accuracy:0.3643, Validation Loss:1.4983, Validation Accuracy:0.3383\n",
    "Epoch #92: Loss:1.4639, Accuracy:0.3561, Validation Loss:1.5004, Validation Accuracy:0.3350\n",
    "Epoch #93: Loss:1.4609, Accuracy:0.3556, Validation Loss:1.4960, Validation Accuracy:0.3366\n",
    "Epoch #94: Loss:1.4603, Accuracy:0.3573, Validation Loss:1.4943, Validation Accuracy:0.3465\n",
    "Epoch #95: Loss:1.4591, Accuracy:0.3606, Validation Loss:1.4978, Validation Accuracy:0.3366\n",
    "Epoch #96: Loss:1.4598, Accuracy:0.3630, Validation Loss:1.4943, Validation Accuracy:0.3432\n",
    "Epoch #97: Loss:1.4574, Accuracy:0.3610, Validation Loss:1.5159, Validation Accuracy:0.3317\n",
    "Epoch #98: Loss:1.4626, Accuracy:0.3602, Validation Loss:1.5077, Validation Accuracy:0.3317\n",
    "Epoch #99: Loss:1.4615, Accuracy:0.3589, Validation Loss:1.4934, Validation Accuracy:0.3383\n",
    "Epoch #100: Loss:1.4595, Accuracy:0.3589, Validation Loss:1.5039, Validation Accuracy:0.3333\n",
    "Epoch #101: Loss:1.4597, Accuracy:0.3556, Validation Loss:1.5020, Validation Accuracy:0.3350\n",
    "Epoch #102: Loss:1.4554, Accuracy:0.3593, Validation Loss:1.4927, Validation Accuracy:0.3366\n",
    "Epoch #103: Loss:1.4555, Accuracy:0.3593, Validation Loss:1.4939, Validation Accuracy:0.3399\n",
    "Epoch #104: Loss:1.4566, Accuracy:0.3589, Validation Loss:1.4924, Validation Accuracy:0.3415\n",
    "Epoch #105: Loss:1.4533, Accuracy:0.3639, Validation Loss:1.5196, Validation Accuracy:0.3284\n",
    "Epoch #106: Loss:1.4665, Accuracy:0.3474, Validation Loss:1.5329, Validation Accuracy:0.3186\n",
    "Epoch #107: Loss:1.4757, Accuracy:0.3446, Validation Loss:1.5136, Validation Accuracy:0.3300\n",
    "Epoch #108: Loss:1.4679, Accuracy:0.3585, Validation Loss:1.4985, Validation Accuracy:0.3366\n",
    "Epoch #109: Loss:1.4687, Accuracy:0.3540, Validation Loss:1.4900, Validation Accuracy:0.3498\n",
    "Epoch #110: Loss:1.4576, Accuracy:0.3618, Validation Loss:1.4924, Validation Accuracy:0.3366\n",
    "Epoch #111: Loss:1.4598, Accuracy:0.3573, Validation Loss:1.5086, Validation Accuracy:0.3366\n",
    "Epoch #112: Loss:1.4630, Accuracy:0.3647, Validation Loss:1.5050, Validation Accuracy:0.3366\n",
    "Epoch #113: Loss:1.4608, Accuracy:0.3680, Validation Loss:1.5102, Validation Accuracy:0.3284\n",
    "Epoch #114: Loss:1.4701, Accuracy:0.3524, Validation Loss:1.4892, Validation Accuracy:0.3432\n",
    "Epoch #115: Loss:1.4600, Accuracy:0.3503, Validation Loss:1.4976, Validation Accuracy:0.3415\n",
    "Epoch #116: Loss:1.4517, Accuracy:0.3651, Validation Loss:1.4890, Validation Accuracy:0.3366\n",
    "Epoch #117: Loss:1.4515, Accuracy:0.3630, Validation Loss:1.4956, Validation Accuracy:0.3317\n",
    "Epoch #118: Loss:1.4516, Accuracy:0.3667, Validation Loss:1.4960, Validation Accuracy:0.3350\n",
    "Epoch #119: Loss:1.4511, Accuracy:0.3643, Validation Loss:1.4932, Validation Accuracy:0.3399\n",
    "Epoch #120: Loss:1.4495, Accuracy:0.3647, Validation Loss:1.5017, Validation Accuracy:0.3350\n",
    "Epoch #121: Loss:1.4509, Accuracy:0.3630, Validation Loss:1.4942, Validation Accuracy:0.3366\n",
    "Epoch #122: Loss:1.4495, Accuracy:0.3630, Validation Loss:1.4917, Validation Accuracy:0.3383\n",
    "Epoch #123: Loss:1.4498, Accuracy:0.3630, Validation Loss:1.4929, Validation Accuracy:0.3514\n",
    "Epoch #124: Loss:1.4526, Accuracy:0.3680, Validation Loss:1.4921, Validation Accuracy:0.3399\n",
    "Epoch #125: Loss:1.4557, Accuracy:0.3577, Validation Loss:1.4910, Validation Accuracy:0.3415\n",
    "Epoch #126: Loss:1.4538, Accuracy:0.3700, Validation Loss:1.4936, Validation Accuracy:0.3383\n",
    "Epoch #127: Loss:1.4485, Accuracy:0.3655, Validation Loss:1.5012, Validation Accuracy:0.3350\n",
    "Epoch #128: Loss:1.4495, Accuracy:0.3639, Validation Loss:1.4898, Validation Accuracy:0.3383\n",
    "Epoch #129: Loss:1.4461, Accuracy:0.3618, Validation Loss:1.5045, Validation Accuracy:0.3350\n",
    "Epoch #130: Loss:1.4502, Accuracy:0.3659, Validation Loss:1.5078, Validation Accuracy:0.3350\n",
    "Epoch #131: Loss:1.4507, Accuracy:0.3647, Validation Loss:1.4910, Validation Accuracy:0.3366\n",
    "Epoch #132: Loss:1.4613, Accuracy:0.3581, Validation Loss:1.4963, Validation Accuracy:0.3333\n",
    "Epoch #133: Loss:1.4570, Accuracy:0.3634, Validation Loss:1.4917, Validation Accuracy:0.3415\n",
    "Epoch #134: Loss:1.4528, Accuracy:0.3630, Validation Loss:1.4892, Validation Accuracy:0.3317\n",
    "Epoch #135: Loss:1.4522, Accuracy:0.3622, Validation Loss:1.4885, Validation Accuracy:0.3350\n",
    "Epoch #136: Loss:1.4496, Accuracy:0.3618, Validation Loss:1.5108, Validation Accuracy:0.3366\n",
    "Epoch #137: Loss:1.4502, Accuracy:0.3667, Validation Loss:1.4914, Validation Accuracy:0.3366\n",
    "Epoch #138: Loss:1.4467, Accuracy:0.3667, Validation Loss:1.4934, Validation Accuracy:0.3448\n",
    "Epoch #139: Loss:1.4494, Accuracy:0.3639, Validation Loss:1.4908, Validation Accuracy:0.3333\n",
    "Epoch #140: Loss:1.4467, Accuracy:0.3630, Validation Loss:1.5048, Validation Accuracy:0.3383\n",
    "Epoch #141: Loss:1.4521, Accuracy:0.3585, Validation Loss:1.5189, Validation Accuracy:0.3300\n",
    "Epoch #142: Loss:1.4533, Accuracy:0.3593, Validation Loss:1.4931, Validation Accuracy:0.3300\n",
    "Epoch #143: Loss:1.4460, Accuracy:0.3618, Validation Loss:1.4899, Validation Accuracy:0.3399\n",
    "Epoch #144: Loss:1.4459, Accuracy:0.3602, Validation Loss:1.4904, Validation Accuracy:0.3383\n",
    "Epoch #145: Loss:1.4535, Accuracy:0.3540, Validation Loss:1.4912, Validation Accuracy:0.3366\n",
    "Epoch #146: Loss:1.4470, Accuracy:0.3692, Validation Loss:1.5053, Validation Accuracy:0.3383\n",
    "Epoch #147: Loss:1.4482, Accuracy:0.3622, Validation Loss:1.5061, Validation Accuracy:0.3366\n",
    "Epoch #148: Loss:1.4518, Accuracy:0.3573, Validation Loss:1.4907, Validation Accuracy:0.3350\n",
    "Epoch #149: Loss:1.4589, Accuracy:0.3626, Validation Loss:1.5078, Validation Accuracy:0.3317\n",
    "Epoch #150: Loss:1.4630, Accuracy:0.3520, Validation Loss:1.4886, Validation Accuracy:0.3415\n",
    "Epoch #151: Loss:1.4594, Accuracy:0.3561, Validation Loss:1.5413, Validation Accuracy:0.3120\n",
    "Epoch #152: Loss:1.4649, Accuracy:0.3565, Validation Loss:1.4887, Validation Accuracy:0.3415\n",
    "Epoch #153: Loss:1.4595, Accuracy:0.3577, Validation Loss:1.4887, Validation Accuracy:0.3498\n",
    "Epoch #154: Loss:1.4497, Accuracy:0.3634, Validation Loss:1.4967, Validation Accuracy:0.3366\n",
    "Epoch #155: Loss:1.4475, Accuracy:0.3606, Validation Loss:1.4889, Validation Accuracy:0.3350\n",
    "Epoch #156: Loss:1.4450, Accuracy:0.3708, Validation Loss:1.4886, Validation Accuracy:0.3399\n",
    "Epoch #157: Loss:1.4451, Accuracy:0.3659, Validation Loss:1.4897, Validation Accuracy:0.3383\n",
    "Epoch #158: Loss:1.4446, Accuracy:0.3610, Validation Loss:1.4900, Validation Accuracy:0.3432\n",
    "Epoch #159: Loss:1.4453, Accuracy:0.3622, Validation Loss:1.4956, Validation Accuracy:0.3399\n",
    "Epoch #160: Loss:1.4432, Accuracy:0.3626, Validation Loss:1.4979, Validation Accuracy:0.3350\n",
    "Epoch #161: Loss:1.4418, Accuracy:0.3659, Validation Loss:1.4911, Validation Accuracy:0.3350\n",
    "Epoch #162: Loss:1.4427, Accuracy:0.3708, Validation Loss:1.5070, Validation Accuracy:0.3317\n",
    "Epoch #163: Loss:1.4489, Accuracy:0.3606, Validation Loss:1.5152, Validation Accuracy:0.3317\n",
    "Epoch #164: Loss:1.4527, Accuracy:0.3610, Validation Loss:1.4914, Validation Accuracy:0.3333\n",
    "Epoch #165: Loss:1.4509, Accuracy:0.3548, Validation Loss:1.4886, Validation Accuracy:0.3415\n",
    "Epoch #166: Loss:1.4432, Accuracy:0.3667, Validation Loss:1.4878, Validation Accuracy:0.3415\n",
    "Epoch #167: Loss:1.4410, Accuracy:0.3655, Validation Loss:1.4944, Validation Accuracy:0.3284\n",
    "Epoch #168: Loss:1.4410, Accuracy:0.3647, Validation Loss:1.4914, Validation Accuracy:0.3333\n",
    "Epoch #169: Loss:1.4395, Accuracy:0.3663, Validation Loss:1.4899, Validation Accuracy:0.3415\n",
    "Epoch #170: Loss:1.4435, Accuracy:0.3639, Validation Loss:1.4904, Validation Accuracy:0.3383\n",
    "Epoch #171: Loss:1.4405, Accuracy:0.3643, Validation Loss:1.4924, Validation Accuracy:0.3333\n",
    "Epoch #172: Loss:1.4411, Accuracy:0.3634, Validation Loss:1.4909, Validation Accuracy:0.3383\n",
    "Epoch #173: Loss:1.4403, Accuracy:0.3647, Validation Loss:1.4910, Validation Accuracy:0.3366\n",
    "Epoch #174: Loss:1.4400, Accuracy:0.3659, Validation Loss:1.4911, Validation Accuracy:0.3415\n",
    "Epoch #175: Loss:1.4405, Accuracy:0.3647, Validation Loss:1.4963, Validation Accuracy:0.3366\n",
    "Epoch #176: Loss:1.4416, Accuracy:0.3639, Validation Loss:1.4945, Validation Accuracy:0.3317\n",
    "Epoch #177: Loss:1.4503, Accuracy:0.3655, Validation Loss:1.5071, Validation Accuracy:0.3284\n",
    "Epoch #178: Loss:1.4652, Accuracy:0.3462, Validation Loss:1.4949, Validation Accuracy:0.3383\n",
    "Epoch #179: Loss:1.4616, Accuracy:0.3540, Validation Loss:1.5000, Validation Accuracy:0.3300\n",
    "Epoch #180: Loss:1.4421, Accuracy:0.3655, Validation Loss:1.4854, Validation Accuracy:0.3333\n",
    "Epoch #181: Loss:1.4391, Accuracy:0.3688, Validation Loss:1.4876, Validation Accuracy:0.3350\n",
    "Epoch #182: Loss:1.4404, Accuracy:0.3676, Validation Loss:1.4984, Validation Accuracy:0.3350\n",
    "Epoch #183: Loss:1.4410, Accuracy:0.3688, Validation Loss:1.5098, Validation Accuracy:0.3399\n",
    "Epoch #184: Loss:1.4507, Accuracy:0.3618, Validation Loss:1.4962, Validation Accuracy:0.3383\n",
    "Epoch #185: Loss:1.4476, Accuracy:0.3700, Validation Loss:1.5138, Validation Accuracy:0.3251\n",
    "Epoch #186: Loss:1.4637, Accuracy:0.3503, Validation Loss:1.4879, Validation Accuracy:0.3350\n",
    "Epoch #187: Loss:1.4436, Accuracy:0.3667, Validation Loss:1.5015, Validation Accuracy:0.3300\n",
    "Epoch #188: Loss:1.4453, Accuracy:0.3630, Validation Loss:1.4960, Validation Accuracy:0.3300\n",
    "Epoch #189: Loss:1.4461, Accuracy:0.3626, Validation Loss:1.4888, Validation Accuracy:0.3498\n",
    "Epoch #190: Loss:1.4380, Accuracy:0.3663, Validation Loss:1.4944, Validation Accuracy:0.3300\n",
    "Epoch #191: Loss:1.4367, Accuracy:0.3774, Validation Loss:1.4890, Validation Accuracy:0.3399\n",
    "Epoch #192: Loss:1.4379, Accuracy:0.3692, Validation Loss:1.4896, Validation Accuracy:0.3465\n",
    "Epoch #193: Loss:1.4359, Accuracy:0.3655, Validation Loss:1.4980, Validation Accuracy:0.3333\n",
    "Epoch #194: Loss:1.4394, Accuracy:0.3676, Validation Loss:1.4920, Validation Accuracy:0.3399\n",
    "Epoch #195: Loss:1.4377, Accuracy:0.3692, Validation Loss:1.4935, Validation Accuracy:0.3350\n",
    "Epoch #196: Loss:1.4356, Accuracy:0.3725, Validation Loss:1.5052, Validation Accuracy:0.3399\n",
    "Epoch #197: Loss:1.4401, Accuracy:0.3704, Validation Loss:1.4915, Validation Accuracy:0.3481\n",
    "Epoch #198: Loss:1.4434, Accuracy:0.3696, Validation Loss:1.4957, Validation Accuracy:0.3415\n",
    "Epoch #199: Loss:1.4478, Accuracy:0.3606, Validation Loss:1.4882, Validation Accuracy:0.3333\n",
    "Epoch #200: Loss:1.4408, Accuracy:0.3655, Validation Loss:1.4929, Validation Accuracy:0.3284\n",
    "Epoch #201: Loss:1.4348, Accuracy:0.3684, Validation Loss:1.4885, Validation Accuracy:0.3350\n",
    "Epoch #202: Loss:1.4341, Accuracy:0.3671, Validation Loss:1.4911, Validation Accuracy:0.3350\n",
    "Epoch #203: Loss:1.4335, Accuracy:0.3676, Validation Loss:1.4902, Validation Accuracy:0.3366\n",
    "Epoch #204: Loss:1.4343, Accuracy:0.3671, Validation Loss:1.4946, Validation Accuracy:0.3333\n",
    "Epoch #205: Loss:1.4361, Accuracy:0.3647, Validation Loss:1.4934, Validation Accuracy:0.3432\n",
    "Epoch #206: Loss:1.4358, Accuracy:0.3692, Validation Loss:1.4907, Validation Accuracy:0.3415\n",
    "Epoch #207: Loss:1.4334, Accuracy:0.3692, Validation Loss:1.5048, Validation Accuracy:0.3317\n",
    "Epoch #208: Loss:1.4399, Accuracy:0.3704, Validation Loss:1.5039, Validation Accuracy:0.3333\n",
    "Epoch #209: Loss:1.4450, Accuracy:0.3647, Validation Loss:1.4926, Validation Accuracy:0.3498\n",
    "Epoch #210: Loss:1.4413, Accuracy:0.3622, Validation Loss:1.4920, Validation Accuracy:0.3432\n",
    "Epoch #211: Loss:1.4404, Accuracy:0.3622, Validation Loss:1.5035, Validation Accuracy:0.3284\n",
    "Epoch #212: Loss:1.4388, Accuracy:0.3704, Validation Loss:1.4921, Validation Accuracy:0.3350\n",
    "Epoch #213: Loss:1.4324, Accuracy:0.3713, Validation Loss:1.4882, Validation Accuracy:0.3415\n",
    "Epoch #214: Loss:1.4327, Accuracy:0.3671, Validation Loss:1.4906, Validation Accuracy:0.3465\n",
    "Epoch #215: Loss:1.4396, Accuracy:0.3655, Validation Loss:1.4903, Validation Accuracy:0.3350\n",
    "Epoch #216: Loss:1.4333, Accuracy:0.3676, Validation Loss:1.4998, Validation Accuracy:0.3317\n",
    "Epoch #217: Loss:1.4361, Accuracy:0.3692, Validation Loss:1.5058, Validation Accuracy:0.3268\n",
    "Epoch #218: Loss:1.4365, Accuracy:0.3708, Validation Loss:1.4942, Validation Accuracy:0.3317\n",
    "Epoch #219: Loss:1.4337, Accuracy:0.3692, Validation Loss:1.5045, Validation Accuracy:0.3300\n",
    "Epoch #220: Loss:1.4363, Accuracy:0.3684, Validation Loss:1.4974, Validation Accuracy:0.3268\n",
    "Epoch #221: Loss:1.4305, Accuracy:0.3671, Validation Loss:1.4910, Validation Accuracy:0.3333\n",
    "Epoch #222: Loss:1.4318, Accuracy:0.3647, Validation Loss:1.4905, Validation Accuracy:0.3366\n",
    "Epoch #223: Loss:1.4303, Accuracy:0.3688, Validation Loss:1.4907, Validation Accuracy:0.3383\n",
    "Epoch #224: Loss:1.4283, Accuracy:0.3721, Validation Loss:1.4927, Validation Accuracy:0.3300\n",
    "Epoch #225: Loss:1.4281, Accuracy:0.3741, Validation Loss:1.4922, Validation Accuracy:0.3366\n",
    "Epoch #226: Loss:1.4277, Accuracy:0.3733, Validation Loss:1.4994, Validation Accuracy:0.3284\n",
    "Epoch #227: Loss:1.4289, Accuracy:0.3749, Validation Loss:1.4963, Validation Accuracy:0.3300\n",
    "Epoch #228: Loss:1.4288, Accuracy:0.3692, Validation Loss:1.4949, Validation Accuracy:0.3333\n",
    "Epoch #229: Loss:1.4322, Accuracy:0.3663, Validation Loss:1.4913, Validation Accuracy:0.3350\n",
    "Epoch #230: Loss:1.4342, Accuracy:0.3655, Validation Loss:1.4987, Validation Accuracy:0.3399\n",
    "Epoch #231: Loss:1.4372, Accuracy:0.3696, Validation Loss:1.4949, Validation Accuracy:0.3366\n",
    "Epoch #232: Loss:1.4364, Accuracy:0.3639, Validation Loss:1.5199, Validation Accuracy:0.3317\n",
    "Epoch #233: Loss:1.4447, Accuracy:0.3643, Validation Loss:1.5001, Validation Accuracy:0.3251\n",
    "Epoch #234: Loss:1.4521, Accuracy:0.3676, Validation Loss:1.4915, Validation Accuracy:0.3366\n",
    "Epoch #235: Loss:1.4352, Accuracy:0.3684, Validation Loss:1.4865, Validation Accuracy:0.3383\n",
    "Epoch #236: Loss:1.4273, Accuracy:0.3667, Validation Loss:1.4985, Validation Accuracy:0.3284\n",
    "Epoch #237: Loss:1.4286, Accuracy:0.3713, Validation Loss:1.4908, Validation Accuracy:0.3366\n",
    "Epoch #238: Loss:1.4283, Accuracy:0.3708, Validation Loss:1.4927, Validation Accuracy:0.3268\n",
    "Epoch #239: Loss:1.4313, Accuracy:0.3663, Validation Loss:1.5026, Validation Accuracy:0.3317\n",
    "Epoch #240: Loss:1.4300, Accuracy:0.3749, Validation Loss:1.4920, Validation Accuracy:0.3218\n",
    "Epoch #241: Loss:1.4280, Accuracy:0.3729, Validation Loss:1.4944, Validation Accuracy:0.3268\n",
    "Epoch #242: Loss:1.4274, Accuracy:0.3791, Validation Loss:1.4975, Validation Accuracy:0.3448\n",
    "Epoch #243: Loss:1.4277, Accuracy:0.3708, Validation Loss:1.4905, Validation Accuracy:0.3399\n",
    "Epoch #244: Loss:1.4244, Accuracy:0.3766, Validation Loss:1.4952, Validation Accuracy:0.3268\n",
    "Epoch #245: Loss:1.4274, Accuracy:0.3696, Validation Loss:1.4907, Validation Accuracy:0.3251\n",
    "Epoch #246: Loss:1.4245, Accuracy:0.3729, Validation Loss:1.4993, Validation Accuracy:0.3366\n",
    "Epoch #247: Loss:1.4300, Accuracy:0.3692, Validation Loss:1.4917, Validation Accuracy:0.3300\n",
    "Epoch #248: Loss:1.4292, Accuracy:0.3688, Validation Loss:1.4996, Validation Accuracy:0.3268\n",
    "Epoch #249: Loss:1.4256, Accuracy:0.3819, Validation Loss:1.4956, Validation Accuracy:0.3268\n",
    "Epoch #250: Loss:1.4241, Accuracy:0.3733, Validation Loss:1.4902, Validation Accuracy:0.3284\n",
    "Epoch #251: Loss:1.4206, Accuracy:0.3733, Validation Loss:1.4943, Validation Accuracy:0.3218\n",
    "Epoch #252: Loss:1.4214, Accuracy:0.3774, Validation Loss:1.4949, Validation Accuracy:0.3366\n",
    "Epoch #253: Loss:1.4231, Accuracy:0.3828, Validation Loss:1.4921, Validation Accuracy:0.3350\n",
    "Epoch #254: Loss:1.4218, Accuracy:0.3782, Validation Loss:1.5042, Validation Accuracy:0.3432\n",
    "Epoch #255: Loss:1.4299, Accuracy:0.3762, Validation Loss:1.4994, Validation Accuracy:0.3366\n",
    "Epoch #256: Loss:1.4224, Accuracy:0.3807, Validation Loss:1.4917, Validation Accuracy:0.3333\n",
    "Epoch #257: Loss:1.4213, Accuracy:0.3774, Validation Loss:1.4955, Validation Accuracy:0.3399\n",
    "Epoch #258: Loss:1.4190, Accuracy:0.3799, Validation Loss:1.4929, Validation Accuracy:0.3251\n",
    "Epoch #259: Loss:1.4216, Accuracy:0.3721, Validation Loss:1.4932, Validation Accuracy:0.3251\n",
    "Epoch #260: Loss:1.4190, Accuracy:0.3749, Validation Loss:1.4958, Validation Accuracy:0.3268\n",
    "Epoch #261: Loss:1.4181, Accuracy:0.3786, Validation Loss:1.4977, Validation Accuracy:0.3317\n",
    "Epoch #262: Loss:1.4177, Accuracy:0.3795, Validation Loss:1.4913, Validation Accuracy:0.3350\n",
    "Epoch #263: Loss:1.4153, Accuracy:0.3774, Validation Loss:1.4950, Validation Accuracy:0.3448\n",
    "Epoch #264: Loss:1.4190, Accuracy:0.3799, Validation Loss:1.4969, Validation Accuracy:0.3350\n",
    "Epoch #265: Loss:1.4182, Accuracy:0.3786, Validation Loss:1.4958, Validation Accuracy:0.3333\n",
    "Epoch #266: Loss:1.4186, Accuracy:0.3758, Validation Loss:1.4959, Validation Accuracy:0.3300\n",
    "Epoch #267: Loss:1.4168, Accuracy:0.3754, Validation Loss:1.4951, Validation Accuracy:0.3383\n",
    "Epoch #268: Loss:1.4181, Accuracy:0.3737, Validation Loss:1.4933, Validation Accuracy:0.3498\n",
    "Epoch #269: Loss:1.4155, Accuracy:0.3815, Validation Loss:1.4922, Validation Accuracy:0.3268\n",
    "Epoch #270: Loss:1.4153, Accuracy:0.3778, Validation Loss:1.5074, Validation Accuracy:0.3448\n",
    "Epoch #271: Loss:1.4191, Accuracy:0.3836, Validation Loss:1.4900, Validation Accuracy:0.3333\n",
    "Epoch #272: Loss:1.4166, Accuracy:0.3856, Validation Loss:1.5012, Validation Accuracy:0.3366\n",
    "Epoch #273: Loss:1.4189, Accuracy:0.3832, Validation Loss:1.4911, Validation Accuracy:0.3268\n",
    "Epoch #274: Loss:1.4162, Accuracy:0.3786, Validation Loss:1.4923, Validation Accuracy:0.3448\n",
    "Epoch #275: Loss:1.4122, Accuracy:0.3819, Validation Loss:1.4898, Validation Accuracy:0.3350\n",
    "Epoch #276: Loss:1.4121, Accuracy:0.3840, Validation Loss:1.5048, Validation Accuracy:0.3415\n",
    "Epoch #277: Loss:1.4178, Accuracy:0.3754, Validation Loss:1.4896, Validation Accuracy:0.3432\n",
    "Epoch #278: Loss:1.4134, Accuracy:0.3840, Validation Loss:1.5196, Validation Accuracy:0.3300\n",
    "Epoch #279: Loss:1.4245, Accuracy:0.3819, Validation Loss:1.4900, Validation Accuracy:0.3530\n",
    "Epoch #280: Loss:1.4175, Accuracy:0.3737, Validation Loss:1.5018, Validation Accuracy:0.3300\n",
    "Epoch #281: Loss:1.4171, Accuracy:0.3799, Validation Loss:1.5100, Validation Accuracy:0.3300\n",
    "Epoch #282: Loss:1.4168, Accuracy:0.3778, Validation Loss:1.5008, Validation Accuracy:0.3317\n",
    "Epoch #283: Loss:1.4298, Accuracy:0.3717, Validation Loss:1.5108, Validation Accuracy:0.3415\n",
    "Epoch #284: Loss:1.4330, Accuracy:0.3754, Validation Loss:1.4905, Validation Accuracy:0.3333\n",
    "Epoch #285: Loss:1.4217, Accuracy:0.3700, Validation Loss:1.5119, Validation Accuracy:0.3284\n",
    "Epoch #286: Loss:1.4260, Accuracy:0.3700, Validation Loss:1.4937, Validation Accuracy:0.3317\n",
    "Epoch #287: Loss:1.4135, Accuracy:0.3803, Validation Loss:1.4938, Validation Accuracy:0.3448\n",
    "Epoch #288: Loss:1.4070, Accuracy:0.3836, Validation Loss:1.4975, Validation Accuracy:0.3333\n",
    "Epoch #289: Loss:1.4117, Accuracy:0.3741, Validation Loss:1.5074, Validation Accuracy:0.3251\n",
    "Epoch #290: Loss:1.4182, Accuracy:0.3795, Validation Loss:1.4901, Validation Accuracy:0.3333\n",
    "Epoch #291: Loss:1.4188, Accuracy:0.3684, Validation Loss:1.4930, Validation Accuracy:0.3284\n",
    "Epoch #292: Loss:1.4121, Accuracy:0.3782, Validation Loss:1.4922, Validation Accuracy:0.3300\n",
    "Epoch #293: Loss:1.4106, Accuracy:0.3774, Validation Loss:1.4935, Validation Accuracy:0.3415\n",
    "Epoch #294: Loss:1.4109, Accuracy:0.3893, Validation Loss:1.5026, Validation Accuracy:0.3087\n",
    "Epoch #295: Loss:1.4096, Accuracy:0.3856, Validation Loss:1.4995, Validation Accuracy:0.3432\n",
    "Epoch #296: Loss:1.4129, Accuracy:0.3811, Validation Loss:1.4897, Validation Accuracy:0.3251\n",
    "Epoch #297: Loss:1.4141, Accuracy:0.3782, Validation Loss:1.4924, Validation Accuracy:0.3498\n",
    "Epoch #298: Loss:1.4118, Accuracy:0.3717, Validation Loss:1.4928, Validation Accuracy:0.3268\n",
    "Epoch #299: Loss:1.4052, Accuracy:0.3832, Validation Loss:1.4910, Validation Accuracy:0.3415\n",
    "Epoch #300: Loss:1.4035, Accuracy:0.3930, Validation Loss:1.4942, Validation Accuracy:0.3218\n",
    "\n",
    "Test:\n",
    "Test Loss:1.49419272, Accuracy:0.3218\n",
    "Labels: ['05', '03', '01', '04', '02']\n",
    "Confusion Matrix:\n",
    "      05  03  01  04  02\n",
    "t:05  76   4  24   9  29\n",
    "t:03  15  10  21  48  21\n",
    "t:01  14   2  32  53  25\n",
    "t:04   6   5  22  64  15\n",
    "t:02  24   5  25  46  14\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.56      0.54      0.55       142\n",
    "          03       0.38      0.09      0.14       115\n",
    "          01       0.26      0.25      0.26       126\n",
    "          04       0.29      0.57      0.39       112\n",
    "          02       0.13      0.12      0.13       114\n",
    "\n",
    "    accuracy                           0.32       609\n",
    "   macro avg       0.33      0.31      0.29       609\n",
    "weighted avg       0.34      0.32      0.30       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 02:25:03 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 27 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.607161327731629, 1.6060812250146725, 1.6056238258217748, 1.6054938510916699, 1.6054080084626898, 1.6053518464021104, 1.6053251734703828, 1.6053457547878396, 1.6052806917669737, 1.6052588839053326, 1.605232449979422, 1.6052009487778487, 1.605135432017848, 1.6050978963598241, 1.6050194764176418, 1.6048795908738436, 1.6047790267784607, 1.6045745709063777, 1.604299593637338, 1.6039783075721001, 1.6035363977570056, 1.6028911472345613, 1.6019484971348679, 1.6004073592438095, 1.5977736758481105, 1.5931360334011133, 1.5860036539131002, 1.5742471901262531, 1.5622961307785586, 1.5559170457529905, 1.5540895751740154, 1.5553228862962896, 1.5512750178135086, 1.5513922026983427, 1.5499852797863714, 1.549050797969837, 1.5508618333265307, 1.5473267501602423, 1.548790391638557, 1.5459573339358927, 1.548985076851054, 1.545305754946566, 1.5453050075884915, 1.5440975083114674, 1.5444107437368684, 1.5432611292806164, 1.5434726214369725, 1.541493417398487, 1.5406851627556561, 1.5426798166312607, 1.5407227205329732, 1.5438194676181562, 1.5398953994506686, 1.53738865946314, 1.5480299812232332, 1.5497612424671943, 1.536998728421717, 1.537142365436836, 1.5337633569839553, 1.5343300511292832, 1.5305093366328524, 1.5284554625575375, 1.5268221733409588, 1.5338979820508283, 1.5284225347594087, 1.5215304340243536, 1.526369221104777, 1.516857336307394, 1.5133257505341704, 1.5123891112056664, 1.5096256415832219, 1.5071978308688636, 1.5078659178979683, 1.524027622587771, 1.5590475962079804, 1.5336165416416863, 1.4977084715378108, 1.4977579418270068, 1.5090835329347057, 1.4967292915228356, 1.4966734596856905, 1.5127977113222646, 1.499572412525296, 1.4999326539940043, 1.5072770314459338, 1.4977600979687544, 1.4972379090163508, 1.5030715518397064, 1.4984719244325886, 1.500876871431598, 1.4982634955047582, 1.5003980382518425, 1.4960042105128222, 1.4942997357332453, 1.497750076362848, 1.4943411921828447, 1.5158503678044661, 1.5076719616434258, 1.4933939539935985, 1.5038683876419694, 1.501957284014409, 1.4927056413174458, 1.4938677048252524, 1.492371477321255, 1.519602909268221, 1.5328953475591975, 1.5135934979261827, 1.4984798253267662, 1.4900432316149006, 1.4923962454490474, 1.50863174029759, 1.5049644023522564, 1.5102280333320104, 1.4891574474782583, 1.4975519123531522, 1.4889702669701161, 1.4956186318828164, 1.496045440680092, 1.4932234727690372, 1.5016975320618728, 1.4942185183855505, 1.4917110408272454, 1.4928764273184665, 1.4920816210103152, 1.4910007161264154, 1.4936129741480786, 1.5011745851811125, 1.4897500276565552, 1.5045296022261696, 1.5077536965434384, 1.4909534675538638, 1.4962730534949717, 1.4916862963847144, 1.4892385146888019, 1.4885188918591328, 1.5108258043016707, 1.4913605913544328, 1.493449926180597, 1.4908210657696026, 1.5047907600261894, 1.5189079398592118, 1.4930502939694033, 1.4899391157090762, 1.4904271567787835, 1.4912441890619463, 1.5052614460633502, 1.506052500704435, 1.490689574008309, 1.5077757598535573, 1.4886362067192842, 1.5412744491166865, 1.4887087630912392, 1.4887335059678026, 1.4966865419754254, 1.4888517705873512, 1.4885630441220914, 1.4897094189827078, 1.490013444169206, 1.4955610945111228, 1.4979372012791374, 1.4911135881405158, 1.507022499842401, 1.5152385872964593, 1.491389967537866, 1.4886227818741196, 1.487822282294726, 1.4944419332325751, 1.4913862213516862, 1.4898804226532358, 1.4904242466235984, 1.4923634938222825, 1.4909200163310385, 1.4909621134571645, 1.4910749439731217, 1.4962528348947786, 1.4944767237492578, 1.5070715629800004, 1.4948941220595136, 1.4999582007992247, 1.4853969058771244, 1.4875863334423998, 1.4984310297738939, 1.5098276594393751, 1.4961552639508677, 1.513772896749437, 1.4878745192573184, 1.501532139449284, 1.4959602543873152, 1.4888467958995275, 1.4943701533848428, 1.4890130944244184, 1.4896111691918084, 1.4979755631808578, 1.4919744027267732, 1.4935287939895354, 1.5052318574955506, 1.4915401692852401, 1.4957340006366349, 1.4881649641763597, 1.492938785717405, 1.488459801047502, 1.4911217918536934, 1.4902197420107712, 1.4946465870038237, 1.4934261500933292, 1.4907089693009952, 1.5048442605289527, 1.5038718237665487, 1.4926094333526536, 1.491986047262433, 1.503503562977357, 1.4921323203883932, 1.4882296766162115, 1.4905503162218041, 1.4903444883662884, 1.4997865308094493, 1.5058274310210655, 1.4942319352051308, 1.504515177706388, 1.4973831587824329, 1.491045423916408, 1.4905252867731555, 1.4907287030384457, 1.4927127927003432, 1.4922470065760496, 1.4994158044041475, 1.4963056892401283, 1.4949340861419151, 1.491282680547492, 1.4987326237955705, 1.4949007302473722, 1.5198863059624859, 1.500140256873884, 1.4915144909387348, 1.4864726644039936, 1.4984659025038796, 1.4907630799439153, 1.492678886759653, 1.5025771161409827, 1.4920037336928895, 1.4943771188090784, 1.4974557699632567, 1.4904546156305398, 1.4951632457413697, 1.490677749777858, 1.4993104686095013, 1.4917355462639594, 1.4995909711997497, 1.4956041151666877, 1.4901867601867576, 1.494330887724026, 1.4949017870798096, 1.4920534570816115, 1.5042359306307262, 1.4994308355406587, 1.4916731929544158, 1.4955089039403229, 1.4928987294386564, 1.4931548974784137, 1.495841022978471, 1.497650721977497, 1.491291899986455, 1.4950266152571379, 1.4969332173148595, 1.4958278663052713, 1.495908913745473, 1.4950925090238574, 1.4933366223509088, 1.49216238872954, 1.5073850702965397, 1.490047344237517, 1.501177327386264, 1.4910570400689036, 1.492256770188781, 1.4898074487551485, 1.5047706561331287, 1.4895509959050195, 1.5195536458825047, 1.4899673076294522, 1.501786865232809, 1.510018078760169, 1.5007592314374074, 1.5107759785378116, 1.4904783611814376, 1.5119081447864402, 1.4936683581184675, 1.4937697611810343, 1.4975158418536383, 1.5074426873368387, 1.4900837937012095, 1.4929609837007445, 1.492169541678405, 1.493534963119206, 1.5025896076694107, 1.4994541394886711, 1.4896790478225608, 1.4923588977071451, 1.4928273998066317, 1.490964861730441, 1.4941927897323333], 'val_acc': [0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.25779966968425194, 0.2643678142813039, 0.31691297017686276, 0.315270934052068, 0.31198686180247853, 0.3037766811785048, 0.3136288979272733, 0.30870278955288905, 0.3103448256776838, 0.30870278955288905, 0.3037766811785048, 0.3054187170096806, 0.30541871730329956, 0.30870278935714307, 0.30541871720542657, 0.30870278935714307, 0.3054187171075536, 0.3054187171075536, 0.30870278935714307, 0.31198686160673256, 0.3021346447600911, 0.3136288978294003, 0.30377668098275884, 0.30049260873316935, 0.3136288978294003, 0.31198686150885957, 0.31198686160673256, 0.315270934149941, 0.3169129699811168, 0.2922824283049416, 0.31527093444355997, 0.3037766812763778, 0.31691297017686276, 0.3103448257755568, 0.315270934149941, 0.3119868619982245, 0.32019704252432524, 0.3169129704704817, 0.31362889812301925, 0.32019704252432524, 0.33004925986033157, 0.315270934345687, 0.33004925956671266, 0.33497536813684287, 0.33825944028855937, 0.3333333321099211, 0.3333333321099211, 0.33825944038643235, 0.3234811152632796, 0.2922824283049416, 0.29720853677719883, 0.33497536813684287, 0.339901476804846, 0.3234811153611526, 0.3349753683325888, 0.339901476804846, 0.3234811150675337, 0.33169129598512637, 0.339901476804846, 0.3366174043595106, 0.34154351292964075, 0.3448275851792303, 0.3366174043595106, 0.33497536823471585, 0.33825944038643235, 0.33825944048430534, 0.33497536813684287, 0.33661740445738353, 0.34646962120615205, 0.3366174045552565, 0.34318554895656256, 0.3316912957893804, 0.3316912957893804, 0.33825944048430534, 0.33333333191417513, 0.3349753680389699, 0.3366174042616376, 0.3399014765112271, 0.3415435127338948, 0.3284072236376639, 0.31855500669314946, 0.33004925976245864, 0.3366174047510025, 0.3497536938472334, 0.3366174042616376, 0.3366174041637646, 0.3366174042616376, 0.3284072237355368, 0.34318554885868957, 0.34154351253814885, 0.3366174041637646, 0.3316912956915074, 0.33497536813684287, 0.3399014765112271, 0.33497536813684287, 0.3366174042616376, 0.3382594401906864, 0.35139572967840926, 0.3399014766091001, 0.34154351253814885, 0.33825944048430534, 0.33497536813684287, 0.33825944038643235, 0.3349753680389699, 0.3349753679410969, 0.3366174041637646, 0.33333333230566703, 0.34154351292964075, 0.3316912956915074, 0.3349753679410969, 0.3366174042616376, 0.3366174041637646, 0.3448275852771032, 0.33333333181630215, 0.33825944038643235, 0.33004925986033157, 0.33004925966458565, 0.3399014766091001, 0.33825944028855937, 0.3366174042616376, 0.33825944028855937, 0.33661740406589163, 0.3349753680389699, 0.3316912958872534, 0.3415435127338948, 0.3119868620960975, 0.3415435128317678, 0.3497536935536145, 0.3366174041637646, 0.33497536813684287, 0.3399014765112271, 0.3382594401906864, 0.34318554885868957, 0.3399014764133541, 0.3349753679410969, 0.33497536823471585, 0.3316912956915074, 0.3316912958872534, 0.33333333191417513, 0.3415435128317678, 0.3415435127338948, 0.3284072234419179, 0.33333333191417513, 0.3415435128317678, 0.33825944048430534, 0.3333333320120481, 0.33825944048430534, 0.3366174042616376, 0.34154351292964075, 0.3366174041637646, 0.3316912957893804, 0.3284072235397909, 0.33825944038643235, 0.33004925956671266, 0.33333333191417513, 0.3349753680389699, 0.3349753679410969, 0.3399014764133541, 0.3382594401906864, 0.32512315119232843, 0.3349753683325888, 0.33004925966458565, 0.33004925956671266, 0.34975369345574153, 0.33004925956671266, 0.3399014766091001, 0.34646962130402503, 0.33333333181630215, 0.3399014766091001, 0.3349753680389699, 0.3399014764133541, 0.3481116574288198, 0.3415435128317678, 0.3333333321099211, 0.3284072234419179, 0.33497536813684287, 0.3349753680389699, 0.3366174043595106, 0.33333333191417513, 0.34318554895656256, 0.34154351292964075, 0.3316912957893804, 0.33333333181630215, 0.34975369345574153, 0.3431855487608166, 0.3284072235397909, 0.3349753679410969, 0.3415435128317678, 0.34646962130402503, 0.3349753684304618, 0.3316912956915074, 0.32676518741499616, 0.3316912957893804, 0.33004925986033157, 0.3267651873171232, 0.33333333191417513, 0.33661740445738353, 0.33825944058217833, 0.33004925956671266, 0.33661740445738353, 0.3284072234419179, 0.33004925956671266, 0.33333333181630215, 0.33497536823471585, 0.33990147670697307, 0.3366174042616376, 0.3316912960829993, 0.3251231512902014, 0.3366174042616376, 0.33825944038643235, 0.3284072234419179, 0.3366174041637646, 0.3267651873171232, 0.3316912957893804, 0.32183907904061193, 0.3267651872192502, 0.3448275849834843, 0.33990147670697307, 0.3267651873171232, 0.3251231512902014, 0.3366174043595106, 0.33004925956671266, 0.3267651873171232, 0.3267651873171232, 0.3284072234419179, 0.32183907894273894, 0.3366174042616376, 0.3349753683325888, 0.34318554895656256, 0.3366174041637646, 0.3333333320120481, 0.3399014765112271, 0.32512315119232843, 0.32512315119232843, 0.3267651873171232, 0.3316912957893804, 0.33497536813684287, 0.3448275850813573, 0.33497536813684287, 0.33333333181630215, 0.33004925976245864, 0.33825944038643235, 0.3497536935536145, 0.3267651872192502, 0.3448275850813573, 0.33333333171842916, 0.3366174042616376, 0.3267651872192502, 0.3448275850813573, 0.3349753680389699, 0.34154351292964075, 0.34318554885868957, 0.33004925976245864, 0.353037765901077, 0.33004925976245864, 0.33004925986033157, 0.33169129598512637, 0.34154351302751373, 0.33333333181630215, 0.3284072235397909, 0.33169129598512637, 0.3448275850813573, 0.33333333181630215, 0.3251231512902014, 0.33333333181630215, 0.3284072235397909, 0.33004925995820455, 0.3415435127338948, 0.30870278994438094, 0.3431855487608166, 0.32512315109445544, 0.3497536935536145, 0.32676518751286915, 0.3415435127338948, 0.32183907894273894], 'loss': [1.6086872158109284, 1.6065946708224883, 1.605817115869855, 1.6057312411694067, 1.6056710774404068, 1.6056101642839717, 1.6055171103448105, 1.6057405960878062, 1.6055230378125482, 1.6054905403321285, 1.605468165360437, 1.605454985512845, 1.6054300040924574, 1.6055089631364576, 1.6053061522007968, 1.605266197065553, 1.605166562479869, 1.6050113680915912, 1.6048484786579986, 1.604573728661273, 1.6042685516561082, 1.6041696484328785, 1.6032137315131312, 1.6023952102759047, 1.6005618827788493, 1.598023654988659, 1.5935034762172973, 1.5852852794913541, 1.574122398443046, 1.5629918152301954, 1.5593074962098985, 1.556656861892716, 1.5552519356690393, 1.553253279133744, 1.551524168852663, 1.5504556038541226, 1.5470225105540218, 1.5486937658987496, 1.5442364789622032, 1.5450977122759182, 1.5420008026354122, 1.5420379159386888, 1.5412620306015015, 1.5395322089322538, 1.5383614381480755, 1.5372726571143775, 1.5369090558567087, 1.5357485267905484, 1.5349556069599284, 1.5328620752514754, 1.5318271362316438, 1.5305130611454927, 1.5310059391252804, 1.5294840713301234, 1.527401177741174, 1.533833880444082, 1.5293713727281324, 1.5217171820771767, 1.5219289844285782, 1.5201637756653146, 1.5184228625875233, 1.5150779481296421, 1.513663046560738, 1.509995706419191, 1.509277599009645, 1.5083865350766348, 1.5011264765287082, 1.4963987035673012, 1.49503222226607, 1.4882853199569106, 1.485745014742904, 1.4808382261460322, 1.474196722620077, 1.4960298250343276, 1.4887831683031587, 1.5030098313423643, 1.4830714590985183, 1.4774421648323168, 1.4749034472804294, 1.483084389854995, 1.4679853158320244, 1.4666419626506202, 1.4660278330593384, 1.4687600586203824, 1.4647441474808804, 1.4621518328449319, 1.464223421476705, 1.4677521390347021, 1.4657376111165699, 1.4597268032097475, 1.4600203912116174, 1.4639277656465095, 1.4609146585209902, 1.4602950083401658, 1.4591330411253034, 1.459805049054187, 1.4574410990278335, 1.4626178392394613, 1.4615402730338627, 1.459526578895365, 1.459656013796217, 1.4554481438298, 1.4554756185112547, 1.4565828187264946, 1.4532790826820985, 1.4665498611618606, 1.4756825824537807, 1.4678628004551912, 1.4686659596539131, 1.4575806134535303, 1.4597877275772408, 1.46296173875092, 1.4608150982514054, 1.470135410561454, 1.4599974656741477, 1.4516881957435999, 1.4515111834116785, 1.4515650821662291, 1.4511250429329685, 1.449482709181627, 1.4508621299046511, 1.4495053269779903, 1.449760526602273, 1.452556789484357, 1.4557133076372089, 1.453797435173019, 1.4484831951971662, 1.449503631660336, 1.4461255341339894, 1.4501781430332568, 1.4507091944712143, 1.461309177038361, 1.4570485999207232, 1.452776905398594, 1.4522142848439774, 1.4496476442172541, 1.4501745420314938, 1.4466813351339383, 1.4494132063472052, 1.4466835506153302, 1.452053815040745, 1.4533459559603148, 1.4459619117223752, 1.4459459387056637, 1.453480009915158, 1.4470434198144524, 1.44823734207075, 1.4517889110949005, 1.458871274807125, 1.4630102047930018, 1.4594359335713318, 1.4648617953000862, 1.4594734094470923, 1.449696004855804, 1.4475157972723551, 1.4450377713973028, 1.4451146623192381, 1.44461467319935, 1.4452895122387082, 1.4432056435324574, 1.441824276393444, 1.4427020661884755, 1.4488696278487878, 1.4526878278603055, 1.4509009081724977, 1.4431879166949702, 1.4410354307789577, 1.4410391956873743, 1.4395391764826844, 1.4434925963501666, 1.440537869661006, 1.441052646264893, 1.4402643071063002, 1.4400190673080069, 1.4405212498788227, 1.4416149000857155, 1.4503313720103896, 1.465246200120914, 1.4616201576021417, 1.442059590684315, 1.4390989630129303, 1.440371715067838, 1.4410421983417299, 1.4507393219632534, 1.4476033160818675, 1.4637140578068257, 1.443569784291716, 1.4453010894434652, 1.446143108569621, 1.4380293448602883, 1.4367142350766693, 1.437903638493109, 1.43587938079599, 1.4393770221812034, 1.4376864893480492, 1.435562490241973, 1.4401402808802328, 1.4434251737300865, 1.4478475841408638, 1.4408221183616279, 1.4348245880687016, 1.4341198098243384, 1.43352032735852, 1.4342851717614051, 1.436130197826597, 1.4358477825501617, 1.4333681120275228, 1.439923009049966, 1.4449517850268794, 1.4412783130238433, 1.4403576536100258, 1.438789702881533, 1.4323504699084304, 1.4327316910824002, 1.4395968754923074, 1.4332794071957315, 1.436089367396533, 1.4365237214971618, 1.4337080205490458, 1.4362672094446922, 1.4304991229114101, 1.4318332622672987, 1.4302878058666566, 1.4283402413558177, 1.428128941396913, 1.4277392441731949, 1.4288913272979078, 1.4288126456419301, 1.4321641692390676, 1.4342006190846344, 1.437222037569943, 1.436368104026058, 1.4447351841466383, 1.4520640404072631, 1.4352373943191779, 1.4273148420655017, 1.4286119015554628, 1.4283194536545927, 1.431315018998524, 1.4300112928942734, 1.4279819750932699, 1.4274223619417978, 1.42766755714064, 1.4243529614971404, 1.4274364353939737, 1.424477434305195, 1.4300430798677448, 1.4292169719750876, 1.4256384488248728, 1.4240703937698929, 1.4205903787142933, 1.4214437721201527, 1.4230804553511696, 1.4218287042034234, 1.4299173352165144, 1.422416997592307, 1.4212829666216027, 1.4189664268395739, 1.4215801696268195, 1.4190474609085177, 1.4181234468424835, 1.4176540517709093, 1.4153015000129872, 1.4189851128345152, 1.4181623140154922, 1.4186155197311965, 1.4168332966446142, 1.4180503716948585, 1.4155284392515493, 1.415326675252503, 1.4191347336622233, 1.4165738825435756, 1.4188593710717234, 1.4161969524144638, 1.4122288980523174, 1.4120905939803232, 1.4178017309803737, 1.413382440279152, 1.4244668440162769, 1.417500642633536, 1.4170679110031599, 1.4167849505461707, 1.4298465696937983, 1.433006989784554, 1.4216848298509508, 1.4260309217891654, 1.4134640470422513, 1.4070092829835488, 1.4116756575308296, 1.4181922207133237, 1.4187826043526495, 1.4121118262073586, 1.4105568211181452, 1.4108659939599477, 1.4095620151417945, 1.412890357305382, 1.414103338507901, 1.4117954002023967, 1.4051714250439247, 1.4034817893891853], 'acc': [0.20657084195643236, 0.2328542071079082, 0.2328542091028891, 0.23285420890706277, 0.2328542094945418, 0.23285420933543288, 0.23285420833794243, 0.2328542114528053, 0.23285421029620593, 0.23285420892542147, 0.23285420970872686, 0.23285420890706277, 0.23285421049203225, 0.2328542083195837, 0.23285420990455322, 0.23285420970872686, 0.23285421049203225, 0.23285421086532623, 0.23285420892542147, 0.2328542087112364, 0.23285421011873828, 0.23285421029620593, 0.23285421049203225, 0.2328542085337688, 0.23285421049203225, 0.2451745397990734, 0.23860369556004016, 0.27926078209886807, 0.2891170423500837, 0.29240246309881585, 0.290349074029335, 0.29240246525290564, 0.2936344983396589, 0.2973305943198273, 0.2985626285815386, 0.29691991611427837, 0.2973305960822644, 0.2960985608781388, 0.2981519487726615, 0.29691991908839105, 0.30061601506855945, 0.2997946594407671, 0.3014373722629625, 0.3010266924540855, 0.29650924123777744, 0.29938398476009254, 0.30102669585656827, 0.3006160162435175, 0.3006160164393439, 0.30513346945480646, 0.2977412733086815, 0.3059548262575569, 0.3055441476603553, 0.3047227918367366, 0.3117043097773127, 0.3043121150019722, 0.31006160145912326, 0.3141683792064322, 0.3125256650134523, 0.31170431271470794, 0.3125256697132847, 0.31293634279063104, 0.3195071878496871, 0.3260780277805407, 0.3199178640969725, 0.32484599551381027, 0.32731006043892374, 0.33716632660164725, 0.346611911229774, 0.346201231849267, 0.34948665416460994, 0.3548254617920157, 0.35359343011276434, 0.33880903671899126, 0.34948665177797633, 0.338398355894265, 0.34127309863327465, 0.3453798754014519, 0.35071868404470674, 0.3420944558643952, 0.3568788479241013, 0.35441478558144773, 0.3535934311286135, 0.35359342893780626, 0.3577002031602409, 0.3572895278920873, 0.35277207389749293, 0.35975359125059, 0.35400410557674433, 0.36303901434923835, 0.3642710464201424, 0.3560574968370324, 0.35564681624484995, 0.35728952769626093, 0.3605749464867296, 0.36303901239097486, 0.3609856260630629, 0.36016427121857597, 0.35893223836436655, 0.35893224051845635, 0.3556468166365026, 0.35934291778159094, 0.35934291735322077, 0.35893223679775577, 0.36386036997703064, 0.34743326290432186, 0.3445585215360967, 0.3585215598038824, 0.35400410894250967, 0.3618069793409391, 0.3572895275004346, 0.36468172325490683, 0.367967144591118, 0.35236139471281236, 0.35030800799324774, 0.36509240064043286, 0.36303901536508754, 0.3667351136951721, 0.3642710489658849, 0.36468172580064934, 0.3630390151325437, 0.3630390133701066, 0.36303901493671736, 0.367967144003639, 0.3577002039435463, 0.3700205346397306, 0.36550308166098544, 0.36386036997703064, 0.3618069793409391, 0.36591375552163724, 0.36468172677978106, 0.35811088433990246, 0.36344969196730814, 0.36303901634421926, 0.3622176591130987, 0.3618069827067044, 0.3667351152985003, 0.3667351142826511, 0.36386036821459355, 0.36303901239097486, 0.35852156211708114, 0.3593429165699154, 0.3618069812992025, 0.36016427262607786, 0.3540041047934389, 0.36919918097020177, 0.362217661303906, 0.3572895275004346, 0.3626283381386704, 0.35195072104798697, 0.35605749268796166, 0.35646817425927585, 0.35770020707676786, 0.36344969157565543, 0.36057494727003503, 0.370841886938475, 0.3659137578715534, 0.36098562567141024, 0.36221766149973234, 0.3626283377103003, 0.36591375552163724, 0.3708418906958931, 0.36057494903247217, 0.3609856264547156, 0.3548254627711474, 0.3667351145151949, 0.36550308064513626, 0.3646817236465595, 0.36632443646875495, 0.3638603680187672, 0.36427104681179506, 0.3634496939622891, 0.36468172619230205, 0.36591375571746354, 0.3646817248215176, 0.3638603676271145, 0.3655030788826991, 0.34620123044176515, 0.35400410698424617, 0.3655030788826991, 0.3687885010022158, 0.3675564669730482, 0.3687884990439523, 0.36180698090754987, 0.37002053248564076, 0.3503080072099423, 0.3667351113452559, 0.36303901536508754, 0.3626283373186476, 0.3663244343146651, 0.37741273059002917, 0.36919917901193827, 0.3655030798618309, 0.36755646873548536, 0.3691991788161119, 0.37248459858571237, 0.3704312106911896, 0.36960985803750995, 0.360574950439974, 0.3655030818200944, 0.3683778220133615, 0.36714579232909106, 0.36755646795217994, 0.36714579190072094, 0.3646817228632541, 0.3691991764661957, 0.3691991770536748, 0.37043121147449504, 0.36468172622901945, 0.36221765868472855, 0.3622176579381406, 0.37043121346947594, 0.3712525680814191, 0.36714578837584666, 0.3655030818568118, 0.36755646951879073, 0.3691991764661957, 0.37084188713430133, 0.3691991770536748, 0.36837782338414593, 0.3671457893549784, 0.36468172423403855, 0.3687885004147367, 0.37207392273007966, 0.37412730905799163, 0.37330595656342086, 0.3749486662523947, 0.36919917999107, 0.3663244343146651, 0.3655030790785255, 0.3696098568258344, 0.36386036880207256, 0.364271045049358, 0.36755646638556916, 0.36837782416745135, 0.36673511349934573, 0.3712525665148083, 0.3708418873301277, 0.36632443748460414, 0.37494866351082584, 0.3728952756163031, 0.3790554430205719, 0.37084188987587025, 0.3765913735914524, 0.369609858624989, 0.3728952750288241, 0.3691991795994173, 0.36878850260554397, 0.38193018360549175, 0.37330595558428914, 0.37330595264689387, 0.3774127298067238, 0.3827515407998949, 0.37823408758860594, 0.3761806961324915, 0.38069815427615655, 0.37741273000255016, 0.37987679884419057, 0.3720739211634689, 0.3749486656649157, 0.378644764227544, 0.37946611750542014, 0.3774127307858555, 0.3798767986850817, 0.3786447638358913, 0.3757700201177499, 0.3753593424996801, 0.3737166322232272, 0.3815195053999429, 0.37782340762061994, 0.3835728966235135, 0.3856262815439236, 0.38316221998457545, 0.37864476242838946, 0.3819301849762762, 0.3839835752207151, 0.37535934152054834, 0.3839835750248887, 0.3819301867387133, 0.3737166318315745, 0.3798767980608852, 0.3778234093830571, 0.3716632421378972, 0.375359340737243, 0.3700205334647725, 0.37002053561886233, 0.3802874764622604, 0.38357289388194465, 0.3741273116404516, 0.3794661184845519, 0.36837782322503704, 0.3782340868053006, 0.37741273000255016, 0.38932238405979636, 0.38562628514223274, 0.38110882856517847, 0.37823408504286343, 0.3716632435453991, 0.38316221544385204, 0.3930184818391193]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
