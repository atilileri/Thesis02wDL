{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf28.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 02:28:43 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '0Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eb', 'sg', 'eo', 'eg', 'ek', 'ck', 'sk', 'ib', 'mb', 'by', 'yd', 'aa', 'my', 'ds', 'ce'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000017594E2E278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000175ADBA6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7001, Accuracy:0.0784, Validation Loss:2.6931, Validation Accuracy:0.0887\n",
    "Epoch #2: Loss:2.6892, Accuracy:0.0891, Validation Loss:2.6830, Validation Accuracy:0.0903\n",
    "Epoch #3: Loss:2.6804, Accuracy:0.0903, Validation Loss:2.6750, Validation Accuracy:0.0952\n",
    "Epoch #4: Loss:2.6709, Accuracy:0.0969, Validation Loss:2.6642, Validation Accuracy:0.1133\n",
    "Epoch #5: Loss:2.6606, Accuracy:0.1088, Validation Loss:2.6536, Validation Accuracy:0.1149\n",
    "Epoch #6: Loss:2.6520, Accuracy:0.1146, Validation Loss:2.6458, Validation Accuracy:0.1149\n",
    "Epoch #7: Loss:2.6425, Accuracy:0.1216, Validation Loss:2.6351, Validation Accuracy:0.1248\n",
    "Epoch #8: Loss:2.6311, Accuracy:0.1228, Validation Loss:2.6212, Validation Accuracy:0.1314\n",
    "Epoch #9: Loss:2.6166, Accuracy:0.1343, Validation Loss:2.6056, Validation Accuracy:0.1363\n",
    "Epoch #10: Loss:2.5994, Accuracy:0.1396, Validation Loss:2.5880, Validation Accuracy:0.1527\n",
    "Epoch #11: Loss:2.5853, Accuracy:0.1503, Validation Loss:2.5723, Validation Accuracy:0.1511\n",
    "Epoch #12: Loss:2.5669, Accuracy:0.1528, Validation Loss:2.5511, Validation Accuracy:0.1445\n",
    "Epoch #13: Loss:2.5499, Accuracy:0.1565, Validation Loss:2.5388, Validation Accuracy:0.1494\n",
    "Epoch #14: Loss:2.5346, Accuracy:0.1573, Validation Loss:2.5224, Validation Accuracy:0.1560\n",
    "Epoch #15: Loss:2.5223, Accuracy:0.1520, Validation Loss:2.5088, Validation Accuracy:0.1494\n",
    "Epoch #16: Loss:2.5141, Accuracy:0.1589, Validation Loss:2.5021, Validation Accuracy:0.1544\n",
    "Epoch #17: Loss:2.5066, Accuracy:0.1544, Validation Loss:2.4985, Validation Accuracy:0.1609\n",
    "Epoch #18: Loss:2.5015, Accuracy:0.1540, Validation Loss:2.4984, Validation Accuracy:0.1626\n",
    "Epoch #19: Loss:2.4975, Accuracy:0.1618, Validation Loss:2.4879, Validation Accuracy:0.1609\n",
    "Epoch #20: Loss:2.4873, Accuracy:0.1651, Validation Loss:2.4855, Validation Accuracy:0.1527\n",
    "Epoch #21: Loss:2.4844, Accuracy:0.1717, Validation Loss:2.4757, Validation Accuracy:0.1626\n",
    "Epoch #22: Loss:2.4793, Accuracy:0.1729, Validation Loss:2.4777, Validation Accuracy:0.1609\n",
    "Epoch #23: Loss:2.4751, Accuracy:0.1708, Validation Loss:2.4774, Validation Accuracy:0.1544\n",
    "Epoch #24: Loss:2.4744, Accuracy:0.1717, Validation Loss:2.4857, Validation Accuracy:0.1642\n",
    "Epoch #25: Loss:2.4914, Accuracy:0.1721, Validation Loss:2.5039, Validation Accuracy:0.1609\n",
    "Epoch #26: Loss:2.4800, Accuracy:0.1684, Validation Loss:2.4776, Validation Accuracy:0.1560\n",
    "Epoch #27: Loss:2.4711, Accuracy:0.1713, Validation Loss:2.4721, Validation Accuracy:0.1609\n",
    "Epoch #28: Loss:2.4625, Accuracy:0.1786, Validation Loss:2.4743, Validation Accuracy:0.1708\n",
    "Epoch #29: Loss:2.4614, Accuracy:0.1758, Validation Loss:2.4669, Validation Accuracy:0.1724\n",
    "Epoch #30: Loss:2.4577, Accuracy:0.1721, Validation Loss:2.4633, Validation Accuracy:0.1773\n",
    "Epoch #31: Loss:2.4563, Accuracy:0.1717, Validation Loss:2.4626, Validation Accuracy:0.1708\n",
    "Epoch #32: Loss:2.4582, Accuracy:0.1758, Validation Loss:2.4622, Validation Accuracy:0.1675\n",
    "Epoch #33: Loss:2.4565, Accuracy:0.1737, Validation Loss:2.4631, Validation Accuracy:0.1609\n",
    "Epoch #34: Loss:2.4640, Accuracy:0.1749, Validation Loss:2.4726, Validation Accuracy:0.1494\n",
    "Epoch #35: Loss:2.4705, Accuracy:0.1704, Validation Loss:2.4836, Validation Accuracy:0.1658\n",
    "Epoch #36: Loss:2.4606, Accuracy:0.1762, Validation Loss:2.4603, Validation Accuracy:0.1757\n",
    "Epoch #37: Loss:2.4567, Accuracy:0.1774, Validation Loss:2.4629, Validation Accuracy:0.1576\n",
    "Epoch #38: Loss:2.4521, Accuracy:0.1791, Validation Loss:2.4586, Validation Accuracy:0.1757\n",
    "Epoch #39: Loss:2.4516, Accuracy:0.1799, Validation Loss:2.4559, Validation Accuracy:0.1757\n",
    "Epoch #40: Loss:2.4509, Accuracy:0.1786, Validation Loss:2.4550, Validation Accuracy:0.1741\n",
    "Epoch #41: Loss:2.4504, Accuracy:0.1795, Validation Loss:2.4562, Validation Accuracy:0.1757\n",
    "Epoch #42: Loss:2.4492, Accuracy:0.1799, Validation Loss:2.4567, Validation Accuracy:0.1790\n",
    "Epoch #43: Loss:2.4500, Accuracy:0.1795, Validation Loss:2.4555, Validation Accuracy:0.1741\n",
    "Epoch #44: Loss:2.4485, Accuracy:0.1795, Validation Loss:2.4536, Validation Accuracy:0.1790\n",
    "Epoch #45: Loss:2.4503, Accuracy:0.1758, Validation Loss:2.4516, Validation Accuracy:0.1790\n",
    "Epoch #46: Loss:2.4485, Accuracy:0.1774, Validation Loss:2.4517, Validation Accuracy:0.1757\n",
    "Epoch #47: Loss:2.4469, Accuracy:0.1766, Validation Loss:2.4507, Validation Accuracy:0.1757\n",
    "Epoch #48: Loss:2.4464, Accuracy:0.1762, Validation Loss:2.4543, Validation Accuracy:0.1691\n",
    "Epoch #49: Loss:2.4462, Accuracy:0.1786, Validation Loss:2.4537, Validation Accuracy:0.1724\n",
    "Epoch #50: Loss:2.4448, Accuracy:0.1795, Validation Loss:2.4541, Validation Accuracy:0.1724\n",
    "Epoch #51: Loss:2.4435, Accuracy:0.1807, Validation Loss:2.4553, Validation Accuracy:0.1724\n",
    "Epoch #52: Loss:2.4435, Accuracy:0.1782, Validation Loss:2.4555, Validation Accuracy:0.1691\n",
    "Epoch #53: Loss:2.4438, Accuracy:0.1758, Validation Loss:2.4573, Validation Accuracy:0.1675\n",
    "Epoch #54: Loss:2.4441, Accuracy:0.1762, Validation Loss:2.4562, Validation Accuracy:0.1724\n",
    "Epoch #55: Loss:2.4436, Accuracy:0.1770, Validation Loss:2.4588, Validation Accuracy:0.1773\n",
    "Epoch #56: Loss:2.4437, Accuracy:0.1795, Validation Loss:2.4576, Validation Accuracy:0.1626\n",
    "Epoch #57: Loss:2.4444, Accuracy:0.1770, Validation Loss:2.4581, Validation Accuracy:0.1658\n",
    "Epoch #58: Loss:2.4425, Accuracy:0.1778, Validation Loss:2.4593, Validation Accuracy:0.1691\n",
    "Epoch #59: Loss:2.4416, Accuracy:0.1811, Validation Loss:2.4590, Validation Accuracy:0.1691\n",
    "Epoch #60: Loss:2.4410, Accuracy:0.1819, Validation Loss:2.4571, Validation Accuracy:0.1790\n",
    "Epoch #61: Loss:2.4393, Accuracy:0.1844, Validation Loss:2.4558, Validation Accuracy:0.1658\n",
    "Epoch #62: Loss:2.4409, Accuracy:0.1782, Validation Loss:2.4603, Validation Accuracy:0.1773\n",
    "Epoch #63: Loss:2.4442, Accuracy:0.1799, Validation Loss:2.4627, Validation Accuracy:0.1724\n",
    "Epoch #64: Loss:2.4427, Accuracy:0.1828, Validation Loss:2.4631, Validation Accuracy:0.1691\n",
    "Epoch #65: Loss:2.4628, Accuracy:0.1676, Validation Loss:2.4626, Validation Accuracy:0.1691\n",
    "Epoch #66: Loss:2.4491, Accuracy:0.1819, Validation Loss:2.4705, Validation Accuracy:0.1905\n",
    "Epoch #67: Loss:2.4457, Accuracy:0.1819, Validation Loss:2.4599, Validation Accuracy:0.1626\n",
    "Epoch #68: Loss:2.4448, Accuracy:0.1754, Validation Loss:2.4565, Validation Accuracy:0.1658\n",
    "Epoch #69: Loss:2.4401, Accuracy:0.1864, Validation Loss:2.4596, Validation Accuracy:0.1823\n",
    "Epoch #70: Loss:2.4417, Accuracy:0.1807, Validation Loss:2.4544, Validation Accuracy:0.1593\n",
    "Epoch #71: Loss:2.4397, Accuracy:0.1762, Validation Loss:2.4539, Validation Accuracy:0.1741\n",
    "Epoch #72: Loss:2.4387, Accuracy:0.1795, Validation Loss:2.4542, Validation Accuracy:0.1790\n",
    "Epoch #73: Loss:2.4407, Accuracy:0.1848, Validation Loss:2.4520, Validation Accuracy:0.1773\n",
    "Epoch #74: Loss:2.4402, Accuracy:0.1836, Validation Loss:2.4526, Validation Accuracy:0.1691\n",
    "Epoch #75: Loss:2.4404, Accuracy:0.1823, Validation Loss:2.4510, Validation Accuracy:0.1790\n",
    "Epoch #76: Loss:2.4403, Accuracy:0.1832, Validation Loss:2.4512, Validation Accuracy:0.1773\n",
    "Epoch #77: Loss:2.4396, Accuracy:0.1823, Validation Loss:2.4486, Validation Accuracy:0.1691\n",
    "Epoch #78: Loss:2.4388, Accuracy:0.1786, Validation Loss:2.4510, Validation Accuracy:0.1708\n",
    "Epoch #79: Loss:2.4377, Accuracy:0.1819, Validation Loss:2.4525, Validation Accuracy:0.1741\n",
    "Epoch #80: Loss:2.4366, Accuracy:0.1844, Validation Loss:2.4542, Validation Accuracy:0.1708\n",
    "Epoch #81: Loss:2.4372, Accuracy:0.1844, Validation Loss:2.4548, Validation Accuracy:0.1675\n",
    "Epoch #82: Loss:2.4373, Accuracy:0.1819, Validation Loss:2.4551, Validation Accuracy:0.1741\n",
    "Epoch #83: Loss:2.4366, Accuracy:0.1848, Validation Loss:2.4552, Validation Accuracy:0.1741\n",
    "Epoch #84: Loss:2.4358, Accuracy:0.1840, Validation Loss:2.4551, Validation Accuracy:0.1724\n",
    "Epoch #85: Loss:2.4369, Accuracy:0.1832, Validation Loss:2.4554, Validation Accuracy:0.1708\n",
    "Epoch #86: Loss:2.4363, Accuracy:0.1795, Validation Loss:2.4589, Validation Accuracy:0.1856\n",
    "Epoch #87: Loss:2.4371, Accuracy:0.1782, Validation Loss:2.4563, Validation Accuracy:0.1741\n",
    "Epoch #88: Loss:2.4358, Accuracy:0.1815, Validation Loss:2.4560, Validation Accuracy:0.1724\n",
    "Epoch #89: Loss:2.4358, Accuracy:0.1807, Validation Loss:2.4575, Validation Accuracy:0.1691\n",
    "Epoch #90: Loss:2.4354, Accuracy:0.1819, Validation Loss:2.4560, Validation Accuracy:0.1708\n",
    "Epoch #91: Loss:2.4350, Accuracy:0.1823, Validation Loss:2.4549, Validation Accuracy:0.1724\n",
    "Epoch #92: Loss:2.4347, Accuracy:0.1828, Validation Loss:2.4562, Validation Accuracy:0.1724\n",
    "Epoch #93: Loss:2.4353, Accuracy:0.1819, Validation Loss:2.4545, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4351, Accuracy:0.1832, Validation Loss:2.4564, Validation Accuracy:0.1839\n",
    "Epoch #95: Loss:2.4354, Accuracy:0.1856, Validation Loss:2.4544, Validation Accuracy:0.1708\n",
    "Epoch #96: Loss:2.4353, Accuracy:0.1819, Validation Loss:2.4551, Validation Accuracy:0.1790\n",
    "Epoch #97: Loss:2.4355, Accuracy:0.1844, Validation Loss:2.4571, Validation Accuracy:0.1806\n",
    "Epoch #98: Loss:2.4358, Accuracy:0.1860, Validation Loss:2.4563, Validation Accuracy:0.1724\n",
    "Epoch #99: Loss:2.4353, Accuracy:0.1832, Validation Loss:2.4576, Validation Accuracy:0.1708\n",
    "Epoch #100: Loss:2.4345, Accuracy:0.1836, Validation Loss:2.4588, Validation Accuracy:0.1708\n",
    "Epoch #101: Loss:2.4342, Accuracy:0.1856, Validation Loss:2.4593, Validation Accuracy:0.1773\n",
    "Epoch #102: Loss:2.4346, Accuracy:0.1869, Validation Loss:2.4591, Validation Accuracy:0.1724\n",
    "Epoch #103: Loss:2.4349, Accuracy:0.1832, Validation Loss:2.4592, Validation Accuracy:0.1708\n",
    "Epoch #104: Loss:2.4352, Accuracy:0.1836, Validation Loss:2.4593, Validation Accuracy:0.1708\n",
    "Epoch #105: Loss:2.4349, Accuracy:0.1836, Validation Loss:2.4590, Validation Accuracy:0.1773\n",
    "Epoch #106: Loss:2.4347, Accuracy:0.1856, Validation Loss:2.4612, Validation Accuracy:0.1708\n",
    "Epoch #107: Loss:2.4382, Accuracy:0.1836, Validation Loss:2.4588, Validation Accuracy:0.1708\n",
    "Epoch #108: Loss:2.4406, Accuracy:0.1811, Validation Loss:2.4623, Validation Accuracy:0.1806\n",
    "Epoch #109: Loss:2.4435, Accuracy:0.1832, Validation Loss:2.4641, Validation Accuracy:0.1790\n",
    "Epoch #110: Loss:2.4463, Accuracy:0.1807, Validation Loss:2.4600, Validation Accuracy:0.1823\n",
    "Epoch #111: Loss:2.4344, Accuracy:0.1815, Validation Loss:2.4594, Validation Accuracy:0.1691\n",
    "Epoch #112: Loss:2.4384, Accuracy:0.1791, Validation Loss:2.4562, Validation Accuracy:0.1741\n",
    "Epoch #113: Loss:2.4380, Accuracy:0.1786, Validation Loss:2.4598, Validation Accuracy:0.1691\n",
    "Epoch #114: Loss:2.4350, Accuracy:0.1803, Validation Loss:2.4602, Validation Accuracy:0.1691\n",
    "Epoch #115: Loss:2.4362, Accuracy:0.1807, Validation Loss:2.4558, Validation Accuracy:0.1741\n",
    "Epoch #116: Loss:2.4351, Accuracy:0.1799, Validation Loss:2.4535, Validation Accuracy:0.1757\n",
    "Epoch #117: Loss:2.4346, Accuracy:0.1815, Validation Loss:2.4531, Validation Accuracy:0.1724\n",
    "Epoch #118: Loss:2.4337, Accuracy:0.1815, Validation Loss:2.4537, Validation Accuracy:0.1741\n",
    "Epoch #119: Loss:2.4343, Accuracy:0.1795, Validation Loss:2.4541, Validation Accuracy:0.1708\n",
    "Epoch #120: Loss:2.4351, Accuracy:0.1762, Validation Loss:2.4553, Validation Accuracy:0.1790\n",
    "Epoch #121: Loss:2.4361, Accuracy:0.1749, Validation Loss:2.4552, Validation Accuracy:0.1741\n",
    "Epoch #122: Loss:2.4350, Accuracy:0.1782, Validation Loss:2.4541, Validation Accuracy:0.1708\n",
    "Epoch #123: Loss:2.4338, Accuracy:0.1819, Validation Loss:2.4549, Validation Accuracy:0.1741\n",
    "Epoch #124: Loss:2.4334, Accuracy:0.1803, Validation Loss:2.4549, Validation Accuracy:0.1741\n",
    "Epoch #125: Loss:2.4332, Accuracy:0.1795, Validation Loss:2.4543, Validation Accuracy:0.1741\n",
    "Epoch #126: Loss:2.4337, Accuracy:0.1795, Validation Loss:2.4560, Validation Accuracy:0.1741\n",
    "Epoch #127: Loss:2.4345, Accuracy:0.1799, Validation Loss:2.4576, Validation Accuracy:0.1741\n",
    "Epoch #128: Loss:2.4351, Accuracy:0.1782, Validation Loss:2.4579, Validation Accuracy:0.1757\n",
    "Epoch #129: Loss:2.4343, Accuracy:0.1803, Validation Loss:2.4595, Validation Accuracy:0.1691\n",
    "Epoch #130: Loss:2.4341, Accuracy:0.1823, Validation Loss:2.4581, Validation Accuracy:0.1741\n",
    "Epoch #131: Loss:2.4339, Accuracy:0.1795, Validation Loss:2.4575, Validation Accuracy:0.1741\n",
    "Epoch #132: Loss:2.4334, Accuracy:0.1754, Validation Loss:2.4559, Validation Accuracy:0.1724\n",
    "Epoch #133: Loss:2.4333, Accuracy:0.1803, Validation Loss:2.4558, Validation Accuracy:0.1724\n",
    "Epoch #134: Loss:2.4326, Accuracy:0.1786, Validation Loss:2.4561, Validation Accuracy:0.1675\n",
    "Epoch #135: Loss:2.4317, Accuracy:0.1799, Validation Loss:2.4525, Validation Accuracy:0.1691\n",
    "Epoch #136: Loss:2.4319, Accuracy:0.1815, Validation Loss:2.4568, Validation Accuracy:0.1708\n",
    "Epoch #137: Loss:2.4317, Accuracy:0.1811, Validation Loss:2.4560, Validation Accuracy:0.1691\n",
    "Epoch #138: Loss:2.4339, Accuracy:0.1807, Validation Loss:2.4561, Validation Accuracy:0.1757\n",
    "Epoch #139: Loss:2.4325, Accuracy:0.1811, Validation Loss:2.4554, Validation Accuracy:0.1675\n",
    "Epoch #140: Loss:2.4331, Accuracy:0.1758, Validation Loss:2.4542, Validation Accuracy:0.1691\n",
    "Epoch #141: Loss:2.4335, Accuracy:0.1774, Validation Loss:2.4536, Validation Accuracy:0.1708\n",
    "Epoch #142: Loss:2.4338, Accuracy:0.1807, Validation Loss:2.4535, Validation Accuracy:0.1741\n",
    "Epoch #143: Loss:2.4354, Accuracy:0.1803, Validation Loss:2.4503, Validation Accuracy:0.1708\n",
    "Epoch #144: Loss:2.4401, Accuracy:0.1799, Validation Loss:2.4591, Validation Accuracy:0.1642\n",
    "Epoch #145: Loss:2.4415, Accuracy:0.1836, Validation Loss:2.4648, Validation Accuracy:0.1642\n",
    "Epoch #146: Loss:2.4387, Accuracy:0.1737, Validation Loss:2.4620, Validation Accuracy:0.1757\n",
    "Epoch #147: Loss:2.4393, Accuracy:0.1758, Validation Loss:2.4582, Validation Accuracy:0.1724\n",
    "Epoch #148: Loss:2.4383, Accuracy:0.1795, Validation Loss:2.4588, Validation Accuracy:0.1675\n",
    "Epoch #149: Loss:2.4363, Accuracy:0.1782, Validation Loss:2.4625, Validation Accuracy:0.1823\n",
    "Epoch #150: Loss:2.4361, Accuracy:0.1803, Validation Loss:2.4575, Validation Accuracy:0.1741\n",
    "Epoch #151: Loss:2.4355, Accuracy:0.1811, Validation Loss:2.4568, Validation Accuracy:0.1724\n",
    "Epoch #152: Loss:2.4322, Accuracy:0.1803, Validation Loss:2.4600, Validation Accuracy:0.1806\n",
    "Epoch #153: Loss:2.4342, Accuracy:0.1778, Validation Loss:2.4588, Validation Accuracy:0.1708\n",
    "Epoch #154: Loss:2.4341, Accuracy:0.1840, Validation Loss:2.4572, Validation Accuracy:0.1724\n",
    "Epoch #155: Loss:2.4344, Accuracy:0.1795, Validation Loss:2.4578, Validation Accuracy:0.1691\n",
    "Epoch #156: Loss:2.4358, Accuracy:0.1778, Validation Loss:2.4601, Validation Accuracy:0.1691\n",
    "Epoch #157: Loss:2.4394, Accuracy:0.1774, Validation Loss:2.4761, Validation Accuracy:0.1790\n",
    "Epoch #158: Loss:2.4428, Accuracy:0.1741, Validation Loss:2.4617, Validation Accuracy:0.1757\n",
    "Epoch #159: Loss:2.4375, Accuracy:0.1729, Validation Loss:2.4624, Validation Accuracy:0.1741\n",
    "Epoch #160: Loss:2.4442, Accuracy:0.1680, Validation Loss:2.4744, Validation Accuracy:0.1675\n",
    "Epoch #161: Loss:2.4424, Accuracy:0.1758, Validation Loss:2.4629, Validation Accuracy:0.1675\n",
    "Epoch #162: Loss:2.4360, Accuracy:0.1774, Validation Loss:2.4599, Validation Accuracy:0.1609\n",
    "Epoch #163: Loss:2.4379, Accuracy:0.1774, Validation Loss:2.4550, Validation Accuracy:0.1724\n",
    "Epoch #164: Loss:2.4349, Accuracy:0.1791, Validation Loss:2.4580, Validation Accuracy:0.1724\n",
    "Epoch #165: Loss:2.4337, Accuracy:0.1799, Validation Loss:2.4553, Validation Accuracy:0.1675\n",
    "Epoch #166: Loss:2.4347, Accuracy:0.1803, Validation Loss:2.4559, Validation Accuracy:0.1675\n",
    "Epoch #167: Loss:2.4349, Accuracy:0.1782, Validation Loss:2.4567, Validation Accuracy:0.1724\n",
    "Epoch #168: Loss:2.4334, Accuracy:0.1737, Validation Loss:2.4543, Validation Accuracy:0.1675\n",
    "Epoch #169: Loss:2.4334, Accuracy:0.1791, Validation Loss:2.4544, Validation Accuracy:0.1658\n",
    "Epoch #170: Loss:2.4324, Accuracy:0.1795, Validation Loss:2.4553, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4342, Accuracy:0.1758, Validation Loss:2.4574, Validation Accuracy:0.1691\n",
    "Epoch #172: Loss:2.4344, Accuracy:0.1749, Validation Loss:2.4624, Validation Accuracy:0.1642\n",
    "Epoch #173: Loss:2.4369, Accuracy:0.1762, Validation Loss:2.4650, Validation Accuracy:0.1675\n",
    "Epoch #174: Loss:2.4378, Accuracy:0.1778, Validation Loss:2.4686, Validation Accuracy:0.1626\n",
    "Epoch #175: Loss:2.4380, Accuracy:0.1782, Validation Loss:2.4569, Validation Accuracy:0.1576\n",
    "Epoch #176: Loss:2.4345, Accuracy:0.1815, Validation Loss:2.4564, Validation Accuracy:0.1642\n",
    "Epoch #177: Loss:2.4323, Accuracy:0.1766, Validation Loss:2.4532, Validation Accuracy:0.1626\n",
    "Epoch #178: Loss:2.4319, Accuracy:0.1860, Validation Loss:2.4546, Validation Accuracy:0.1708\n",
    "Epoch #179: Loss:2.4305, Accuracy:0.1786, Validation Loss:2.4535, Validation Accuracy:0.1658\n",
    "Epoch #180: Loss:2.4317, Accuracy:0.1799, Validation Loss:2.4589, Validation Accuracy:0.1675\n",
    "Epoch #181: Loss:2.4325, Accuracy:0.1782, Validation Loss:2.4593, Validation Accuracy:0.1691\n",
    "Epoch #182: Loss:2.4308, Accuracy:0.1799, Validation Loss:2.4597, Validation Accuracy:0.1642\n",
    "Epoch #183: Loss:2.4304, Accuracy:0.1811, Validation Loss:2.4594, Validation Accuracy:0.1675\n",
    "Epoch #184: Loss:2.4310, Accuracy:0.1819, Validation Loss:2.4601, Validation Accuracy:0.1675\n",
    "Epoch #185: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4606, Validation Accuracy:0.1675\n",
    "Epoch #186: Loss:2.4310, Accuracy:0.1823, Validation Loss:2.4590, Validation Accuracy:0.1642\n",
    "Epoch #187: Loss:2.4319, Accuracy:0.1754, Validation Loss:2.4598, Validation Accuracy:0.1658\n",
    "Epoch #188: Loss:2.4304, Accuracy:0.1811, Validation Loss:2.4573, Validation Accuracy:0.1741\n",
    "Epoch #189: Loss:2.4303, Accuracy:0.1815, Validation Loss:2.4559, Validation Accuracy:0.1708\n",
    "Epoch #190: Loss:2.4303, Accuracy:0.1815, Validation Loss:2.4560, Validation Accuracy:0.1708\n",
    "Epoch #191: Loss:2.4302, Accuracy:0.1803, Validation Loss:2.4561, Validation Accuracy:0.1741\n",
    "Epoch #192: Loss:2.4299, Accuracy:0.1819, Validation Loss:2.4567, Validation Accuracy:0.1675\n",
    "Epoch #193: Loss:2.4303, Accuracy:0.1823, Validation Loss:2.4570, Validation Accuracy:0.1724\n",
    "Epoch #194: Loss:2.4299, Accuracy:0.1803, Validation Loss:2.4572, Validation Accuracy:0.1658\n",
    "Epoch #195: Loss:2.4302, Accuracy:0.1766, Validation Loss:2.4576, Validation Accuracy:0.1708\n",
    "Epoch #196: Loss:2.4304, Accuracy:0.1791, Validation Loss:2.4577, Validation Accuracy:0.1724\n",
    "Epoch #197: Loss:2.4311, Accuracy:0.1782, Validation Loss:2.4569, Validation Accuracy:0.1708\n",
    "Epoch #198: Loss:2.4302, Accuracy:0.1799, Validation Loss:2.4574, Validation Accuracy:0.1708\n",
    "Epoch #199: Loss:2.4301, Accuracy:0.1749, Validation Loss:2.4581, Validation Accuracy:0.1708\n",
    "Epoch #200: Loss:2.4303, Accuracy:0.1766, Validation Loss:2.4574, Validation Accuracy:0.1658\n",
    "Epoch #201: Loss:2.4304, Accuracy:0.1782, Validation Loss:2.4557, Validation Accuracy:0.1708\n",
    "Epoch #202: Loss:2.4299, Accuracy:0.1803, Validation Loss:2.4544, Validation Accuracy:0.1708\n",
    "Epoch #203: Loss:2.4295, Accuracy:0.1828, Validation Loss:2.4552, Validation Accuracy:0.1708\n",
    "Epoch #204: Loss:2.4294, Accuracy:0.1799, Validation Loss:2.4540, Validation Accuracy:0.1708\n",
    "Epoch #205: Loss:2.4290, Accuracy:0.1807, Validation Loss:2.4544, Validation Accuracy:0.1691\n",
    "Epoch #206: Loss:2.4292, Accuracy:0.1786, Validation Loss:2.4544, Validation Accuracy:0.1708\n",
    "Epoch #207: Loss:2.4288, Accuracy:0.1823, Validation Loss:2.4536, Validation Accuracy:0.1708\n",
    "Epoch #208: Loss:2.4291, Accuracy:0.1819, Validation Loss:2.4539, Validation Accuracy:0.1708\n",
    "Epoch #209: Loss:2.4287, Accuracy:0.1799, Validation Loss:2.4546, Validation Accuracy:0.1708\n",
    "Epoch #210: Loss:2.4292, Accuracy:0.1819, Validation Loss:2.4544, Validation Accuracy:0.1724\n",
    "Epoch #211: Loss:2.4288, Accuracy:0.1819, Validation Loss:2.4550, Validation Accuracy:0.1724\n",
    "Epoch #212: Loss:2.4289, Accuracy:0.1828, Validation Loss:2.4555, Validation Accuracy:0.1724\n",
    "Epoch #213: Loss:2.4289, Accuracy:0.1815, Validation Loss:2.4559, Validation Accuracy:0.1741\n",
    "Epoch #214: Loss:2.4282, Accuracy:0.1836, Validation Loss:2.4558, Validation Accuracy:0.1741\n",
    "Epoch #215: Loss:2.4293, Accuracy:0.1823, Validation Loss:2.4567, Validation Accuracy:0.1708\n",
    "Epoch #216: Loss:2.4285, Accuracy:0.1811, Validation Loss:2.4566, Validation Accuracy:0.1658\n",
    "Epoch #217: Loss:2.4282, Accuracy:0.1791, Validation Loss:2.4569, Validation Accuracy:0.1839\n",
    "Epoch #218: Loss:2.4289, Accuracy:0.1803, Validation Loss:2.4571, Validation Accuracy:0.1856\n",
    "Epoch #219: Loss:2.4274, Accuracy:0.1811, Validation Loss:2.4566, Validation Accuracy:0.1724\n",
    "Epoch #220: Loss:2.4283, Accuracy:0.1823, Validation Loss:2.4569, Validation Accuracy:0.1724\n",
    "Epoch #221: Loss:2.4276, Accuracy:0.1815, Validation Loss:2.4579, Validation Accuracy:0.1708\n",
    "Epoch #222: Loss:2.4279, Accuracy:0.1828, Validation Loss:2.4564, Validation Accuracy:0.1708\n",
    "Epoch #223: Loss:2.4284, Accuracy:0.1815, Validation Loss:2.4569, Validation Accuracy:0.1708\n",
    "Epoch #224: Loss:2.4279, Accuracy:0.1786, Validation Loss:2.4581, Validation Accuracy:0.1773\n",
    "Epoch #225: Loss:2.4275, Accuracy:0.1791, Validation Loss:2.4570, Validation Accuracy:0.1691\n",
    "Epoch #226: Loss:2.4280, Accuracy:0.1836, Validation Loss:2.4575, Validation Accuracy:0.1708\n",
    "Epoch #227: Loss:2.4280, Accuracy:0.1840, Validation Loss:2.4584, Validation Accuracy:0.1708\n",
    "Epoch #228: Loss:2.4279, Accuracy:0.1811, Validation Loss:2.4579, Validation Accuracy:0.1741\n",
    "Epoch #229: Loss:2.4276, Accuracy:0.1795, Validation Loss:2.4581, Validation Accuracy:0.1691\n",
    "Epoch #230: Loss:2.4279, Accuracy:0.1815, Validation Loss:2.4580, Validation Accuracy:0.1691\n",
    "Epoch #231: Loss:2.4273, Accuracy:0.1815, Validation Loss:2.4579, Validation Accuracy:0.1691\n",
    "Epoch #232: Loss:2.4276, Accuracy:0.1807, Validation Loss:2.4574, Validation Accuracy:0.1691\n",
    "Epoch #233: Loss:2.4274, Accuracy:0.1836, Validation Loss:2.4574, Validation Accuracy:0.1708\n",
    "Epoch #234: Loss:2.4270, Accuracy:0.1840, Validation Loss:2.4579, Validation Accuracy:0.1708\n",
    "Epoch #235: Loss:2.4273, Accuracy:0.1832, Validation Loss:2.4577, Validation Accuracy:0.1708\n",
    "Epoch #236: Loss:2.4274, Accuracy:0.1795, Validation Loss:2.4580, Validation Accuracy:0.1823\n",
    "Epoch #237: Loss:2.4267, Accuracy:0.1832, Validation Loss:2.4569, Validation Accuracy:0.1691\n",
    "Epoch #238: Loss:2.4276, Accuracy:0.1819, Validation Loss:2.4590, Validation Accuracy:0.1757\n",
    "Epoch #239: Loss:2.4282, Accuracy:0.1819, Validation Loss:2.4592, Validation Accuracy:0.1724\n",
    "Epoch #240: Loss:2.4285, Accuracy:0.1754, Validation Loss:2.4600, Validation Accuracy:0.1872\n",
    "Epoch #241: Loss:2.4273, Accuracy:0.1819, Validation Loss:2.4587, Validation Accuracy:0.1675\n",
    "Epoch #242: Loss:2.4298, Accuracy:0.1811, Validation Loss:2.4568, Validation Accuracy:0.1691\n",
    "Epoch #243: Loss:2.4249, Accuracy:0.1848, Validation Loss:2.4551, Validation Accuracy:0.1708\n",
    "Epoch #244: Loss:2.4261, Accuracy:0.1745, Validation Loss:2.4554, Validation Accuracy:0.1839\n",
    "Epoch #245: Loss:2.4268, Accuracy:0.1811, Validation Loss:2.4541, Validation Accuracy:0.1724\n",
    "Epoch #246: Loss:2.4259, Accuracy:0.1795, Validation Loss:2.4553, Validation Accuracy:0.1642\n",
    "Epoch #247: Loss:2.4256, Accuracy:0.1848, Validation Loss:2.4542, Validation Accuracy:0.1823\n",
    "Epoch #248: Loss:2.4252, Accuracy:0.1774, Validation Loss:2.4541, Validation Accuracy:0.1691\n",
    "Epoch #249: Loss:2.4254, Accuracy:0.1803, Validation Loss:2.4551, Validation Accuracy:0.1741\n",
    "Epoch #250: Loss:2.4268, Accuracy:0.1811, Validation Loss:2.4552, Validation Accuracy:0.1724\n",
    "Epoch #251: Loss:2.4248, Accuracy:0.1819, Validation Loss:2.4546, Validation Accuracy:0.1675\n",
    "Epoch #252: Loss:2.4260, Accuracy:0.1778, Validation Loss:2.4543, Validation Accuracy:0.1741\n",
    "Epoch #253: Loss:2.4264, Accuracy:0.1807, Validation Loss:2.4551, Validation Accuracy:0.1856\n",
    "Epoch #254: Loss:2.4249, Accuracy:0.1828, Validation Loss:2.4564, Validation Accuracy:0.1658\n",
    "Epoch #255: Loss:2.4244, Accuracy:0.1807, Validation Loss:2.4567, Validation Accuracy:0.1856\n",
    "Epoch #256: Loss:2.4239, Accuracy:0.1832, Validation Loss:2.4538, Validation Accuracy:0.1872\n",
    "Epoch #257: Loss:2.4241, Accuracy:0.1815, Validation Loss:2.4548, Validation Accuracy:0.1790\n",
    "Epoch #258: Loss:2.4231, Accuracy:0.1828, Validation Loss:2.4552, Validation Accuracy:0.1724\n",
    "Epoch #259: Loss:2.4231, Accuracy:0.1844, Validation Loss:2.4554, Validation Accuracy:0.1741\n",
    "Epoch #260: Loss:2.4234, Accuracy:0.1832, Validation Loss:2.4546, Validation Accuracy:0.1708\n",
    "Epoch #261: Loss:2.4232, Accuracy:0.1828, Validation Loss:2.4542, Validation Accuracy:0.1741\n",
    "Epoch #262: Loss:2.4232, Accuracy:0.1840, Validation Loss:2.4549, Validation Accuracy:0.1708\n",
    "Epoch #263: Loss:2.4235, Accuracy:0.1840, Validation Loss:2.4558, Validation Accuracy:0.1806\n",
    "Epoch #264: Loss:2.4238, Accuracy:0.1840, Validation Loss:2.4551, Validation Accuracy:0.1691\n",
    "Epoch #265: Loss:2.4233, Accuracy:0.1840, Validation Loss:2.4546, Validation Accuracy:0.1741\n",
    "Epoch #266: Loss:2.4223, Accuracy:0.1852, Validation Loss:2.4545, Validation Accuracy:0.1708\n",
    "Epoch #267: Loss:2.4233, Accuracy:0.1828, Validation Loss:2.4543, Validation Accuracy:0.1790\n",
    "Epoch #268: Loss:2.4237, Accuracy:0.1836, Validation Loss:2.4522, Validation Accuracy:0.1741\n",
    "Epoch #269: Loss:2.4236, Accuracy:0.1844, Validation Loss:2.4526, Validation Accuracy:0.1757\n",
    "Epoch #270: Loss:2.4243, Accuracy:0.1815, Validation Loss:2.4532, Validation Accuracy:0.1757\n",
    "Epoch #271: Loss:2.4232, Accuracy:0.1848, Validation Loss:2.4545, Validation Accuracy:0.1856\n",
    "Epoch #272: Loss:2.4241, Accuracy:0.1778, Validation Loss:2.4538, Validation Accuracy:0.1806\n",
    "Epoch #273: Loss:2.4241, Accuracy:0.1799, Validation Loss:2.4534, Validation Accuracy:0.1757\n",
    "Epoch #274: Loss:2.4245, Accuracy:0.1828, Validation Loss:2.4559, Validation Accuracy:0.1757\n",
    "Epoch #275: Loss:2.4230, Accuracy:0.1836, Validation Loss:2.4576, Validation Accuracy:0.1773\n",
    "Epoch #276: Loss:2.4236, Accuracy:0.1786, Validation Loss:2.4574, Validation Accuracy:0.1839\n",
    "Epoch #277: Loss:2.4225, Accuracy:0.1828, Validation Loss:2.4544, Validation Accuracy:0.1773\n",
    "Epoch #278: Loss:2.4228, Accuracy:0.1844, Validation Loss:2.4533, Validation Accuracy:0.1823\n",
    "Epoch #279: Loss:2.4220, Accuracy:0.1811, Validation Loss:2.4569, Validation Accuracy:0.1741\n",
    "Epoch #280: Loss:2.4220, Accuracy:0.1828, Validation Loss:2.4550, Validation Accuracy:0.1741\n",
    "Epoch #281: Loss:2.4231, Accuracy:0.1832, Validation Loss:2.4534, Validation Accuracy:0.1741\n",
    "Epoch #282: Loss:2.4227, Accuracy:0.1823, Validation Loss:2.4544, Validation Accuracy:0.1839\n",
    "Epoch #283: Loss:2.4223, Accuracy:0.1823, Validation Loss:2.4546, Validation Accuracy:0.1757\n",
    "Epoch #284: Loss:2.4219, Accuracy:0.1852, Validation Loss:2.4548, Validation Accuracy:0.1757\n",
    "Epoch #285: Loss:2.4223, Accuracy:0.1836, Validation Loss:2.4545, Validation Accuracy:0.1757\n",
    "Epoch #286: Loss:2.4231, Accuracy:0.1799, Validation Loss:2.4555, Validation Accuracy:0.1757\n",
    "Epoch #287: Loss:2.4240, Accuracy:0.1799, Validation Loss:2.4576, Validation Accuracy:0.1823\n",
    "Epoch #288: Loss:2.4254, Accuracy:0.1782, Validation Loss:2.4591, Validation Accuracy:0.1823\n",
    "Epoch #289: Loss:2.4269, Accuracy:0.1778, Validation Loss:2.4581, Validation Accuracy:0.1741\n",
    "Epoch #290: Loss:2.4233, Accuracy:0.1770, Validation Loss:2.4534, Validation Accuracy:0.1741\n",
    "Epoch #291: Loss:2.4214, Accuracy:0.1848, Validation Loss:2.4541, Validation Accuracy:0.1856\n",
    "Epoch #292: Loss:2.4209, Accuracy:0.1836, Validation Loss:2.4527, Validation Accuracy:0.1773\n",
    "Epoch #293: Loss:2.4213, Accuracy:0.1844, Validation Loss:2.4530, Validation Accuracy:0.1773\n",
    "Epoch #294: Loss:2.4220, Accuracy:0.1836, Validation Loss:2.4533, Validation Accuracy:0.1773\n",
    "Epoch #295: Loss:2.4215, Accuracy:0.1864, Validation Loss:2.4525, Validation Accuracy:0.1872\n",
    "Epoch #296: Loss:2.4208, Accuracy:0.1795, Validation Loss:2.4540, Validation Accuracy:0.1773\n",
    "Epoch #297: Loss:2.4208, Accuracy:0.1848, Validation Loss:2.4564, Validation Accuracy:0.1741\n",
    "Epoch #298: Loss:2.4231, Accuracy:0.1832, Validation Loss:2.4583, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.4243, Accuracy:0.1844, Validation Loss:2.4625, Validation Accuracy:0.1609\n",
    "Epoch #300: Loss:2.4273, Accuracy:0.1860, Validation Loss:2.4652, Validation Accuracy:0.1724\n",
    "\n",
    "Test:\n",
    "Test Loss:2.46517038, Accuracy:0.1724\n",
    "Labels: ['eb', 'sg', 'eo', 'eg', 'ek', 'ck', 'sk', 'ib', 'mb', 'by', 'yd', 'aa', 'my', 'ds', 'ce']\n",
    "Confusion Matrix:\n",
    "      eb  sg  eo  eg  ek  ck  sk  ib  mb  by  yd  aa  my  ds  ce\n",
    "t:eb   0  19   0  19   0   0   0   0   0   6   5   0   0   1   0\n",
    "t:sg   0  40   0   4   0   0   0   2   0   1   4   0   0   0   0\n",
    "t:eo   1  23   0   4   0   0   0   0   0   4   2   0   0   0   0\n",
    "t:eg   1  11   0  26   0   0   0   0   0   6   1   0   0   5   0\n",
    "t:ek   0  20   0  16   0   0   0   0   0   4   8   0   0   0   0\n",
    "t:ck   1   8   0   8   0   0   0   0   0   4   0   0   0   2   0\n",
    "t:sk   0  14   0  10   0   0   0   0   0   3   2   0   0   4   0\n",
    "t:ib   0  22   0   2   0   0   0   5   0   2  23   0   0   0   0\n",
    "t:mb   0  25   0  11   0   0   0   2   0   5   6   0   0   3   0\n",
    "t:by   1  24   0   8   0   0   0   0   0   4   3   0   0   0   0\n",
    "t:yd   0  35   0   2   0   0   0   2   0   0  23   0   0   0   0\n",
    "t:aa   0   7   0  18   0   0   0   0   0   2   1   0   0   6   0\n",
    "t:my   0   9   0   3   0   0   0   1   0   2   3   0   0   2   0\n",
    "t:ds   1   8   0  13   0   0   0   0   0   1   1   0   0   7   0\n",
    "t:ce   0  16   0   4   0   0   0   0   0   3   1   0   0   3   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          sg       0.14      0.78      0.24        51\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          eg       0.18      0.52      0.26        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ib       0.42      0.09      0.15        54\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          by       0.09      0.10      0.09        40\n",
    "          yd       0.28      0.37      0.32        62\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ds       0.21      0.23      0.22        31\n",
    "          ce       0.00      0.00      0.00        27\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.09      0.14      0.09       609\n",
    "weighted avg       0.11      0.17      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 03:09:12 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 29 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.693092548984221, 2.683042072897474, 2.675048375560341, 2.6642380058276047, 2.653583907141474, 2.6458408934337947, 2.6351393001224412, 2.6212113483003012, 2.605633297968772, 2.5880253647740057, 2.5722870509612736, 2.551116104000699, 2.5387602431825034, 2.522408708562992, 2.508802300016281, 2.5021434374435, 2.4985246901050187, 2.4984108653953316, 2.4878809017500854, 2.485509166576592, 2.4757328777281913, 2.4777421078266966, 2.477376610970458, 2.485741746836695, 2.5038936443516775, 2.477632735554612, 2.4721201444885805, 2.4742618503633196, 2.4669432385605936, 2.463315809497301, 2.4625860615121122, 2.46221123188, 2.463076774318426, 2.472572273417255, 2.483611915303373, 2.460265347131563, 2.4629085063934326, 2.4585929637276283, 2.4558659480710334, 2.4549695208350624, 2.456155543648355, 2.4566931262587874, 2.455517181230492, 2.453563786884051, 2.4516484670842615, 2.451694389477935, 2.450709157780865, 2.454277198302922, 2.4537018876161873, 2.4540889960008694, 2.4552827198517146, 2.455504434645078, 2.4572536514701904, 2.4561652784864303, 2.4588374069758823, 2.4575932119867483, 2.4580670178230175, 2.4593114637584716, 2.4590030475986024, 2.45707752042999, 2.455762302542751, 2.4602630921381055, 2.462700240130495, 2.463070829513625, 2.462649272971944, 2.470473501286875, 2.4598989106947173, 2.4565040474063267, 2.459593814777819, 2.4544416311730695, 2.453926332674199, 2.454200815097452, 2.452013772109459, 2.4525806304856475, 2.450966511649647, 2.4511755734241656, 2.448586860509537, 2.4509974399027956, 2.452531016714663, 2.454193740446971, 2.4547625613721524, 2.455116019460368, 2.455184535635712, 2.455101962551499, 2.4554336458591406, 2.458850267485445, 2.456291645814241, 2.455953408540372, 2.457470353992506, 2.4560227178783447, 2.4548593554003486, 2.4561691288094605, 2.4545351848226464, 2.4564293601438525, 2.454375522673032, 2.4550542306821725, 2.4571162519000826, 2.45628332308752, 2.4575784429540777, 2.4587943632222946, 2.459275854044947, 2.459092074427111, 2.459203785863416, 2.4592558668164783, 2.4589648943620754, 2.4611861431735687, 2.4587941870509304, 2.4623073317930224, 2.464058635661559, 2.4600302049483376, 2.4593738797067224, 2.456212132239381, 2.4598354956590875, 2.4601533737871644, 2.455790192035619, 2.453540744844133, 2.4531204183700637, 2.4536601111219434, 2.4540887168671306, 2.455330148314803, 2.4552262459678214, 2.4540627930551913, 2.4549129118864563, 2.45485358441796, 2.454346847455881, 2.4560219728692214, 2.457575037365868, 2.4579191438865973, 2.4594937467026985, 2.458132802950729, 2.4574527646520457, 2.4558877244175754, 2.455793628943182, 2.4560697423217723, 2.452477807090396, 2.4568438463414637, 2.4559880536178063, 2.4561422591530433, 2.4554165400112007, 2.4542285550404066, 2.4536107189353857, 2.4534877027784074, 2.450313447535723, 2.4590626397156363, 2.46480034685683, 2.4620470910628245, 2.4582334770553413, 2.458771995722954, 2.4625122206551686, 2.457472792204182, 2.45675211234633, 2.4600154171240547, 2.4587819701540843, 2.4572446287559173, 2.4577979141072492, 2.4600550866087865, 2.476073040750814, 2.46169629042176, 2.462418638426682, 2.4743717852092924, 2.462886554267019, 2.4599091071017662, 2.4550482369408817, 2.458008821374677, 2.455333402004148, 2.455874575378468, 2.456662820086299, 2.4543034513595656, 2.454407561588757, 2.4552746378925243, 2.457447661554872, 2.462358930036548, 2.46496069959819, 2.468648564052112, 2.4569439371231154, 2.45638086838871, 2.4531827920371874, 2.4545894581304593, 2.453475185215767, 2.4588863011847186, 2.4593153203453726, 2.4596756314996426, 2.459428973190107, 2.4601036609687243, 2.460564749581473, 2.4590244269723374, 2.4597537133885528, 2.4573220055678795, 2.455923259356143, 2.4560007187729007, 2.4561450755459138, 2.45672822977326, 2.456962887289489, 2.457240378328145, 2.4575501192966707, 2.45773383236088, 2.4569088304767077, 2.457350531235117, 2.4580705067990056, 2.4574109403957873, 2.4556775425846746, 2.4543672187379233, 2.4551578190526353, 2.453981468047219, 2.4544314856599705, 2.4544134159589244, 2.453567033526541, 2.453851341613995, 2.454619254188976, 2.454399259219616, 2.454957273401846, 2.4555183596211703, 2.4559452862575135, 2.4557680292865522, 2.4567053145767237, 2.4565511070840267, 2.4569118985988823, 2.457122309063064, 2.456562252467489, 2.4569480716692795, 2.4578728362648747, 2.456384095065112, 2.45694025905653, 2.4580659435691894, 2.4569916952224005, 2.457504475253752, 2.458443538308731, 2.457925275433044, 2.458077087386684, 2.4580424383943305, 2.45785056781299, 2.4573739076091345, 2.457448143872917, 2.4579472878492132, 2.4576997478998743, 2.458038611952307, 2.4569019652744037, 2.4589922028809346, 2.459178914381757, 2.459951087954792, 2.4587172909910455, 2.456754905641177, 2.455115896923397, 2.4553780712321864, 2.454065740597855, 2.4553179098859013, 2.4542201454024792, 2.4540546425849152, 2.4550869053807753, 2.4551740276010947, 2.454595769371697, 2.4542850798182494, 2.455053711172395, 2.456419245558615, 2.456708453558936, 2.45379983732853, 2.454801911399478, 2.4551692161654017, 2.455369478767533, 2.4545899772487445, 2.4542297937208404, 2.454933553297923, 2.455765870600107, 2.4550566896429205, 2.4545573878953806, 2.454466539063477, 2.454278754483303, 2.4522220552065495, 2.4526365899491585, 2.453185249823459, 2.4544535438806943, 2.453834482014473, 2.453444541185752, 2.455854433902183, 2.4575587360338234, 2.4573875659792295, 2.4543876080285933, 2.45328756741115, 2.456907107912261, 2.455036613545786, 2.4534324617025693, 2.454361465372671, 2.4545715250600932, 2.4547521324189034, 2.4545270492290627, 2.455526819761555, 2.457597355145735, 2.45909427656916, 2.458091866206653, 2.453354430316117, 2.4540811258388073, 2.4526552731180424, 2.4529902371279713, 2.4532935149563944, 2.4525476602106453, 2.454033433510165, 2.456382625012954, 2.4583263056618825, 2.4625098031925647, 2.4651701332900324], 'val_acc': [0.08866995023731723, 0.09031198645998496, 0.09523809493224218, 0.11330049250073033, 0.11494252833190614, 0.11494252833190614, 0.12479474537429355, 0.1313628889803816, 0.13628899725689286, 0.15270935860271329, 0.15106732286941046, 0.1444991783702315, 0.1494252872339806, 0.15599343153741363, 0.14942528694036167, 0.15435139560836486, 0.16091954000967085, 0.16256157633021157, 0.16091954020541682, 0.1527093590920782, 0.16256157633021157, 0.16091954010754383, 0.15435139541261889, 0.16420361235713332, 0.16091954000967085, 0.15599343163528662, 0.16091954020541682, 0.1707717568563123, 0.17241379307898003, 0.17733989937356345, 0.1707717568563123, 0.16748768450884982, 0.1609195398139249, 0.1494252868424887, 0.16584564857980105, 0.17569786334664167, 0.15763546746646243, 0.1756978653285695, 0.1756978632487687, 0.17405582712397397, 0.17569786315089572, 0.1789819354004852, 0.17405582920377477, 0.1789819354983582, 0.17898193559623116, 0.17569786334664167, 0.17569786334664167, 0.16912972082939054, 0.17241379080343325, 0.17241379090130624, 0.17241379090130624, 0.16912971855384376, 0.16748768242904902, 0.17241379080343325, 0.17733989917781748, 0.16256157633021157, 0.16584564630425427, 0.16912971855384376, 0.16912971855384376, 0.1789819354004852, 0.16584564848192807, 0.17733989927569047, 0.1724137909991792, 0.16912971865171675, 0.1691297206336446, 0.19047618856766738, 0.16256157603659263, 0.16584564848192807, 0.1822660076500747, 0.15927750378700312, 0.17405582920377477, 0.17898193530261222, 0.17733989917781748, 0.16912972073151755, 0.17898193530261222, 0.17733990145336426, 0.16912972073151755, 0.1707717546786385, 0.174055826928228, 0.17077175675843934, 0.16748768450884982, 0.17405582920377477, 0.17405582920377477, 0.17241379307898003, 0.17077175695418528, 0.18555008009541016, 0.174055826928228, 0.17241379307898003, 0.16912972082939054, 0.17077175695418528, 0.17241379307898003, 0.17241379298110704, 0.17733989917781748, 0.18390804387274243, 0.1707717568563123, 0.1789819354004852, 0.18062397162315294, 0.17241379080343325, 0.1707717546786385, 0.1707717546786385, 0.1773398994714364, 0.17241379090130624, 0.1707717546786385, 0.1707717546786385, 0.1773398994714364, 0.17077175487438445, 0.17077175497225744, 0.1806239718188989, 0.178981937578159, 0.18226600794369363, 0.1691297206336446, 0.17405582920377477, 0.16912972073151755, 0.16912971855384376, 0.174055826928228, 0.17569786315089572, 0.17241379307898003, 0.17405582920377477, 0.1707717568563123, 0.17898193530261222, 0.17405582910590178, 0.17077175675843934, 0.17405582910590178, 0.17405582910590178, 0.17405582910590178, 0.17405582910590178, 0.17405582910590178, 0.17569786523069655, 0.1691297206336446, 0.17405582910590178, 0.17405582910590178, 0.17241379298110704, 0.17241379298110704, 0.16748768431310387, 0.16912972043789862, 0.17077175666056635, 0.1691297205357716, 0.17569786523069655, 0.16748768441097686, 0.16912972073151755, 0.17077175675843934, 0.17405582910590178, 0.17077175656269336, 0.16420361216138737, 0.1642036124550063, 0.17569786315089572, 0.17241379288323408, 0.16748768450884982, 0.18226600774794768, 0.17405582891015584, 0.1724137927853611, 0.18062397162315294, 0.1707717568563123, 0.17241379298110704, 0.16912971855384376, 0.1691297205357716, 0.17898193530261222, 0.17569786305302273, 0.17405582900802882, 0.1674876846067228, 0.1674876847045958, 0.16091954000967085, 0.17241379080343325, 0.17241379080343325, 0.16748768450884982, 0.16748768450884982, 0.17241379307898003, 0.16748768450884982, 0.16584564838405508, 0.17077175695418528, 0.16912972073151755, 0.16420361225926033, 0.1674876846067228, 0.16256157603659263, 0.15763546785795435, 0.16420361225926033, 0.16256157603659263, 0.17077175666056635, 0.16584564828618212, 0.16748768441097686, 0.16912972073151755, 0.16420361225926033, 0.1674876846067228, 0.1674876846067228, 0.1674876846067228, 0.16420361225926033, 0.16584564838405508, 0.17405582910590178, 0.17077175675843934, 0.17077175675843934, 0.17405582910590178, 0.16748768450884982, 0.17241379298110704, 0.16584564838405508, 0.1707717568563123, 0.17241379298110704, 0.1707717568563123, 0.1707717568563123, 0.1707717568563123, 0.16584564838405508, 0.1707717568563123, 0.1707717568563123, 0.1707717568563123, 0.17077175695418528, 0.16912972073151755, 0.17077175695418528, 0.1707717568563123, 0.1707717568563123, 0.1707717568563123, 0.17241379298110704, 0.17241379298110704, 0.17241379298110704, 0.17405582920377477, 0.17405582920377477, 0.17077175675843934, 0.16584564838405508, 0.18390804377486944, 0.18555007999753717, 0.17241379298110704, 0.17241379298110704, 0.17077175675843934, 0.17077175675843934, 0.1707717568563123, 0.17733989917781748, 0.1691297206336446, 0.1707717568563123, 0.1707717568563123, 0.174055826928228, 0.1691297206336446, 0.1691297206336446, 0.1691297206336446, 0.1691297206336446, 0.17077175675843934, 0.17077175675843934, 0.17077175675843934, 0.1822660076500747, 0.1691297206336446, 0.17569786523069655, 0.17241379288323408, 0.18719211602445893, 0.16748768441097686, 0.1691297206336446, 0.1707717568563123, 0.18390804387274243, 0.17241379298110704, 0.16420361225926033, 0.1822660075522017, 0.1691297206336446, 0.17405582910590178, 0.17241379288323408, 0.16748768441097686, 0.17405582910590178, 0.18555007989966418, 0.16584564838405508, 0.18555007980179122, 0.18719211602445893, 0.17898193728454007, 0.17241379288323408, 0.17405582910590178, 0.17077175675843934, 0.17405582900802882, 0.17077175675843934, 0.18062397142740697, 0.1691297206336446, 0.17405582900802882, 0.1707717568563123, 0.17898193748028604, 0.17405582910590178, 0.17569786523069655, 0.17569786523069655, 0.18555007980179122, 0.18062397360508078, 0.17569786523069655, 0.17569786523069655, 0.1773399013554913, 0.18390804585467027, 0.1773399013554913, 0.18226600972987553, 0.17405582910590178, 0.17405582900802882, 0.17405582900802882, 0.18390804585467027, 0.17569786523069655, 0.17569786523069655, 0.17569786523069655, 0.17569786523069655, 0.18226600963200254, 0.18226600963200254, 0.17405582900802882, 0.1740558287144099, 0.18555007980179122, 0.1773399013554913, 0.1773399013554913, 0.1773399013554913, 0.18719211810425976, 0.1773399013554913, 0.17405582900802882, 0.17405582910590178, 0.16091953961817893, 0.17241379288323408], 'loss': [2.7001452991360266, 2.689218451208158, 2.6803580740394044, 2.6708591709881104, 2.660554587229077, 2.65195707834232, 2.6425127655084126, 2.631077966161332, 2.6166483918254624, 2.59935050715656, 2.5852897305263385, 2.5669201875369407, 2.549919054787262, 2.5345675971718538, 2.522314762138978, 2.5141348782995645, 2.5066082295451078, 2.501508314359849, 2.4975130251545683, 2.4872553986935158, 2.4844201787051725, 2.479335968009745, 2.475067439950712, 2.4744067747245335, 2.4914361653141905, 2.4799570020953734, 2.471140110810924, 2.462488887442211, 2.4614176753120502, 2.457678350237116, 2.456284156521243, 2.4581759554649527, 2.4565002692063977, 2.464018918650351, 2.470533091135828, 2.460612269250764, 2.4566899122398738, 2.4521445038382277, 2.451594650671957, 2.450851612854787, 2.450374404605654, 2.4491893558776354, 2.4499878145096483, 2.448484633835434, 2.450256177972719, 2.448464332126249, 2.4469259309083284, 2.446442831517245, 2.4461930530516764, 2.444788480245602, 2.443478272534004, 2.443450242244243, 2.4438061091444574, 2.4441141360594263, 2.4436173935445673, 2.4436746492522943, 2.4444440498978697, 2.4424584771573423, 2.4416164840760906, 2.44096783641917, 2.439284539957066, 2.440874011903328, 2.4442119250796903, 2.442674490950191, 2.462834309944137, 2.449149736095013, 2.4457050225573154, 2.444781626127584, 2.440128531348289, 2.4416737184387456, 2.4396573532778136, 2.4387055077836743, 2.4406852965971773, 2.4402358583845887, 2.4403539031927592, 2.4403444438010027, 2.4395612754860942, 2.438801864138374, 2.4377214860622396, 2.43664574779769, 2.437151031376645, 2.4372894445728717, 2.436597535840295, 2.435780754813913, 2.436908734065062, 2.4363089586919826, 2.437091038604047, 2.4358222980029285, 2.4358165273920958, 2.435437778085164, 2.4349569648687845, 2.434687388457312, 2.435251426109298, 2.4351042156101985, 2.435376743222654, 2.435292626504291, 2.435477539841889, 2.4357665421292034, 2.4352824206224946, 2.4345094266613407, 2.4342064013471347, 2.4345658080044226, 2.434853973427837, 2.4352281943483765, 2.434912625377428, 2.434709901143883, 2.438223775750068, 2.440617290708319, 2.4434675271506183, 2.4463133148099363, 2.434393557288074, 2.438415466441756, 2.4379804713035758, 2.4349793836566214, 2.4362457928471497, 2.4350655555725096, 2.434561118159206, 2.4337426515575307, 2.4342686135176517, 2.4351258299433964, 2.436090270596608, 2.4349549581382797, 2.433807989606133, 2.4333608091734273, 2.4332349459982994, 2.4337455629078515, 2.4345020656468197, 2.435064892308668, 2.434253720042642, 2.4341462453532756, 2.433851907875014, 2.4334037847342675, 2.433289880184667, 2.432598679560166, 2.4317215334952977, 2.4319405730010546, 2.4316778041009295, 2.433857074165736, 2.4324888346865925, 2.4331074310279237, 2.4334599143425786, 2.433807152252667, 2.4353632599910915, 2.4401272272427224, 2.4415044652363114, 2.4386688502662235, 2.4393298237230745, 2.438290036630337, 2.436333098010116, 2.436107181376745, 2.435491846131593, 2.432175572256288, 2.4341539806898616, 2.4340859569808053, 2.434397081574865, 2.435759174064934, 2.439363558923929, 2.4427816736624717, 2.4375286920849057, 2.444233484385684, 2.4424324307843155, 2.4360150053271035, 2.4379227045869927, 2.434854579314559, 2.433686862687066, 2.434726307083694, 2.434876604883088, 2.4334301279800385, 2.433403606532291, 2.432428052244245, 2.434184274830123, 2.434402367711312, 2.4369132376794207, 2.4377777178919047, 2.4379847336598734, 2.434525248646981, 2.4323289682243394, 2.4318964310984836, 2.4305076829217054, 2.4316753574465335, 2.4324642766917264, 2.4308271853586, 2.4304432294207188, 2.4310211024979544, 2.4307631638505374, 2.431034281121632, 2.4319258900393694, 2.430364420771354, 2.4303288348156813, 2.430347317200177, 2.430229483590723, 2.4299145974662513, 2.4303043477099533, 2.429948226822965, 2.4301834100570523, 2.4304020804301425, 2.431143409760336, 2.4301798425905514, 2.430074543336089, 2.430346788762776, 2.4303685297956212, 2.42993580555769, 2.429468960536824, 2.4294037654414558, 2.42895756130101, 2.4291868265649375, 2.4288440796384085, 2.4290581826556634, 2.4286864831951855, 2.4291733164072524, 2.4288007719560816, 2.4289268404551354, 2.4288622647585076, 2.4281600062117685, 2.4293051808766517, 2.4284710004833934, 2.428247491828715, 2.4288615326617045, 2.427449773224472, 2.428306858544477, 2.4276239053424624, 2.427923381597844, 2.4283718843479667, 2.4279462859371113, 2.427520492287387, 2.427981758900981, 2.427985115462505, 2.427866880311124, 2.427578660252158, 2.4278862051405703, 2.427332374545338, 2.4275675649270876, 2.427405818923543, 2.4269518906086134, 2.427307130179121, 2.4273932412909285, 2.4266938200721504, 2.427586407632064, 2.4282024211217736, 2.4284818381009896, 2.427284462104343, 2.429803628647352, 2.4249335138215176, 2.4261469396477606, 2.426842298840595, 2.4259384969911046, 2.4256083071354233, 2.425174896721967, 2.4254193572293072, 2.42678110898153, 2.4248380381958197, 2.4259611197320834, 2.4264320142460067, 2.424877338879407, 2.4244070205845136, 2.423914004938803, 2.4240832666596837, 2.4230911036536433, 2.4230853577169307, 2.42339557632039, 2.423198033358282, 2.4231976171293788, 2.4234920204052934, 2.423772192686735, 2.4232788763496664, 2.4223001473248615, 2.4233170966592903, 2.4237248400153564, 2.423559106497794, 2.424318244667758, 2.4232116400583568, 2.4241146046522952, 2.4241024378633598, 2.4244533954949348, 2.422988061395759, 2.423566211077712, 2.422488660929874, 2.422815028451062, 2.422011551964699, 2.421987321097748, 2.423094963293056, 2.4227199833495905, 2.4222708522906293, 2.421920146883391, 2.422299898625399, 2.423148313538005, 2.4239672756782547, 2.425409389178611, 2.42691190247663, 2.4232965353822804, 2.421407537244918, 2.4209484357853444, 2.4212947821959823, 2.421977576876568, 2.421535351340041, 2.420821171864347, 2.420838924697782, 2.4230520259183534, 2.424284917862753, 2.4273382330822013], 'acc': [0.0784394248004322, 0.0891170428702474, 0.09034907632111525, 0.09691991760439451, 0.10882956912507756, 0.11457905615134895, 0.12156057508023613, 0.12279260851274526, 0.13429158201452643, 0.13963039047113435, 0.15030800890506416, 0.15277207422183034, 0.1564681731944701, 0.15728952684564024, 0.15195071837985294, 0.15893223888453029, 0.15441478449828325, 0.15400410705768108, 0.16180698123188725, 0.1650924023539134, 0.17166324463468313, 0.1728952767055872, 0.17084189000438127, 0.17166324367391012, 0.17207392225275295, 0.1683778240817773, 0.17125256664331934, 0.17864476277108554, 0.17577002120703397, 0.17207392227111165, 0.1716632448488682, 0.17577002024626096, 0.17371663293921727, 0.17494866600761177, 0.17043121060551558, 0.17618069782761334, 0.1774127299168761, 0.17905544177829852, 0.179876795625295, 0.17864476474770774, 0.17946611898635692, 0.17987679719190577, 0.17946611939636833, 0.17946611978802104, 0.17577002120703397, 0.17741273052271386, 0.17659137626570598, 0.17618069806015713, 0.1786447637502173, 0.17946611898635692, 0.18069815285641555, 0.1782340857588535, 0.1757700212253927, 0.17618069903928885, 0.17700205351048182, 0.17946612017967373, 0.17700205292300278, 0.17782340872826272, 0.18110882945863618, 0.18193018530061358, 0.18439425020736835, 0.178234087343823, 0.17987679699607942, 0.1827515389517837, 0.1675564688639964, 0.1819301843031231, 0.18193018469477582, 0.17535934301984385, 0.1864476390626641, 0.18069815107561968, 0.17618069825598345, 0.17946611798886647, 0.1848049284129172, 0.18357289497122872, 0.18234086231284563, 0.1831622169431475, 0.18234086331033608, 0.17864476355439093, 0.18193018449894946, 0.1843942512048588, 0.18439425140068516, 0.18193018410729678, 0.18480492762961181, 0.1839835735684303, 0.18316221715733255, 0.179466120161315, 0.1782340857588535, 0.18151950648922696, 0.18069815184056637, 0.18193018367892663, 0.18234086231284563, 0.1827515409284059, 0.18193018410729678, 0.18316221774481162, 0.18562628304321907, 0.18193018471313452, 0.18439424942406296, 0.18603696048382126, 0.18316221794063794, 0.183572894579576, 0.18562628386324193, 0.18685831669909264, 0.18316221733480018, 0.18357289616454553, 0.18357289575453412, 0.1856262830615778, 0.18357289596871917, 0.18110882967282124, 0.18316221674732114, 0.18069815303388317, 0.1815195078600114, 0.1790554409582757, 0.17864476335856458, 0.18028747361665878, 0.18069815086143462, 0.17987679660442674, 0.18151950746835868, 0.18151950746835868, 0.17946611920054198, 0.17618069745431936, 0.1749486653834153, 0.17823408712963792, 0.18193018489060217, 0.18028747361665878, 0.17946611978802104, 0.1794661184172366, 0.17987679621277403, 0.17823408613214747, 0.1802874752016283, 0.18234086152954024, 0.17946611918218328, 0.17535934221817973, 0.18028747363501751, 0.17864476474770774, 0.17987679699607942, 0.18151950609757425, 0.18110883045612663, 0.18069815127144603, 0.18110883006447395, 0.17577002024626096, 0.17741273152020434, 0.18069815262387176, 0.18028747422249655, 0.17987679660442674, 0.18357289438374969, 0.1737166317642592, 0.1757700196404232, 0.17946611998384737, 0.17823408632797383, 0.18028747363501751, 0.1811088286753308, 0.1802874740266702, 0.17782340931574178, 0.18398357178763441, 0.17946611879053057, 0.17782340851407766, 0.17741273009434372, 0.17412731114476612, 0.17289527770307764, 0.16796714471962906, 0.17577001983624954, 0.17741273050435516, 0.17741273050435516, 0.17905544078080807, 0.179876795625295, 0.18028747343919116, 0.17823408515301573, 0.1737166315317154, 0.17905544236577756, 0.1794661195921947, 0.17577002161704539, 0.17494866479593627, 0.1761806992167565, 0.1778234091015567, 0.17823408691545287, 0.18151950689923838, 0.17659137587405327, 0.18603695989634222, 0.17864476496189283, 0.1798767968186118, 0.17823408691545287, 0.17987679621277403, 0.18110882904862477, 0.18193018449894946, 0.18234086329197738, 0.1823408629186834, 0.17535934341149653, 0.18110882906698347, 0.18151950805583775, 0.18151950689923838, 0.1802874732250061, 0.18193018412565548, 0.18234086192119292, 0.1802874732433648, 0.1765913754824006, 0.17905544156411346, 0.1782340867196265, 0.1798767960169477, 0.17494866440428355, 0.1765913764431736, 0.17823408712963792, 0.18028747541581336, 0.18275153993091545, 0.1798767964269591, 0.18069815283805682, 0.17864476316273825, 0.1823408625270307, 0.18193018391147042, 0.17987679660442674, 0.18193018588809262, 0.18193018510478723, 0.18275153953926274, 0.18151950766418504, 0.18357289596871917, 0.18234086133371388, 0.181108830651953, 0.17905544236577756, 0.1802874732433648, 0.18110882928116856, 0.18234086331033608, 0.18151950688087964, 0.18275154034092686, 0.18151950650758567, 0.1786447645518814, 0.17905544177829852, 0.1835728953445227, 0.18398357180599315, 0.1811088296361038, 0.17946611898635692, 0.1815195082700228, 0.18151950727253235, 0.18069815283805682, 0.18357289497122872, 0.18398357337260393, 0.18316221676567987, 0.17946611978802104, 0.18316221874230207, 0.18193018510478723, 0.1819301854964399, 0.17535934358896416, 0.18193018391147042, 0.18110882906698347, 0.18480492880456992, 0.17453798717786645, 0.18110883045612663, 0.179466120161315, 0.18480492921458133, 0.1774127303085288, 0.18028747383084384, 0.18110883006447395, 0.18193018569226627, 0.17782340970739446, 0.180698152018034, 0.1827515409284059, 0.18069815205475143, 0.18316221696150622, 0.18151950705834727, 0.18275154053675322, 0.1843942504031947, 0.1831622181364643, 0.1827515397350891, 0.1839835727851249, 0.18398357376425664, 0.1839835720018195, 0.18398357339096266, 0.18521560485602895, 0.18275154112423225, 0.18357289614618683, 0.1843942511865001, 0.18151950648922696, 0.18480492900039625, 0.17782340794495732, 0.179876795625295, 0.18275154073257954, 0.18357289516705508, 0.17864476396440235, 0.18275153914761005, 0.18439425020736835, 0.18110882945863618, 0.1827515409284059, 0.1831622185464757, 0.18234086190283422, 0.18234086311450973, 0.18521560524768163, 0.1835728955770665, 0.17987679740609086, 0.179876795625295, 0.17823408613214747, 0.17782340792659862, 0.17700205407960215, 0.18480492823544958, 0.18357289555870776, 0.1843942511865001, 0.18357289575453412, 0.1864476377102384, 0.17946611820305153, 0.1848049286271023, 0.18316221753062653, 0.1843942507948474, 0.18603696067964762]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
