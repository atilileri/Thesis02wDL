{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf28.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 14:23:00 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '3', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ck', 'ib', 'ce', 'my', 'sg', 'aa', 'yd', 'ek', 'eg', 'mb', 'sk', 'eb', 'ds', 'by', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000185D7E7E240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000185D55D6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7092, Accuracy:0.0575, Validation Loss:2.7029, Validation Accuracy:0.0608\n",
    "Epoch #2: Loss:2.6993, Accuracy:0.0608, Validation Loss:2.6940, Validation Accuracy:0.0624\n",
    "Epoch #3: Loss:2.6916, Accuracy:0.0608, Validation Loss:2.6871, Validation Accuracy:0.0575\n",
    "Epoch #4: Loss:2.6841, Accuracy:0.0587, Validation Loss:2.6796, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6770, Accuracy:0.1055, Validation Loss:2.6724, Validation Accuracy:0.1067\n",
    "Epoch #6: Loss:2.6701, Accuracy:0.1109, Validation Loss:2.6654, Validation Accuracy:0.1051\n",
    "Epoch #7: Loss:2.6628, Accuracy:0.1138, Validation Loss:2.6581, Validation Accuracy:0.1051\n",
    "Epoch #8: Loss:2.6556, Accuracy:0.1199, Validation Loss:2.6504, Validation Accuracy:0.1199\n",
    "Epoch #9: Loss:2.6477, Accuracy:0.1179, Validation Loss:2.6411, Validation Accuracy:0.1199\n",
    "Epoch #10: Loss:2.6366, Accuracy:0.1236, Validation Loss:2.6322, Validation Accuracy:0.1232\n",
    "Epoch #11: Loss:2.6266, Accuracy:0.1248, Validation Loss:2.6195, Validation Accuracy:0.1215\n",
    "Epoch #12: Loss:2.6138, Accuracy:0.1339, Validation Loss:2.6065, Validation Accuracy:0.1379\n",
    "Epoch #13: Loss:2.6008, Accuracy:0.1466, Validation Loss:2.5963, Validation Accuracy:0.1642\n",
    "Epoch #14: Loss:2.5898, Accuracy:0.1598, Validation Loss:2.5801, Validation Accuracy:0.1642\n",
    "Epoch #15: Loss:2.5767, Accuracy:0.1552, Validation Loss:2.5658, Validation Accuracy:0.1642\n",
    "Epoch #16: Loss:2.5610, Accuracy:0.1606, Validation Loss:2.5571, Validation Accuracy:0.1741\n",
    "Epoch #17: Loss:2.5485, Accuracy:0.1598, Validation Loss:2.6186, Validation Accuracy:0.1264\n",
    "Epoch #18: Loss:2.5641, Accuracy:0.1520, Validation Loss:2.5396, Validation Accuracy:0.1757\n",
    "Epoch #19: Loss:2.5301, Accuracy:0.1610, Validation Loss:2.5252, Validation Accuracy:0.1691\n",
    "Epoch #20: Loss:2.5211, Accuracy:0.1598, Validation Loss:2.5154, Validation Accuracy:0.1724\n",
    "Epoch #21: Loss:2.5112, Accuracy:0.1606, Validation Loss:2.5149, Validation Accuracy:0.1741\n",
    "Epoch #22: Loss:2.5063, Accuracy:0.1610, Validation Loss:2.5034, Validation Accuracy:0.1708\n",
    "Epoch #23: Loss:2.5012, Accuracy:0.1614, Validation Loss:2.4962, Validation Accuracy:0.1724\n",
    "Epoch #24: Loss:2.4929, Accuracy:0.1610, Validation Loss:2.4956, Validation Accuracy:0.1741\n",
    "Epoch #25: Loss:2.4890, Accuracy:0.1651, Validation Loss:2.4871, Validation Accuracy:0.1741\n",
    "Epoch #26: Loss:2.4829, Accuracy:0.1634, Validation Loss:2.4843, Validation Accuracy:0.1872\n",
    "Epoch #27: Loss:2.4808, Accuracy:0.1655, Validation Loss:2.4813, Validation Accuracy:0.1741\n",
    "Epoch #28: Loss:2.4765, Accuracy:0.1704, Validation Loss:2.4776, Validation Accuracy:0.1806\n",
    "Epoch #29: Loss:2.4737, Accuracy:0.1725, Validation Loss:2.4791, Validation Accuracy:0.1757\n",
    "Epoch #30: Loss:2.4723, Accuracy:0.1737, Validation Loss:2.4769, Validation Accuracy:0.1741\n",
    "Epoch #31: Loss:2.4702, Accuracy:0.1733, Validation Loss:2.4765, Validation Accuracy:0.1773\n",
    "Epoch #32: Loss:2.4676, Accuracy:0.1737, Validation Loss:2.4740, Validation Accuracy:0.1708\n",
    "Epoch #33: Loss:2.4662, Accuracy:0.1688, Validation Loss:2.4693, Validation Accuracy:0.1823\n",
    "Epoch #34: Loss:2.4659, Accuracy:0.1708, Validation Loss:2.4650, Validation Accuracy:0.1790\n",
    "Epoch #35: Loss:2.4645, Accuracy:0.1717, Validation Loss:2.4662, Validation Accuracy:0.1806\n",
    "Epoch #36: Loss:2.4647, Accuracy:0.1745, Validation Loss:2.4643, Validation Accuracy:0.1806\n",
    "Epoch #37: Loss:2.4628, Accuracy:0.1676, Validation Loss:2.4631, Validation Accuracy:0.1773\n",
    "Epoch #38: Loss:2.4620, Accuracy:0.1684, Validation Loss:2.4636, Validation Accuracy:0.1806\n",
    "Epoch #39: Loss:2.4614, Accuracy:0.1729, Validation Loss:2.4616, Validation Accuracy:0.1806\n",
    "Epoch #40: Loss:2.4596, Accuracy:0.1725, Validation Loss:2.4614, Validation Accuracy:0.1790\n",
    "Epoch #41: Loss:2.4584, Accuracy:0.1671, Validation Loss:2.4576, Validation Accuracy:0.1741\n",
    "Epoch #42: Loss:2.4586, Accuracy:0.1684, Validation Loss:2.4613, Validation Accuracy:0.1741\n",
    "Epoch #43: Loss:2.4573, Accuracy:0.1758, Validation Loss:2.4561, Validation Accuracy:0.1790\n",
    "Epoch #44: Loss:2.4564, Accuracy:0.1745, Validation Loss:2.4572, Validation Accuracy:0.1790\n",
    "Epoch #45: Loss:2.4563, Accuracy:0.1749, Validation Loss:2.4568, Validation Accuracy:0.1757\n",
    "Epoch #46: Loss:2.4545, Accuracy:0.1737, Validation Loss:2.4532, Validation Accuracy:0.1790\n",
    "Epoch #47: Loss:2.4541, Accuracy:0.1745, Validation Loss:2.4558, Validation Accuracy:0.1675\n",
    "Epoch #48: Loss:2.4552, Accuracy:0.1762, Validation Loss:2.4532, Validation Accuracy:0.1773\n",
    "Epoch #49: Loss:2.4529, Accuracy:0.1754, Validation Loss:2.4582, Validation Accuracy:0.1724\n",
    "Epoch #50: Loss:2.4527, Accuracy:0.1725, Validation Loss:2.4543, Validation Accuracy:0.1708\n",
    "Epoch #51: Loss:2.4521, Accuracy:0.1729, Validation Loss:2.4553, Validation Accuracy:0.1642\n",
    "Epoch #52: Loss:2.4509, Accuracy:0.1778, Validation Loss:2.4531, Validation Accuracy:0.1658\n",
    "Epoch #53: Loss:2.4491, Accuracy:0.1786, Validation Loss:2.4512, Validation Accuracy:0.1773\n",
    "Epoch #54: Loss:2.4498, Accuracy:0.1758, Validation Loss:2.4484, Validation Accuracy:0.1773\n",
    "Epoch #55: Loss:2.4503, Accuracy:0.1786, Validation Loss:2.4496, Validation Accuracy:0.1790\n",
    "Epoch #56: Loss:2.4486, Accuracy:0.1754, Validation Loss:2.4496, Validation Accuracy:0.1741\n",
    "Epoch #57: Loss:2.4499, Accuracy:0.1754, Validation Loss:2.4559, Validation Accuracy:0.1773\n",
    "Epoch #58: Loss:2.4511, Accuracy:0.1725, Validation Loss:2.4517, Validation Accuracy:0.1691\n",
    "Epoch #59: Loss:2.4494, Accuracy:0.1733, Validation Loss:2.4530, Validation Accuracy:0.1675\n",
    "Epoch #60: Loss:2.4474, Accuracy:0.1770, Validation Loss:2.4520, Validation Accuracy:0.1708\n",
    "Epoch #61: Loss:2.4487, Accuracy:0.1762, Validation Loss:2.4535, Validation Accuracy:0.1773\n",
    "Epoch #62: Loss:2.4495, Accuracy:0.1729, Validation Loss:2.4520, Validation Accuracy:0.1741\n",
    "Epoch #63: Loss:2.4491, Accuracy:0.1741, Validation Loss:2.4516, Validation Accuracy:0.1691\n",
    "Epoch #64: Loss:2.4505, Accuracy:0.1770, Validation Loss:2.4548, Validation Accuracy:0.1773\n",
    "Epoch #65: Loss:2.4501, Accuracy:0.1729, Validation Loss:2.4526, Validation Accuracy:0.1724\n",
    "Epoch #66: Loss:2.4483, Accuracy:0.1762, Validation Loss:2.4523, Validation Accuracy:0.1757\n",
    "Epoch #67: Loss:2.4477, Accuracy:0.1758, Validation Loss:2.4505, Validation Accuracy:0.1724\n",
    "Epoch #68: Loss:2.4497, Accuracy:0.1741, Validation Loss:2.4501, Validation Accuracy:0.1757\n",
    "Epoch #69: Loss:2.4478, Accuracy:0.1737, Validation Loss:2.4489, Validation Accuracy:0.1823\n",
    "Epoch #70: Loss:2.4469, Accuracy:0.1729, Validation Loss:2.4479, Validation Accuracy:0.1773\n",
    "Epoch #71: Loss:2.4453, Accuracy:0.1741, Validation Loss:2.4478, Validation Accuracy:0.1823\n",
    "Epoch #72: Loss:2.4461, Accuracy:0.1721, Validation Loss:2.4473, Validation Accuracy:0.1806\n",
    "Epoch #73: Loss:2.4468, Accuracy:0.1737, Validation Loss:2.4493, Validation Accuracy:0.1856\n",
    "Epoch #74: Loss:2.4492, Accuracy:0.1749, Validation Loss:2.4480, Validation Accuracy:0.1839\n",
    "Epoch #75: Loss:2.4492, Accuracy:0.1741, Validation Loss:2.4459, Validation Accuracy:0.1691\n",
    "Epoch #76: Loss:2.4441, Accuracy:0.1770, Validation Loss:2.4496, Validation Accuracy:0.1872\n",
    "Epoch #77: Loss:2.4444, Accuracy:0.1774, Validation Loss:2.4460, Validation Accuracy:0.1658\n",
    "Epoch #78: Loss:2.4444, Accuracy:0.1713, Validation Loss:2.4463, Validation Accuracy:0.1823\n",
    "Epoch #79: Loss:2.4442, Accuracy:0.1754, Validation Loss:2.4463, Validation Accuracy:0.1856\n",
    "Epoch #80: Loss:2.4430, Accuracy:0.1725, Validation Loss:2.4442, Validation Accuracy:0.1724\n",
    "Epoch #81: Loss:2.4433, Accuracy:0.1733, Validation Loss:2.4476, Validation Accuracy:0.1856\n",
    "Epoch #82: Loss:2.4447, Accuracy:0.1758, Validation Loss:2.4455, Validation Accuracy:0.1724\n",
    "Epoch #83: Loss:2.4438, Accuracy:0.1721, Validation Loss:2.4474, Validation Accuracy:0.1741\n",
    "Epoch #84: Loss:2.4436, Accuracy:0.1745, Validation Loss:2.4462, Validation Accuracy:0.1757\n",
    "Epoch #85: Loss:2.4426, Accuracy:0.1729, Validation Loss:2.4457, Validation Accuracy:0.1757\n",
    "Epoch #86: Loss:2.4425, Accuracy:0.1786, Validation Loss:2.4468, Validation Accuracy:0.1773\n",
    "Epoch #87: Loss:2.4425, Accuracy:0.1745, Validation Loss:2.4467, Validation Accuracy:0.1724\n",
    "Epoch #88: Loss:2.4430, Accuracy:0.1754, Validation Loss:2.4482, Validation Accuracy:0.1823\n",
    "Epoch #89: Loss:2.4428, Accuracy:0.1799, Validation Loss:2.4461, Validation Accuracy:0.1773\n",
    "Epoch #90: Loss:2.4429, Accuracy:0.1815, Validation Loss:2.4485, Validation Accuracy:0.1806\n",
    "Epoch #91: Loss:2.4447, Accuracy:0.1782, Validation Loss:2.4493, Validation Accuracy:0.1773\n",
    "Epoch #92: Loss:2.4445, Accuracy:0.1758, Validation Loss:2.4481, Validation Accuracy:0.1806\n",
    "Epoch #93: Loss:2.4448, Accuracy:0.1762, Validation Loss:2.4522, Validation Accuracy:0.1658\n",
    "Epoch #94: Loss:2.4433, Accuracy:0.1782, Validation Loss:2.4486, Validation Accuracy:0.1724\n",
    "Epoch #95: Loss:2.4460, Accuracy:0.1713, Validation Loss:2.4509, Validation Accuracy:0.1790\n",
    "Epoch #96: Loss:2.4414, Accuracy:0.1791, Validation Loss:2.4485, Validation Accuracy:0.1708\n",
    "Epoch #97: Loss:2.4440, Accuracy:0.1725, Validation Loss:2.4497, Validation Accuracy:0.1741\n",
    "Epoch #98: Loss:2.4460, Accuracy:0.1778, Validation Loss:2.4524, Validation Accuracy:0.1691\n",
    "Epoch #99: Loss:2.4425, Accuracy:0.1795, Validation Loss:2.4454, Validation Accuracy:0.1790\n",
    "Epoch #100: Loss:2.4463, Accuracy:0.1713, Validation Loss:2.4462, Validation Accuracy:0.1642\n",
    "Epoch #101: Loss:2.4436, Accuracy:0.1786, Validation Loss:2.4508, Validation Accuracy:0.1691\n",
    "Epoch #102: Loss:2.4416, Accuracy:0.1766, Validation Loss:2.4443, Validation Accuracy:0.1741\n",
    "Epoch #103: Loss:2.4430, Accuracy:0.1762, Validation Loss:2.4442, Validation Accuracy:0.1741\n",
    "Epoch #104: Loss:2.4432, Accuracy:0.1745, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #105: Loss:2.4417, Accuracy:0.1758, Validation Loss:2.4443, Validation Accuracy:0.1724\n",
    "Epoch #106: Loss:2.4408, Accuracy:0.1766, Validation Loss:2.4438, Validation Accuracy:0.1691\n",
    "Epoch #107: Loss:2.4404, Accuracy:0.1688, Validation Loss:2.4451, Validation Accuracy:0.1708\n",
    "Epoch #108: Loss:2.4482, Accuracy:0.1717, Validation Loss:2.4608, Validation Accuracy:0.1872\n",
    "Epoch #109: Loss:2.4476, Accuracy:0.1717, Validation Loss:2.4566, Validation Accuracy:0.1724\n",
    "Epoch #110: Loss:2.4467, Accuracy:0.1729, Validation Loss:2.4439, Validation Accuracy:0.1806\n",
    "Epoch #111: Loss:2.4428, Accuracy:0.1766, Validation Loss:2.4447, Validation Accuracy:0.1773\n",
    "Epoch #112: Loss:2.4420, Accuracy:0.1795, Validation Loss:2.4453, Validation Accuracy:0.1773\n",
    "Epoch #113: Loss:2.4399, Accuracy:0.1799, Validation Loss:2.4446, Validation Accuracy:0.1757\n",
    "Epoch #114: Loss:2.4415, Accuracy:0.1782, Validation Loss:2.4447, Validation Accuracy:0.1757\n",
    "Epoch #115: Loss:2.4411, Accuracy:0.1778, Validation Loss:2.4429, Validation Accuracy:0.1790\n",
    "Epoch #116: Loss:2.4419, Accuracy:0.1803, Validation Loss:2.4414, Validation Accuracy:0.1773\n",
    "Epoch #117: Loss:2.4407, Accuracy:0.1815, Validation Loss:2.4407, Validation Accuracy:0.1856\n",
    "Epoch #118: Loss:2.4396, Accuracy:0.1782, Validation Loss:2.4404, Validation Accuracy:0.1806\n",
    "Epoch #119: Loss:2.4400, Accuracy:0.1766, Validation Loss:2.4395, Validation Accuracy:0.1823\n",
    "Epoch #120: Loss:2.4403, Accuracy:0.1774, Validation Loss:2.4410, Validation Accuracy:0.1839\n",
    "Epoch #121: Loss:2.4404, Accuracy:0.1791, Validation Loss:2.4400, Validation Accuracy:0.1806\n",
    "Epoch #122: Loss:2.4399, Accuracy:0.1770, Validation Loss:2.4418, Validation Accuracy:0.1806\n",
    "Epoch #123: Loss:2.4391, Accuracy:0.1766, Validation Loss:2.4385, Validation Accuracy:0.1790\n",
    "Epoch #124: Loss:2.4400, Accuracy:0.1774, Validation Loss:2.4387, Validation Accuracy:0.1823\n",
    "Epoch #125: Loss:2.4393, Accuracy:0.1766, Validation Loss:2.4388, Validation Accuracy:0.1839\n",
    "Epoch #126: Loss:2.4389, Accuracy:0.1795, Validation Loss:2.4390, Validation Accuracy:0.1790\n",
    "Epoch #127: Loss:2.4394, Accuracy:0.1762, Validation Loss:2.4416, Validation Accuracy:0.1839\n",
    "Epoch #128: Loss:2.4390, Accuracy:0.1791, Validation Loss:2.4421, Validation Accuracy:0.1856\n",
    "Epoch #129: Loss:2.4398, Accuracy:0.1770, Validation Loss:2.4415, Validation Accuracy:0.1790\n",
    "Epoch #130: Loss:2.4398, Accuracy:0.1803, Validation Loss:2.4436, Validation Accuracy:0.1839\n",
    "Epoch #131: Loss:2.4372, Accuracy:0.1803, Validation Loss:2.4422, Validation Accuracy:0.1790\n",
    "Epoch #132: Loss:2.4385, Accuracy:0.1782, Validation Loss:2.4432, Validation Accuracy:0.1839\n",
    "Epoch #133: Loss:2.4386, Accuracy:0.1803, Validation Loss:2.4400, Validation Accuracy:0.1773\n",
    "Epoch #134: Loss:2.4376, Accuracy:0.1799, Validation Loss:2.4400, Validation Accuracy:0.1806\n",
    "Epoch #135: Loss:2.4373, Accuracy:0.1795, Validation Loss:2.4399, Validation Accuracy:0.1823\n",
    "Epoch #136: Loss:2.4369, Accuracy:0.1811, Validation Loss:2.4394, Validation Accuracy:0.1790\n",
    "Epoch #137: Loss:2.4368, Accuracy:0.1795, Validation Loss:2.4390, Validation Accuracy:0.1806\n",
    "Epoch #138: Loss:2.4375, Accuracy:0.1799, Validation Loss:2.4394, Validation Accuracy:0.1790\n",
    "Epoch #139: Loss:2.4387, Accuracy:0.1811, Validation Loss:2.4410, Validation Accuracy:0.1856\n",
    "Epoch #140: Loss:2.4365, Accuracy:0.1807, Validation Loss:2.4393, Validation Accuracy:0.1806\n",
    "Epoch #141: Loss:2.4376, Accuracy:0.1819, Validation Loss:2.4397, Validation Accuracy:0.1856\n",
    "Epoch #142: Loss:2.4370, Accuracy:0.1815, Validation Loss:2.4389, Validation Accuracy:0.1823\n",
    "Epoch #143: Loss:2.4367, Accuracy:0.1811, Validation Loss:2.4401, Validation Accuracy:0.1856\n",
    "Epoch #144: Loss:2.4368, Accuracy:0.1774, Validation Loss:2.4387, Validation Accuracy:0.1823\n",
    "Epoch #145: Loss:2.4367, Accuracy:0.1819, Validation Loss:2.4388, Validation Accuracy:0.1872\n",
    "Epoch #146: Loss:2.4359, Accuracy:0.1807, Validation Loss:2.4399, Validation Accuracy:0.1839\n",
    "Epoch #147: Loss:2.4355, Accuracy:0.1799, Validation Loss:2.4392, Validation Accuracy:0.1823\n",
    "Epoch #148: Loss:2.4361, Accuracy:0.1803, Validation Loss:2.4404, Validation Accuracy:0.1856\n",
    "Epoch #149: Loss:2.4364, Accuracy:0.1811, Validation Loss:2.4390, Validation Accuracy:0.1790\n",
    "Epoch #150: Loss:2.4363, Accuracy:0.1778, Validation Loss:2.4389, Validation Accuracy:0.1823\n",
    "Epoch #151: Loss:2.4372, Accuracy:0.1778, Validation Loss:2.4415, Validation Accuracy:0.1790\n",
    "Epoch #152: Loss:2.4357, Accuracy:0.1778, Validation Loss:2.4394, Validation Accuracy:0.1773\n",
    "Epoch #153: Loss:2.4356, Accuracy:0.1811, Validation Loss:2.4412, Validation Accuracy:0.1856\n",
    "Epoch #154: Loss:2.4357, Accuracy:0.1811, Validation Loss:2.4407, Validation Accuracy:0.1872\n",
    "Epoch #155: Loss:2.4354, Accuracy:0.1828, Validation Loss:2.4407, Validation Accuracy:0.1872\n",
    "Epoch #156: Loss:2.4354, Accuracy:0.1815, Validation Loss:2.4420, Validation Accuracy:0.1757\n",
    "Epoch #157: Loss:2.4352, Accuracy:0.1774, Validation Loss:2.4407, Validation Accuracy:0.1806\n",
    "Epoch #158: Loss:2.4356, Accuracy:0.1799, Validation Loss:2.4421, Validation Accuracy:0.1806\n",
    "Epoch #159: Loss:2.4352, Accuracy:0.1811, Validation Loss:2.4413, Validation Accuracy:0.1872\n",
    "Epoch #160: Loss:2.4348, Accuracy:0.1815, Validation Loss:2.4412, Validation Accuracy:0.1872\n",
    "Epoch #161: Loss:2.4359, Accuracy:0.1823, Validation Loss:2.4424, Validation Accuracy:0.1741\n",
    "Epoch #162: Loss:2.4338, Accuracy:0.1811, Validation Loss:2.4406, Validation Accuracy:0.1823\n",
    "Epoch #163: Loss:2.4343, Accuracy:0.1828, Validation Loss:2.4418, Validation Accuracy:0.1806\n",
    "Epoch #164: Loss:2.4344, Accuracy:0.1811, Validation Loss:2.4412, Validation Accuracy:0.1872\n",
    "Epoch #165: Loss:2.4346, Accuracy:0.1807, Validation Loss:2.4416, Validation Accuracy:0.1806\n",
    "Epoch #166: Loss:2.4340, Accuracy:0.1819, Validation Loss:2.4431, Validation Accuracy:0.1741\n",
    "Epoch #167: Loss:2.4345, Accuracy:0.1815, Validation Loss:2.4404, Validation Accuracy:0.1872\n",
    "Epoch #168: Loss:2.4361, Accuracy:0.1811, Validation Loss:2.4415, Validation Accuracy:0.1806\n",
    "Epoch #169: Loss:2.4342, Accuracy:0.1803, Validation Loss:2.4442, Validation Accuracy:0.1741\n",
    "Epoch #170: Loss:2.4335, Accuracy:0.1819, Validation Loss:2.4415, Validation Accuracy:0.1823\n",
    "Epoch #171: Loss:2.4334, Accuracy:0.1791, Validation Loss:2.4426, Validation Accuracy:0.1806\n",
    "Epoch #172: Loss:2.4347, Accuracy:0.1836, Validation Loss:2.4411, Validation Accuracy:0.1872\n",
    "Epoch #173: Loss:2.4336, Accuracy:0.1815, Validation Loss:2.4417, Validation Accuracy:0.1806\n",
    "Epoch #174: Loss:2.4343, Accuracy:0.1844, Validation Loss:2.4429, Validation Accuracy:0.1806\n",
    "Epoch #175: Loss:2.4339, Accuracy:0.1819, Validation Loss:2.4413, Validation Accuracy:0.1872\n",
    "Epoch #176: Loss:2.4337, Accuracy:0.1860, Validation Loss:2.4425, Validation Accuracy:0.1872\n",
    "Epoch #177: Loss:2.4340, Accuracy:0.1823, Validation Loss:2.4415, Validation Accuracy:0.1872\n",
    "Epoch #178: Loss:2.4328, Accuracy:0.1823, Validation Loss:2.4427, Validation Accuracy:0.1806\n",
    "Epoch #179: Loss:2.4329, Accuracy:0.1815, Validation Loss:2.4420, Validation Accuracy:0.1872\n",
    "Epoch #180: Loss:2.4333, Accuracy:0.1823, Validation Loss:2.4429, Validation Accuracy:0.1806\n",
    "Epoch #181: Loss:2.4331, Accuracy:0.1795, Validation Loss:2.4415, Validation Accuracy:0.1806\n",
    "Epoch #182: Loss:2.4330, Accuracy:0.1811, Validation Loss:2.4415, Validation Accuracy:0.1872\n",
    "Epoch #183: Loss:2.4337, Accuracy:0.1823, Validation Loss:2.4425, Validation Accuracy:0.1806\n",
    "Epoch #184: Loss:2.4336, Accuracy:0.1836, Validation Loss:2.4413, Validation Accuracy:0.1872\n",
    "Epoch #185: Loss:2.4333, Accuracy:0.1823, Validation Loss:2.4424, Validation Accuracy:0.1872\n",
    "Epoch #186: Loss:2.4333, Accuracy:0.1828, Validation Loss:2.4421, Validation Accuracy:0.1872\n",
    "Epoch #187: Loss:2.4319, Accuracy:0.1823, Validation Loss:2.4416, Validation Accuracy:0.1872\n",
    "Epoch #188: Loss:2.4323, Accuracy:0.1815, Validation Loss:2.4438, Validation Accuracy:0.1757\n",
    "Epoch #189: Loss:2.4324, Accuracy:0.1815, Validation Loss:2.4421, Validation Accuracy:0.1872\n",
    "Epoch #190: Loss:2.4326, Accuracy:0.1832, Validation Loss:2.4436, Validation Accuracy:0.1806\n",
    "Epoch #191: Loss:2.4324, Accuracy:0.1828, Validation Loss:2.4418, Validation Accuracy:0.1872\n",
    "Epoch #192: Loss:2.4320, Accuracy:0.1815, Validation Loss:2.4441, Validation Accuracy:0.1741\n",
    "Epoch #193: Loss:2.4314, Accuracy:0.1807, Validation Loss:2.4421, Validation Accuracy:0.1872\n",
    "Epoch #194: Loss:2.4322, Accuracy:0.1803, Validation Loss:2.4438, Validation Accuracy:0.1741\n",
    "Epoch #195: Loss:2.4321, Accuracy:0.1819, Validation Loss:2.4418, Validation Accuracy:0.1872\n",
    "Epoch #196: Loss:2.4310, Accuracy:0.1819, Validation Loss:2.4436, Validation Accuracy:0.1806\n",
    "Epoch #197: Loss:2.4315, Accuracy:0.1832, Validation Loss:2.4426, Validation Accuracy:0.1872\n",
    "Epoch #198: Loss:2.4307, Accuracy:0.1819, Validation Loss:2.4425, Validation Accuracy:0.1872\n",
    "Epoch #199: Loss:2.4311, Accuracy:0.1819, Validation Loss:2.4428, Validation Accuracy:0.1872\n",
    "Epoch #200: Loss:2.4312, Accuracy:0.1819, Validation Loss:2.4426, Validation Accuracy:0.1872\n",
    "Epoch #201: Loss:2.4308, Accuracy:0.1795, Validation Loss:2.4433, Validation Accuracy:0.1806\n",
    "Epoch #202: Loss:2.4310, Accuracy:0.1823, Validation Loss:2.4430, Validation Accuracy:0.1872\n",
    "Epoch #203: Loss:2.4308, Accuracy:0.1819, Validation Loss:2.4433, Validation Accuracy:0.1806\n",
    "Epoch #204: Loss:2.4308, Accuracy:0.1828, Validation Loss:2.4422, Validation Accuracy:0.1872\n",
    "Epoch #205: Loss:2.4302, Accuracy:0.1819, Validation Loss:2.4425, Validation Accuracy:0.1872\n",
    "Epoch #206: Loss:2.4301, Accuracy:0.1836, Validation Loss:2.4440, Validation Accuracy:0.1757\n",
    "Epoch #207: Loss:2.4301, Accuracy:0.1795, Validation Loss:2.4426, Validation Accuracy:0.1872\n",
    "Epoch #208: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4437, Validation Accuracy:0.1888\n",
    "Epoch #209: Loss:2.4300, Accuracy:0.1828, Validation Loss:2.4434, Validation Accuracy:0.1888\n",
    "Epoch #210: Loss:2.4298, Accuracy:0.1815, Validation Loss:2.4430, Validation Accuracy:0.1888\n",
    "Epoch #211: Loss:2.4315, Accuracy:0.1819, Validation Loss:2.4447, Validation Accuracy:0.1757\n",
    "Epoch #212: Loss:2.4328, Accuracy:0.1807, Validation Loss:2.4432, Validation Accuracy:0.1872\n",
    "Epoch #213: Loss:2.4301, Accuracy:0.1819, Validation Loss:2.4430, Validation Accuracy:0.1872\n",
    "Epoch #214: Loss:2.4298, Accuracy:0.1819, Validation Loss:2.4441, Validation Accuracy:0.1806\n",
    "Epoch #215: Loss:2.4302, Accuracy:0.1828, Validation Loss:2.4427, Validation Accuracy:0.1872\n",
    "Epoch #216: Loss:2.4302, Accuracy:0.1819, Validation Loss:2.4430, Validation Accuracy:0.1872\n",
    "Epoch #217: Loss:2.4299, Accuracy:0.1819, Validation Loss:2.4452, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.4307, Accuracy:0.1811, Validation Loss:2.4428, Validation Accuracy:0.1872\n",
    "Epoch #219: Loss:2.4298, Accuracy:0.1815, Validation Loss:2.4438, Validation Accuracy:0.1888\n",
    "Epoch #220: Loss:2.4292, Accuracy:0.1819, Validation Loss:2.4431, Validation Accuracy:0.1872\n",
    "Epoch #221: Loss:2.4302, Accuracy:0.1799, Validation Loss:2.4439, Validation Accuracy:0.1823\n",
    "Epoch #222: Loss:2.4293, Accuracy:0.1819, Validation Loss:2.4435, Validation Accuracy:0.1872\n",
    "Epoch #223: Loss:2.4298, Accuracy:0.1815, Validation Loss:2.4439, Validation Accuracy:0.1872\n",
    "Epoch #224: Loss:2.4287, Accuracy:0.1819, Validation Loss:2.4429, Validation Accuracy:0.1872\n",
    "Epoch #225: Loss:2.4299, Accuracy:0.1819, Validation Loss:2.4466, Validation Accuracy:0.1757\n",
    "Epoch #226: Loss:2.4312, Accuracy:0.1799, Validation Loss:2.4432, Validation Accuracy:0.1757\n",
    "Epoch #227: Loss:2.4286, Accuracy:0.1807, Validation Loss:2.4435, Validation Accuracy:0.1757\n",
    "Epoch #228: Loss:2.4288, Accuracy:0.1799, Validation Loss:2.4443, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4289, Accuracy:0.1807, Validation Loss:2.4442, Validation Accuracy:0.1790\n",
    "Epoch #230: Loss:2.4282, Accuracy:0.1836, Validation Loss:2.4443, Validation Accuracy:0.1790\n",
    "Epoch #231: Loss:2.4284, Accuracy:0.1815, Validation Loss:2.4444, Validation Accuracy:0.1888\n",
    "Epoch #232: Loss:2.4289, Accuracy:0.1791, Validation Loss:2.4434, Validation Accuracy:0.1757\n",
    "Epoch #233: Loss:2.4290, Accuracy:0.1815, Validation Loss:2.4435, Validation Accuracy:0.1757\n",
    "Epoch #234: Loss:2.4286, Accuracy:0.1811, Validation Loss:2.4434, Validation Accuracy:0.1757\n",
    "Epoch #235: Loss:2.4285, Accuracy:0.1741, Validation Loss:2.4448, Validation Accuracy:0.1806\n",
    "Epoch #236: Loss:2.4291, Accuracy:0.1795, Validation Loss:2.4451, Validation Accuracy:0.1675\n",
    "Epoch #237: Loss:2.4282, Accuracy:0.1819, Validation Loss:2.4442, Validation Accuracy:0.1757\n",
    "Epoch #238: Loss:2.4276, Accuracy:0.1807, Validation Loss:2.4439, Validation Accuracy:0.1741\n",
    "Epoch #239: Loss:2.4291, Accuracy:0.1803, Validation Loss:2.4441, Validation Accuracy:0.1757\n",
    "Epoch #240: Loss:2.4293, Accuracy:0.1819, Validation Loss:2.4452, Validation Accuracy:0.1691\n",
    "Epoch #241: Loss:2.4303, Accuracy:0.1823, Validation Loss:2.4445, Validation Accuracy:0.1757\n",
    "Epoch #242: Loss:2.4305, Accuracy:0.1799, Validation Loss:2.4448, Validation Accuracy:0.1757\n",
    "Epoch #243: Loss:2.4287, Accuracy:0.1754, Validation Loss:2.4449, Validation Accuracy:0.1757\n",
    "Epoch #244: Loss:2.4278, Accuracy:0.1811, Validation Loss:2.4444, Validation Accuracy:0.1872\n",
    "Epoch #245: Loss:2.4285, Accuracy:0.1799, Validation Loss:2.4451, Validation Accuracy:0.1757\n",
    "Epoch #246: Loss:2.4277, Accuracy:0.1823, Validation Loss:2.4446, Validation Accuracy:0.1757\n",
    "Epoch #247: Loss:2.4286, Accuracy:0.1786, Validation Loss:2.4448, Validation Accuracy:0.1691\n",
    "Epoch #248: Loss:2.4275, Accuracy:0.1815, Validation Loss:2.4443, Validation Accuracy:0.1757\n",
    "Epoch #249: Loss:2.4294, Accuracy:0.1795, Validation Loss:2.4449, Validation Accuracy:0.1675\n",
    "Epoch #250: Loss:2.4273, Accuracy:0.1840, Validation Loss:2.4444, Validation Accuracy:0.1757\n",
    "Epoch #251: Loss:2.4280, Accuracy:0.1758, Validation Loss:2.4461, Validation Accuracy:0.1626\n",
    "Epoch #252: Loss:2.4272, Accuracy:0.1819, Validation Loss:2.4450, Validation Accuracy:0.1872\n",
    "Epoch #253: Loss:2.4273, Accuracy:0.1782, Validation Loss:2.4457, Validation Accuracy:0.1609\n",
    "Epoch #254: Loss:2.4272, Accuracy:0.1819, Validation Loss:2.4439, Validation Accuracy:0.1757\n",
    "Epoch #255: Loss:2.4267, Accuracy:0.1799, Validation Loss:2.4459, Validation Accuracy:0.1609\n",
    "Epoch #256: Loss:2.4278, Accuracy:0.1832, Validation Loss:2.4452, Validation Accuracy:0.1872\n",
    "Epoch #257: Loss:2.4273, Accuracy:0.1803, Validation Loss:2.4464, Validation Accuracy:0.1691\n",
    "Epoch #258: Loss:2.4272, Accuracy:0.1762, Validation Loss:2.4450, Validation Accuracy:0.1757\n",
    "Epoch #259: Loss:2.4277, Accuracy:0.1807, Validation Loss:2.4453, Validation Accuracy:0.1757\n",
    "Epoch #260: Loss:2.4294, Accuracy:0.1786, Validation Loss:2.4463, Validation Accuracy:0.1691\n",
    "Epoch #261: Loss:2.4294, Accuracy:0.1782, Validation Loss:2.4449, Validation Accuracy:0.1757\n",
    "Epoch #262: Loss:2.4287, Accuracy:0.1749, Validation Loss:2.4491, Validation Accuracy:0.1626\n",
    "Epoch #263: Loss:2.4272, Accuracy:0.1840, Validation Loss:2.4460, Validation Accuracy:0.1872\n",
    "Epoch #264: Loss:2.4259, Accuracy:0.1832, Validation Loss:2.4478, Validation Accuracy:0.1609\n",
    "Epoch #265: Loss:2.4314, Accuracy:0.1799, Validation Loss:2.4443, Validation Accuracy:0.1675\n",
    "Epoch #266: Loss:2.4271, Accuracy:0.1823, Validation Loss:2.4457, Validation Accuracy:0.1757\n",
    "Epoch #267: Loss:2.4265, Accuracy:0.1778, Validation Loss:2.4481, Validation Accuracy:0.1691\n",
    "Epoch #268: Loss:2.4264, Accuracy:0.1791, Validation Loss:2.4451, Validation Accuracy:0.1741\n",
    "Epoch #269: Loss:2.4271, Accuracy:0.1791, Validation Loss:2.4467, Validation Accuracy:0.1691\n",
    "Epoch #270: Loss:2.4260, Accuracy:0.1807, Validation Loss:2.4452, Validation Accuracy:0.1757\n",
    "Epoch #271: Loss:2.4262, Accuracy:0.1791, Validation Loss:2.4459, Validation Accuracy:0.1675\n",
    "Epoch #272: Loss:2.4271, Accuracy:0.1819, Validation Loss:2.4459, Validation Accuracy:0.1675\n",
    "Epoch #273: Loss:2.4255, Accuracy:0.1791, Validation Loss:2.4472, Validation Accuracy:0.1609\n",
    "Epoch #274: Loss:2.4256, Accuracy:0.1823, Validation Loss:2.4450, Validation Accuracy:0.1757\n",
    "Epoch #275: Loss:2.4252, Accuracy:0.1782, Validation Loss:2.4464, Validation Accuracy:0.1691\n",
    "Epoch #276: Loss:2.4256, Accuracy:0.1791, Validation Loss:2.4454, Validation Accuracy:0.1757\n",
    "Epoch #277: Loss:2.4266, Accuracy:0.1823, Validation Loss:2.4450, Validation Accuracy:0.1757\n",
    "Epoch #278: Loss:2.4260, Accuracy:0.1807, Validation Loss:2.4463, Validation Accuracy:0.1675\n",
    "Epoch #279: Loss:2.4264, Accuracy:0.1828, Validation Loss:2.4461, Validation Accuracy:0.1675\n",
    "Epoch #280: Loss:2.4259, Accuracy:0.1799, Validation Loss:2.4457, Validation Accuracy:0.1757\n",
    "Epoch #281: Loss:2.4263, Accuracy:0.1749, Validation Loss:2.4469, Validation Accuracy:0.1691\n",
    "Epoch #282: Loss:2.4242, Accuracy:0.1795, Validation Loss:2.4462, Validation Accuracy:0.1757\n",
    "Epoch #283: Loss:2.4266, Accuracy:0.1815, Validation Loss:2.4487, Validation Accuracy:0.1609\n",
    "Epoch #284: Loss:2.4263, Accuracy:0.1803, Validation Loss:2.4450, Validation Accuracy:0.1757\n",
    "Epoch #285: Loss:2.4251, Accuracy:0.1782, Validation Loss:2.4469, Validation Accuracy:0.1675\n",
    "Epoch #286: Loss:2.4249, Accuracy:0.1823, Validation Loss:2.4458, Validation Accuracy:0.1675\n",
    "Epoch #287: Loss:2.4250, Accuracy:0.1823, Validation Loss:2.4468, Validation Accuracy:0.1691\n",
    "Epoch #288: Loss:2.4256, Accuracy:0.1811, Validation Loss:2.4466, Validation Accuracy:0.1675\n",
    "Epoch #289: Loss:2.4246, Accuracy:0.1795, Validation Loss:2.4452, Validation Accuracy:0.1757\n",
    "Epoch #290: Loss:2.4246, Accuracy:0.1823, Validation Loss:2.4463, Validation Accuracy:0.1691\n",
    "Epoch #291: Loss:2.4267, Accuracy:0.1819, Validation Loss:2.4467, Validation Accuracy:0.1675\n",
    "Epoch #292: Loss:2.4243, Accuracy:0.1823, Validation Loss:2.4465, Validation Accuracy:0.1675\n",
    "Epoch #293: Loss:2.4241, Accuracy:0.1823, Validation Loss:2.4454, Validation Accuracy:0.1757\n",
    "Epoch #294: Loss:2.4248, Accuracy:0.1819, Validation Loss:2.4461, Validation Accuracy:0.1675\n",
    "Epoch #295: Loss:2.4244, Accuracy:0.1823, Validation Loss:2.4470, Validation Accuracy:0.1675\n",
    "Epoch #296: Loss:2.4244, Accuracy:0.1823, Validation Loss:2.4470, Validation Accuracy:0.1675\n",
    "Epoch #297: Loss:2.4243, Accuracy:0.1807, Validation Loss:2.4464, Validation Accuracy:0.1691\n",
    "Epoch #298: Loss:2.4248, Accuracy:0.1815, Validation Loss:2.4462, Validation Accuracy:0.1675\n",
    "Epoch #299: Loss:2.4241, Accuracy:0.1819, Validation Loss:2.4465, Validation Accuracy:0.1675\n",
    "Epoch #300: Loss:2.4241, Accuracy:0.1823, Validation Loss:2.4464, Validation Accuracy:0.1675\n",
    "\n",
    "Test:\n",
    "Test Loss:2.44644761, Accuracy:0.1675\n",
    "Labels: ['ck', 'ib', 'ce', 'my', 'sg', 'aa', 'yd', 'ek', 'eg', 'mb', 'sk', 'eb', 'ds', 'by', 'eo']\n",
    "Confusion Matrix:\n",
    "      ck  ib  ce  my  sg  aa  yd  ek  eg  mb  sk  eb  ds  by  eo\n",
    "t:ck   0   0   0   0   2   0   0   6   7   0   0   1   2   5   0\n",
    "t:ib   0   1   0   0   9   0  31   8   2   0   0   0   1   2   0\n",
    "t:ce   0   0   0   0   6   0   2   4  11   0   0   0   1   3   0\n",
    "t:my   0   1   0   0   2   0   5   3   4   0   0   0   4   1   0\n",
    "t:sg   0   6   0   0  15   0   7  16   4   0   0   0   1   2   0\n",
    "t:aa   0   2   0   0   0   0   3   3  16   0   0   0   8   2   0\n",
    "t:yd   0   8   0   0  12   0  29   9   4   0   0   0   0   0   0\n",
    "t:ek   0   2   0   0   9   0   6  11  12   0   0   0   1   7   0\n",
    "t:eg   0   0   0   0   0   0   1   5  29   0   0   0   6   9   0\n",
    "t:mb   0   8   0   0  14   0   3  11   6   0   0   1   2   7   0\n",
    "t:sk   0   1   0   0   7   0   2   5  13   0   0   0   3   2   0\n",
    "t:eb   0   1   0   0   9   0   6  13  17   0   0   0   0   4   0\n",
    "t:ds   0   1   0   0   6   0   1   6   8   0   0   0   9   0   0\n",
    "t:by   0   1   0   0   7   0   0  12  11   0   0   1   0   8   0\n",
    "t:eo   0   1   0   0   7   0   3  15   3   0   0   0   0   5   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ib       0.03      0.02      0.02        54\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          my       0.00      0.00      0.00        20\n",
    "          sg       0.14      0.29      0.19        51\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          yd       0.29      0.47      0.36        62\n",
    "          ek       0.09      0.23      0.13        48\n",
    "          eg       0.20      0.58      0.29        50\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          ds       0.24      0.29      0.26        31\n",
    "          by       0.14      0.20      0.16        40\n",
    "          eo       0.00      0.00      0.00        34\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.08      0.14      0.09       609\n",
    "weighted avg       0.09      0.17      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 14:38:37 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7028545327178755, 2.693997255882802, 2.687062285803809, 2.6796364326195174, 2.6723905345684984, 2.6653850192115423, 2.6581332773606374, 2.650370625635282, 2.6411449271078378, 2.6322150668878663, 2.6195037427598424, 2.606481315271412, 2.5963035878681002, 2.5800934370319637, 2.5657974698860655, 2.5571310939068472, 2.618582630000874, 2.5395758978056007, 2.5251873948891173, 2.5154421399966838, 2.5148806904728582, 2.5034170988549542, 2.4962390565324104, 2.495639367252344, 2.487087281466705, 2.4843421381682598, 2.4812957935145334, 2.4776016291726397, 2.479123657364368, 2.4768761237853854, 2.476543130545781, 2.473956078731368, 2.4692924363272533, 2.464980262449418, 2.466213449077262, 2.464340565044109, 2.4631297232090743, 2.463606950684721, 2.4615821783570038, 2.461358953579306, 2.457621645261893, 2.4612983467152163, 2.456058650964195, 2.4571718762465102, 2.456798536632644, 2.4532387922158576, 2.4557531952662224, 2.453207678395539, 2.4582354865833653, 2.454328480220976, 2.455272074403434, 2.4530944123448215, 2.451159793167866, 2.4484016335460743, 2.4496211989955556, 2.4495738437414563, 2.4559179111850282, 2.451735755297155, 2.453040322646719, 2.451991247621859, 2.4535278588875955, 2.452034342269396, 2.4516387776592485, 2.4547760815455995, 2.452632599472021, 2.452261204790012, 2.4505211277352568, 2.4500668749629178, 2.4489050389119167, 2.4479100516276993, 2.4477660397590673, 2.447266384885816, 2.449281150680066, 2.447963687586667, 2.4459440876501928, 2.4496253100522045, 2.44598601173689, 2.4463095876383663, 2.44628101929851, 2.4442395901640843, 2.447643694227748, 2.4455320459281285, 2.447408952932248, 2.4461986094664274, 2.4456708051496734, 2.4467552733930265, 2.446686083068597, 2.448157595491957, 2.446131306524543, 2.448489795568933, 2.449320419668564, 2.4480926430675587, 2.4522163214158934, 2.4485994626344327, 2.4509218803963244, 2.4484919507319507, 2.4496669107665765, 2.452411818386886, 2.4453624950841144, 2.446243806034082, 2.4507903124898527, 2.4442860023141493, 2.4441904704559025, 2.4470417644394247, 2.4443240298817703, 2.4438081296598186, 2.445067038481263, 2.460831470677418, 2.456641587522034, 2.4439018199400753, 2.4447219089921473, 2.4453394996317344, 2.444607479035952, 2.444703288070478, 2.44292520341419, 2.44135103711158, 2.4407182820324826, 2.4404312734337665, 2.439480752193282, 2.4410268763211755, 2.4399772375479514, 2.4418226400228167, 2.4385285076053664, 2.4387150950032503, 2.43880445029348, 2.4389683366409076, 2.4416195187466876, 2.4421199937955107, 2.441459164047868, 2.4435710965706208, 2.442194539729402, 2.4432148162171563, 2.439951976531832, 2.4399799376677214, 2.4399234798349965, 2.439425617603246, 2.4389544594268298, 2.439395765169892, 2.4409851753848724, 2.4393292080201148, 2.439693773125584, 2.4388999825432185, 2.440117067108405, 2.4387052055258667, 2.438834435442594, 2.439921244416135, 2.4391714336445376, 2.4404285603947633, 2.4390205757567056, 2.438929124027246, 2.4414891452820626, 2.439405825337753, 2.4411882649501555, 2.440666315395061, 2.4407133343576013, 2.4419632588309805, 2.440737501153805, 2.442102738397658, 2.4413453538233814, 2.441161831415737, 2.442401266254619, 2.440640316416673, 2.441757989047196, 2.4412089047956544, 2.4416030044430386, 2.443100264898466, 2.4404200708924844, 2.4414783408880627, 2.4441952822830877, 2.4415396062415615, 2.4426432493676495, 2.4410916960298135, 2.4416678112324433, 2.4428723339964016, 2.4413424587406354, 2.4425384904363474, 2.4415050116665844, 2.4427075209876, 2.4419985389083085, 2.4428845814296176, 2.441506929585499, 2.4414539912651323, 2.4425175726315853, 2.4412640883221806, 2.4423796018747663, 2.4420686625494747, 2.441576357545524, 2.4437991586224785, 2.4421012088387273, 2.4436414989540336, 2.4417875388572954, 2.4441043097397377, 2.442120798311406, 2.443781826883701, 2.441816124031305, 2.4436039266915155, 2.4425894407607456, 2.4425429491378208, 2.4428199214496833, 2.4426438722312938, 2.443272488849308, 2.4430037531359443, 2.443341182761983, 2.4421926351212124, 2.4424955367259007, 2.443959790106086, 2.442552544604773, 2.4436563227955737, 2.443423569887534, 2.4430305014298663, 2.444660519927202, 2.4431817175328048, 2.4430252452593524, 2.44412891070048, 2.442733454586837, 2.443011561833775, 2.445160400691291, 2.442815582936229, 2.443826903263336, 2.4431043418953178, 2.443871549393352, 2.4434821640916646, 2.4438935449753685, 2.4428701310713694, 2.4465606564958695, 2.443228821840584, 2.443480659588217, 2.444346195762772, 2.4441594658618295, 2.444258148838538, 2.444367240410916, 2.4434168667628846, 2.443537360537424, 2.4434489659683654, 2.444808469813054, 2.4450848850319145, 2.444226713211861, 2.443865432723598, 2.4441352270311127, 2.4451653475831883, 2.4444582748099895, 2.444775353120074, 2.4449034340080176, 2.444356147095879, 2.4451260030367497, 2.444569699674209, 2.4448447630714703, 2.444302899105404, 2.444869836954452, 2.444378098830801, 2.446054589376465, 2.445045164653233, 2.4457330202625696, 2.4438741888318742, 2.4458742865983685, 2.4452109634386887, 2.4463578237492856, 2.444994433955802, 2.4452836039814065, 2.4462606538888463, 2.4448681805521395, 2.449113320051547, 2.4459675889101327, 2.447766816479036, 2.444328404803973, 2.445708418518843, 2.4481270873096386, 2.445113244706578, 2.4467423870449974, 2.445156112288802, 2.445935055540113, 2.445891827785323, 2.4471631461176377, 2.4449631294789183, 2.4464110435523425, 2.445444458223916, 2.4449824828819686, 2.4463007473593277, 2.4461344834814716, 2.445677653126333, 2.4469359805822766, 2.4462313683357926, 2.4487394668002826, 2.4450322352410927, 2.4468660937937217, 2.4457964971539226, 2.4467681566091204, 2.446620909451264, 2.445237164426907, 2.4462683337858353, 2.4467187836056663, 2.446491936353236, 2.4454241186527197, 2.4460850180859244, 2.4470087381810783, 2.4470020522820732, 2.446382262632373, 2.4461727338079946, 2.4464627592434436, 2.446447628863731], 'val_acc': [0.06075533641554256, 0.06239737254033731, 0.05747126416595307, 0.10180623883194916, 0.10673234720633339, 0.10509031108153864, 0.10509031117941163, 0.11986863640043732, 0.11986863640043732, 0.12315270865002681, 0.12151067252523207, 0.13793103406679846, 0.1642036124550063, 0.16420361235713332, 0.16420361235713332, 0.17405582910590178, 0.12643678099748928, 0.17569786523069655, 0.16912972073151755, 0.17241379288323408, 0.17405582910590178, 0.17077175675843934, 0.17241379298110704, 0.17405582910590178, 0.17405582910590178, 0.1871921162202049, 0.17405582920377477, 0.18062397172102593, 0.1756978632487687, 0.17405582712397397, 0.17733989937356345, 0.17077175487438445, 0.1822660076500747, 0.1789819354004852, 0.18062397152527995, 0.18062397152527995, 0.17733989927569047, 0.18062397152527995, 0.18062397152527995, 0.1789819354004852, 0.17405582702610098, 0.17405582900802882, 0.1789819354004852, 0.1789819354004852, 0.17569786305302273, 0.1789819354004852, 0.16748768450884982, 0.17733989927569047, 0.17241379288323408, 0.1707717546786385, 0.16420361225926033, 0.16584564838405508, 0.17733989917781748, 0.17733989917781748, 0.1789819354983582, 0.174055826928228, 0.1773399012576183, 0.16912971855384376, 0.167487682526922, 0.1707717546786385, 0.17733989927569047, 0.174055826928228, 0.16912971855384376, 0.1773399013554913, 0.17241379307898003, 0.17569786315089572, 0.17241379080343325, 0.17569786305302273, 0.1822660075522017, 0.17733989917781748, 0.1822660075522017, 0.18062397142740697, 0.18555007980179122, 0.18390804367699645, 0.16912971865171675, 0.18719211602445893, 0.16584564640212723, 0.1822660076500747, 0.18555007999753717, 0.17241379090130624, 0.18555007999753717, 0.17241379080343325, 0.17405582702610098, 0.17569786315089572, 0.17569786315089572, 0.17733989927569047, 0.17241379307898003, 0.1822660076500747, 0.17733989917781748, 0.18062397152527995, 0.17733989927569047, 0.18062397142740697, 0.16584564828618212, 0.17241379307898003, 0.17898193530261222, 0.1707717568563123, 0.174055826928228, 0.1691297205357716, 0.17898193748028604, 0.1642036120635144, 0.1691297205357716, 0.17405582920377477, 0.17405582920377477, 0.1642036120635144, 0.17241379307898003, 0.16912972073151755, 0.1707717568563123, 0.18719211592658597, 0.17241379258961514, 0.18062397142740697, 0.17733990145336426, 0.17733990145336426, 0.1756978653285695, 0.17569786305302273, 0.1789819354004852, 0.17733989927569047, 0.18555007999753717, 0.18062397152527995, 0.1822660076500747, 0.18390804387274243, 0.18062397152527995, 0.18062397152527995, 0.1789819354004852, 0.1822660076500747, 0.18390804387274243, 0.1789819354004852, 0.18390804387274243, 0.18555007999753717, 0.1789819354004852, 0.18390804585467027, 0.17898193530261222, 0.18390804377486944, 0.17733989927569047, 0.18062397152527995, 0.18226600774794768, 0.1789819354983582, 0.18062397152527995, 0.1789819354983582, 0.18555007999753717, 0.18062397152527995, 0.18555007999753717, 0.18226600774794768, 0.18555007999753717, 0.18226600774794768, 0.18719211612233191, 0.18390804387274243, 0.18226600774794768, 0.18555007999753717, 0.1789819354983582, 0.18226600774794768, 0.17898193748028604, 0.17733989927569047, 0.18555007999753717, 0.18719211612233191, 0.18719211612233191, 0.17569786315089572, 0.18062397152527995, 0.18062397152527995, 0.18719211612233191, 0.18719211612233191, 0.17405582900802882, 0.18226600774794768, 0.18062397152527995, 0.18719211612233191, 0.18062397152527995, 0.17405582900802882, 0.18719211612233191, 0.18062397152527995, 0.17405582900802882, 0.18226600774794768, 0.18062397152527995, 0.18719211612233191, 0.18062397152527995, 0.18062397152527995, 0.18719211612233191, 0.18719211612233191, 0.18719211612233191, 0.18062397152527995, 0.18719211612233191, 0.18062397152527995, 0.18062397152527995, 0.18719211612233191, 0.18062397152527995, 0.18719211612233191, 0.18719211612233191, 0.18719211612233191, 0.18719211612233191, 0.17569786513282357, 0.18719211612233191, 0.18062397360508078, 0.18719211612233191, 0.17405582900802882, 0.18719211612233191, 0.17405582900802882, 0.18719211612233191, 0.18062397152527995, 0.18719211612233191, 0.18719211612233191, 0.18719211612233191, 0.18719211612233191, 0.18062397360508078, 0.18719211612233191, 0.18062397360508078, 0.18719211612233191, 0.18719211612233191, 0.17569786513282357, 0.18719211612233191, 0.18883415224712666, 0.18883415224712666, 0.18883415224712666, 0.17569786513282357, 0.18719211612233191, 0.18719211612233191, 0.18062397360508078, 0.18719211612233191, 0.18719211612233191, 0.16912972043789862, 0.18719211612233191, 0.18883415224712666, 0.18719211612233191, 0.1822660076500747, 0.18719211612233191, 0.18719211612233191, 0.18719211612233191, 0.17569786513282357, 0.1756978653285695, 0.1756978653285695, 0.16912972043789862, 0.17898193738241305, 0.17898193738241305, 0.18883415224712666, 0.1756978653285695, 0.1756978653285695, 0.1756978653285695, 0.18062397360508078, 0.1674876842152309, 0.1756978653285695, 0.17405582920377477, 0.1756978653285695, 0.16912972043789862, 0.1756978653285695, 0.1756978653285695, 0.1756978653285695, 0.18719211612233191, 0.1756978653285695, 0.1756978653285695, 0.16912972043789862, 0.1756978653285695, 0.1674876842152309, 0.1756978653285695, 0.16256157584084666, 0.18719211612233191, 0.16091953961817893, 0.1756978653285695, 0.16091953961817893, 0.18719211612233191, 0.16912972043789862, 0.1756978653285695, 0.1756978653285695, 0.16912972043789862, 0.1756978653285695, 0.16256157584084666, 0.18719211612233191, 0.16091953961817893, 0.1674876842152309, 0.1756978653285695, 0.16912972043789862, 0.17405582920377477, 0.16912972043789862, 0.1756978653285695, 0.1674876842152309, 0.1674876842152309, 0.16091953961817893, 0.1756978653285695, 0.16912972043789862, 0.1756978653285695, 0.1756978653285695, 0.1674876842152309, 0.1674876842152309, 0.1756978653285695, 0.16912972043789862, 0.1756978653285695, 0.16091953961817893, 0.1756978653285695, 0.1674876842152309, 0.1674876842152309, 0.16912972043789862, 0.1674876842152309, 0.1756978653285695, 0.16912972043789862, 0.1674876842152309, 0.1674876842152309, 0.1756978653285695, 0.1674876842152309, 0.1674876842152309, 0.1674876842152309, 0.16912972043789862, 0.1674876842152309, 0.1674876842152309, 0.1674876842152309], 'loss': [2.7092132788664016, 2.699335612898245, 2.6915658671753118, 2.6841120954901285, 2.6769887112494124, 2.6700620346735144, 2.662849961756681, 2.655593458974631, 2.64769499120771, 2.636648353560994, 2.626553060680444, 2.613831579905516, 2.6007969474400827, 2.589834408397792, 2.5767287984765774, 2.5610256394811235, 2.5484814252207166, 2.564070052728516, 2.5301079997781364, 2.5210765394097234, 2.511226642107327, 2.506255814573848, 2.5011847701650383, 2.4929152181261127, 2.4890180964734276, 2.482892717130375, 2.480842144435436, 2.4765260562270086, 2.4736760444954435, 2.4723458239185248, 2.4701685143692047, 2.4675611840626055, 2.466214383652078, 2.4658546922632802, 2.4645108756588225, 2.4646995038222483, 2.462777912151642, 2.461968048103536, 2.4613817717260402, 2.4596238866723783, 2.458370110239581, 2.458596634326285, 2.4572617615028083, 2.4564104402334537, 2.4563398861542374, 2.454465560208111, 2.454100949759356, 2.4551742169891293, 2.452882530067491, 2.452672628798279, 2.4520544471192407, 2.450857508304917, 2.449147857043288, 2.449773486781659, 2.450275584559666, 2.4486178776077177, 2.449915648583759, 2.4510727256720073, 2.4493580281612073, 2.447446473374259, 2.448678132298056, 2.4494687891104383, 2.449093645162406, 2.450454041992125, 2.450063927903068, 2.448332179008813, 2.447661205679484, 2.449659894672997, 2.447810492819095, 2.446850808004579, 2.44529286165257, 2.446054456366161, 2.4467840512919965, 2.4492274143367823, 2.449165140480966, 2.4440800000020366, 2.4444398750759495, 2.444382465644539, 2.444163391967084, 2.443037301265239, 2.443321937506204, 2.4446925426655484, 2.443805598918907, 2.443637090346162, 2.4426010758480254, 2.4425463490417605, 2.4425436769912374, 2.442975276402624, 2.442772818688739, 2.442938613010383, 2.444703389780722, 2.4445307563217757, 2.444804326317883, 2.443313074503591, 2.445963370188061, 2.441391372093185, 2.4440396111114313, 2.4459534954486197, 2.442476528332219, 2.446308913416931, 2.443638925141133, 2.4416392626948427, 2.443004351230128, 2.4431662558530145, 2.441682415380615, 2.4408350439287063, 2.440398798830945, 2.448222068888451, 2.4476231868751728, 2.4466621164913294, 2.442819943222422, 2.4420189229859464, 2.439868347747615, 2.4414527121510594, 2.441117415437953, 2.441937180074578, 2.440690682605062, 2.4396141834572354, 2.439981390857109, 2.4402710418192024, 2.440357707609141, 2.4398741086650433, 2.439121108867794, 2.4400380947261864, 2.4392818845517827, 2.438872062694855, 2.439425233942772, 2.439029804441229, 2.439799645304435, 2.4397542330763424, 2.4372249007959383, 2.43845232056886, 2.43862668248907, 2.437550740760944, 2.437337391381391, 2.4368640179996373, 2.4367506104573087, 2.437530485560517, 2.438718696100756, 2.4365491549826745, 2.4375772074752273, 2.4370277220708387, 2.4367074708919017, 2.4368368814613297, 2.436714125658697, 2.4359035732809766, 2.4354829426908395, 2.4360820698787053, 2.4364354412658504, 2.436291645097047, 2.4372003164624285, 2.435747280160015, 2.4355525920523267, 2.4356505891380857, 2.435415913290067, 2.435383706513861, 2.4351766408101736, 2.4356061917800433, 2.43521345861149, 2.4347969977517883, 2.435921491979329, 2.433848301732809, 2.434347821065288, 2.4344133128375733, 2.4346028385710667, 2.4340134000876112, 2.4345380305264763, 2.43606355665156, 2.4341776043972194, 2.433497533465313, 2.433398903566709, 2.4346729167922567, 2.433585280118782, 2.4343357894944457, 2.433853526869349, 2.433729398421928, 2.433954515692145, 2.4327557391454553, 2.4328517487896053, 2.4332937812413524, 2.4331103645066214, 2.432998030582248, 2.4336959415882276, 2.433629564974587, 2.433334970474243, 2.433306687372666, 2.4318990148313238, 2.432307823482725, 2.43238521503472, 2.4326488508580892, 2.432417255848095, 2.4320055596392747, 2.431429717183358, 2.432247357887409, 2.432073815500467, 2.4309599283049974, 2.431532324021357, 2.430703776181356, 2.4311215295928705, 2.43117546810017, 2.430768176613402, 2.4309558466964187, 2.430797824330888, 2.4308198841690283, 2.430196102835559, 2.4301155383092423, 2.430110144174564, 2.430824418919777, 2.4300309683507963, 2.4298115352340792, 2.4314940581821074, 2.4327622697583458, 2.430106657584345, 2.4298047999826546, 2.43019339142394, 2.4301852242902564, 2.4298526813851735, 2.430687544918648, 2.4298100370646014, 2.4291799951627757, 2.4302021974410852, 2.429333184436117, 2.4298252266290494, 2.4287165725010866, 2.429899796323365, 2.431243796906677, 2.4286030182848233, 2.4288045178203856, 2.4289337281573724, 2.4281691630518165, 2.428385901304241, 2.428906058873484, 2.4289879439547812, 2.4286297857883774, 2.4284958059048507, 2.429061950452519, 2.428224451683875, 2.4275758941070746, 2.4290712601350317, 2.4292831790031104, 2.430289084906451, 2.4305145606367984, 2.428662120047536, 2.427830193125981, 2.428546464712468, 2.4276814461733527, 2.42858679828213, 2.427519644261386, 2.429438802397961, 2.427316015701764, 2.428044457014581, 2.427245532805425, 2.427261297707685, 2.4272400556403753, 2.426702523280463, 2.427773399470523, 2.42730605665908, 2.4272010818888763, 2.4276502043314783, 2.4293882712691226, 2.429415180942606, 2.4286512022390503, 2.4271964187739568, 2.4258780901436934, 2.431448251559749, 2.427139979368363, 2.426495111649531, 2.426445455472817, 2.427054533282834, 2.4259534568512464, 2.4262205432327866, 2.427094122516546, 2.4254622568095243, 2.4255643142567034, 2.4252076924459156, 2.425563790029569, 2.4265646152183016, 2.425982157748338, 2.4263986078375908, 2.425885697899413, 2.4263067991581786, 2.4241946953767624, 2.426610437506768, 2.4262969899226507, 2.425101256811154, 2.4248911203544976, 2.4250354262592855, 2.4256186921004153, 2.4245602077037645, 2.424618501291138, 2.4266741528403344, 2.4243397813558087, 2.424108736519941, 2.424760188801822, 2.4243915101585936, 2.42440934200796, 2.42431110151005, 2.4248414265791247, 2.4240803268166293, 2.4240752578516025], 'acc': [0.05749486668888303, 0.060780287036783155, 0.06078028782926791, 0.05872689894643408, 0.10554414816216032, 0.11088295719706792, 0.11375769994525695, 0.11991786401129846, 0.11786447632178144, 0.12361396276975314, 0.12484599621144163, 0.13388090341732486, 0.14661191036079455, 0.15975359235823278, 0.15523614014443432, 0.16057494876933048, 0.1597535937290172, 0.1519507197689961, 0.16098562579992126, 0.1597535925357004, 0.16057494757601368, 0.1609856267974117, 0.16139630302633837, 0.16098562597738889, 0.16509240336976258, 0.16344969129415507, 0.16550308095111493, 0.17043121158464733, 0.17248459850003833, 0.17371663135424775, 0.17330595371781923, 0.1737166325659233, 0.16878850072071538, 0.17084188920271715, 0.17166324424303042, 0.17453798780206292, 0.1675564684723437, 0.16837782410013602, 0.1728952767055872, 0.17248460006664912, 0.1671457908542739, 0.16837782427760364, 0.17577002159868668, 0.17453798758787786, 0.17494866440428355, 0.1737166335266963, 0.17453798876283594, 0.17618069925347393, 0.17535934319731147, 0.17248459928334373, 0.17289527811308905, 0.17782340890573037, 0.17864476474770774, 0.17577002161704539, 0.17864476472934904, 0.17535934341149653, 0.1753593420407121, 0.17248459850003833, 0.1733059551253211, 0.17700205448961356, 0.176180697258493, 0.17289527829055668, 0.17412730996980805, 0.17700205368794944, 0.17289527709723987, 0.1761806980417984, 0.17577001985460824, 0.17412731114476612, 0.17371663235173823, 0.17289527768471893, 0.17412730957815534, 0.1720739224944761, 0.1737166317642592, 0.1749486646001099, 0.17412731055728708, 0.17700205308211167, 0.17741273091436657, 0.1712525666249606, 0.17535934241400608, 0.17248459908751737, 0.173305954323657, 0.1757700196404232, 0.17207392285859072, 0.1745379881753569, 0.17289527691977224, 0.1786447649251754, 0.1745379877653455, 0.17535934319731147, 0.17987679564365372, 0.181519507076706, 0.1782340869338116, 0.17577002042372858, 0.1761806992167565, 0.17823408595467985, 0.17125256564582889, 0.1790554425616039, 0.17248459850003833, 0.1778234094932094, 0.17946611998384737, 0.17125256623330792, 0.1786447629669119, 0.17659137466237776, 0.17618069784597204, 0.17453798719622515, 0.17577001983624954, 0.17659137546404186, 0.16878849991905126, 0.17166324463468313, 0.17166324345972503, 0.17289527770307764, 0.17659137489492155, 0.17946611978802104, 0.17987679701443815, 0.17823408613214747, 0.17782340851407766, 0.18028747541581336, 0.18151950807419645, 0.17823408691545287, 0.1765913766757174, 0.17741273130601926, 0.17905544117246075, 0.17700205327793803, 0.17659137526821553, 0.1774127315018456, 0.17659137526821553, 0.1794661193780096, 0.1761806988251038, 0.17905544197412487, 0.17700205406124342, 0.1802874752016283, 0.1802874740266702, 0.17823408634633253, 0.1802874748099756, 0.17987679699607942, 0.17946611800722517, 0.18110882906698347, 0.17946612017967373, 0.17987679640860038, 0.18110883043776793, 0.18069815303388317, 0.1819301854780812, 0.18151950648922696, 0.18110882924445112, 0.17741273208932465, 0.18193018410729678, 0.1806981526422305, 0.17987679660442674, 0.18028747439996418, 0.18110882924445112, 0.1778234085324364, 0.1778234085324364, 0.1778234095115681, 0.18110882967282124, 0.18110882945863618, 0.18275153955762147, 0.18151950807419645, 0.17741273167931323, 0.1798767968002531, 0.18110883004611522, 0.1815195082700228, 0.182340862135378, 0.18110883024194158, 0.18275154051839448, 0.1811088286753308, 0.18069815301552444, 0.18193018410729678, 0.18151950627504188, 0.18110882924445112, 0.180287475219987, 0.18193018449894946, 0.17905544078080807, 0.18357289575453412, 0.18151950766418504, 0.18439424961988932, 0.18193018528225485, 0.1860369618546057, 0.18234086209866054, 0.18234086211701928, 0.18151950807419645, 0.18234086250867199, 0.17946611920054198, 0.18110882985028887, 0.18234086133371388, 0.18357289596871917, 0.18234086331033608, 0.18275153914761005, 0.18234086250867199, 0.18151950607921552, 0.18151950768254377, 0.18316221676567987, 0.18275154071422084, 0.1815195062934006, 0.18069815086143462, 0.18028747541581336, 0.1819301842847644, 0.18193018569226627, 0.1831622173531589, 0.1819301838931117, 0.18193018489060217, 0.18193018588809262, 0.17946611839887788, 0.18234086350616244, 0.18193018530061358, 0.18275154051839448, 0.18193018490896087, 0.18357289536288143, 0.1794661195921947, 0.18234086231284563, 0.18275153975344782, 0.18151950607921552, 0.1819301850864285, 0.18069815225057778, 0.18193018510478723, 0.18193018449894946, 0.18275154073257954, 0.18193018489060217, 0.1819301837340028, 0.18110882926280983, 0.18151950648922696, 0.18193018410729678, 0.17987679660442674, 0.1819301842847644, 0.1815195082700228, 0.1819301858697339, 0.1819301854964399, 0.1798767972102645, 0.18069815103890224, 0.1798767960169477, 0.18069815086143462, 0.18357289477540237, 0.18151950648922696, 0.1790554425616039, 0.18151950746835868, 0.181108830651953, 0.17412730936397028, 0.17946611800722517, 0.18193018490896087, 0.18069815225057778, 0.1802874744366816, 0.18193018391147042, 0.18234086211701928, 0.17987679660442674, 0.17535934319731147, 0.18110883004611522, 0.17987679560693628, 0.18234086250867199, 0.17864476357274967, 0.1815195074867174, 0.17946611820305153, 0.18398357237511348, 0.1757700212253927, 0.18193018490896087, 0.1782340869338116, 0.18193018449894946, 0.17987679740609086, 0.18316221833229065, 0.18028747482833432, 0.1761806976501457, 0.1806981526422305, 0.17864476433769633, 0.17823408732546428, 0.17494866579342672, 0.1839835731951363, 0.18316221811810557, 0.17987679660442674, 0.18234086329197738, 0.1778234083182513, 0.17905544099499313, 0.17905544138664584, 0.18069815283805682, 0.1790554417599398, 0.1819301843031231, 0.17905544117246075, 0.18234086250867199, 0.17823408554466844, 0.17905544038915536, 0.18234086331033608, 0.18069815283805682, 0.1827515397350891, 0.17987679660442674, 0.1749486646001099, 0.17946611820305153, 0.1815195070950647, 0.18028747482833432, 0.1782340867012678, 0.18234086290032467, 0.18234086327361865, 0.1811088294769949, 0.17946612019803246, 0.18234086211701928, 0.1819301843031231, 0.18234086250867199, 0.18234086192119292, 0.18193018371564407, 0.18234086211701928, 0.18234086272285704, 0.18069815223221905, 0.18151950607921552, 0.1819301850864285, 0.18234086170700786]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
