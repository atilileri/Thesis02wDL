{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf32.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 07:28:17 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '2', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '04', '05', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000237C6E5BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000237BC716EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6183, Accuracy:0.2000, Validation Loss:1.6134, Validation Accuracy:0.2069\n",
    "Epoch #2: Loss:1.6119, Accuracy:0.2066, Validation Loss:1.6106, Validation Accuracy:0.2069\n",
    "Epoch #3: Loss:1.6068, Accuracy:0.2160, Validation Loss:1.6078, Validation Accuracy:0.2069\n",
    "Epoch #4: Loss:1.6053, Accuracy:0.2353, Validation Loss:1.6073, Validation Accuracy:0.2151\n",
    "Epoch #5: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6076, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6071, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6029, Accuracy:0.2357, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6027, Accuracy:0.2357, Validation Loss:1.6065, Validation Accuracy:0.2348\n",
    "Epoch #14: Loss:1.6029, Accuracy:0.2476, Validation Loss:1.6068, Validation Accuracy:0.2282\n",
    "Epoch #15: Loss:1.6026, Accuracy:0.2464, Validation Loss:1.6065, Validation Accuracy:0.2299\n",
    "Epoch #16: Loss:1.6029, Accuracy:0.2402, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6022, Accuracy:0.2398, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #18: Loss:1.6014, Accuracy:0.2468, Validation Loss:1.6065, Validation Accuracy:0.2266\n",
    "Epoch #19: Loss:1.6011, Accuracy:0.2468, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #20: Loss:1.6011, Accuracy:0.2468, Validation Loss:1.6067, Validation Accuracy:0.2266\n",
    "Epoch #21: Loss:1.6007, Accuracy:0.2452, Validation Loss:1.6063, Validation Accuracy:0.2282\n",
    "Epoch #22: Loss:1.6014, Accuracy:0.2452, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #23: Loss:1.6008, Accuracy:0.2464, Validation Loss:1.6066, Validation Accuracy:0.2250\n",
    "Epoch #24: Loss:1.6001, Accuracy:0.2439, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #25: Loss:1.6000, Accuracy:0.2431, Validation Loss:1.6074, Validation Accuracy:0.2266\n",
    "Epoch #26: Loss:1.6010, Accuracy:0.2468, Validation Loss:1.6071, Validation Accuracy:0.2266\n",
    "Epoch #27: Loss:1.6003, Accuracy:0.2460, Validation Loss:1.6077, Validation Accuracy:0.2282\n",
    "Epoch #28: Loss:1.6003, Accuracy:0.2456, Validation Loss:1.6079, Validation Accuracy:0.2282\n",
    "Epoch #29: Loss:1.6001, Accuracy:0.2452, Validation Loss:1.6078, Validation Accuracy:0.2299\n",
    "Epoch #30: Loss:1.6006, Accuracy:0.2480, Validation Loss:1.6078, Validation Accuracy:0.2233\n",
    "Epoch #31: Loss:1.6006, Accuracy:0.2427, Validation Loss:1.6073, Validation Accuracy:0.2315\n",
    "Epoch #32: Loss:1.6004, Accuracy:0.2431, Validation Loss:1.6070, Validation Accuracy:0.2282\n",
    "Epoch #33: Loss:1.6002, Accuracy:0.2460, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #34: Loss:1.5995, Accuracy:0.2423, Validation Loss:1.6071, Validation Accuracy:0.2348\n",
    "Epoch #35: Loss:1.5994, Accuracy:0.2423, Validation Loss:1.6061, Validation Accuracy:0.2266\n",
    "Epoch #36: Loss:1.6003, Accuracy:0.2431, Validation Loss:1.6064, Validation Accuracy:0.2266\n",
    "Epoch #37: Loss:1.5989, Accuracy:0.2509, Validation Loss:1.6066, Validation Accuracy:0.2315\n",
    "Epoch #38: Loss:1.5996, Accuracy:0.2464, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #39: Loss:1.5982, Accuracy:0.2456, Validation Loss:1.6078, Validation Accuracy:0.2282\n",
    "Epoch #40: Loss:1.6061, Accuracy:0.2341, Validation Loss:1.6141, Validation Accuracy:0.2332\n",
    "Epoch #41: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #42: Loss:1.6015, Accuracy:0.2444, Validation Loss:1.6091, Validation Accuracy:0.2282\n",
    "Epoch #43: Loss:1.6026, Accuracy:0.2407, Validation Loss:1.6054, Validation Accuracy:0.2282\n",
    "Epoch #44: Loss:1.6003, Accuracy:0.2411, Validation Loss:1.6056, Validation Accuracy:0.2217\n",
    "Epoch #45: Loss:1.6008, Accuracy:0.2390, Validation Loss:1.6059, Validation Accuracy:0.2217\n",
    "Epoch #46: Loss:1.5996, Accuracy:0.2444, Validation Loss:1.6056, Validation Accuracy:0.2299\n",
    "Epoch #47: Loss:1.5986, Accuracy:0.2464, Validation Loss:1.6062, Validation Accuracy:0.2282\n",
    "Epoch #48: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6063, Validation Accuracy:0.2118\n",
    "Epoch #49: Loss:1.6002, Accuracy:0.2419, Validation Loss:1.6065, Validation Accuracy:0.2348\n",
    "Epoch #50: Loss:1.5993, Accuracy:0.2480, Validation Loss:1.6055, Validation Accuracy:0.2266\n",
    "Epoch #51: Loss:1.5984, Accuracy:0.2427, Validation Loss:1.6064, Validation Accuracy:0.2266\n",
    "Epoch #52: Loss:1.5984, Accuracy:0.2439, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #53: Loss:1.5975, Accuracy:0.2448, Validation Loss:1.6059, Validation Accuracy:0.2266\n",
    "Epoch #54: Loss:1.5982, Accuracy:0.2464, Validation Loss:1.6065, Validation Accuracy:0.2266\n",
    "Epoch #55: Loss:1.5981, Accuracy:0.2419, Validation Loss:1.6073, Validation Accuracy:0.2233\n",
    "Epoch #56: Loss:1.5978, Accuracy:0.2411, Validation Loss:1.6071, Validation Accuracy:0.2233\n",
    "Epoch #57: Loss:1.5980, Accuracy:0.2419, Validation Loss:1.6064, Validation Accuracy:0.2282\n",
    "Epoch #58: Loss:1.5975, Accuracy:0.2435, Validation Loss:1.6063, Validation Accuracy:0.2315\n",
    "Epoch #59: Loss:1.5975, Accuracy:0.2439, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #60: Loss:1.5974, Accuracy:0.2444, Validation Loss:1.6065, Validation Accuracy:0.2315\n",
    "Epoch #61: Loss:1.5978, Accuracy:0.2452, Validation Loss:1.6073, Validation Accuracy:0.2036\n",
    "Epoch #62: Loss:1.5974, Accuracy:0.2419, Validation Loss:1.6073, Validation Accuracy:0.2299\n",
    "Epoch #63: Loss:1.5977, Accuracy:0.2460, Validation Loss:1.6072, Validation Accuracy:0.2282\n",
    "Epoch #64: Loss:1.5976, Accuracy:0.2452, Validation Loss:1.6065, Validation Accuracy:0.2315\n",
    "Epoch #65: Loss:1.5970, Accuracy:0.2448, Validation Loss:1.6076, Validation Accuracy:0.2315\n",
    "Epoch #66: Loss:1.5976, Accuracy:0.2423, Validation Loss:1.6071, Validation Accuracy:0.2332\n",
    "Epoch #67: Loss:1.5974, Accuracy:0.2456, Validation Loss:1.6083, Validation Accuracy:0.2069\n",
    "Epoch #68: Loss:1.5982, Accuracy:0.2435, Validation Loss:1.6083, Validation Accuracy:0.2036\n",
    "Epoch #69: Loss:1.5975, Accuracy:0.2435, Validation Loss:1.6094, Validation Accuracy:0.2299\n",
    "Epoch #70: Loss:1.5975, Accuracy:0.2431, Validation Loss:1.6090, Validation Accuracy:0.2102\n",
    "Epoch #71: Loss:1.5973, Accuracy:0.2444, Validation Loss:1.6086, Validation Accuracy:0.2102\n",
    "Epoch #72: Loss:1.5970, Accuracy:0.2427, Validation Loss:1.6087, Validation Accuracy:0.2085\n",
    "Epoch #73: Loss:1.5965, Accuracy:0.2444, Validation Loss:1.6090, Validation Accuracy:0.2085\n",
    "Epoch #74: Loss:1.5965, Accuracy:0.2480, Validation Loss:1.6092, Validation Accuracy:0.2266\n",
    "Epoch #75: Loss:1.5963, Accuracy:0.2439, Validation Loss:1.6088, Validation Accuracy:0.2069\n",
    "Epoch #76: Loss:1.5969, Accuracy:0.2427, Validation Loss:1.6093, Validation Accuracy:0.2003\n",
    "Epoch #77: Loss:1.5962, Accuracy:0.2423, Validation Loss:1.6090, Validation Accuracy:0.2069\n",
    "Epoch #78: Loss:1.5962, Accuracy:0.2415, Validation Loss:1.6089, Validation Accuracy:0.2036\n",
    "Epoch #79: Loss:1.5960, Accuracy:0.2448, Validation Loss:1.6092, Validation Accuracy:0.2069\n",
    "Epoch #80: Loss:1.5961, Accuracy:0.2460, Validation Loss:1.6099, Validation Accuracy:0.2069\n",
    "Epoch #81: Loss:1.5956, Accuracy:0.2460, Validation Loss:1.6096, Validation Accuracy:0.2102\n",
    "Epoch #82: Loss:1.5959, Accuracy:0.2468, Validation Loss:1.6095, Validation Accuracy:0.2069\n",
    "Epoch #83: Loss:1.5959, Accuracy:0.2452, Validation Loss:1.6098, Validation Accuracy:0.2069\n",
    "Epoch #84: Loss:1.5955, Accuracy:0.2431, Validation Loss:1.6097, Validation Accuracy:0.2036\n",
    "Epoch #85: Loss:1.5955, Accuracy:0.2427, Validation Loss:1.6094, Validation Accuracy:0.2053\n",
    "Epoch #86: Loss:1.5959, Accuracy:0.2456, Validation Loss:1.6111, Validation Accuracy:0.2036\n",
    "Epoch #87: Loss:1.5959, Accuracy:0.2435, Validation Loss:1.6111, Validation Accuracy:0.2102\n",
    "Epoch #88: Loss:1.5960, Accuracy:0.2423, Validation Loss:1.6111, Validation Accuracy:0.2135\n",
    "Epoch #89: Loss:1.5963, Accuracy:0.2415, Validation Loss:1.6117, Validation Accuracy:0.2036\n",
    "Epoch #90: Loss:1.5961, Accuracy:0.2411, Validation Loss:1.6114, Validation Accuracy:0.2085\n",
    "Epoch #91: Loss:1.5954, Accuracy:0.2435, Validation Loss:1.6114, Validation Accuracy:0.2069\n",
    "Epoch #92: Loss:1.5955, Accuracy:0.2411, Validation Loss:1.6112, Validation Accuracy:0.2069\n",
    "Epoch #93: Loss:1.5958, Accuracy:0.2423, Validation Loss:1.6108, Validation Accuracy:0.2069\n",
    "Epoch #94: Loss:1.5962, Accuracy:0.2431, Validation Loss:1.6100, Validation Accuracy:0.2053\n",
    "Epoch #95: Loss:1.5953, Accuracy:0.2439, Validation Loss:1.6108, Validation Accuracy:0.2036\n",
    "Epoch #96: Loss:1.5959, Accuracy:0.2460, Validation Loss:1.6106, Validation Accuracy:0.2053\n",
    "Epoch #97: Loss:1.5945, Accuracy:0.2444, Validation Loss:1.6105, Validation Accuracy:0.2069\n",
    "Epoch #98: Loss:1.5958, Accuracy:0.2427, Validation Loss:1.6101, Validation Accuracy:0.2102\n",
    "Epoch #99: Loss:1.5959, Accuracy:0.2448, Validation Loss:1.6084, Validation Accuracy:0.1987\n",
    "Epoch #100: Loss:1.5950, Accuracy:0.2419, Validation Loss:1.6092, Validation Accuracy:0.2233\n",
    "Epoch #101: Loss:1.5955, Accuracy:0.2423, Validation Loss:1.6086, Validation Accuracy:0.2053\n",
    "Epoch #102: Loss:1.5953, Accuracy:0.2452, Validation Loss:1.6088, Validation Accuracy:0.2069\n",
    "Epoch #103: Loss:1.5948, Accuracy:0.2398, Validation Loss:1.6093, Validation Accuracy:0.2053\n",
    "Epoch #104: Loss:1.5954, Accuracy:0.2439, Validation Loss:1.6102, Validation Accuracy:0.2020\n",
    "Epoch #105: Loss:1.5956, Accuracy:0.2452, Validation Loss:1.6093, Validation Accuracy:0.2036\n",
    "Epoch #106: Loss:1.5959, Accuracy:0.2415, Validation Loss:1.6089, Validation Accuracy:0.2053\n",
    "Epoch #107: Loss:1.5958, Accuracy:0.2415, Validation Loss:1.6086, Validation Accuracy:0.2085\n",
    "Epoch #108: Loss:1.5954, Accuracy:0.2423, Validation Loss:1.6081, Validation Accuracy:0.2069\n",
    "Epoch #109: Loss:1.5953, Accuracy:0.2444, Validation Loss:1.6077, Validation Accuracy:0.2085\n",
    "Epoch #110: Loss:1.5954, Accuracy:0.2460, Validation Loss:1.6080, Validation Accuracy:0.2069\n",
    "Epoch #111: Loss:1.5953, Accuracy:0.2464, Validation Loss:1.6079, Validation Accuracy:0.2085\n",
    "Epoch #112: Loss:1.5951, Accuracy:0.2456, Validation Loss:1.6073, Validation Accuracy:0.2069\n",
    "Epoch #113: Loss:1.5957, Accuracy:0.2419, Validation Loss:1.6075, Validation Accuracy:0.2053\n",
    "Epoch #114: Loss:1.5955, Accuracy:0.2452, Validation Loss:1.6077, Validation Accuracy:0.2053\n",
    "Epoch #115: Loss:1.5955, Accuracy:0.2546, Validation Loss:1.6080, Validation Accuracy:0.2036\n",
    "Epoch #116: Loss:1.5950, Accuracy:0.2493, Validation Loss:1.6071, Validation Accuracy:0.2036\n",
    "Epoch #117: Loss:1.5949, Accuracy:0.2517, Validation Loss:1.6076, Validation Accuracy:0.2003\n",
    "Epoch #118: Loss:1.5951, Accuracy:0.2423, Validation Loss:1.6084, Validation Accuracy:0.2069\n",
    "Epoch #119: Loss:1.5950, Accuracy:0.2460, Validation Loss:1.6077, Validation Accuracy:0.2003\n",
    "Epoch #120: Loss:1.5952, Accuracy:0.2517, Validation Loss:1.6077, Validation Accuracy:0.2003\n",
    "Epoch #121: Loss:1.5946, Accuracy:0.2538, Validation Loss:1.6080, Validation Accuracy:0.2036\n",
    "Epoch #122: Loss:1.5945, Accuracy:0.2509, Validation Loss:1.6077, Validation Accuracy:0.2102\n",
    "Epoch #123: Loss:1.5941, Accuracy:0.2526, Validation Loss:1.6080, Validation Accuracy:0.2102\n",
    "Epoch #124: Loss:1.5941, Accuracy:0.2526, Validation Loss:1.6077, Validation Accuracy:0.2118\n",
    "Epoch #125: Loss:1.5938, Accuracy:0.2550, Validation Loss:1.6072, Validation Accuracy:0.2135\n",
    "Epoch #126: Loss:1.5936, Accuracy:0.2554, Validation Loss:1.6074, Validation Accuracy:0.2135\n",
    "Epoch #127: Loss:1.5931, Accuracy:0.2550, Validation Loss:1.6081, Validation Accuracy:0.2102\n",
    "Epoch #128: Loss:1.5926, Accuracy:0.2583, Validation Loss:1.6081, Validation Accuracy:0.2118\n",
    "Epoch #129: Loss:1.5936, Accuracy:0.2559, Validation Loss:1.6089, Validation Accuracy:0.2118\n",
    "Epoch #130: Loss:1.5932, Accuracy:0.2571, Validation Loss:1.6085, Validation Accuracy:0.2085\n",
    "Epoch #131: Loss:1.5929, Accuracy:0.2517, Validation Loss:1.6097, Validation Accuracy:0.1970\n",
    "Epoch #132: Loss:1.5932, Accuracy:0.2509, Validation Loss:1.6106, Validation Accuracy:0.2151\n",
    "Epoch #133: Loss:1.5955, Accuracy:0.2575, Validation Loss:1.6117, Validation Accuracy:0.1921\n",
    "Epoch #134: Loss:1.5944, Accuracy:0.2579, Validation Loss:1.6126, Validation Accuracy:0.2053\n",
    "Epoch #135: Loss:1.5939, Accuracy:0.2587, Validation Loss:1.6125, Validation Accuracy:0.1987\n",
    "Epoch #136: Loss:1.5941, Accuracy:0.2600, Validation Loss:1.6102, Validation Accuracy:0.2020\n",
    "Epoch #137: Loss:1.5937, Accuracy:0.2608, Validation Loss:1.6111, Validation Accuracy:0.1987\n",
    "Epoch #138: Loss:1.5934, Accuracy:0.2632, Validation Loss:1.6113, Validation Accuracy:0.2036\n",
    "Epoch #139: Loss:1.5931, Accuracy:0.2620, Validation Loss:1.6117, Validation Accuracy:0.2003\n",
    "Epoch #140: Loss:1.5930, Accuracy:0.2608, Validation Loss:1.6099, Validation Accuracy:0.2069\n",
    "Epoch #141: Loss:1.5935, Accuracy:0.2591, Validation Loss:1.6094, Validation Accuracy:0.2085\n",
    "Epoch #142: Loss:1.5945, Accuracy:0.2542, Validation Loss:1.6111, Validation Accuracy:0.2053\n",
    "Epoch #143: Loss:1.5947, Accuracy:0.2534, Validation Loss:1.6123, Validation Accuracy:0.2102\n",
    "Epoch #144: Loss:1.5962, Accuracy:0.2480, Validation Loss:1.6110, Validation Accuracy:0.2069\n",
    "Epoch #145: Loss:1.5965, Accuracy:0.2431, Validation Loss:1.6106, Validation Accuracy:0.2053\n",
    "Epoch #146: Loss:1.5944, Accuracy:0.2497, Validation Loss:1.6100, Validation Accuracy:0.2053\n",
    "Epoch #147: Loss:1.5935, Accuracy:0.2513, Validation Loss:1.6088, Validation Accuracy:0.1987\n",
    "Epoch #148: Loss:1.5936, Accuracy:0.2517, Validation Loss:1.6092, Validation Accuracy:0.1987\n",
    "Epoch #149: Loss:1.5941, Accuracy:0.2517, Validation Loss:1.6087, Validation Accuracy:0.2036\n",
    "Epoch #150: Loss:1.5943, Accuracy:0.2530, Validation Loss:1.6093, Validation Accuracy:0.2053\n",
    "Epoch #151: Loss:1.5929, Accuracy:0.2587, Validation Loss:1.6087, Validation Accuracy:0.2069\n",
    "Epoch #152: Loss:1.5929, Accuracy:0.2595, Validation Loss:1.6079, Validation Accuracy:0.2102\n",
    "Epoch #153: Loss:1.5923, Accuracy:0.2632, Validation Loss:1.6094, Validation Accuracy:0.2020\n",
    "Epoch #154: Loss:1.5924, Accuracy:0.2628, Validation Loss:1.6097, Validation Accuracy:0.2003\n",
    "Epoch #155: Loss:1.5929, Accuracy:0.2637, Validation Loss:1.6096, Validation Accuracy:0.2020\n",
    "Epoch #156: Loss:1.5926, Accuracy:0.2637, Validation Loss:1.6088, Validation Accuracy:0.2020\n",
    "Epoch #157: Loss:1.5921, Accuracy:0.2624, Validation Loss:1.6080, Validation Accuracy:0.2020\n",
    "Epoch #158: Loss:1.5923, Accuracy:0.2637, Validation Loss:1.6086, Validation Accuracy:0.2069\n",
    "Epoch #159: Loss:1.5923, Accuracy:0.2649, Validation Loss:1.6084, Validation Accuracy:0.2036\n",
    "Epoch #160: Loss:1.5920, Accuracy:0.2583, Validation Loss:1.6077, Validation Accuracy:0.2036\n",
    "Epoch #161: Loss:1.5927, Accuracy:0.2604, Validation Loss:1.6062, Validation Accuracy:0.2085\n",
    "Epoch #162: Loss:1.5912, Accuracy:0.2571, Validation Loss:1.6053, Validation Accuracy:0.2135\n",
    "Epoch #163: Loss:1.5921, Accuracy:0.2534, Validation Loss:1.6074, Validation Accuracy:0.2102\n",
    "Epoch #164: Loss:1.5928, Accuracy:0.2608, Validation Loss:1.6080, Validation Accuracy:0.2053\n",
    "Epoch #165: Loss:1.5917, Accuracy:0.2632, Validation Loss:1.6078, Validation Accuracy:0.2036\n",
    "Epoch #166: Loss:1.5916, Accuracy:0.2567, Validation Loss:1.6088, Validation Accuracy:0.2003\n",
    "Epoch #167: Loss:1.5915, Accuracy:0.2637, Validation Loss:1.6088, Validation Accuracy:0.1921\n",
    "Epoch #168: Loss:1.5916, Accuracy:0.2583, Validation Loss:1.6096, Validation Accuracy:0.1905\n",
    "Epoch #169: Loss:1.5912, Accuracy:0.2604, Validation Loss:1.6087, Validation Accuracy:0.1921\n",
    "Epoch #170: Loss:1.5924, Accuracy:0.2595, Validation Loss:1.6087, Validation Accuracy:0.1905\n",
    "Epoch #171: Loss:1.5920, Accuracy:0.2575, Validation Loss:1.6104, Validation Accuracy:0.1921\n",
    "Epoch #172: Loss:1.5918, Accuracy:0.2632, Validation Loss:1.6102, Validation Accuracy:0.1938\n",
    "Epoch #173: Loss:1.5928, Accuracy:0.2476, Validation Loss:1.6108, Validation Accuracy:0.2053\n",
    "Epoch #174: Loss:1.5937, Accuracy:0.2563, Validation Loss:1.6092, Validation Accuracy:0.2036\n",
    "Epoch #175: Loss:1.5932, Accuracy:0.2583, Validation Loss:1.6097, Validation Accuracy:0.2036\n",
    "Epoch #176: Loss:1.5933, Accuracy:0.2571, Validation Loss:1.6085, Validation Accuracy:0.2053\n",
    "Epoch #177: Loss:1.5930, Accuracy:0.2559, Validation Loss:1.6087, Validation Accuracy:0.2069\n",
    "Epoch #178: Loss:1.5920, Accuracy:0.2575, Validation Loss:1.6083, Validation Accuracy:0.2053\n",
    "Epoch #179: Loss:1.5925, Accuracy:0.2567, Validation Loss:1.6079, Validation Accuracy:0.2036\n",
    "Epoch #180: Loss:1.5922, Accuracy:0.2571, Validation Loss:1.6094, Validation Accuracy:0.2036\n",
    "Epoch #181: Loss:1.5919, Accuracy:0.2563, Validation Loss:1.6096, Validation Accuracy:0.2020\n",
    "Epoch #182: Loss:1.5921, Accuracy:0.2538, Validation Loss:1.6126, Validation Accuracy:0.1970\n",
    "Epoch #183: Loss:1.5923, Accuracy:0.2509, Validation Loss:1.6095, Validation Accuracy:0.2102\n",
    "Epoch #184: Loss:1.5935, Accuracy:0.2550, Validation Loss:1.6106, Validation Accuracy:0.2102\n",
    "Epoch #185: Loss:1.5939, Accuracy:0.2485, Validation Loss:1.6109, Validation Accuracy:0.1888\n",
    "Epoch #186: Loss:1.5933, Accuracy:0.2472, Validation Loss:1.6103, Validation Accuracy:0.1938\n",
    "Epoch #187: Loss:1.5931, Accuracy:0.2468, Validation Loss:1.6101, Validation Accuracy:0.2085\n",
    "Epoch #188: Loss:1.5925, Accuracy:0.2509, Validation Loss:1.6093, Validation Accuracy:0.2102\n",
    "Epoch #189: Loss:1.5928, Accuracy:0.2522, Validation Loss:1.6094, Validation Accuracy:0.2102\n",
    "Epoch #190: Loss:1.5926, Accuracy:0.2513, Validation Loss:1.6105, Validation Accuracy:0.2118\n",
    "Epoch #191: Loss:1.5925, Accuracy:0.2485, Validation Loss:1.6115, Validation Accuracy:0.1987\n",
    "Epoch #192: Loss:1.5934, Accuracy:0.2456, Validation Loss:1.6115, Validation Accuracy:0.1970\n",
    "Epoch #193: Loss:1.5931, Accuracy:0.2501, Validation Loss:1.6118, Validation Accuracy:0.1987\n",
    "Epoch #194: Loss:1.5922, Accuracy:0.2542, Validation Loss:1.6108, Validation Accuracy:0.2118\n",
    "Epoch #195: Loss:1.5926, Accuracy:0.2468, Validation Loss:1.6106, Validation Accuracy:0.1987\n",
    "Epoch #196: Loss:1.5933, Accuracy:0.2452, Validation Loss:1.6115, Validation Accuracy:0.1970\n",
    "Epoch #197: Loss:1.5934, Accuracy:0.2394, Validation Loss:1.6120, Validation Accuracy:0.2085\n",
    "Epoch #198: Loss:1.5932, Accuracy:0.2411, Validation Loss:1.6109, Validation Accuracy:0.1970\n",
    "Epoch #199: Loss:1.5930, Accuracy:0.2460, Validation Loss:1.6109, Validation Accuracy:0.2020\n",
    "Epoch #200: Loss:1.5927, Accuracy:0.2550, Validation Loss:1.6104, Validation Accuracy:0.2020\n",
    "Epoch #201: Loss:1.5923, Accuracy:0.2538, Validation Loss:1.6106, Validation Accuracy:0.2135\n",
    "Epoch #202: Loss:1.5922, Accuracy:0.2517, Validation Loss:1.6111, Validation Accuracy:0.2036\n",
    "Epoch #203: Loss:1.5922, Accuracy:0.2517, Validation Loss:1.6117, Validation Accuracy:0.2135\n",
    "Epoch #204: Loss:1.5931, Accuracy:0.2517, Validation Loss:1.6110, Validation Accuracy:0.2135\n",
    "Epoch #205: Loss:1.5914, Accuracy:0.2493, Validation Loss:1.6114, Validation Accuracy:0.2102\n",
    "Epoch #206: Loss:1.5929, Accuracy:0.2546, Validation Loss:1.6107, Validation Accuracy:0.2085\n",
    "Epoch #207: Loss:1.5910, Accuracy:0.2534, Validation Loss:1.6107, Validation Accuracy:0.2151\n",
    "Epoch #208: Loss:1.5925, Accuracy:0.2505, Validation Loss:1.6110, Validation Accuracy:0.2118\n",
    "Epoch #209: Loss:1.5910, Accuracy:0.2517, Validation Loss:1.6103, Validation Accuracy:0.2348\n",
    "Epoch #210: Loss:1.5928, Accuracy:0.2554, Validation Loss:1.6103, Validation Accuracy:0.2365\n",
    "Epoch #211: Loss:1.5908, Accuracy:0.2546, Validation Loss:1.6104, Validation Accuracy:0.2003\n",
    "Epoch #212: Loss:1.5917, Accuracy:0.2534, Validation Loss:1.6112, Validation Accuracy:0.2069\n",
    "Epoch #213: Loss:1.5924, Accuracy:0.2464, Validation Loss:1.6103, Validation Accuracy:0.2365\n",
    "Epoch #214: Loss:1.5913, Accuracy:0.2546, Validation Loss:1.6106, Validation Accuracy:0.2332\n",
    "Epoch #215: Loss:1.5909, Accuracy:0.2534, Validation Loss:1.6109, Validation Accuracy:0.2020\n",
    "Epoch #216: Loss:1.5917, Accuracy:0.2542, Validation Loss:1.6112, Validation Accuracy:0.2003\n",
    "Epoch #217: Loss:1.5909, Accuracy:0.2493, Validation Loss:1.6105, Validation Accuracy:0.2365\n",
    "Epoch #218: Loss:1.5922, Accuracy:0.2567, Validation Loss:1.6106, Validation Accuracy:0.2102\n",
    "Epoch #219: Loss:1.5919, Accuracy:0.2460, Validation Loss:1.6125, Validation Accuracy:0.2217\n",
    "Epoch #220: Loss:1.5921, Accuracy:0.2522, Validation Loss:1.6106, Validation Accuracy:0.2036\n",
    "Epoch #221: Loss:1.5918, Accuracy:0.2509, Validation Loss:1.6112, Validation Accuracy:0.2135\n",
    "Epoch #222: Loss:1.5914, Accuracy:0.2509, Validation Loss:1.6103, Validation Accuracy:0.2118\n",
    "Epoch #223: Loss:1.5921, Accuracy:0.2550, Validation Loss:1.6122, Validation Accuracy:0.2069\n",
    "Epoch #224: Loss:1.5922, Accuracy:0.2493, Validation Loss:1.6113, Validation Accuracy:0.2348\n",
    "Epoch #225: Loss:1.5912, Accuracy:0.2591, Validation Loss:1.6111, Validation Accuracy:0.2151\n",
    "Epoch #226: Loss:1.5912, Accuracy:0.2505, Validation Loss:1.6111, Validation Accuracy:0.2151\n",
    "Epoch #227: Loss:1.5908, Accuracy:0.2534, Validation Loss:1.6117, Validation Accuracy:0.2250\n",
    "Epoch #228: Loss:1.5908, Accuracy:0.2559, Validation Loss:1.6115, Validation Accuracy:0.2250\n",
    "Epoch #229: Loss:1.5902, Accuracy:0.2550, Validation Loss:1.6109, Validation Accuracy:0.2348\n",
    "Epoch #230: Loss:1.5905, Accuracy:0.2505, Validation Loss:1.6109, Validation Accuracy:0.2381\n",
    "Epoch #231: Loss:1.5903, Accuracy:0.2542, Validation Loss:1.6112, Validation Accuracy:0.2365\n",
    "Epoch #232: Loss:1.5904, Accuracy:0.2583, Validation Loss:1.6116, Validation Accuracy:0.2003\n",
    "Epoch #233: Loss:1.5901, Accuracy:0.2538, Validation Loss:1.6112, Validation Accuracy:0.2118\n",
    "Epoch #234: Loss:1.5904, Accuracy:0.2501, Validation Loss:1.6112, Validation Accuracy:0.2135\n",
    "Epoch #235: Loss:1.5903, Accuracy:0.2509, Validation Loss:1.6112, Validation Accuracy:0.2118\n",
    "Epoch #236: Loss:1.5900, Accuracy:0.2534, Validation Loss:1.6114, Validation Accuracy:0.2348\n",
    "Epoch #237: Loss:1.5899, Accuracy:0.2554, Validation Loss:1.6123, Validation Accuracy:0.2332\n",
    "Epoch #238: Loss:1.5897, Accuracy:0.2493, Validation Loss:1.6120, Validation Accuracy:0.2332\n",
    "Epoch #239: Loss:1.5896, Accuracy:0.2509, Validation Loss:1.6124, Validation Accuracy:0.2365\n",
    "Epoch #240: Loss:1.5902, Accuracy:0.2530, Validation Loss:1.6122, Validation Accuracy:0.2348\n",
    "Epoch #241: Loss:1.5895, Accuracy:0.2554, Validation Loss:1.6124, Validation Accuracy:0.2250\n",
    "Epoch #242: Loss:1.5898, Accuracy:0.2559, Validation Loss:1.6123, Validation Accuracy:0.2250\n",
    "Epoch #243: Loss:1.5902, Accuracy:0.2554, Validation Loss:1.6121, Validation Accuracy:0.2365\n",
    "Epoch #244: Loss:1.5894, Accuracy:0.2563, Validation Loss:1.6128, Validation Accuracy:0.2348\n",
    "Epoch #245: Loss:1.5900, Accuracy:0.2530, Validation Loss:1.6128, Validation Accuracy:0.2233\n",
    "Epoch #246: Loss:1.5895, Accuracy:0.2517, Validation Loss:1.6127, Validation Accuracy:0.2118\n",
    "Epoch #247: Loss:1.5894, Accuracy:0.2509, Validation Loss:1.6124, Validation Accuracy:0.2003\n",
    "Epoch #248: Loss:1.5901, Accuracy:0.2534, Validation Loss:1.6133, Validation Accuracy:0.2332\n",
    "Epoch #249: Loss:1.5892, Accuracy:0.2526, Validation Loss:1.6127, Validation Accuracy:0.2118\n",
    "Epoch #250: Loss:1.5893, Accuracy:0.2526, Validation Loss:1.6130, Validation Accuracy:0.2184\n",
    "Epoch #251: Loss:1.5897, Accuracy:0.2501, Validation Loss:1.6127, Validation Accuracy:0.2315\n",
    "Epoch #252: Loss:1.5896, Accuracy:0.2550, Validation Loss:1.6142, Validation Accuracy:0.2233\n",
    "Epoch #253: Loss:1.5903, Accuracy:0.2505, Validation Loss:1.6138, Validation Accuracy:0.2135\n",
    "Epoch #254: Loss:1.5893, Accuracy:0.2522, Validation Loss:1.6130, Validation Accuracy:0.2365\n",
    "Epoch #255: Loss:1.5893, Accuracy:0.2554, Validation Loss:1.6125, Validation Accuracy:0.2365\n",
    "Epoch #256: Loss:1.5899, Accuracy:0.2534, Validation Loss:1.6148, Validation Accuracy:0.2233\n",
    "Epoch #257: Loss:1.5896, Accuracy:0.2559, Validation Loss:1.6130, Validation Accuracy:0.2266\n",
    "Epoch #258: Loss:1.5891, Accuracy:0.2472, Validation Loss:1.6133, Validation Accuracy:0.2365\n",
    "Epoch #259: Loss:1.5888, Accuracy:0.2538, Validation Loss:1.6130, Validation Accuracy:0.2315\n",
    "Epoch #260: Loss:1.5888, Accuracy:0.2559, Validation Loss:1.6136, Validation Accuracy:0.2414\n",
    "Epoch #261: Loss:1.5890, Accuracy:0.2546, Validation Loss:1.6136, Validation Accuracy:0.2233\n",
    "Epoch #262: Loss:1.5889, Accuracy:0.2501, Validation Loss:1.6129, Validation Accuracy:0.2135\n",
    "Epoch #263: Loss:1.5887, Accuracy:0.2559, Validation Loss:1.6135, Validation Accuracy:0.2266\n",
    "Epoch #264: Loss:1.5887, Accuracy:0.2505, Validation Loss:1.6136, Validation Accuracy:0.2118\n",
    "Epoch #265: Loss:1.5880, Accuracy:0.2591, Validation Loss:1.6143, Validation Accuracy:0.2348\n",
    "Epoch #266: Loss:1.5889, Accuracy:0.2563, Validation Loss:1.6136, Validation Accuracy:0.2315\n",
    "Epoch #267: Loss:1.5885, Accuracy:0.2554, Validation Loss:1.6131, Validation Accuracy:0.2085\n",
    "Epoch #268: Loss:1.5881, Accuracy:0.2485, Validation Loss:1.6134, Validation Accuracy:0.2332\n",
    "Epoch #269: Loss:1.5881, Accuracy:0.2546, Validation Loss:1.6151, Validation Accuracy:0.2315\n",
    "Epoch #270: Loss:1.5883, Accuracy:0.2567, Validation Loss:1.6142, Validation Accuracy:0.2266\n",
    "Epoch #271: Loss:1.5878, Accuracy:0.2522, Validation Loss:1.6135, Validation Accuracy:0.2315\n",
    "Epoch #272: Loss:1.5878, Accuracy:0.2538, Validation Loss:1.6132, Validation Accuracy:0.2332\n",
    "Epoch #273: Loss:1.5876, Accuracy:0.2546, Validation Loss:1.6140, Validation Accuracy:0.2315\n",
    "Epoch #274: Loss:1.5875, Accuracy:0.2530, Validation Loss:1.6143, Validation Accuracy:0.1954\n",
    "Epoch #275: Loss:1.5875, Accuracy:0.2563, Validation Loss:1.6131, Validation Accuracy:0.2381\n",
    "Epoch #276: Loss:1.5876, Accuracy:0.2546, Validation Loss:1.6142, Validation Accuracy:0.2315\n",
    "Epoch #277: Loss:1.5872, Accuracy:0.2546, Validation Loss:1.6145, Validation Accuracy:0.2003\n",
    "Epoch #278: Loss:1.5881, Accuracy:0.2480, Validation Loss:1.6151, Validation Accuracy:0.2135\n",
    "Epoch #279: Loss:1.5876, Accuracy:0.2485, Validation Loss:1.6142, Validation Accuracy:0.1987\n",
    "Epoch #280: Loss:1.5876, Accuracy:0.2567, Validation Loss:1.6144, Validation Accuracy:0.2348\n",
    "Epoch #281: Loss:1.5874, Accuracy:0.2554, Validation Loss:1.6139, Validation Accuracy:0.2020\n",
    "Epoch #282: Loss:1.5881, Accuracy:0.2501, Validation Loss:1.6147, Validation Accuracy:0.2020\n",
    "Epoch #283: Loss:1.5877, Accuracy:0.2546, Validation Loss:1.6153, Validation Accuracy:0.1954\n",
    "Epoch #284: Loss:1.5878, Accuracy:0.2563, Validation Loss:1.6129, Validation Accuracy:0.2135\n",
    "Epoch #285: Loss:1.5876, Accuracy:0.2448, Validation Loss:1.6141, Validation Accuracy:0.2118\n",
    "Epoch #286: Loss:1.5872, Accuracy:0.2600, Validation Loss:1.6147, Validation Accuracy:0.2020\n",
    "Epoch #287: Loss:1.5866, Accuracy:0.2550, Validation Loss:1.6148, Validation Accuracy:0.2020\n",
    "Epoch #288: Loss:1.5871, Accuracy:0.2513, Validation Loss:1.6143, Validation Accuracy:0.1823\n",
    "Epoch #289: Loss:1.5867, Accuracy:0.2579, Validation Loss:1.6148, Validation Accuracy:0.2365\n",
    "Epoch #290: Loss:1.5882, Accuracy:0.2513, Validation Loss:1.6146, Validation Accuracy:0.1938\n",
    "Epoch #291: Loss:1.5865, Accuracy:0.2522, Validation Loss:1.6135, Validation Accuracy:0.1954\n",
    "Epoch #292: Loss:1.5868, Accuracy:0.2542, Validation Loss:1.6144, Validation Accuracy:0.2118\n",
    "Epoch #293: Loss:1.5863, Accuracy:0.2563, Validation Loss:1.6154, Validation Accuracy:0.1954\n",
    "Epoch #294: Loss:1.5863, Accuracy:0.2542, Validation Loss:1.6146, Validation Accuracy:0.2003\n",
    "Epoch #295: Loss:1.5867, Accuracy:0.2559, Validation Loss:1.6147, Validation Accuracy:0.1987\n",
    "Epoch #296: Loss:1.5862, Accuracy:0.2546, Validation Loss:1.6143, Validation Accuracy:0.2020\n",
    "Epoch #297: Loss:1.5871, Accuracy:0.2530, Validation Loss:1.6153, Validation Accuracy:0.1954\n",
    "Epoch #298: Loss:1.5864, Accuracy:0.2501, Validation Loss:1.6146, Validation Accuracy:0.2266\n",
    "Epoch #299: Loss:1.5864, Accuracy:0.2526, Validation Loss:1.6138, Validation Accuracy:0.2003\n",
    "Epoch #300: Loss:1.5862, Accuracy:0.2550, Validation Loss:1.6149, Validation Accuracy:0.1938\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61486185, Accuracy:0.1938\n",
    "Labels: ['03', '04', '05', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03  04  05  01  02\n",
    "t:03  10   6  60  39   0\n",
    "t:04  11  10  47  44   0\n",
    "t:05  25  15  57  45   0\n",
    "t:01  14  10  61  41   0\n",
    "t:02  14  16  44  40   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.14      0.09      0.11       115\n",
    "          04       0.18      0.09      0.12       112\n",
    "          05       0.21      0.40      0.28       142\n",
    "          01       0.20      0.33      0.24       126\n",
    "          02       0.00      0.00      0.00       114\n",
    "\n",
    "    accuracy                           0.19       609\n",
    "   macro avg       0.14      0.18      0.15       609\n",
    "weighted avg       0.15      0.19      0.16       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 07:44:02 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6133593639912471, 1.6106234577489016, 1.6078494156914196, 1.6073301462900071, 1.6075642610026895, 1.607121550586619, 1.6069186628354202, 1.606868653070359, 1.6063899629809, 1.6066869432703028, 1.6058922546054735, 1.6057690701069698, 1.6064925510895076, 1.606771205054911, 1.6064742778126633, 1.6063795590831338, 1.6062758096137462, 1.606471763847301, 1.6063281442535726, 1.6066543685978856, 1.6062600364043012, 1.6067367864555522, 1.6065855965825724, 1.6066543065464163, 1.6074158616841132, 1.607065255027295, 1.607727421524098, 1.6078693931325903, 1.6078014074287978, 1.607753160747597, 1.607317903358948, 1.6070163612099508, 1.6071338420626762, 1.6070877266634862, 1.6061184905432715, 1.606426905920157, 1.606598373900102, 1.6072235874746037, 1.6077643174843248, 1.6140950360321646, 1.6050278155870235, 1.6090722745666755, 1.605439333884391, 1.605579494059771, 1.6059025022979636, 1.605560545850857, 1.6061690327373437, 1.6063110836229497, 1.606503689817607, 1.6054754856184785, 1.6063952320706472, 1.6055104352766265, 1.6058580176583652, 1.6065250733020076, 1.607276902214451, 1.6070847789250766, 1.6063946984671607, 1.6063463411894925, 1.6067340236970749, 1.606477409748021, 1.6072556126881115, 1.607324135127326, 1.6071674422481768, 1.6064534758895097, 1.607557822330832, 1.6070970279242605, 1.608332535707696, 1.6083273216225635, 1.6094464106708521, 1.6090124980569473, 1.608623543396372, 1.6086693134996888, 1.6089795833737979, 1.6091576156945064, 1.6087575158462148, 1.6092882665311565, 1.6090034316913249, 1.608857000011137, 1.609196126754648, 1.6098893980674556, 1.609569899945815, 1.6095346527538081, 1.609780651203713, 1.6096995689207305, 1.6093946748179169, 1.6110671737119677, 1.6110525101863693, 1.6111173862698434, 1.6117474864464871, 1.6113605139095013, 1.6114128060724544, 1.6112090043833691, 1.6107668285495151, 1.6099729207153195, 1.6107637058142175, 1.6106006899490732, 1.610500147385746, 1.6101007050481335, 1.6084241743745475, 1.6091847521526668, 1.608643903912386, 1.6088237981686646, 1.609310838780771, 1.610176867060669, 1.6093000870424343, 1.6088734411057972, 1.6086248376686585, 1.6080693063281832, 1.60772472081709, 1.6080347606896963, 1.6079158093933206, 1.6072617767283874, 1.6075399458310482, 1.6077499264371022, 1.607955515286801, 1.6070552962558415, 1.6076124385855664, 1.6084098044678887, 1.6076705571270145, 1.6077078103236182, 1.607959859672634, 1.6076555439991316, 1.607979835352091, 1.607748421737909, 1.607225356822335, 1.6073704129956625, 1.608061847036891, 1.6080773794788055, 1.6088925132219036, 1.608503594382839, 1.6096706288592961, 1.6105916703667351, 1.6117184586908626, 1.6125872630399631, 1.6124691788981897, 1.6101812110550102, 1.611073813806418, 1.611307603189315, 1.6117076676075877, 1.6099284343140075, 1.6093733999725242, 1.611086572723827, 1.6122642027333451, 1.6109597281673662, 1.610592695096835, 1.6100360979196082, 1.6088170398436548, 1.6092097770991585, 1.6087194921935133, 1.6092754083705458, 1.6087406627063094, 1.6079207118508851, 1.6093911018669116, 1.6097177913036254, 1.6096478287613842, 1.608805707914293, 1.608013362524349, 1.6086100529763108, 1.6084199799301198, 1.6076871170394722, 1.606243718238104, 1.6053050339515573, 1.60741116750025, 1.6080387462731849, 1.6078153051962014, 1.6088247232640709, 1.6088080887724026, 1.60956958381609, 1.6086927069036048, 1.6087111794498363, 1.6104298880926298, 1.6101762653375886, 1.6108039707581594, 1.6091895608479165, 1.6097451671590945, 1.608484825868716, 1.608686849792994, 1.6082899566550168, 1.6078773639080755, 1.6093618627056503, 1.6096216293391337, 1.612585230413916, 1.609508415748333, 1.610608574987828, 1.6109472078642821, 1.6103499761747413, 1.6100718450467966, 1.6093401413637234, 1.6093811470103774, 1.6104602837210218, 1.6114986191437946, 1.6114843482845913, 1.611758559991182, 1.610805747935729, 1.6106485388744836, 1.6114719884931943, 1.612043115697275, 1.6108840196982197, 1.6109047585911742, 1.610441184591973, 1.6106209903710778, 1.6111447676061996, 1.6117375231728766, 1.610963264709623, 1.611365527942263, 1.6107237391871185, 1.6107162421168562, 1.6109837542222247, 1.6103120455013706, 1.6102600191614311, 1.6104052241016882, 1.6111524089412346, 1.610314833119585, 1.6106123333102573, 1.6109308364551838, 1.6111949257466984, 1.61050291660384, 1.6106168431014263, 1.6124597044022408, 1.6106467170668353, 1.6112286453372349, 1.6102752198139434, 1.612198478678373, 1.6113320632129664, 1.611105515843346, 1.611076568539311, 1.6117160416197502, 1.6115334090732394, 1.6109107687751256, 1.6109046201987807, 1.6111806045807837, 1.6116009226377765, 1.6111999709030678, 1.6111774918285302, 1.6112357147025749, 1.6113701262106057, 1.6122671968635471, 1.6119909270839348, 1.6124381149930906, 1.6122381593206245, 1.6124244983168854, 1.6122998472896508, 1.6121351685625775, 1.6127606743857974, 1.612832940466494, 1.6126853834427832, 1.6123811088759323, 1.613334415972918, 1.612701221052649, 1.613017674737376, 1.6126762346681116, 1.6141544825142045, 1.613819863800149, 1.6130051131318943, 1.612519178288715, 1.614835213362094, 1.6129984528756103, 1.6132903858554384, 1.6129518786478905, 1.6135831949941826, 1.613614520220138, 1.6128945237114316, 1.6135392490474658, 1.613573676063901, 1.614342446593424, 1.613550817829439, 1.613136327325417, 1.613359540358357, 1.6150711355929697, 1.6141689882685595, 1.6135066199576718, 1.6132205063094842, 1.613979399497873, 1.6142847181736737, 1.6130909256159967, 1.6142431571957316, 1.6144931052118687, 1.6151194977642866, 1.6142031320405907, 1.6144022362181314, 1.6138682038521728, 1.6146767143349734, 1.6153266772456554, 1.6128608123422257, 1.6141316998376831, 1.6147289131271036, 1.6147570610046387, 1.6142637481047406, 1.614778892355795, 1.6145791189228176, 1.6134832641369798, 1.614361876728891, 1.6153842367366422, 1.6146044306371403, 1.6146954558361535, 1.6142585698411187, 1.615259836851474, 1.6146046210979592, 1.6137994713775434, 1.614861943255896], 'val_acc': [0.20689655081881286, 0.20689655081881286, 0.20689655081881286, 0.2151067318342785, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.2348111656254344, 0.22824302102838243, 0.22988505725105016, 0.23316912950063964, 0.22660098490358768, 0.22660098490358768, 0.22660098490358768, 0.22660098490358768, 0.22824302102838243, 0.22824302102838243, 0.22495894877879294, 0.22824302102838243, 0.22660098500146067, 0.22660098490358768, 0.2282430211262554, 0.2282430211262554, 0.22988505725105016, 0.22331691275187118, 0.2315270933758449, 0.2282430211262554, 0.2282430211262554, 0.23481116572330737, 0.22660098490358768, 0.22660098490358768, 0.2315270933758449, 0.23316912950063964, 0.2282430212241284, 0.23316912950063964, 0.23316912950063964, 0.2282430211262554, 0.2282430213220014, 0.22167487662707644, 0.22167487662707644, 0.22988505725105016, 0.2282430211262554, 0.211822659682562, 0.23481116552756143, 0.22660098509933366, 0.22660098519720664, 0.2282430213220014, 0.22660098509933366, 0.22660098519720664, 0.22331691294761713, 0.22331691294761713, 0.2282430213220014, 0.23152709357159088, 0.2282430213220014, 0.23152709357159088, 0.2036124794500802, 0.22988505744679613, 0.2282430212241284, 0.23152709357159088, 0.23152709357159088, 0.23316912969638562, 0.2068965516017967, 0.20361247915646125, 0.22988505744679613, 0.21018062394925918, 0.21018062394925918, 0.20853858782446444, 0.20853858782446444, 0.22660098509933366, 0.2068965516996697, 0.20032840710261773, 0.2068965516996697, 0.20361247935220722, 0.2068965516996697, 0.2068965516996697, 0.21018062394925918, 0.2068965516996697, 0.2068965516996697, 0.20361247935220722, 0.20525451557487495, 0.2036124794500802, 0.21018062177158536, 0.21346469402117488, 0.2036124794500802, 0.20853858564679062, 0.2068965516996697, 0.2068965516996697, 0.20689654942412292, 0.20525451329932815, 0.2036124794500802, 0.20525451557487495, 0.20689654942412292, 0.2101806238513862, 0.19868637078207702, 0.22331691284974417, 0.20525451329932815, 0.2068965516017967, 0.20525451329932815, 0.2019704431295395, 0.2036124771745334, 0.20525451329932815, 0.20853858772659145, 0.2068965516017967, 0.20853858772659145, 0.20689654942412292, 0.20853858772659145, 0.20689654942412292, 0.20525451547700196, 0.20525451329932815, 0.20361247935220722, 0.2036124794500802, 0.2003284050228169, 0.2068965516017967, 0.2003284072004907, 0.2003284072004907, 0.20361247935220722, 0.2101806238513862, 0.2101806238513862, 0.21182265997618094, 0.21346469610097568, 0.21346469610097568, 0.2101806237535132, 0.21182265997618094, 0.21182265997618094, 0.20853858772659145, 0.19704433455940928, 0.21510673222577043, 0.19211822638077103, 0.20525451547700196, 0.198686370977823, 0.20197044332528544, 0.19868637087995, 0.2036124794500802, 0.2003284072004907, 0.2068965516996697, 0.20853858782446444, 0.20525451557487495, 0.2101806216737124, 0.2068965516017967, 0.20525451329932815, 0.20525451339720113, 0.19868637107569595, 0.19868637107569595, 0.2036124794500802, 0.20525451557487495, 0.2068965516996697, 0.21018062177158536, 0.20197044332528544, 0.20032840710261773, 0.20197044322741248, 0.20197044104973866, 0.20197044322741248, 0.20689654942412292, 0.2036124794500802, 0.2036124771745334, 0.20853858554891766, 0.2134646939233019, 0.21018062394925918, 0.205254515281256, 0.20361247925433423, 0.20032840710261773, 0.19211822618502505, 0.19047618996235732, 0.19211822638077103, 0.1904761900602303, 0.19211822618502505, 0.1937602623098198, 0.20525451547700196, 0.20361247905858829, 0.2036124789607153, 0.205254515281256, 0.20689655140605073, 0.20525451508551004, 0.2036124789607153, 0.20361247915646125, 0.2019704430316665, 0.19704433475515526, 0.21018062355776726, 0.21018062355776726, 0.1888341542290545, 0.19376026240769278, 0.20853858733509953, 0.21018062345989427, 0.21018062345989427, 0.21182265958468902, 0.19868637078207702, 0.19704433465728227, 0.19868637048845808, 0.21182265948681606, 0.19868637078207702, 0.19704433465728227, 0.20853858743297252, 0.19704433465728227, 0.20197044273804757, 0.20197044273804757, 0.2134646956116108, 0.2036124788628423, 0.21346469580735675, 0.21346469580735675, 0.21018062336202128, 0.20853858723722654, 0.2151067319321515, 0.21182265958468902, 0.2348111652339425, 0.23645320135873724, 0.20032840661325282, 0.20689655121030479, 0.23645320135873724, 0.23316912910914772, 0.20197044273804757, 0.20032840661325282, 0.23645320135873724, 0.21018062326414833, 0.22167487613771153, 0.20361247876496932, 0.21346469590522973, 0.21182265938894307, 0.20689655121030479, 0.23481116533181545, 0.21510673203002448, 0.21510673203002448, 0.224958948485174, 0.224958948485174, 0.2348111651360695, 0.23809523748353198, 0.23645320135873724, 0.20032840661325282, 0.211822659682562, 0.2134646956116108, 0.211822659682562, 0.23481116533181545, 0.23316912910914772, 0.23316912901127476, 0.2364532014566102, 0.2348111651360695, 0.224958948485174, 0.224958948485174, 0.23645320155448318, 0.2348111652339425, 0.22331691236037926, 0.211822659682562, 0.20032840661325282, 0.23316912910914772, 0.211822659682562, 0.21839080408386802, 0.23152709298435298, 0.22331691236037926, 0.2134646956116108, 0.2364532014566102, 0.2364532014566102, 0.22331691236037926, 0.22660098460996875, 0.2364532014566102, 0.23152709298435298, 0.2413793100267404, 0.22331691236037926, 0.21346469600310272, 0.2266009844142228, 0.211822659682562, 0.23481116533181545, 0.23152709298435298, 0.20853858743297252, 0.23316912910914772, 0.23152709298435298, 0.2266009844142228, 0.23152709298435298, 0.23316912910914772, 0.23152709298435298, 0.1954022981409956, 0.238095237385659, 0.23152709298435298, 0.20032840690687176, 0.2134646956116108, 0.19868637078207702, 0.23481116533181545, 0.20197044254230162, 0.20197044283592056, 0.1954022981409956, 0.2134646955137378, 0.21182265987830795, 0.20197044264017458, 0.20197044254230162, 0.1822660094362566, 0.2364532014566102, 0.19376026201620086, 0.19540229853248753, 0.21182265958468902, 0.1954022981409956, 0.20032840641750688, 0.19868637078207702, 0.20197044264017458, 0.1954022981409956, 0.22660098451209576, 0.20032840641750688, 0.19376026201620086], 'loss': [1.6183186162913359, 1.6119415361044098, 1.6068168652375865, 1.6053344733905988, 1.604199466176591, 1.6043445928385616, 1.60394542599116, 1.6033046681778143, 1.6029341331987166, 1.6044272508464554, 1.6039806383591169, 1.6030886555599235, 1.6027239999731953, 1.602884895502909, 1.6026059876232421, 1.6029034358030472, 1.602182751465627, 1.6014252375283524, 1.6011370803296443, 1.601074661219634, 1.600728945957317, 1.6013743823558644, 1.6008125828032131, 1.6001225531713184, 1.600007427742349, 1.6009804796144458, 1.6002708883990497, 1.6002810033684638, 1.6000677266894425, 1.6005816484134054, 1.6006101527497998, 1.6003707289940523, 1.6001724479134811, 1.5995065739022634, 1.5994339804874553, 1.600280460097217, 1.5989022510007667, 1.599609417934927, 1.5982255404489976, 1.6060774669510138, 1.605763940497835, 1.6015452484820167, 1.6025509710429386, 1.600270393839607, 1.600773179163923, 1.5996401323674885, 1.5985581686364552, 1.5984071291937232, 1.600220348555939, 1.5993341708330158, 1.598431792494208, 1.5983735865391255, 1.5975165306420296, 1.5981966356477209, 1.5980784909191563, 1.597840211034066, 1.5980321304509282, 1.5974630997166253, 1.5975150000632912, 1.5973568228480752, 1.5977980563772778, 1.5973970676594447, 1.5977334081759442, 1.5975959388627163, 1.5970173260514495, 1.597586161008361, 1.5974209568583746, 1.5981760206653353, 1.5974745970731887, 1.5975485460469365, 1.5972530587742706, 1.596952547087072, 1.5964676622003011, 1.596472336330453, 1.5963327361328157, 1.5968963149636677, 1.596183130775389, 1.5962219881570805, 1.595995421967712, 1.596139111215329, 1.595649837125743, 1.5958820763065096, 1.5959030802489795, 1.595520038624319, 1.595470757210279, 1.595896061489959, 1.5959406722497647, 1.5960263225821745, 1.596322799169552, 1.5960867795121743, 1.595385394693645, 1.5954645655727975, 1.5957571475168029, 1.596183902869724, 1.5952840078537958, 1.595931731944701, 1.5944907654971803, 1.5958273979672661, 1.595864477686324, 1.5950353194066387, 1.5954973795085963, 1.5952615123020306, 1.5948465525492017, 1.5953870286686953, 1.5955621582771473, 1.5958974650753108, 1.5957977501029106, 1.595396650008842, 1.5952829994949718, 1.5953518274628407, 1.5953391660655059, 1.5950863838685367, 1.5956515991712252, 1.5954679163084873, 1.5954507286298936, 1.5949607348784773, 1.59488299407019, 1.5950603966840238, 1.5949583949983976, 1.5951614832731242, 1.5946435378562254, 1.5944573025439064, 1.5940734945528316, 1.5940967297407147, 1.5938195257461047, 1.5935589000184922, 1.5930853772701914, 1.5925963058119192, 1.593612861731214, 1.593246977089367, 1.5929317526748783, 1.5931882937096473, 1.5955007139906991, 1.5943635796129827, 1.5939484171309266, 1.594078089422269, 1.593745286998318, 1.5934490557813545, 1.5931254368298353, 1.5929886432154223, 1.593454550325993, 1.5944607117827179, 1.5947094503124637, 1.5961851476399072, 1.5964517161097125, 1.5943851351493192, 1.5935303252825257, 1.5936152469450933, 1.5940587287076444, 1.5943020027520964, 1.5929310783468478, 1.5928611716695389, 1.5923013655801572, 1.5923886794573963, 1.5929137833064586, 1.5925740061354587, 1.592114918922252, 1.592294573147439, 1.5922937118542022, 1.591965697384468, 1.5926912921654859, 1.591232360461899, 1.5920589558152938, 1.5928186872901369, 1.5916976465092059, 1.5915532634977931, 1.591497384204512, 1.5915850983997635, 1.5912274023345854, 1.5923955685793743, 1.5919878744736344, 1.5917896514555756, 1.5928186718198554, 1.5936599205162, 1.5932215112925066, 1.5932540507287216, 1.592953326472022, 1.5920145634508232, 1.5924982636860998, 1.5922049274190007, 1.5918875909194319, 1.5920812453577406, 1.5922785086798228, 1.5935384758199265, 1.5939016528687682, 1.593313863438992, 1.5930856484896838, 1.5925153590815269, 1.5927881028373139, 1.592559082552148, 1.5925234281062102, 1.5934302593403529, 1.593053870083615, 1.5921865498505579, 1.5925867180559914, 1.5933430980607959, 1.5934459321063157, 1.593171087476507, 1.5930195759454058, 1.5926604596006797, 1.592344169303377, 1.5922333046151382, 1.592241008521595, 1.5931276698376853, 1.5914441013238267, 1.5929140035131875, 1.591047449376304, 1.592534315072046, 1.5910205161547024, 1.5927826791818136, 1.5907735428036605, 1.5916534499221269, 1.592363856264698, 1.5913065181375774, 1.590878661261447, 1.5917406587874865, 1.5909403279087138, 1.592175844758443, 1.591854700072835, 1.592061514971927, 1.5917659252331242, 1.591444995173194, 1.5921138129929497, 1.5922162296835647, 1.591166746151276, 1.591201704630372, 1.5907690495191413, 1.5907566515082452, 1.5902352433919418, 1.5905380547658619, 1.5902830059768238, 1.5904288320815538, 1.5900557067115204, 1.5903712627579298, 1.5902554877240065, 1.5900332708867913, 1.589925835705391, 1.5896869318196416, 1.5895630106544103, 1.5902419142164979, 1.5894980197569673, 1.589769134139623, 1.5902188148341874, 1.5893777744970772, 1.5900400891685877, 1.5895374206546886, 1.5894204675294534, 1.590140379625669, 1.5892068781647104, 1.5893104841576955, 1.5896558316091738, 1.589552604638086, 1.5903154429958586, 1.589255714024851, 1.589341906404593, 1.5899397716385137, 1.5896153309996368, 1.589124735375939, 1.588780412732698, 1.5888390176839653, 1.5890340953881736, 1.5889231756237743, 1.5887370393995877, 1.5887135677024324, 1.5879942143477455, 1.5888773391378979, 1.5884530064996019, 1.5880987893384584, 1.5880922814903808, 1.5883232340430822, 1.5877639231985354, 1.587763533308276, 1.5875896805365717, 1.58753312906935, 1.5875369481237518, 1.5876104351431437, 1.5872043071096682, 1.5880979023430137, 1.5876275281886545, 1.5875972379159633, 1.5874497751435706, 1.5881190732764023, 1.5877022729027688, 1.5877627860349306, 1.5875573781481513, 1.587211935280285, 1.5865567823210291, 1.5870607750616523, 1.5866506929025512, 1.5881722928072637, 1.5864909288574782, 1.5867905058165594, 1.5862728714208583, 1.586349937558419, 1.5867332468287412, 1.586236177920316, 1.5871311654789981, 1.5863921917929051, 1.5863653137453773, 1.5861816652012066], 'acc': [0.20000000026314166, 0.20657084158313838, 0.21601642676202668, 0.23531827404636133, 0.2328542102778472, 0.2328542087112364, 0.2328542094945418, 0.23285420951290053, 0.23572895303521557, 0.2328542091028891, 0.2328542098861945, 0.23285421049203225, 0.23572895223355148, 0.24763860452836053, 0.24640657186997744, 0.24024640683398354, 0.2398357299808604, 0.24681724831308918, 0.24681724831308918, 0.24681724833144789, 0.24517453842828896, 0.2451745376633423, 0.2464065712824984, 0.24394250480913285, 0.24312115112124527, 0.24681724733395743, 0.24599589425190763, 0.24558521682966417, 0.245174539015768, 0.24804928136312496, 0.2427104725240437, 0.24312114937716686, 0.24599589327277588, 0.24229979431849485, 0.2422997941226685, 0.24312114898551415, 0.25092402625622445, 0.2464065716741511, 0.24558521565470606, 0.23408624195709854, 0.23285420970872686, 0.24435318319214933, 0.24065708484370604, 0.24106776109099143, 0.2390143743714268, 0.24435318221301758, 0.2464065716741511, 0.244353180413863, 0.2418891168962514, 0.24804928175477767, 0.24271047193656467, 0.24394250422165378, 0.2447638594394347, 0.24640657147832476, 0.24188911671878377, 0.24106776185593812, 0.24188911826703582, 0.24353182797436843, 0.24394250578826457, 0.24435318201719122, 0.24517453686167817, 0.24188911748373043, 0.24599589386025494, 0.24517453881994164, 0.2447638592619671, 0.24229979353518946, 0.24558521626054383, 0.24353182776018334, 0.2435318267810516, 0.2431211507479513, 0.24435318321050806, 0.24271047271987006, 0.2443531837979871, 0.2480492815589513, 0.24394250537825315, 0.24271047334406656, 0.24229979549345296, 0.24147844006148697, 0.24476386139769818, 0.24599589327277588, 0.24599589427026636, 0.24681724931057963, 0.24517453999489974, 0.24312115131707163, 0.2427104725240437, 0.24558521467557434, 0.24353182738688936, 0.2422997953159853, 0.24147844084479236, 0.2410677614642854, 0.24353182854348873, 0.24106776087680637, 0.2422997945143212, 0.24312115094377765, 0.24394250420329508, 0.24599589464356034, 0.24435318221301758, 0.24271047115325928, 0.24476385924360836, 0.24188911650459868, 0.24229979568927928, 0.2451745397990734, 0.23983572902008737, 0.2439425063573849, 0.24517453903412673, 0.24147843910071395, 0.24147843908235522, 0.24229979549345296, 0.24435318340633438, 0.2459958920978178, 0.24640657226163015, 0.24558521565470606, 0.24188911789374185, 0.24517453825082133, 0.25462012344806834, 0.24928131637142426, 0.25174538207984315, 0.24229979510180025, 0.24599589346860223, 0.25174538129653773, 0.25379876641277416, 0.2509240241021346, 0.25256673516189293, 0.25256673594519835, 0.2550307998911801, 0.2554414800549924, 0.2550308014761496, 0.2583162226165345, 0.2558521545031232, 0.25708418993979265, 0.2517453787691539, 0.250924023924667, 0.2574948671662098, 0.25790554499846463, 0.25872690158703, 0.2599589334621077, 0.26078028832495337, 0.2632443530358818, 0.26201232037749866, 0.260780285332482, 0.25913757724683634, 0.25420944582999855, 0.2533880913771643, 0.24804927897649134, 0.2431211499462872, 0.24969199203123058, 0.25133470367846794, 0.2517453807090587, 0.25174538013993836, 0.25297741356326814, 0.2587268994512989, 0.25954825228244616, 0.2632443510409009, 0.26283367582782335, 0.26365503006647256, 0.26365503165144205, 0.2624229991705266, 0.26365503222056236, 0.26488706389981376, 0.2583162239689602, 0.2603696106885248, 0.25708418837318187, 0.25338809078968527, 0.260780288129127, 0.26324435325006684, 0.2566735117342438, 0.26365503222056236, 0.2583162231856548, 0.26036961108017753, 0.2595482554523852, 0.25749486438792346, 0.26324435205675006, 0.2476386047241869, 0.2562628354869584, 0.25831622281236083, 0.25708418657402726, 0.25585215649810417, 0.2574948663461869, 0.25667351251754916, 0.2570841887648346, 0.25626283313704223, 0.25379876901359283, 0.2509240250812664, 0.2550308010844969, 0.2484599589995535, 0.24722792612698533, 0.24681724890056822, 0.2509240239063083, 0.25215605754382314, 0.2513347040701206, 0.24845996093945827, 0.2455852164563702, 0.2501026678501458, 0.254209447200783, 0.24681724811726283, 0.24517453881994164, 0.23942505181202897, 0.24106776068098001, 0.24599589346860223, 0.2550308020452699, 0.2537987688177665, 0.251745379944112, 0.25174537914244793, 0.25174538168819044, 0.2492813153922925, 0.2546201230564157, 0.25338809139552304, 0.2505133476590229, 0.251745379356633, 0.25544147629757435, 0.2546201228605893, 0.25338809157299064, 0.2464065716741511, 0.2546201228605893, 0.2533880898105535, 0.25420944543834584, 0.24928131323820266, 0.25667351193007015, 0.24599589425190763, 0.2521560579354758, 0.25092402429796096, 0.2509240241021346, 0.2550308030244017, 0.2492813142173344, 0.25913757822596806, 0.2505133480690343, 0.2533880898105535, 0.2558521553415048, 0.2550308008703118, 0.2505133480690343, 0.2542094456341722, 0.258316221619044, 0.2537987680161024, 0.2501026696676591, 0.2509240254912778, 0.253388088598878, 0.25544147868420797, 0.24928131300565887, 0.2509240241021346, 0.2529774111766345, 0.2554414800549924, 0.2558521578688886, 0.2554414794858721, 0.2562628333512273, 0.25297741356326814, 0.2517453807090587, 0.2509240228904591, 0.2533880911996967, 0.2525667353577193, 0.2525667343418701, 0.2501026676543195, 0.2550307998911801, 0.2505133459149445, 0.25215605715217043, 0.25544147868420797, 0.2533880902022062, 0.25585215567808134, 0.24722792612698533, 0.2537987691910605, 0.25585215532314604, 0.2546201240539061, 0.25010267025513816, 0.2558521568897568, 0.2505133468757175, 0.2591375790092735, 0.25626283372452124, 0.2554414764934007, 0.24845995937284748, 0.2546201240355474, 0.25667351251754916, 0.25215605658305007, 0.25379876879940777, 0.2546201248188528, 0.25297741317161543, 0.25626283133788763, 0.25462012227311026, 0.2546201230564157, 0.2480492805981783, 0.24845996015615288, 0.2566735109692971, 0.2554414794675134, 0.25010267023677946, 0.2546201234664271, 0.256262834703653, 0.2447638602410988, 0.25995893444123946, 0.2550308000870064, 0.2513347029135212, 0.2579055446068119, 0.25133470172020445, 0.25215605813130215, 0.2542094466133039, 0.2562628329412159, 0.2542094446550405, 0.2558521563022778, 0.2546201228605893, 0.25297741317161543, 0.2501026700409531, 0.2525667349660666, 0.25503079969535375]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
