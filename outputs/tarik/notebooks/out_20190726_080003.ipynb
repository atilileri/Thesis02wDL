{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf34.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 08:00:03 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '3', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['yd', 'ek', 'ib', 'sk', 'eo', 'eb', 'eg', 'sg', 'by', 'my', 'ck', 'ds', 'ce', 'mb', 'aa'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001FD8130D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001FDFBE66EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7049, Accuracy:0.0797, Validation Loss:2.7005, Validation Accuracy:0.1182\n",
    "Epoch #2: Loss:2.6977, Accuracy:0.0977, Validation Loss:2.6931, Validation Accuracy:0.1067\n",
    "Epoch #3: Loss:2.6899, Accuracy:0.1031, Validation Loss:2.6874, Validation Accuracy:0.1084\n",
    "Epoch #4: Loss:2.6847, Accuracy:0.1121, Validation Loss:2.6796, Validation Accuracy:0.1264\n",
    "Epoch #5: Loss:2.6765, Accuracy:0.1175, Validation Loss:2.6694, Validation Accuracy:0.1182\n",
    "Epoch #6: Loss:2.6669, Accuracy:0.1092, Validation Loss:2.6606, Validation Accuracy:0.1182\n",
    "Epoch #7: Loss:2.6584, Accuracy:0.1105, Validation Loss:2.6508, Validation Accuracy:0.1166\n",
    "Epoch #8: Loss:2.6490, Accuracy:0.1138, Validation Loss:2.6395, Validation Accuracy:0.1149\n",
    "Epoch #9: Loss:2.6375, Accuracy:0.1191, Validation Loss:2.6257, Validation Accuracy:0.1232\n",
    "Epoch #10: Loss:2.6246, Accuracy:0.1261, Validation Loss:2.6100, Validation Accuracy:0.1363\n",
    "Epoch #11: Loss:2.6086, Accuracy:0.1339, Validation Loss:2.5921, Validation Accuracy:0.1363\n",
    "Epoch #12: Loss:2.5914, Accuracy:0.1347, Validation Loss:2.5701, Validation Accuracy:0.1363\n",
    "Epoch #13: Loss:2.5746, Accuracy:0.1318, Validation Loss:2.5546, Validation Accuracy:0.1363\n",
    "Epoch #14: Loss:2.5589, Accuracy:0.1343, Validation Loss:2.5482, Validation Accuracy:0.1363\n",
    "Epoch #15: Loss:2.5462, Accuracy:0.1396, Validation Loss:2.5213, Validation Accuracy:0.1544\n",
    "Epoch #16: Loss:2.5386, Accuracy:0.1585, Validation Loss:2.5071, Validation Accuracy:0.1708\n",
    "Epoch #17: Loss:2.5200, Accuracy:0.1655, Validation Loss:2.5045, Validation Accuracy:0.1757\n",
    "Epoch #18: Loss:2.5142, Accuracy:0.1651, Validation Loss:2.4884, Validation Accuracy:0.1773\n",
    "Epoch #19: Loss:2.5074, Accuracy:0.1630, Validation Loss:2.4817, Validation Accuracy:0.1708\n",
    "Epoch #20: Loss:2.5013, Accuracy:0.1713, Validation Loss:2.4775, Validation Accuracy:0.1790\n",
    "Epoch #21: Loss:2.4982, Accuracy:0.1692, Validation Loss:2.4692, Validation Accuracy:0.1773\n",
    "Epoch #22: Loss:2.4939, Accuracy:0.1692, Validation Loss:2.4663, Validation Accuracy:0.1839\n",
    "Epoch #23: Loss:2.4889, Accuracy:0.1717, Validation Loss:2.4575, Validation Accuracy:0.1823\n",
    "Epoch #24: Loss:2.4850, Accuracy:0.1696, Validation Loss:2.4544, Validation Accuracy:0.1823\n",
    "Epoch #25: Loss:2.4830, Accuracy:0.1700, Validation Loss:2.4522, Validation Accuracy:0.1905\n",
    "Epoch #26: Loss:2.4812, Accuracy:0.1704, Validation Loss:2.4495, Validation Accuracy:0.1839\n",
    "Epoch #27: Loss:2.4780, Accuracy:0.1713, Validation Loss:2.4446, Validation Accuracy:0.1839\n",
    "Epoch #28: Loss:2.4760, Accuracy:0.1700, Validation Loss:2.4419, Validation Accuracy:0.1806\n",
    "Epoch #29: Loss:2.4719, Accuracy:0.1708, Validation Loss:2.4412, Validation Accuracy:0.1839\n",
    "Epoch #30: Loss:2.4704, Accuracy:0.1717, Validation Loss:2.4395, Validation Accuracy:0.1839\n",
    "Epoch #31: Loss:2.4691, Accuracy:0.1717, Validation Loss:2.4367, Validation Accuracy:0.1888\n",
    "Epoch #32: Loss:2.4692, Accuracy:0.1696, Validation Loss:2.4359, Validation Accuracy:0.1856\n",
    "Epoch #33: Loss:2.4672, Accuracy:0.1737, Validation Loss:2.4372, Validation Accuracy:0.1872\n",
    "Epoch #34: Loss:2.4666, Accuracy:0.1741, Validation Loss:2.4334, Validation Accuracy:0.1921\n",
    "Epoch #35: Loss:2.4685, Accuracy:0.1684, Validation Loss:2.4342, Validation Accuracy:0.1921\n",
    "Epoch #36: Loss:2.4680, Accuracy:0.1721, Validation Loss:2.4411, Validation Accuracy:0.1905\n",
    "Epoch #37: Loss:2.4670, Accuracy:0.1745, Validation Loss:2.4313, Validation Accuracy:0.1905\n",
    "Epoch #38: Loss:2.4684, Accuracy:0.1733, Validation Loss:2.4342, Validation Accuracy:0.1938\n",
    "Epoch #39: Loss:2.4666, Accuracy:0.1741, Validation Loss:2.4327, Validation Accuracy:0.1905\n",
    "Epoch #40: Loss:2.4659, Accuracy:0.1717, Validation Loss:2.4309, Validation Accuracy:0.1888\n",
    "Epoch #41: Loss:2.4668, Accuracy:0.1708, Validation Loss:2.4319, Validation Accuracy:0.1888\n",
    "Epoch #42: Loss:2.4667, Accuracy:0.1704, Validation Loss:2.4261, Validation Accuracy:0.1938\n",
    "Epoch #43: Loss:2.4660, Accuracy:0.1713, Validation Loss:2.4301, Validation Accuracy:0.1905\n",
    "Epoch #44: Loss:2.4640, Accuracy:0.1704, Validation Loss:2.4275, Validation Accuracy:0.1905\n",
    "Epoch #45: Loss:2.4643, Accuracy:0.1713, Validation Loss:2.4288, Validation Accuracy:0.1905\n",
    "Epoch #46: Loss:2.4634, Accuracy:0.1717, Validation Loss:2.4338, Validation Accuracy:0.1872\n",
    "Epoch #47: Loss:2.4635, Accuracy:0.1713, Validation Loss:2.4267, Validation Accuracy:0.1905\n",
    "Epoch #48: Loss:2.4651, Accuracy:0.1700, Validation Loss:2.4271, Validation Accuracy:0.1905\n",
    "Epoch #49: Loss:2.4660, Accuracy:0.1717, Validation Loss:2.4295, Validation Accuracy:0.1921\n",
    "Epoch #50: Loss:2.4607, Accuracy:0.1717, Validation Loss:2.4262, Validation Accuracy:0.1856\n",
    "Epoch #51: Loss:2.4600, Accuracy:0.1733, Validation Loss:2.4330, Validation Accuracy:0.1856\n",
    "Epoch #52: Loss:2.4619, Accuracy:0.1770, Validation Loss:2.4302, Validation Accuracy:0.1724\n",
    "Epoch #53: Loss:2.4590, Accuracy:0.1741, Validation Loss:2.4327, Validation Accuracy:0.1938\n",
    "Epoch #54: Loss:2.4611, Accuracy:0.1708, Validation Loss:2.4363, Validation Accuracy:0.1806\n",
    "Epoch #55: Loss:2.4635, Accuracy:0.1721, Validation Loss:2.4285, Validation Accuracy:0.1888\n",
    "Epoch #56: Loss:2.4625, Accuracy:0.1713, Validation Loss:2.4353, Validation Accuracy:0.1839\n",
    "Epoch #57: Loss:2.4850, Accuracy:0.1655, Validation Loss:2.4978, Validation Accuracy:0.1642\n",
    "Epoch #58: Loss:2.5364, Accuracy:0.1544, Validation Loss:2.6367, Validation Accuracy:0.1100\n",
    "Epoch #59: Loss:2.5835, Accuracy:0.1454, Validation Loss:2.4404, Validation Accuracy:0.1856\n",
    "Epoch #60: Loss:2.4739, Accuracy:0.1713, Validation Loss:2.4592, Validation Accuracy:0.1708\n",
    "Epoch #61: Loss:2.5026, Accuracy:0.1569, Validation Loss:2.4596, Validation Accuracy:0.1741\n",
    "Epoch #62: Loss:2.4841, Accuracy:0.1618, Validation Loss:2.4454, Validation Accuracy:0.1872\n",
    "Epoch #63: Loss:2.4767, Accuracy:0.1749, Validation Loss:2.4570, Validation Accuracy:0.1658\n",
    "Epoch #64: Loss:2.4744, Accuracy:0.1770, Validation Loss:2.4392, Validation Accuracy:0.1872\n",
    "Epoch #65: Loss:2.4627, Accuracy:0.1741, Validation Loss:2.4301, Validation Accuracy:0.1888\n",
    "Epoch #66: Loss:2.4623, Accuracy:0.1676, Validation Loss:2.4269, Validation Accuracy:0.1839\n",
    "Epoch #67: Loss:2.4597, Accuracy:0.1688, Validation Loss:2.4285, Validation Accuracy:0.1856\n",
    "Epoch #68: Loss:2.4575, Accuracy:0.1733, Validation Loss:2.4290, Validation Accuracy:0.1856\n",
    "Epoch #69: Loss:2.4563, Accuracy:0.1733, Validation Loss:2.4265, Validation Accuracy:0.1856\n",
    "Epoch #70: Loss:2.4551, Accuracy:0.1733, Validation Loss:2.4240, Validation Accuracy:0.1856\n",
    "Epoch #71: Loss:2.4544, Accuracy:0.1729, Validation Loss:2.4225, Validation Accuracy:0.1856\n",
    "Epoch #72: Loss:2.4540, Accuracy:0.1733, Validation Loss:2.4229, Validation Accuracy:0.1856\n",
    "Epoch #73: Loss:2.4536, Accuracy:0.1729, Validation Loss:2.4230, Validation Accuracy:0.1938\n",
    "Epoch #74: Loss:2.4553, Accuracy:0.1717, Validation Loss:2.4285, Validation Accuracy:0.1773\n",
    "Epoch #75: Loss:2.4552, Accuracy:0.1717, Validation Loss:2.4289, Validation Accuracy:0.1757\n",
    "Epoch #76: Loss:2.4548, Accuracy:0.1737, Validation Loss:2.4272, Validation Accuracy:0.1905\n",
    "Epoch #77: Loss:2.4543, Accuracy:0.1721, Validation Loss:2.4265, Validation Accuracy:0.1773\n",
    "Epoch #78: Loss:2.4541, Accuracy:0.1766, Validation Loss:2.4270, Validation Accuracy:0.1724\n",
    "Epoch #79: Loss:2.4537, Accuracy:0.1799, Validation Loss:2.4273, Validation Accuracy:0.1757\n",
    "Epoch #80: Loss:2.4513, Accuracy:0.1786, Validation Loss:2.4256, Validation Accuracy:0.1839\n",
    "Epoch #81: Loss:2.4503, Accuracy:0.1745, Validation Loss:2.4222, Validation Accuracy:0.1905\n",
    "Epoch #82: Loss:2.4517, Accuracy:0.1758, Validation Loss:2.4220, Validation Accuracy:0.1921\n",
    "Epoch #83: Loss:2.4526, Accuracy:0.1704, Validation Loss:2.4220, Validation Accuracy:0.1905\n",
    "Epoch #84: Loss:2.4505, Accuracy:0.1729, Validation Loss:2.4266, Validation Accuracy:0.1905\n",
    "Epoch #85: Loss:2.4622, Accuracy:0.1643, Validation Loss:2.4251, Validation Accuracy:0.1872\n",
    "Epoch #86: Loss:2.4530, Accuracy:0.1774, Validation Loss:2.4418, Validation Accuracy:0.1691\n",
    "Epoch #87: Loss:2.4569, Accuracy:0.1795, Validation Loss:2.4263, Validation Accuracy:0.1905\n",
    "Epoch #88: Loss:2.4493, Accuracy:0.1717, Validation Loss:2.4224, Validation Accuracy:0.1938\n",
    "Epoch #89: Loss:2.4507, Accuracy:0.1741, Validation Loss:2.4263, Validation Accuracy:0.1905\n",
    "Epoch #90: Loss:2.4489, Accuracy:0.1786, Validation Loss:2.4308, Validation Accuracy:0.1773\n",
    "Epoch #91: Loss:2.4506, Accuracy:0.1758, Validation Loss:2.4245, Validation Accuracy:0.1905\n",
    "Epoch #92: Loss:2.4511, Accuracy:0.1749, Validation Loss:2.4223, Validation Accuracy:0.1938\n",
    "Epoch #93: Loss:2.4489, Accuracy:0.1733, Validation Loss:2.4294, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4496, Accuracy:0.1754, Validation Loss:2.4260, Validation Accuracy:0.1773\n",
    "Epoch #95: Loss:2.4492, Accuracy:0.1692, Validation Loss:2.4219, Validation Accuracy:0.1938\n",
    "Epoch #96: Loss:2.4483, Accuracy:0.1721, Validation Loss:2.4270, Validation Accuracy:0.1773\n",
    "Epoch #97: Loss:2.4487, Accuracy:0.1696, Validation Loss:2.4250, Validation Accuracy:0.1905\n",
    "Epoch #98: Loss:2.4476, Accuracy:0.1717, Validation Loss:2.4250, Validation Accuracy:0.1921\n",
    "Epoch #99: Loss:2.4490, Accuracy:0.1717, Validation Loss:2.4253, Validation Accuracy:0.1888\n",
    "Epoch #100: Loss:2.4480, Accuracy:0.1745, Validation Loss:2.4232, Validation Accuracy:0.1839\n",
    "Epoch #101: Loss:2.4492, Accuracy:0.1758, Validation Loss:2.4226, Validation Accuracy:0.1790\n",
    "Epoch #102: Loss:2.4494, Accuracy:0.1721, Validation Loss:2.4209, Validation Accuracy:0.1856\n",
    "Epoch #103: Loss:2.4486, Accuracy:0.1749, Validation Loss:2.4256, Validation Accuracy:0.1790\n",
    "Epoch #104: Loss:2.4495, Accuracy:0.1758, Validation Loss:2.4243, Validation Accuracy:0.1790\n",
    "Epoch #105: Loss:2.4493, Accuracy:0.1733, Validation Loss:2.4208, Validation Accuracy:0.1839\n",
    "Epoch #106: Loss:2.4498, Accuracy:0.1676, Validation Loss:2.4238, Validation Accuracy:0.1790\n",
    "Epoch #107: Loss:2.4493, Accuracy:0.1749, Validation Loss:2.4265, Validation Accuracy:0.1790\n",
    "Epoch #108: Loss:2.4495, Accuracy:0.1762, Validation Loss:2.4235, Validation Accuracy:0.1806\n",
    "Epoch #109: Loss:2.4491, Accuracy:0.1758, Validation Loss:2.4243, Validation Accuracy:0.1790\n",
    "Epoch #110: Loss:2.4491, Accuracy:0.1774, Validation Loss:2.4243, Validation Accuracy:0.1790\n",
    "Epoch #111: Loss:2.4489, Accuracy:0.1770, Validation Loss:2.4257, Validation Accuracy:0.1790\n",
    "Epoch #112: Loss:2.4488, Accuracy:0.1762, Validation Loss:2.4248, Validation Accuracy:0.1790\n",
    "Epoch #113: Loss:2.4489, Accuracy:0.1758, Validation Loss:2.4256, Validation Accuracy:0.1790\n",
    "Epoch #114: Loss:2.4502, Accuracy:0.1791, Validation Loss:2.4253, Validation Accuracy:0.1790\n",
    "Epoch #115: Loss:2.4489, Accuracy:0.1717, Validation Loss:2.4235, Validation Accuracy:0.1806\n",
    "Epoch #116: Loss:2.4485, Accuracy:0.1770, Validation Loss:2.4265, Validation Accuracy:0.1790\n",
    "Epoch #117: Loss:2.4482, Accuracy:0.1762, Validation Loss:2.4245, Validation Accuracy:0.1790\n",
    "Epoch #118: Loss:2.4481, Accuracy:0.1762, Validation Loss:2.4234, Validation Accuracy:0.1790\n",
    "Epoch #119: Loss:2.4482, Accuracy:0.1741, Validation Loss:2.4241, Validation Accuracy:0.1790\n",
    "Epoch #120: Loss:2.4488, Accuracy:0.1803, Validation Loss:2.4257, Validation Accuracy:0.1790\n",
    "Epoch #121: Loss:2.4476, Accuracy:0.1770, Validation Loss:2.4235, Validation Accuracy:0.1790\n",
    "Epoch #122: Loss:2.4486, Accuracy:0.1721, Validation Loss:2.4236, Validation Accuracy:0.1790\n",
    "Epoch #123: Loss:2.4471, Accuracy:0.1770, Validation Loss:2.4266, Validation Accuracy:0.1626\n",
    "Epoch #124: Loss:2.4484, Accuracy:0.1782, Validation Loss:2.4261, Validation Accuracy:0.1790\n",
    "Epoch #125: Loss:2.4472, Accuracy:0.1758, Validation Loss:2.4236, Validation Accuracy:0.1806\n",
    "Epoch #126: Loss:2.4477, Accuracy:0.1733, Validation Loss:2.4243, Validation Accuracy:0.1790\n",
    "Epoch #127: Loss:2.4470, Accuracy:0.1766, Validation Loss:2.4263, Validation Accuracy:0.1626\n",
    "Epoch #128: Loss:2.4474, Accuracy:0.1791, Validation Loss:2.4245, Validation Accuracy:0.1790\n",
    "Epoch #129: Loss:2.4471, Accuracy:0.1749, Validation Loss:2.4240, Validation Accuracy:0.1790\n",
    "Epoch #130: Loss:2.4469, Accuracy:0.1766, Validation Loss:2.4247, Validation Accuracy:0.1790\n",
    "Epoch #131: Loss:2.4471, Accuracy:0.1770, Validation Loss:2.4245, Validation Accuracy:0.1790\n",
    "Epoch #132: Loss:2.4467, Accuracy:0.1758, Validation Loss:2.4237, Validation Accuracy:0.1790\n",
    "Epoch #133: Loss:2.4473, Accuracy:0.1733, Validation Loss:2.4255, Validation Accuracy:0.1790\n",
    "Epoch #134: Loss:2.4477, Accuracy:0.1741, Validation Loss:2.4232, Validation Accuracy:0.1806\n",
    "Epoch #135: Loss:2.4466, Accuracy:0.1770, Validation Loss:2.4265, Validation Accuracy:0.1626\n",
    "Epoch #136: Loss:2.4468, Accuracy:0.1807, Validation Loss:2.4247, Validation Accuracy:0.1626\n",
    "Epoch #137: Loss:2.4464, Accuracy:0.1749, Validation Loss:2.4230, Validation Accuracy:0.1806\n",
    "Epoch #138: Loss:2.4467, Accuracy:0.1721, Validation Loss:2.4250, Validation Accuracy:0.1626\n",
    "Epoch #139: Loss:2.4470, Accuracy:0.1770, Validation Loss:2.4260, Validation Accuracy:0.1626\n",
    "Epoch #140: Loss:2.4462, Accuracy:0.1754, Validation Loss:2.4229, Validation Accuracy:0.1806\n",
    "Epoch #141: Loss:2.4465, Accuracy:0.1741, Validation Loss:2.4233, Validation Accuracy:0.1790\n",
    "Epoch #142: Loss:2.4464, Accuracy:0.1803, Validation Loss:2.4253, Validation Accuracy:0.1626\n",
    "Epoch #143: Loss:2.4468, Accuracy:0.1749, Validation Loss:2.4238, Validation Accuracy:0.1790\n",
    "Epoch #144: Loss:2.4458, Accuracy:0.1786, Validation Loss:2.4253, Validation Accuracy:0.1626\n",
    "Epoch #145: Loss:2.4458, Accuracy:0.1795, Validation Loss:2.4245, Validation Accuracy:0.1626\n",
    "Epoch #146: Loss:2.4461, Accuracy:0.1758, Validation Loss:2.4232, Validation Accuracy:0.1790\n",
    "Epoch #147: Loss:2.4461, Accuracy:0.1807, Validation Loss:2.4251, Validation Accuracy:0.1626\n",
    "Epoch #148: Loss:2.4460, Accuracy:0.1799, Validation Loss:2.4244, Validation Accuracy:0.1790\n",
    "Epoch #149: Loss:2.4456, Accuracy:0.1741, Validation Loss:2.4230, Validation Accuracy:0.1790\n",
    "Epoch #150: Loss:2.4459, Accuracy:0.1754, Validation Loss:2.4238, Validation Accuracy:0.1790\n",
    "Epoch #151: Loss:2.4454, Accuracy:0.1770, Validation Loss:2.4258, Validation Accuracy:0.1626\n",
    "Epoch #152: Loss:2.4462, Accuracy:0.1799, Validation Loss:2.4230, Validation Accuracy:0.1806\n",
    "Epoch #153: Loss:2.4463, Accuracy:0.1778, Validation Loss:2.4250, Validation Accuracy:0.1626\n",
    "Epoch #154: Loss:2.4454, Accuracy:0.1803, Validation Loss:2.4225, Validation Accuracy:0.1790\n",
    "Epoch #155: Loss:2.4456, Accuracy:0.1758, Validation Loss:2.4250, Validation Accuracy:0.1626\n",
    "Epoch #156: Loss:2.4456, Accuracy:0.1799, Validation Loss:2.4241, Validation Accuracy:0.1675\n",
    "Epoch #157: Loss:2.4459, Accuracy:0.1770, Validation Loss:2.4227, Validation Accuracy:0.1790\n",
    "Epoch #158: Loss:2.4451, Accuracy:0.1795, Validation Loss:2.4247, Validation Accuracy:0.1626\n",
    "Epoch #159: Loss:2.4454, Accuracy:0.1791, Validation Loss:2.4237, Validation Accuracy:0.1626\n",
    "Epoch #160: Loss:2.4450, Accuracy:0.1803, Validation Loss:2.4236, Validation Accuracy:0.1626\n",
    "Epoch #161: Loss:2.4457, Accuracy:0.1737, Validation Loss:2.4231, Validation Accuracy:0.1790\n",
    "Epoch #162: Loss:2.4458, Accuracy:0.1819, Validation Loss:2.4258, Validation Accuracy:0.1626\n",
    "Epoch #163: Loss:2.4453, Accuracy:0.1770, Validation Loss:2.4223, Validation Accuracy:0.1806\n",
    "Epoch #164: Loss:2.4451, Accuracy:0.1770, Validation Loss:2.4248, Validation Accuracy:0.1626\n",
    "Epoch #165: Loss:2.4461, Accuracy:0.1770, Validation Loss:2.4248, Validation Accuracy:0.1626\n",
    "Epoch #166: Loss:2.4459, Accuracy:0.1729, Validation Loss:2.4216, Validation Accuracy:0.1806\n",
    "Epoch #167: Loss:2.4450, Accuracy:0.1745, Validation Loss:2.4261, Validation Accuracy:0.1626\n",
    "Epoch #168: Loss:2.4452, Accuracy:0.1803, Validation Loss:2.4234, Validation Accuracy:0.1626\n",
    "Epoch #169: Loss:2.4451, Accuracy:0.1737, Validation Loss:2.4227, Validation Accuracy:0.1790\n",
    "Epoch #170: Loss:2.4455, Accuracy:0.1791, Validation Loss:2.4252, Validation Accuracy:0.1675\n",
    "Epoch #171: Loss:2.4440, Accuracy:0.1741, Validation Loss:2.4225, Validation Accuracy:0.1790\n",
    "Epoch #172: Loss:2.4457, Accuracy:0.1725, Validation Loss:2.4228, Validation Accuracy:0.1790\n",
    "Epoch #173: Loss:2.4446, Accuracy:0.1778, Validation Loss:2.4247, Validation Accuracy:0.1626\n",
    "Epoch #174: Loss:2.4444, Accuracy:0.1795, Validation Loss:2.4232, Validation Accuracy:0.1626\n",
    "Epoch #175: Loss:2.4444, Accuracy:0.1811, Validation Loss:2.4234, Validation Accuracy:0.1626\n",
    "Epoch #176: Loss:2.4444, Accuracy:0.1725, Validation Loss:2.4230, Validation Accuracy:0.1790\n",
    "Epoch #177: Loss:2.4453, Accuracy:0.1762, Validation Loss:2.4256, Validation Accuracy:0.1626\n",
    "Epoch #178: Loss:2.4447, Accuracy:0.1770, Validation Loss:2.4218, Validation Accuracy:0.1823\n",
    "Epoch #179: Loss:2.4446, Accuracy:0.1754, Validation Loss:2.4241, Validation Accuracy:0.1626\n",
    "Epoch #180: Loss:2.4446, Accuracy:0.1774, Validation Loss:2.4242, Validation Accuracy:0.1626\n",
    "Epoch #181: Loss:2.4444, Accuracy:0.1791, Validation Loss:2.4219, Validation Accuracy:0.1823\n",
    "Epoch #182: Loss:2.4448, Accuracy:0.1745, Validation Loss:2.4247, Validation Accuracy:0.1626\n",
    "Epoch #183: Loss:2.4444, Accuracy:0.1791, Validation Loss:2.4231, Validation Accuracy:0.1626\n",
    "Epoch #184: Loss:2.4444, Accuracy:0.1725, Validation Loss:2.4203, Validation Accuracy:0.1806\n",
    "Epoch #185: Loss:2.4445, Accuracy:0.1729, Validation Loss:2.4208, Validation Accuracy:0.1691\n",
    "Epoch #186: Loss:2.4435, Accuracy:0.1786, Validation Loss:2.4211, Validation Accuracy:0.1757\n",
    "Epoch #187: Loss:2.4436, Accuracy:0.1717, Validation Loss:2.4196, Validation Accuracy:0.1741\n",
    "Epoch #188: Loss:2.4428, Accuracy:0.1795, Validation Loss:2.4216, Validation Accuracy:0.1823\n",
    "Epoch #189: Loss:2.4439, Accuracy:0.1758, Validation Loss:2.4199, Validation Accuracy:0.1790\n",
    "Epoch #190: Loss:2.4437, Accuracy:0.1786, Validation Loss:2.4196, Validation Accuracy:0.1806\n",
    "Epoch #191: Loss:2.4437, Accuracy:0.1786, Validation Loss:2.4205, Validation Accuracy:0.1806\n",
    "Epoch #192: Loss:2.4437, Accuracy:0.1762, Validation Loss:2.4184, Validation Accuracy:0.1970\n",
    "Epoch #193: Loss:2.4436, Accuracy:0.1778, Validation Loss:2.4202, Validation Accuracy:0.1806\n",
    "Epoch #194: Loss:2.4436, Accuracy:0.1774, Validation Loss:2.4193, Validation Accuracy:0.1790\n",
    "Epoch #195: Loss:2.4432, Accuracy:0.1778, Validation Loss:2.4208, Validation Accuracy:0.1790\n",
    "Epoch #196: Loss:2.4443, Accuracy:0.1782, Validation Loss:2.4184, Validation Accuracy:0.1790\n",
    "Epoch #197: Loss:2.4427, Accuracy:0.1774, Validation Loss:2.4218, Validation Accuracy:0.1757\n",
    "Epoch #198: Loss:2.4441, Accuracy:0.1737, Validation Loss:2.4193, Validation Accuracy:0.1790\n",
    "Epoch #199: Loss:2.4429, Accuracy:0.1770, Validation Loss:2.4197, Validation Accuracy:0.1790\n",
    "Epoch #200: Loss:2.4429, Accuracy:0.1770, Validation Loss:2.4184, Validation Accuracy:0.1790\n",
    "Epoch #201: Loss:2.4429, Accuracy:0.1770, Validation Loss:2.4201, Validation Accuracy:0.1790\n",
    "Epoch #202: Loss:2.4430, Accuracy:0.1791, Validation Loss:2.4202, Validation Accuracy:0.1773\n",
    "Epoch #203: Loss:2.4426, Accuracy:0.1770, Validation Loss:2.4183, Validation Accuracy:0.1790\n",
    "Epoch #204: Loss:2.4425, Accuracy:0.1770, Validation Loss:2.4183, Validation Accuracy:0.1790\n",
    "Epoch #205: Loss:2.4423, Accuracy:0.1766, Validation Loss:2.4195, Validation Accuracy:0.1790\n",
    "Epoch #206: Loss:2.4424, Accuracy:0.1758, Validation Loss:2.4213, Validation Accuracy:0.1691\n",
    "Epoch #207: Loss:2.4422, Accuracy:0.1786, Validation Loss:2.4181, Validation Accuracy:0.1790\n",
    "Epoch #208: Loss:2.4428, Accuracy:0.1766, Validation Loss:2.4196, Validation Accuracy:0.1790\n",
    "Epoch #209: Loss:2.4420, Accuracy:0.1766, Validation Loss:2.4185, Validation Accuracy:0.1790\n",
    "Epoch #210: Loss:2.4426, Accuracy:0.1762, Validation Loss:2.4202, Validation Accuracy:0.1691\n",
    "Epoch #211: Loss:2.4419, Accuracy:0.1770, Validation Loss:2.4182, Validation Accuracy:0.1790\n",
    "Epoch #212: Loss:2.4421, Accuracy:0.1770, Validation Loss:2.4191, Validation Accuracy:0.1773\n",
    "Epoch #213: Loss:2.4429, Accuracy:0.1745, Validation Loss:2.4211, Validation Accuracy:0.1724\n",
    "Epoch #214: Loss:2.4432, Accuracy:0.1749, Validation Loss:2.4173, Validation Accuracy:0.1954\n",
    "Epoch #215: Loss:2.4435, Accuracy:0.1733, Validation Loss:2.4212, Validation Accuracy:0.1691\n",
    "Epoch #216: Loss:2.4418, Accuracy:0.1782, Validation Loss:2.4181, Validation Accuracy:0.1790\n",
    "Epoch #217: Loss:2.4423, Accuracy:0.1741, Validation Loss:2.4185, Validation Accuracy:0.1790\n",
    "Epoch #218: Loss:2.4417, Accuracy:0.1745, Validation Loss:2.4216, Validation Accuracy:0.1675\n",
    "Epoch #219: Loss:2.4420, Accuracy:0.1791, Validation Loss:2.4193, Validation Accuracy:0.1790\n",
    "Epoch #220: Loss:2.4416, Accuracy:0.1770, Validation Loss:2.4185, Validation Accuracy:0.1790\n",
    "Epoch #221: Loss:2.4422, Accuracy:0.1758, Validation Loss:2.4199, Validation Accuracy:0.1675\n",
    "Epoch #222: Loss:2.4414, Accuracy:0.1778, Validation Loss:2.4188, Validation Accuracy:0.1773\n",
    "Epoch #223: Loss:2.4414, Accuracy:0.1758, Validation Loss:2.4223, Validation Accuracy:0.1790\n",
    "Epoch #224: Loss:2.4414, Accuracy:0.1758, Validation Loss:2.4220, Validation Accuracy:0.1741\n",
    "Epoch #225: Loss:2.4409, Accuracy:0.1774, Validation Loss:2.4223, Validation Accuracy:0.1708\n",
    "Epoch #226: Loss:2.4420, Accuracy:0.1766, Validation Loss:2.4211, Validation Accuracy:0.1708\n",
    "Epoch #227: Loss:2.4417, Accuracy:0.1770, Validation Loss:2.4246, Validation Accuracy:0.1609\n",
    "Epoch #228: Loss:2.4421, Accuracy:0.1749, Validation Loss:2.4236, Validation Accuracy:0.1626\n",
    "Epoch #229: Loss:2.4423, Accuracy:0.1704, Validation Loss:2.4219, Validation Accuracy:0.1626\n",
    "Epoch #230: Loss:2.4422, Accuracy:0.1774, Validation Loss:2.4257, Validation Accuracy:0.1609\n",
    "Epoch #231: Loss:2.4416, Accuracy:0.1762, Validation Loss:2.4225, Validation Accuracy:0.1609\n",
    "Epoch #232: Loss:2.4425, Accuracy:0.1704, Validation Loss:2.4221, Validation Accuracy:0.1609\n",
    "Epoch #233: Loss:2.4453, Accuracy:0.1745, Validation Loss:2.4280, Validation Accuracy:0.1626\n",
    "Epoch #234: Loss:2.4451, Accuracy:0.1745, Validation Loss:2.4204, Validation Accuracy:0.1806\n",
    "Epoch #235: Loss:2.4440, Accuracy:0.1782, Validation Loss:2.4267, Validation Accuracy:0.1626\n",
    "Epoch #236: Loss:2.4425, Accuracy:0.1762, Validation Loss:2.4228, Validation Accuracy:0.1658\n",
    "Epoch #237: Loss:2.4423, Accuracy:0.1737, Validation Loss:2.4211, Validation Accuracy:0.1823\n",
    "Epoch #238: Loss:2.4420, Accuracy:0.1766, Validation Loss:2.4238, Validation Accuracy:0.1626\n",
    "Epoch #239: Loss:2.4426, Accuracy:0.1770, Validation Loss:2.4227, Validation Accuracy:0.1658\n",
    "Epoch #240: Loss:2.4415, Accuracy:0.1762, Validation Loss:2.4226, Validation Accuracy:0.1658\n",
    "Epoch #241: Loss:2.4415, Accuracy:0.1762, Validation Loss:2.4228, Validation Accuracy:0.1658\n",
    "Epoch #242: Loss:2.4416, Accuracy:0.1762, Validation Loss:2.4227, Validation Accuracy:0.1658\n",
    "Epoch #243: Loss:2.4417, Accuracy:0.1762, Validation Loss:2.4229, Validation Accuracy:0.1658\n",
    "Epoch #244: Loss:2.4415, Accuracy:0.1758, Validation Loss:2.4241, Validation Accuracy:0.1626\n",
    "Epoch #245: Loss:2.4413, Accuracy:0.1774, Validation Loss:2.4229, Validation Accuracy:0.1658\n",
    "Epoch #246: Loss:2.4411, Accuracy:0.1770, Validation Loss:2.4223, Validation Accuracy:0.1642\n",
    "Epoch #247: Loss:2.4411, Accuracy:0.1758, Validation Loss:2.4236, Validation Accuracy:0.1658\n",
    "Epoch #248: Loss:2.4416, Accuracy:0.1762, Validation Loss:2.4226, Validation Accuracy:0.1658\n",
    "Epoch #249: Loss:2.4415, Accuracy:0.1758, Validation Loss:2.4229, Validation Accuracy:0.1658\n",
    "Epoch #250: Loss:2.4411, Accuracy:0.1762, Validation Loss:2.4221, Validation Accuracy:0.1675\n",
    "Epoch #251: Loss:2.4410, Accuracy:0.1791, Validation Loss:2.4218, Validation Accuracy:0.1642\n",
    "Epoch #252: Loss:2.4411, Accuracy:0.1791, Validation Loss:2.4241, Validation Accuracy:0.1626\n",
    "Epoch #253: Loss:2.4414, Accuracy:0.1786, Validation Loss:2.4226, Validation Accuracy:0.1642\n",
    "Epoch #254: Loss:2.4411, Accuracy:0.1782, Validation Loss:2.4230, Validation Accuracy:0.1626\n",
    "Epoch #255: Loss:2.4408, Accuracy:0.1766, Validation Loss:2.4211, Validation Accuracy:0.1675\n",
    "Epoch #256: Loss:2.4408, Accuracy:0.1766, Validation Loss:2.4228, Validation Accuracy:0.1626\n",
    "Epoch #257: Loss:2.4405, Accuracy:0.1774, Validation Loss:2.4227, Validation Accuracy:0.1642\n",
    "Epoch #258: Loss:2.4407, Accuracy:0.1782, Validation Loss:2.4221, Validation Accuracy:0.1642\n",
    "Epoch #259: Loss:2.4405, Accuracy:0.1791, Validation Loss:2.4222, Validation Accuracy:0.1642\n",
    "Epoch #260: Loss:2.4416, Accuracy:0.1770, Validation Loss:2.4237, Validation Accuracy:0.1626\n",
    "Epoch #261: Loss:2.4411, Accuracy:0.1774, Validation Loss:2.4209, Validation Accuracy:0.1658\n",
    "Epoch #262: Loss:2.4417, Accuracy:0.1795, Validation Loss:2.4241, Validation Accuracy:0.1626\n",
    "Epoch #263: Loss:2.4402, Accuracy:0.1774, Validation Loss:2.4208, Validation Accuracy:0.1658\n",
    "Epoch #264: Loss:2.4409, Accuracy:0.1737, Validation Loss:2.4208, Validation Accuracy:0.1626\n",
    "Epoch #265: Loss:2.4400, Accuracy:0.1713, Validation Loss:2.4256, Validation Accuracy:0.1675\n",
    "Epoch #266: Loss:2.4405, Accuracy:0.1770, Validation Loss:2.4215, Validation Accuracy:0.1658\n",
    "Epoch #267: Loss:2.4409, Accuracy:0.1762, Validation Loss:2.4211, Validation Accuracy:0.1658\n",
    "Epoch #268: Loss:2.4406, Accuracy:0.1745, Validation Loss:2.4245, Validation Accuracy:0.1658\n",
    "Epoch #269: Loss:2.4403, Accuracy:0.1774, Validation Loss:2.4206, Validation Accuracy:0.1642\n",
    "Epoch #270: Loss:2.4401, Accuracy:0.1778, Validation Loss:2.4225, Validation Accuracy:0.1642\n",
    "Epoch #271: Loss:2.4399, Accuracy:0.1774, Validation Loss:2.4223, Validation Accuracy:0.1642\n",
    "Epoch #272: Loss:2.4401, Accuracy:0.1778, Validation Loss:2.4224, Validation Accuracy:0.1642\n",
    "Epoch #273: Loss:2.4403, Accuracy:0.1782, Validation Loss:2.4214, Validation Accuracy:0.1642\n",
    "Epoch #274: Loss:2.4403, Accuracy:0.1782, Validation Loss:2.4226, Validation Accuracy:0.1642\n",
    "Epoch #275: Loss:2.4399, Accuracy:0.1778, Validation Loss:2.4226, Validation Accuracy:0.1626\n",
    "Epoch #276: Loss:2.4397, Accuracy:0.1754, Validation Loss:2.4212, Validation Accuracy:0.1626\n",
    "Epoch #277: Loss:2.4395, Accuracy:0.1778, Validation Loss:2.4216, Validation Accuracy:0.1626\n",
    "Epoch #278: Loss:2.4401, Accuracy:0.1807, Validation Loss:2.4230, Validation Accuracy:0.1675\n",
    "Epoch #279: Loss:2.4395, Accuracy:0.1803, Validation Loss:2.4212, Validation Accuracy:0.1658\n",
    "Epoch #280: Loss:2.4394, Accuracy:0.1766, Validation Loss:2.4214, Validation Accuracy:0.1642\n",
    "Epoch #281: Loss:2.4396, Accuracy:0.1774, Validation Loss:2.4218, Validation Accuracy:0.1724\n",
    "Epoch #282: Loss:2.4392, Accuracy:0.1799, Validation Loss:2.4221, Validation Accuracy:0.1708\n",
    "Epoch #283: Loss:2.4406, Accuracy:0.1807, Validation Loss:2.4217, Validation Accuracy:0.1675\n",
    "Epoch #284: Loss:2.4390, Accuracy:0.1832, Validation Loss:2.4225, Validation Accuracy:0.1658\n",
    "Epoch #285: Loss:2.4393, Accuracy:0.1803, Validation Loss:2.4217, Validation Accuracy:0.1675\n",
    "Epoch #286: Loss:2.4395, Accuracy:0.1807, Validation Loss:2.4212, Validation Accuracy:0.1642\n",
    "Epoch #287: Loss:2.4400, Accuracy:0.1819, Validation Loss:2.4235, Validation Accuracy:0.1675\n",
    "Epoch #288: Loss:2.4404, Accuracy:0.1807, Validation Loss:2.4204, Validation Accuracy:0.1806\n",
    "Epoch #289: Loss:2.4387, Accuracy:0.1762, Validation Loss:2.4254, Validation Accuracy:0.1691\n",
    "Epoch #290: Loss:2.4396, Accuracy:0.1799, Validation Loss:2.4222, Validation Accuracy:0.1675\n",
    "Epoch #291: Loss:2.4387, Accuracy:0.1807, Validation Loss:2.4221, Validation Accuracy:0.1626\n",
    "Epoch #292: Loss:2.4384, Accuracy:0.1786, Validation Loss:2.4207, Validation Accuracy:0.1626\n",
    "Epoch #293: Loss:2.4393, Accuracy:0.1778, Validation Loss:2.4218, Validation Accuracy:0.1675\n",
    "Epoch #294: Loss:2.4387, Accuracy:0.1815, Validation Loss:2.4246, Validation Accuracy:0.1675\n",
    "Epoch #295: Loss:2.4385, Accuracy:0.1819, Validation Loss:2.4211, Validation Accuracy:0.1642\n",
    "Epoch #296: Loss:2.4390, Accuracy:0.1815, Validation Loss:2.4221, Validation Accuracy:0.1675\n",
    "Epoch #297: Loss:2.4383, Accuracy:0.1819, Validation Loss:2.4220, Validation Accuracy:0.1626\n",
    "Epoch #298: Loss:2.4384, Accuracy:0.1791, Validation Loss:2.4212, Validation Accuracy:0.1658\n",
    "Epoch #299: Loss:2.4382, Accuracy:0.1815, Validation Loss:2.4220, Validation Accuracy:0.1658\n",
    "Epoch #300: Loss:2.4387, Accuracy:0.1811, Validation Loss:2.4232, Validation Accuracy:0.1642\n",
    "\n",
    "Test:\n",
    "Test Loss:2.42321157, Accuracy:0.1642\n",
    "Labels: ['yd', 'ek', 'ib', 'sk', 'eo', 'eb', 'eg', 'sg', 'by', 'my', 'ck', 'ds', 'ce', 'mb', 'aa']\n",
    "Confusion Matrix:\n",
    "      yd  ek  ib  sk  eo  eb  eg  sg  by  my  ck  ds  ce  mb  aa\n",
    "t:yd  32   0   2   0   9   0   1  18   0   0   0   0   0   0   0\n",
    "t:ek   5   0   1   0  15   0  17   8   1   0   0   1   0   0   0\n",
    "t:ib  31   0   0   0   9   0   4  10   0   0   0   0   0   0   0\n",
    "t:sk   0   0   1   0   5   1  18   3   1   0   0   4   0   0   0\n",
    "t:eo   4   0   2   0  10   1   6  11   0   0   0   0   0   0   0\n",
    "t:eb   7   0   0   0  10   1  24   6   1   0   0   1   0   0   0\n",
    "t:eg   0   0   1   0   7   1  35   1   0   0   0   5   0   0   0\n",
    "t:sg   9   0   1   0  20   0   5  15   1   0   0   0   0   0   0\n",
    "t:by   3   0   1   0  13   2  19   0   0   0   0   2   0   0   0\n",
    "t:my   3   0   0   0   0   0   7   6   1   0   0   3   0   0   0\n",
    "t:ck   0   0   1   0   3   1  11   5   1   0   0   1   0   0   0\n",
    "t:ds   0   0   1   0   5   0  16   2   0   0   0   7   0   0   0\n",
    "t:ce   0   0   0   0   6   1  10   8   0   0   0   2   0   0   0\n",
    "t:mb   9   0   1   0  10   0  14  17   1   0   0   0   0   0   0\n",
    "t:aa   4   0   0   0   3   0  21   1   0   0   0   5   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.30      0.52      0.38        62\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eo       0.08      0.29      0.13        34\n",
    "          eb       0.12      0.02      0.03        50\n",
    "          eg       0.17      0.70      0.27        50\n",
    "          sg       0.14      0.29      0.19        51\n",
    "          by       0.00      0.00      0.00        40\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ds       0.23      0.23      0.23        31\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          aa       0.00      0.00      0.00        34\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.07      0.14      0.08       609\n",
    "weighted avg       0.08      0.16      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 08:15:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 44 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7005105793769726, 2.693068774071429, 2.687384827775125, 2.6796406454640658, 2.6693684017325467, 2.6605904458583085, 2.6508164100459055, 2.6395262579612546, 2.6256990154779993, 2.6100347464895015, 2.5921058693934347, 2.5701466076479758, 2.5546463757312945, 2.5481990139276913, 2.521325155236255, 2.5070780415840335, 2.5044981740378396, 2.4883713064522577, 2.481682749217367, 2.4774780187309275, 2.4691785795152286, 2.4662697307386225, 2.457522183999248, 2.4544181995986913, 2.4521845980426558, 2.449522198910392, 2.4446373894101097, 2.4418603207285963, 2.4412496465767544, 2.439542206245886, 2.436706060259213, 2.4358668902824663, 2.4372315563396083, 2.433393573917583, 2.434202912993032, 2.4410534985546994, 2.4313472906748452, 2.4342424963495413, 2.432683345719511, 2.4309285618793006, 2.4318670376964, 2.426061412579516, 2.430089923548581, 2.4275003391729397, 2.4287551248014854, 2.433809620601986, 2.4266945180438815, 2.427116812156339, 2.4295274306987893, 2.42619568295471, 2.43303104339562, 2.4302223946269117, 2.4327148706063455, 2.4362929274491685, 2.428501688200852, 2.4353242632986487, 2.4977939860965623, 2.636661117300024, 2.4404083746798912, 2.459194323503717, 2.4595792246569554, 2.44542169805818, 2.456988919153198, 2.43923122737991, 2.4301345046909377, 2.426883626258236, 2.4284885270254954, 2.428989817551987, 2.4265105536418594, 2.4239559420223893, 2.422505449583182, 2.422941076735949, 2.423045985608657, 2.42850858276505, 2.4288511393692693, 2.4271890921349986, 2.4265254495179125, 2.4269778156906905, 2.427297929237629, 2.425602191383224, 2.4221944981216406, 2.421971473396314, 2.4219616316809445, 2.426604438102108, 2.4251065355999324, 2.4417538325774846, 2.426296914152324, 2.4224389903063845, 2.426331204538079, 2.430750066227905, 2.4244789783590535, 2.4222665392902294, 2.4294222008027075, 2.426041620705515, 2.4218835740645335, 2.4270481155032204, 2.425048370862438, 2.4250498749743934, 2.4253171304568086, 2.4232274695178755, 2.4225727118099067, 2.4208899413423586, 2.425585298506889, 2.424328845514257, 2.4208339231550595, 2.4238472795251553, 2.4265028400765654, 2.423471689224243, 2.42427678609325, 2.424329980449332, 2.4257496135379686, 2.424770104669781, 2.42562378294558, 2.425251816685368, 2.4234981994910783, 2.4265316464435096, 2.4244751010230807, 2.4234303605967553, 2.424075414786002, 2.4257058189028786, 2.4235235951804177, 2.423565935031534, 2.4266021901555055, 2.426088941899818, 2.423646085955239, 2.424261625568659, 2.4263157515690246, 2.4245060437614305, 2.4239559173583984, 2.42471963314, 2.4245239220229275, 2.4236897134232795, 2.425505314358741, 2.423246441216304, 2.4264726869774176, 2.424660850628256, 2.4230382293707438, 2.425035038996604, 2.4259933591476215, 2.422895882517246, 2.423273618976862, 2.425333833459563, 2.423847726608928, 2.4253032841705924, 2.424465060429816, 2.4232245031836, 2.425078574464043, 2.424441066281549, 2.423039469225653, 2.42382490693642, 2.4258484167026966, 2.4229689501776486, 2.4249654552227953, 2.422466184901095, 2.4250215000315447, 2.4241460724221464, 2.422656974573245, 2.4247023519036803, 2.423665718100537, 2.423565803490249, 2.4230640572671622, 2.4258009240349327, 2.4222679627548493, 2.4247925261950063, 2.424753580578834, 2.421573394625058, 2.4261036987962394, 2.4234415787977146, 2.4226761823413017, 2.4252352769347443, 2.422493451921811, 2.422837114882195, 2.424740600664236, 2.4232044497929968, 2.423367942691045, 2.423004832369549, 2.4256297273588885, 2.421805031389634, 2.424136171200005, 2.42424570672422, 2.4218787959056534, 2.4247366000083086, 2.423058428396341, 2.4202692520442266, 2.4208045961038622, 2.4211032331870697, 2.4195660405558317, 2.4215745546156158, 2.419880044088379, 2.419591286303766, 2.420454144282098, 2.418378749308719, 2.42019819430334, 2.4192607899996252, 2.4207674078949175, 2.418427629032354, 2.4217961323868074, 2.4192966596637846, 2.4196562630006637, 2.418440150118422, 2.420088212869829, 2.4201769202409316, 2.4182727822333527, 2.4183494751089314, 2.4195267523842294, 2.421335746893546, 2.4181472503492984, 2.4195640302448242, 2.4185258155973086, 2.4202064618297006, 2.418218911770725, 2.419077116084608, 2.421076233555335, 2.4172838771675997, 2.4212235139900047, 2.4181262890889337, 2.4184607253677544, 2.4216271664317213, 2.4193458564958745, 2.418491518556191, 2.4198698488558064, 2.4188383510351574, 2.422290075001458, 2.421958887322587, 2.422257729939052, 2.4211320481668355, 2.4245987799758786, 2.4236434480826845, 2.4218914289584106, 2.4256556766178026, 2.4224635885266834, 2.4220965773796994, 2.428039605198627, 2.4204396598640527, 2.426719381695702, 2.4228320086530863, 2.4210879332913553, 2.42376261352514, 2.4226758859819184, 2.4225733820440736, 2.4228389157450256, 2.4227092360040823, 2.42292418307663, 2.42408914519061, 2.422856799095918, 2.4223097024488527, 2.4236070929685445, 2.422574963279937, 2.422898253392312, 2.4220826199097782, 2.421844911105527, 2.424110081004, 2.4225620369997323, 2.4229582311288866, 2.42111977019725, 2.422811005894578, 2.4226846851543056, 2.422123538253734, 2.422248250353708, 2.423675199251848, 2.4209438270731707, 2.424064564978939, 2.42084144841274, 2.420770622826562, 2.425585408124626, 2.421475677067423, 2.4210552754269052, 2.42454179519503, 2.420553428198904, 2.4225318478833278, 2.4222598161994924, 2.4223561752801652, 2.4214356795124625, 2.4226215660865673, 2.4226476504101933, 2.4212213541290835, 2.421647899843789, 2.4229950795228454, 2.421175031630668, 2.4214189917778928, 2.421836748890493, 2.42208545900918, 2.4217033069122014, 2.4224982751022615, 2.4217405483640473, 2.4211812794502148, 2.4234773086992587, 2.4204109623318626, 2.425418062554596, 2.4222252345437485, 2.4221499878393216, 2.4207338120158277, 2.421813051493101, 2.4246462652052956, 2.421117227848723, 2.4221083178308795, 2.4220158630991215, 2.421232283409006, 2.4220311528160456, 2.4232115690735565], 'val_acc': [0.11822659998202363, 0.10673234710846041, 0.10837438333112814, 0.12643678099748928, 0.11822660037351555, 0.11822660027564257, 0.11658456415084782, 0.11494252783030712, 0.12315270845428085, 0.1362889975505118, 0.1362889975505118, 0.1362889975505118, 0.13628899764838476, 0.13628899824785679, 0.154351394923254, 0.17077175607332848, 0.1756978651450577, 0.17733990067038044, 0.17077175617120144, 0.17898193759039313, 0.1773399007682534, 0.18390804487594048, 0.18226600974210966, 0.18226600983998262, 0.19047618947299244, 0.18390804497381344, 0.18390804487594048, 0.18062397371518787, 0.18390804596477736, 0.18390804596477736, 0.1888341533481977, 0.18555008100073522, 0.18719211712552997, 0.19211822569566014, 0.19211822569566014, 0.19047619046395636, 0.19047618937511945, 0.19376026172258193, 0.19047618947299244, 0.1888341533481977, 0.1888341533481977, 0.19376026182045492, 0.1904761895708654, 0.19047618947299244, 0.19047618937511945, 0.18719211712552997, 0.19047618937511945, 0.19047618947299244, 0.19211822559778718, 0.1855500820895721, 0.18555008119648117, 0.17241379200237725, 0.19376026182045492, 0.180623972626351, 0.1888341533481977, 0.18390804487594048, 0.1642036118677684, 0.1100164203107822, 0.18555008109860818, 0.17077175617120144, 0.17405582832291797, 0.18719211722340295, 0.1658456484941622, 0.18719211821436688, 0.1888341532503247, 0.18390804497381344, 0.1855500820895721, 0.1855500820895721, 0.1855500820895721, 0.1855500820895721, 0.1855500820895721, 0.1855500820895721, 0.19376026172258193, 0.17733990047463447, 0.17569786434983972, 0.19047618947299244, 0.1773399013677254, 0.17241379309121416, 0.17569786425196673, 0.18390804596477736, 0.19047619046395636, 0.1921182254999142, 0.19047619046395636, 0.19047618937511945, 0.18719211712552997, 0.16912971975278776, 0.19047619046395636, 0.19376026162470894, 0.19047619046395636, 0.17733990126985244, 0.19047619046395636, 0.19376026162470894, 0.17733990126985244, 0.17733990126985244, 0.19376026162470894, 0.17733990126985244, 0.19047619046395636, 0.1921182254999142, 0.18883415433916162, 0.18390804487594048, 0.17898193650155622, 0.18555008100073522, 0.17898193650155622, 0.17898193650155622, 0.18390804497381344, 0.17898193650155622, 0.17898193650155622, 0.18062397371518787, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.180623972626351, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.16256157604882673, 0.17898193650155622, 0.180623972626351, 0.17898193650155622, 0.16256157604882673, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.17898193650155622, 0.18062397371518787, 0.16256157604882673, 0.16256157604882673, 0.180623972626351, 0.16256157604882673, 0.16256157604882673, 0.18062397371518787, 0.17898193650155622, 0.16256157604882673, 0.17898193650155622, 0.16256157604882673, 0.16256157604882673, 0.17898193650155622, 0.16256157604882673, 0.17898193650155622, 0.1789819365994292, 0.17898193650155622, 0.16256157604882673, 0.18062397371518787, 0.16256157604882673, 0.1789819365994292, 0.16256157604882673, 0.16748768461895694, 0.1789819365994292, 0.16256157604882673, 0.16256157604882673, 0.16256157604882673, 0.17898193650155622, 0.16256157604882673, 0.180623972626351, 0.16256157604882673, 0.16256157604882673, 0.180623972626351, 0.16256157604882673, 0.16256157604882673, 0.1789819365994292, 0.16748768461895694, 0.1789819365994292, 0.17898193650155622, 0.16256157604882673, 0.16256157604882673, 0.16256157604882673, 0.17898193650155622, 0.16256157604882673, 0.18226600875114574, 0.16256157604882673, 0.16256157604882673, 0.18226600875114574, 0.16256157604882673, 0.16256157604882673, 0.18062397272422395, 0.16912972074375168, 0.17569786524293066, 0.17405582892238997, 0.18226600974210966, 0.1789819372967742, 0.18062397342156894, 0.18062397342156894, 0.19704433387429843, 0.18062397342156894, 0.1789819372967742, 0.1789819372967742, 0.1789819372967742, 0.17569786534080364, 0.1789819372967742, 0.1789819372967742, 0.1789819372967742, 0.1789819372967742, 0.17733990117197945, 0.1789819372967742, 0.1789819372967742, 0.1789819372967742, 0.1691297206458787, 0.1789819372967742, 0.1789819372967742, 0.1789819372967742, 0.16912972054800574, 0.1789819372967742, 0.17733990117197945, 0.17241379309121416, 0.19540229774950368, 0.1691297206458787, 0.1789819372967742, 0.1789819372967742, 0.16748768452108395, 0.1789819372967742, 0.1789819372967742, 0.16748768452108395, 0.17733990117197945, 0.17898193739464718, 0.17405582882451698, 0.1707717565749275, 0.1707717565749275, 0.160919539826159, 0.16256157595095377, 0.16256157595095377, 0.160919539924032, 0.160919539826159, 0.160919539826159, 0.16256157604882673, 0.18062397272422395, 0.16256157604882673, 0.16584564829841622, 0.18226600875114574, 0.16256157604882673, 0.16584564829841622, 0.16584564829841622, 0.16584564829841622, 0.16584564829841622, 0.16584564829841622, 0.16256157604882673, 0.16584564829841622, 0.16420361227149446, 0.16584564829841622, 0.16584564829841622, 0.16584564829841622, 0.16748768442321096, 0.16420361227149446, 0.16256157604882673, 0.16420361227149446, 0.16256157604882673, 0.16748768442321096, 0.16256157604882673, 0.16420361217362148, 0.16420361217362148, 0.16420361227149446, 0.16256157604882673, 0.1658456483962892, 0.16256157604882673, 0.16584564829841622, 0.16256157604882673, 0.16748768461895694, 0.1658456483962892, 0.1658456483962892, 0.1658456483962892, 0.16420361227149446, 0.16420361217362148, 0.16420361217362148, 0.16420361217362148, 0.16420361217362148, 0.16420361217362148, 0.16256157604882673, 0.16256157604882673, 0.16256157604882673, 0.16748768452108395, 0.1658456483962892, 0.16420361217362148, 0.17241379309121416, 0.1707717569664194, 0.16748768452108395, 0.1658456483962892, 0.16748768452108395, 0.16420361236936745, 0.16748768452108395, 0.18062397282209694, 0.16912972084162467, 0.16748768452108395, 0.16256157604882673, 0.16256157604882673, 0.16748768452108395, 0.16748768452108395, 0.16420361236936745, 0.16748768452108395, 0.16256157604882673, 0.1658456483962892, 0.1658456483962892, 0.16420361227149446], 'loss': [2.704911682884796, 2.697725885751556, 2.68991409072641, 2.684724177178416, 2.676549958301521, 2.666937877020552, 2.658405857066599, 2.6489763674060423, 2.63751716936883, 2.6245537892993713, 2.6085831235811208, 2.5914155670260013, 2.5745646302460155, 2.5588558482927954, 2.5461867584095352, 2.53860487252535, 2.519954129753661, 2.5141500329090096, 2.5074082505776407, 2.5012718755851293, 2.4982476155126365, 2.4938783221665837, 2.4888532401599925, 2.4850124400254394, 2.483010100632967, 2.4812385116514486, 2.4780122710938817, 2.476036228144683, 2.471877971077357, 2.4703998491259815, 2.4690598027662087, 2.46917049556787, 2.467219726554667, 2.466605669566004, 2.468458014494095, 2.467971895852373, 2.4670423660434984, 2.4684067619409893, 2.466558438209048, 2.4658716576789685, 2.466766555891879, 2.4666873078081886, 2.4660060327400664, 2.464012034668815, 2.464291065184732, 2.4633685877680533, 2.463545943775216, 2.465138979663105, 2.4660385764355044, 2.460655821030634, 2.4599793055219084, 2.4618718350937234, 2.458998551163096, 2.4610806695244887, 2.4635076014657775, 2.462526590476535, 2.4849955209226824, 2.5363918645181207, 2.5835090688121882, 2.473887536657909, 2.502605595578893, 2.484070524689598, 2.4766809928588556, 2.4743983181595066, 2.462686425117007, 2.4623231276839177, 2.4597079933056842, 2.457528772393291, 2.456275727861471, 2.455134043213768, 2.4544233563009965, 2.453982758375164, 2.4536109917462485, 2.455341583013045, 2.455166299729866, 2.4548346953225577, 2.454297386987988, 2.4541297438697893, 2.453693801911215, 2.4512637709200504, 2.450317895583793, 2.4517219785302573, 2.4526406473202873, 2.4504897914627986, 2.4621501970584876, 2.4530302876074948, 2.45688305361315, 2.449285439446232, 2.4506506933568684, 2.4488788451991774, 2.450566292886127, 2.4511471018898905, 2.44888901886754, 2.449580438621725, 2.4491625654623985, 2.4483253519148307, 2.4487274098445257, 2.447588005105083, 2.449026057362801, 2.44797677210469, 2.4491618735099965, 2.449372995071098, 2.448566781421951, 2.449520881562752, 2.4492719998839454, 2.449812460728984, 2.449291455231653, 2.4494638578113346, 2.4490887541056168, 2.4490722064854427, 2.448936392296511, 2.448811094374138, 2.44893020222564, 2.4501779516130013, 2.4489167171337276, 2.448508296120583, 2.448223449464207, 2.4480566336144167, 2.44816917912916, 2.4487523155290734, 2.4475718971150613, 2.4486393756200644, 2.4471293292740777, 2.448400536014314, 2.4472434201524487, 2.4476833834050864, 2.4470380362054405, 2.4474284442298466, 2.447106147155135, 2.4469280755005824, 2.447107963346603, 2.4466725857595644, 2.447291224105647, 2.447706701133775, 2.4465838745634167, 2.4468484225459166, 2.4464489658755197, 2.4466603796095328, 2.4470229536600914, 2.446192624583626, 2.4464541372577266, 2.4464074131889264, 2.4468344810806997, 2.44577728796299, 2.445843896924593, 2.4461161636963515, 2.4461261754163237, 2.446012817809714, 2.4455824034414744, 2.4459358864496377, 2.445410251715345, 2.44624500607563, 2.4463099536464936, 2.445362698615699, 2.445615855869082, 2.4455837533704066, 2.4459336643101497, 2.445137408724556, 2.445371238798576, 2.4449777862619326, 2.4456859521062957, 2.445763676660996, 2.4452529375558028, 2.4451112270355226, 2.4461489448312372, 2.445907854397439, 2.4450391986776427, 2.4452419593348886, 2.445072283245455, 2.445456848692845, 2.4440242559758056, 2.4457284594463373, 2.444604138623028, 2.4444236857200794, 2.444444003682851, 2.444382108751019, 2.4453040817190246, 2.444725546063339, 2.444572500820277, 2.4445736999629215, 2.4443917290141206, 2.4448314278032743, 2.444397865232746, 2.4444272014884243, 2.444483581166982, 2.443542832858264, 2.443585889295386, 2.442755589005394, 2.4438672692379177, 2.4437386784954973, 2.4437125520294942, 2.4436773918003025, 2.4435975766035076, 2.443561542009671, 2.443200675858609, 2.4443030630538596, 2.4426947260784173, 2.4440536788846434, 2.442885678798511, 2.4428586108973382, 2.442924251301822, 2.44296495645198, 2.4426307260133404, 2.44252321911054, 2.4422675451948415, 2.4424164508647253, 2.4421600751073944, 2.4427821443310997, 2.442035835136868, 2.4426234258029007, 2.441897642734849, 2.4420707617452257, 2.442874693234109, 2.4431938330985195, 2.443536360209972, 2.4418420638881426, 2.442281639796263, 2.4417434126444664, 2.442007191323157, 2.4415642548390726, 2.442173952341569, 2.441378119447148, 2.4413511168540625, 2.441434716786692, 2.440942292144901, 2.4419799461012257, 2.4417031158901583, 2.4420550935811822, 2.442303040189175, 2.442238457638625, 2.4416057066750967, 2.4425461072941337, 2.445270427590278, 2.4451043925980525, 2.4440449424837647, 2.4425225675962787, 2.442283055424935, 2.4419890670071394, 2.442619409991975, 2.441501359038774, 2.4414603351812345, 2.4416236041262898, 2.441736491358011, 2.4414775444986394, 2.4413419815549124, 2.4411498738510162, 2.4411298072313627, 2.4416437627843273, 2.441467222785558, 2.4411196936816895, 2.4409745055792023, 2.4410603781745173, 2.4413839437144005, 2.4410649110649154, 2.440824279745991, 2.4408341867967795, 2.4405325998271024, 2.4407128163676486, 2.4405382213161713, 2.441614704210411, 2.441054187809907, 2.4416776702144554, 2.4401833676214824, 2.44094035894719, 2.4400167052016366, 2.44052045193541, 2.4408588979278503, 2.4405944371859887, 2.4403063791733257, 2.440089311638897, 2.439942350622565, 2.4400652411537247, 2.4403313663216344, 2.4403466389164543, 2.439910083629757, 2.4396943656326076, 2.439547707755463, 2.4400888583009004, 2.4395284420655736, 2.4394406006321527, 2.439571378656971, 2.4392437538327134, 2.440551970675741, 2.438985755066607, 2.4392654061562227, 2.439476081820729, 2.4400497329798077, 2.440406123030112, 2.4386572948471477, 2.4395899300702544, 2.438733286240752, 2.4383942753872097, 2.4392694457600492, 2.4387173007401106, 2.4384715358334645, 2.439035662880179, 2.438301943557708, 2.438392933585071, 2.438169421503431, 2.438716006621688], 'acc': [0.079671457663821, 0.09774127324136621, 0.10308008246292079, 0.11211499025628308, 0.11745379849870592, 0.10924024673396802, 0.11047227978400381, 0.11375770014108329, 0.11909650875680011, 0.12607802828234568, 0.13388090459228297, 0.13470225963259624, 0.1318275149353231, 0.13429158221035278, 0.13963038907281183, 0.15852156107063411, 0.1655030807552886, 0.1650924035472302, 0.16303901428192302, 0.17125256586001394, 0.1691991787487966, 0.1691991779471325, 0.17166324365555138, 0.16960985536937595, 0.17002053476824164, 0.17043121121135335, 0.17125256682078696, 0.17002053476824164, 0.17084188979019618, 0.17166324424303042, 0.17166324504469455, 0.1696098569727042, 0.17371663174590046, 0.17412730916814392, 0.16837782390430966, 0.17207392305441704, 0.1745379869820401, 0.17330595451948333, 0.17412730975562296, 0.17166324365555138, 0.17084188802775907, 0.17043121039133052, 0.17125256603748157, 0.17043121181719112, 0.1712525666249606, 0.1716632448488682, 0.1712525660558403, 0.170020534964068, 0.17166324426138915, 0.17166324426138915, 0.1733059539320043, 0.17700205249463263, 0.17412731114476612, 0.17084188904360825, 0.1720739216652739, 0.17125256703497202, 0.16550307920703652, 0.15441478549577373, 0.14537987748822637, 0.17125256703497202, 0.15687884942339675, 0.16180698123188725, 0.17494866579342672, 0.17700205447125483, 0.17412730936397028, 0.16755646827651735, 0.1687885007390741, 0.17330595492949477, 0.17330595571280016, 0.17330595471530968, 0.1728952771155986, 0.17330595414618938, 0.17289527829055668, 0.17166324386973644, 0.17166324386973644, 0.1737166323333795, 0.1720739212736212, 0.17659137622898854, 0.17987679621277403, 0.17864476376857602, 0.17453798719622515, 0.17577002102956635, 0.17043121040968925, 0.17289527611810812, 0.16427104633446837, 0.1774127303085288, 0.17946611798886647, 0.17166324463468313, 0.17412731014727567, 0.1786447645518814, 0.17577002101120762, 0.17494866481429497, 0.1733059551253211, 0.17535934300148512, 0.1691991775554798, 0.1720739212736212, 0.16960985597521372, 0.17166324365555138, 0.17166324365555138, 0.17453798796117184, 0.17577002120703397, 0.17207392266276436, 0.17494866440428355, 0.17577002161704539, 0.17330595373617794, 0.16755646866817003, 0.1749486646001099, 0.176180697258493, 0.17577002142121906, 0.1774127303085288, 0.17700205427542848, 0.17618069745431936, 0.17577001983624954, 0.17905544234741885, 0.17166324463468313, 0.17700205327793803, 0.17618069782761334, 0.176180697258493, 0.17412731034310203, 0.18028747365337622, 0.17700205445289613, 0.17207392187945897, 0.177002052690459, 0.17823408634633253, 0.1757700200504346, 0.17330595373617794, 0.1765913754824006, 0.17905544177829852, 0.1749486646001099, 0.17659137507238917, 0.177002052690459, 0.17577002083374, 0.17330595532114745, 0.17412731112640742, 0.1770020532595793, 0.18069815184056637, 0.17494866422681593, 0.17207392227111165, 0.1770020538837758, 0.17535934241400608, 0.17412731075311344, 0.18028747381248514, 0.17494866638090575, 0.1786447637502173, 0.1794661185947042, 0.17577002063791364, 0.18069815164474, 0.17987679660442674, 0.1741273109489398, 0.17535934184488575, 0.17700205347376438, 0.1798767968002531, 0.1778234075349459, 0.18028747342083243, 0.17577001944459683, 0.17987679740609086, 0.17700205347376438, 0.17946611879053057, 0.17905544056662298, 0.18028747363501751, 0.17371663333086998, 0.18193018410729678, 0.17700205447125483, 0.17700205267210026, 0.1770020538837758, 0.17289527752561, 0.17453798874447723, 0.18028747502416068, 0.17371663235173823, 0.1790554405849817, 0.17412731036146073, 0.17248459947917005, 0.17782340792659862, 0.1794661193780096, 0.181108830651953, 0.17248459889169102, 0.17618069704430794, 0.17700205447125483, 0.17535934358896416, 0.1774127307001815, 0.17905544099499313, 0.17453798737369278, 0.17905544038915536, 0.17248459910587607, 0.17289527690141354, 0.17864476357274967, 0.1716632440472041, 0.17946611898635692, 0.1757700212253927, 0.1786447641602287, 0.1786447645518814, 0.17618069925347393, 0.177823408709904, 0.1774127299168761, 0.17782340892408907, 0.1782340865421589, 0.1774127315018456, 0.17371663251084712, 0.17700205228044757, 0.17700205386541706, 0.17700205229880628, 0.17905544076244934, 0.17700205290464405, 0.17700205290464405, 0.17659137546404186, 0.1757700200320759, 0.17864476435605506, 0.17659137487656282, 0.17659137626570598, 0.17618069724013427, 0.17700205425706977, 0.17700205288628534, 0.17453798815699817, 0.1749486649917626, 0.17330595549861508, 0.17823408613214747, 0.17412730916814392, 0.17453798815699817, 0.17905544158247216, 0.17700205347376438, 0.17577002063791364, 0.1778234095115681, 0.17577002120703397, 0.17577002161704539, 0.17741273050435516, 0.17659137546404186, 0.17700205308211167, 0.17494866438592485, 0.17043121040968925, 0.1774127307001815, 0.1761806984518098, 0.17043121081970067, 0.17453798717786645, 0.17453798659038738, 0.17823408632797383, 0.17618069784597204, 0.17371663135424775, 0.1765913768348263, 0.17700205229880628, 0.1761806992167565, 0.17618069706266665, 0.176180697258493, 0.17618069884346252, 0.17577002143957776, 0.17741273169767197, 0.1770020524762739, 0.1757700198178908, 0.17618069903928885, 0.17577002022790222, 0.17618069806015713, 0.1790554405849817, 0.17905544216995123, 0.17864476335856458, 0.17823408593632112, 0.17659137587405327, 0.17659137567822694, 0.1774127303268875, 0.1782340869338116, 0.1790554405849817, 0.17700205347376438, 0.17741273167931323, 0.17946611839887788, 0.1774127307001815, 0.17371663135424775, 0.1712525668391457, 0.1770020538837758, 0.17618069903928885, 0.1745379881753569, 0.1774127310918342, 0.17782340892408907, 0.1774127299168761, 0.1778234082998926, 0.17823408613214747, 0.17823408632797383, 0.17782340812242498, 0.17535934182652702, 0.1778234091015567, 0.18069815281969812, 0.18028747363501751, 0.17659137509074788, 0.1774127310918342, 0.17987679719190577, 0.1806981516263813, 0.18316221715733255, 0.18028747523834573, 0.18069815225057778, 0.18193018551479864, 0.18069815184056637, 0.17618069763178698, 0.1798767968002531, 0.18069815144891366, 0.1786447649435341, 0.1778234091015567, 0.181519507076706, 0.18193018526389615, 0.1815195078783701, 0.18193018489060217, 0.17905544038915536, 0.1815195063117593, 0.18110883045612663]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
