{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf41.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 13:32:57 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '01', '05', '03', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001FA80E7BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001FAD0B36EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6098, Accuracy:0.2066, Validation Loss:1.6075, Validation Accuracy:0.2069\n",
    "Epoch #2: Loss:1.6065, Accuracy:0.2324, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6026, Accuracy:0.2329, Validation Loss:1.6016, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6011, Accuracy:0.2329, Validation Loss:1.5995, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.5984, Accuracy:0.2341, Validation Loss:1.5961, Validation Accuracy:0.2348\n",
    "Epoch #13: Loss:1.5944, Accuracy:0.2427, Validation Loss:1.5906, Validation Accuracy:0.2496\n",
    "Epoch #14: Loss:1.5883, Accuracy:0.2559, Validation Loss:1.5817, Validation Accuracy:0.2693\n",
    "Epoch #15: Loss:1.5785, Accuracy:0.2632, Validation Loss:1.5686, Validation Accuracy:0.2808\n",
    "Epoch #16: Loss:1.5653, Accuracy:0.2682, Validation Loss:1.5501, Validation Accuracy:0.2857\n",
    "Epoch #17: Loss:1.5466, Accuracy:0.2846, Validation Loss:1.5262, Validation Accuracy:0.3054\n",
    "Epoch #18: Loss:1.5208, Accuracy:0.2920, Validation Loss:1.4926, Validation Accuracy:0.3268\n",
    "Epoch #19: Loss:1.4833, Accuracy:0.3191, Validation Loss:1.4469, Validation Accuracy:0.3383\n",
    "Epoch #20: Loss:1.4355, Accuracy:0.3417, Validation Loss:1.3861, Validation Accuracy:0.3711\n",
    "Epoch #21: Loss:1.3862, Accuracy:0.3762, Validation Loss:1.3409, Validation Accuracy:0.4368\n",
    "Epoch #22: Loss:1.3404, Accuracy:0.4209, Validation Loss:1.2961, Validation Accuracy:0.4499\n",
    "Epoch #23: Loss:1.3038, Accuracy:0.4378, Validation Loss:1.2501, Validation Accuracy:0.4828\n",
    "Epoch #24: Loss:1.2625, Accuracy:0.4567, Validation Loss:1.2131, Validation Accuracy:0.4778\n",
    "Epoch #25: Loss:1.2187, Accuracy:0.4723, Validation Loss:1.1926, Validation Accuracy:0.4713\n",
    "Epoch #26: Loss:1.1916, Accuracy:0.4793, Validation Loss:1.1439, Validation Accuracy:0.4926\n",
    "Epoch #27: Loss:1.1371, Accuracy:0.4871, Validation Loss:1.0854, Validation Accuracy:0.5057\n",
    "Epoch #28: Loss:1.0885, Accuracy:0.5183, Validation Loss:1.0497, Validation Accuracy:0.5090\n",
    "Epoch #29: Loss:1.0549, Accuracy:0.5285, Validation Loss:1.0271, Validation Accuracy:0.5189\n",
    "Epoch #30: Loss:1.0280, Accuracy:0.5339, Validation Loss:1.0035, Validation Accuracy:0.5255\n",
    "Epoch #31: Loss:1.0108, Accuracy:0.5310, Validation Loss:1.0276, Validation Accuracy:0.5041\n",
    "Epoch #32: Loss:0.9989, Accuracy:0.5421, Validation Loss:0.9667, Validation Accuracy:0.5287\n",
    "Epoch #33: Loss:0.9692, Accuracy:0.5495, Validation Loss:0.9469, Validation Accuracy:0.5435\n",
    "Epoch #34: Loss:0.9512, Accuracy:0.5478, Validation Loss:0.9416, Validation Accuracy:0.5386\n",
    "Epoch #35: Loss:0.9392, Accuracy:0.5585, Validation Loss:0.9202, Validation Accuracy:0.5517\n",
    "Epoch #36: Loss:0.9280, Accuracy:0.5667, Validation Loss:0.9082, Validation Accuracy:0.5599\n",
    "Epoch #37: Loss:0.9091, Accuracy:0.5749, Validation Loss:0.8974, Validation Accuracy:0.5616\n",
    "Epoch #38: Loss:0.8984, Accuracy:0.5774, Validation Loss:0.8866, Validation Accuracy:0.5616\n",
    "Epoch #39: Loss:0.9035, Accuracy:0.5729, Validation Loss:0.8852, Validation Accuracy:0.5665\n",
    "Epoch #40: Loss:0.8913, Accuracy:0.5733, Validation Loss:0.8676, Validation Accuracy:0.5632\n",
    "Epoch #41: Loss:0.8682, Accuracy:0.5893, Validation Loss:0.8597, Validation Accuracy:0.5764\n",
    "Epoch #42: Loss:0.8734, Accuracy:0.5926, Validation Loss:0.8529, Validation Accuracy:0.5813\n",
    "Epoch #43: Loss:0.8594, Accuracy:0.5996, Validation Loss:0.8453, Validation Accuracy:0.5698\n",
    "Epoch #44: Loss:0.8505, Accuracy:0.5992, Validation Loss:0.8373, Validation Accuracy:0.5878\n",
    "Epoch #45: Loss:0.8409, Accuracy:0.6078, Validation Loss:0.8335, Validation Accuracy:0.5846\n",
    "Epoch #46: Loss:0.8376, Accuracy:0.6070, Validation Loss:0.8563, Validation Accuracy:0.5780\n",
    "Epoch #47: Loss:0.8430, Accuracy:0.5996, Validation Loss:0.8158, Validation Accuracy:0.5895\n",
    "Epoch #48: Loss:0.8230, Accuracy:0.6103, Validation Loss:0.8148, Validation Accuracy:0.5878\n",
    "Epoch #49: Loss:0.8197, Accuracy:0.6140, Validation Loss:0.8452, Validation Accuracy:0.5829\n",
    "Epoch #50: Loss:0.8374, Accuracy:0.5922, Validation Loss:0.8117, Validation Accuracy:0.5928\n",
    "Epoch #51: Loss:0.8316, Accuracy:0.6037, Validation Loss:0.8249, Validation Accuracy:0.5846\n",
    "Epoch #52: Loss:0.8364, Accuracy:0.6033, Validation Loss:0.8276, Validation Accuracy:0.5911\n",
    "Epoch #53: Loss:0.8488, Accuracy:0.5910, Validation Loss:0.8480, Validation Accuracy:0.5665\n",
    "Epoch #54: Loss:0.8341, Accuracy:0.6037, Validation Loss:0.8362, Validation Accuracy:0.5698\n",
    "Epoch #55: Loss:0.8122, Accuracy:0.6066, Validation Loss:0.8370, Validation Accuracy:0.5698\n",
    "Epoch #56: Loss:0.7958, Accuracy:0.6234, Validation Loss:0.7780, Validation Accuracy:0.6092\n",
    "Epoch #57: Loss:0.7817, Accuracy:0.6242, Validation Loss:0.7876, Validation Accuracy:0.6092\n",
    "Epoch #58: Loss:0.7736, Accuracy:0.6255, Validation Loss:0.7689, Validation Accuracy:0.6108\n",
    "Epoch #59: Loss:0.7704, Accuracy:0.6300, Validation Loss:0.7569, Validation Accuracy:0.6158\n",
    "Epoch #60: Loss:0.7678, Accuracy:0.6382, Validation Loss:0.7531, Validation Accuracy:0.6108\n",
    "Epoch #61: Loss:0.7576, Accuracy:0.6468, Validation Loss:0.7467, Validation Accuracy:0.6125\n",
    "Epoch #62: Loss:0.7550, Accuracy:0.6435, Validation Loss:0.7542, Validation Accuracy:0.6174\n",
    "Epoch #63: Loss:0.7572, Accuracy:0.6320, Validation Loss:0.7279, Validation Accuracy:0.6289\n",
    "Epoch #64: Loss:0.7408, Accuracy:0.6456, Validation Loss:0.7548, Validation Accuracy:0.6108\n",
    "Epoch #65: Loss:0.7404, Accuracy:0.6407, Validation Loss:0.7316, Validation Accuracy:0.6388\n",
    "Epoch #66: Loss:0.7375, Accuracy:0.6513, Validation Loss:0.7355, Validation Accuracy:0.6141\n",
    "Epoch #67: Loss:0.7354, Accuracy:0.6542, Validation Loss:0.7060, Validation Accuracy:0.6355\n",
    "Epoch #68: Loss:0.7318, Accuracy:0.6530, Validation Loss:0.7062, Validation Accuracy:0.6388\n",
    "Epoch #69: Loss:0.7355, Accuracy:0.6439, Validation Loss:0.7935, Validation Accuracy:0.5780\n",
    "Epoch #70: Loss:0.7443, Accuracy:0.6460, Validation Loss:0.7546, Validation Accuracy:0.6010\n",
    "Epoch #71: Loss:0.7141, Accuracy:0.6542, Validation Loss:0.6909, Validation Accuracy:0.6338\n",
    "Epoch #72: Loss:0.7029, Accuracy:0.6674, Validation Loss:0.6901, Validation Accuracy:0.6256\n",
    "Epoch #73: Loss:0.6983, Accuracy:0.6604, Validation Loss:0.6872, Validation Accuracy:0.6502\n",
    "Epoch #74: Loss:0.7024, Accuracy:0.6628, Validation Loss:0.7051, Validation Accuracy:0.6338\n",
    "Epoch #75: Loss:0.6938, Accuracy:0.6702, Validation Loss:0.6787, Validation Accuracy:0.6502\n",
    "Epoch #76: Loss:0.6846, Accuracy:0.6686, Validation Loss:0.6676, Validation Accuracy:0.6470\n",
    "Epoch #77: Loss:0.6815, Accuracy:0.6727, Validation Loss:0.6706, Validation Accuracy:0.6388\n",
    "Epoch #78: Loss:0.6778, Accuracy:0.6747, Validation Loss:0.6625, Validation Accuracy:0.6470\n",
    "Epoch #79: Loss:0.6733, Accuracy:0.6727, Validation Loss:0.6770, Validation Accuracy:0.6486\n",
    "Epoch #80: Loss:0.6795, Accuracy:0.6669, Validation Loss:0.6572, Validation Accuracy:0.6470\n",
    "Epoch #81: Loss:0.6827, Accuracy:0.6649, Validation Loss:0.6857, Validation Accuracy:0.6453\n",
    "Epoch #82: Loss:0.6876, Accuracy:0.6632, Validation Loss:0.7306, Validation Accuracy:0.6305\n",
    "Epoch #83: Loss:0.7034, Accuracy:0.6600, Validation Loss:0.6559, Validation Accuracy:0.6502\n",
    "Epoch #84: Loss:0.7180, Accuracy:0.6480, Validation Loss:0.7076, Validation Accuracy:0.6470\n",
    "Epoch #85: Loss:0.7418, Accuracy:0.6567, Validation Loss:0.6502, Validation Accuracy:0.6404\n",
    "Epoch #86: Loss:0.6999, Accuracy:0.6567, Validation Loss:0.6495, Validation Accuracy:0.6338\n",
    "Epoch #87: Loss:0.6930, Accuracy:0.6674, Validation Loss:0.6726, Validation Accuracy:0.6453\n",
    "Epoch #88: Loss:0.6868, Accuracy:0.6600, Validation Loss:0.6783, Validation Accuracy:0.6535\n",
    "Epoch #89: Loss:0.6730, Accuracy:0.6657, Validation Loss:0.6609, Validation Accuracy:0.6552\n",
    "Epoch #90: Loss:0.6701, Accuracy:0.6731, Validation Loss:0.6951, Validation Accuracy:0.6453\n",
    "Epoch #91: Loss:0.6830, Accuracy:0.6587, Validation Loss:0.6603, Validation Accuracy:0.6601\n",
    "Epoch #92: Loss:0.6669, Accuracy:0.6731, Validation Loss:0.6617, Validation Accuracy:0.6650\n",
    "Epoch #93: Loss:0.6726, Accuracy:0.6678, Validation Loss:0.6420, Validation Accuracy:0.6601\n",
    "Epoch #94: Loss:0.6622, Accuracy:0.6768, Validation Loss:0.6463, Validation Accuracy:0.6420\n",
    "Epoch #95: Loss:0.6523, Accuracy:0.6821, Validation Loss:0.6507, Validation Accuracy:0.6535\n",
    "Epoch #96: Loss:0.6525, Accuracy:0.6764, Validation Loss:0.6506, Validation Accuracy:0.6535\n",
    "Epoch #97: Loss:0.6609, Accuracy:0.6723, Validation Loss:0.6410, Validation Accuracy:0.6650\n",
    "Epoch #98: Loss:0.6571, Accuracy:0.6760, Validation Loss:0.6602, Validation Accuracy:0.6371\n",
    "Epoch #99: Loss:0.6648, Accuracy:0.6698, Validation Loss:0.6817, Validation Accuracy:0.6519\n",
    "Epoch #100: Loss:0.7076, Accuracy:0.6538, Validation Loss:0.7058, Validation Accuracy:0.6322\n",
    "Epoch #101: Loss:0.7360, Accuracy:0.6505, Validation Loss:0.6467, Validation Accuracy:0.6552\n",
    "Epoch #102: Loss:0.7307, Accuracy:0.6419, Validation Loss:0.6358, Validation Accuracy:0.6617\n",
    "Epoch #103: Loss:0.6767, Accuracy:0.6645, Validation Loss:0.6594, Validation Accuracy:0.6535\n",
    "Epoch #104: Loss:0.6887, Accuracy:0.6559, Validation Loss:0.6661, Validation Accuracy:0.6437\n",
    "Epoch #105: Loss:0.6711, Accuracy:0.6657, Validation Loss:0.6354, Validation Accuracy:0.6617\n",
    "Epoch #106: Loss:0.6563, Accuracy:0.6739, Validation Loss:0.6403, Validation Accuracy:0.6634\n",
    "Epoch #107: Loss:0.6516, Accuracy:0.6784, Validation Loss:0.6293, Validation Accuracy:0.6700\n",
    "Epoch #108: Loss:0.6453, Accuracy:0.6825, Validation Loss:0.6359, Validation Accuracy:0.6732\n",
    "Epoch #109: Loss:0.6432, Accuracy:0.6830, Validation Loss:0.6298, Validation Accuracy:0.6617\n",
    "Epoch #110: Loss:0.6397, Accuracy:0.6846, Validation Loss:0.6291, Validation Accuracy:0.6568\n",
    "Epoch #111: Loss:0.6428, Accuracy:0.6784, Validation Loss:0.6237, Validation Accuracy:0.6716\n",
    "Epoch #112: Loss:0.6370, Accuracy:0.6842, Validation Loss:0.6272, Validation Accuracy:0.6683\n",
    "Epoch #113: Loss:0.6461, Accuracy:0.6862, Validation Loss:0.6246, Validation Accuracy:0.6650\n",
    "Epoch #114: Loss:0.6347, Accuracy:0.6871, Validation Loss:0.6210, Validation Accuracy:0.6700\n",
    "Epoch #115: Loss:0.6324, Accuracy:0.6846, Validation Loss:0.6231, Validation Accuracy:0.6650\n",
    "Epoch #116: Loss:0.6382, Accuracy:0.6817, Validation Loss:0.6546, Validation Accuracy:0.6552\n",
    "Epoch #117: Loss:0.6598, Accuracy:0.6756, Validation Loss:0.6759, Validation Accuracy:0.6437\n",
    "Epoch #118: Loss:0.6575, Accuracy:0.6694, Validation Loss:0.6194, Validation Accuracy:0.6585\n",
    "Epoch #119: Loss:0.6460, Accuracy:0.6809, Validation Loss:0.6275, Validation Accuracy:0.6650\n",
    "Epoch #120: Loss:0.6422, Accuracy:0.6747, Validation Loss:0.6183, Validation Accuracy:0.6749\n",
    "Epoch #121: Loss:0.6302, Accuracy:0.6862, Validation Loss:0.6308, Validation Accuracy:0.6667\n",
    "Epoch #122: Loss:0.6333, Accuracy:0.6862, Validation Loss:0.6388, Validation Accuracy:0.6585\n",
    "Epoch #123: Loss:0.6488, Accuracy:0.6760, Validation Loss:0.6299, Validation Accuracy:0.6700\n",
    "Epoch #124: Loss:0.6304, Accuracy:0.6797, Validation Loss:0.6265, Validation Accuracy:0.6749\n",
    "Epoch #125: Loss:0.6259, Accuracy:0.6830, Validation Loss:0.6278, Validation Accuracy:0.6732\n",
    "Epoch #126: Loss:0.6359, Accuracy:0.6797, Validation Loss:0.6210, Validation Accuracy:0.6814\n",
    "Epoch #127: Loss:0.6327, Accuracy:0.6862, Validation Loss:0.6111, Validation Accuracy:0.6749\n",
    "Epoch #128: Loss:0.6353, Accuracy:0.6805, Validation Loss:0.6131, Validation Accuracy:0.6749\n",
    "Epoch #129: Loss:0.6245, Accuracy:0.6945, Validation Loss:0.6106, Validation Accuracy:0.6700\n",
    "Epoch #130: Loss:0.6185, Accuracy:0.6903, Validation Loss:0.6101, Validation Accuracy:0.6749\n",
    "Epoch #131: Loss:0.6279, Accuracy:0.6846, Validation Loss:0.6075, Validation Accuracy:0.6749\n",
    "Epoch #132: Loss:0.6254, Accuracy:0.6821, Validation Loss:0.6104, Validation Accuracy:0.6749\n",
    "Epoch #133: Loss:0.6257, Accuracy:0.6908, Validation Loss:0.6365, Validation Accuracy:0.6585\n",
    "Epoch #134: Loss:0.6445, Accuracy:0.6776, Validation Loss:0.6088, Validation Accuracy:0.6765\n",
    "Epoch #135: Loss:0.6502, Accuracy:0.6760, Validation Loss:0.6129, Validation Accuracy:0.6716\n",
    "Epoch #136: Loss:0.6474, Accuracy:0.6678, Validation Loss:0.6954, Validation Accuracy:0.6338\n",
    "Epoch #137: Loss:0.6639, Accuracy:0.6686, Validation Loss:0.6711, Validation Accuracy:0.6502\n",
    "Epoch #138: Loss:0.6331, Accuracy:0.6817, Validation Loss:0.6098, Validation Accuracy:0.6782\n",
    "Epoch #139: Loss:0.6206, Accuracy:0.6858, Validation Loss:0.6111, Validation Accuracy:0.6782\n",
    "Epoch #140: Loss:0.6258, Accuracy:0.6838, Validation Loss:0.6072, Validation Accuracy:0.6814\n",
    "Epoch #141: Loss:0.6189, Accuracy:0.6895, Validation Loss:0.6294, Validation Accuracy:0.6683\n",
    "Epoch #142: Loss:0.6276, Accuracy:0.6875, Validation Loss:0.6235, Validation Accuracy:0.6650\n",
    "Epoch #143: Loss:0.6218, Accuracy:0.6825, Validation Loss:0.6312, Validation Accuracy:0.6700\n",
    "Epoch #144: Loss:0.6456, Accuracy:0.6776, Validation Loss:0.6592, Validation Accuracy:0.6568\n",
    "Epoch #145: Loss:0.6535, Accuracy:0.6686, Validation Loss:0.6687, Validation Accuracy:0.6437\n",
    "Epoch #146: Loss:0.6423, Accuracy:0.6780, Validation Loss:0.6394, Validation Accuracy:0.6552\n",
    "Epoch #147: Loss:0.6468, Accuracy:0.6694, Validation Loss:0.6666, Validation Accuracy:0.6437\n",
    "Epoch #148: Loss:0.6711, Accuracy:0.6694, Validation Loss:0.6222, Validation Accuracy:0.6650\n",
    "Epoch #149: Loss:0.6489, Accuracy:0.6719, Validation Loss:0.5986, Validation Accuracy:0.6864\n",
    "Epoch #150: Loss:0.6302, Accuracy:0.6842, Validation Loss:0.6007, Validation Accuracy:0.6880\n",
    "Epoch #151: Loss:0.6260, Accuracy:0.6846, Validation Loss:0.5990, Validation Accuracy:0.6897\n",
    "Epoch #152: Loss:0.6197, Accuracy:0.6879, Validation Loss:0.5993, Validation Accuracy:0.6880\n",
    "Epoch #153: Loss:0.6184, Accuracy:0.6891, Validation Loss:0.5964, Validation Accuracy:0.6913\n",
    "Epoch #154: Loss:0.6067, Accuracy:0.6924, Validation Loss:0.5989, Validation Accuracy:0.6946\n",
    "Epoch #155: Loss:0.6082, Accuracy:0.6903, Validation Loss:0.5969, Validation Accuracy:0.6864\n",
    "Epoch #156: Loss:0.6120, Accuracy:0.6887, Validation Loss:0.6010, Validation Accuracy:0.6864\n",
    "Epoch #157: Loss:0.6209, Accuracy:0.6891, Validation Loss:0.5926, Validation Accuracy:0.6897\n",
    "Epoch #158: Loss:0.6183, Accuracy:0.6875, Validation Loss:0.5975, Validation Accuracy:0.6847\n",
    "Epoch #159: Loss:0.6183, Accuracy:0.6871, Validation Loss:0.5923, Validation Accuracy:0.6979\n",
    "Epoch #160: Loss:0.6278, Accuracy:0.6887, Validation Loss:0.6140, Validation Accuracy:0.6700\n",
    "Epoch #161: Loss:0.6177, Accuracy:0.6867, Validation Loss:0.5942, Validation Accuracy:0.6847\n",
    "Epoch #162: Loss:0.6025, Accuracy:0.6932, Validation Loss:0.5961, Validation Accuracy:0.6847\n",
    "Epoch #163: Loss:0.6064, Accuracy:0.6928, Validation Loss:0.5963, Validation Accuracy:0.6913\n",
    "Epoch #164: Loss:0.6021, Accuracy:0.6953, Validation Loss:0.5935, Validation Accuracy:0.6913\n",
    "Epoch #165: Loss:0.5996, Accuracy:0.6973, Validation Loss:0.5883, Validation Accuracy:0.6913\n",
    "Epoch #166: Loss:0.5969, Accuracy:0.6998, Validation Loss:0.5890, Validation Accuracy:0.7011\n",
    "Epoch #167: Loss:0.5983, Accuracy:0.6973, Validation Loss:0.5964, Validation Accuracy:0.6782\n",
    "Epoch #168: Loss:0.6012, Accuracy:0.6924, Validation Loss:0.5912, Validation Accuracy:0.6864\n",
    "Epoch #169: Loss:0.5938, Accuracy:0.7006, Validation Loss:0.5865, Validation Accuracy:0.6962\n",
    "Epoch #170: Loss:0.5998, Accuracy:0.6936, Validation Loss:0.6025, Validation Accuracy:0.6765\n",
    "Epoch #171: Loss:0.6042, Accuracy:0.6928, Validation Loss:0.6329, Validation Accuracy:0.6634\n",
    "Epoch #172: Loss:0.6337, Accuracy:0.6834, Validation Loss:0.5847, Validation Accuracy:0.6913\n",
    "Epoch #173: Loss:0.6228, Accuracy:0.6842, Validation Loss:0.6028, Validation Accuracy:0.6765\n",
    "Epoch #174: Loss:0.6110, Accuracy:0.6949, Validation Loss:0.5832, Validation Accuracy:0.7011\n",
    "Epoch #175: Loss:0.6014, Accuracy:0.6891, Validation Loss:0.5855, Validation Accuracy:0.7061\n",
    "Epoch #176: Loss:0.6042, Accuracy:0.7027, Validation Loss:0.5844, Validation Accuracy:0.7028\n",
    "Epoch #177: Loss:0.5963, Accuracy:0.6990, Validation Loss:0.5839, Validation Accuracy:0.7028\n",
    "Epoch #178: Loss:0.5959, Accuracy:0.6990, Validation Loss:0.5900, Validation Accuracy:0.6897\n",
    "Epoch #179: Loss:0.5991, Accuracy:0.6953, Validation Loss:0.5857, Validation Accuracy:0.6946\n",
    "Epoch #180: Loss:0.5970, Accuracy:0.6928, Validation Loss:0.5867, Validation Accuracy:0.6979\n",
    "Epoch #181: Loss:0.5936, Accuracy:0.6982, Validation Loss:0.5842, Validation Accuracy:0.6995\n",
    "Epoch #182: Loss:0.5934, Accuracy:0.6994, Validation Loss:0.5825, Validation Accuracy:0.7094\n",
    "Epoch #183: Loss:0.5901, Accuracy:0.7051, Validation Loss:0.5794, Validation Accuracy:0.6929\n",
    "Epoch #184: Loss:0.5893, Accuracy:0.6977, Validation Loss:0.5814, Validation Accuracy:0.6929\n",
    "Epoch #185: Loss:0.6042, Accuracy:0.6990, Validation Loss:0.5791, Validation Accuracy:0.7094\n",
    "Epoch #186: Loss:0.6009, Accuracy:0.6977, Validation Loss:0.5849, Validation Accuracy:0.6913\n",
    "Epoch #187: Loss:0.6109, Accuracy:0.6846, Validation Loss:0.5890, Validation Accuracy:0.6847\n",
    "Epoch #188: Loss:0.6039, Accuracy:0.6908, Validation Loss:0.6007, Validation Accuracy:0.6782\n",
    "Epoch #189: Loss:0.6130, Accuracy:0.6875, Validation Loss:0.5798, Validation Accuracy:0.7061\n",
    "Epoch #190: Loss:0.6010, Accuracy:0.6891, Validation Loss:0.5769, Validation Accuracy:0.7028\n",
    "Epoch #191: Loss:0.5876, Accuracy:0.6994, Validation Loss:0.5772, Validation Accuracy:0.7110\n",
    "Epoch #192: Loss:0.5907, Accuracy:0.6973, Validation Loss:0.5995, Validation Accuracy:0.6831\n",
    "Epoch #193: Loss:0.6065, Accuracy:0.6932, Validation Loss:0.5758, Validation Accuracy:0.7044\n",
    "Epoch #194: Loss:0.5887, Accuracy:0.7092, Validation Loss:0.5802, Validation Accuracy:0.7028\n",
    "Epoch #195: Loss:0.5870, Accuracy:0.7027, Validation Loss:0.5755, Validation Accuracy:0.7061\n",
    "Epoch #196: Loss:0.5896, Accuracy:0.6998, Validation Loss:0.5784, Validation Accuracy:0.6913\n",
    "Epoch #197: Loss:0.5881, Accuracy:0.7043, Validation Loss:0.5801, Validation Accuracy:0.7061\n",
    "Epoch #198: Loss:0.5824, Accuracy:0.7047, Validation Loss:0.5783, Validation Accuracy:0.7077\n",
    "Epoch #199: Loss:0.5853, Accuracy:0.7047, Validation Loss:0.5747, Validation Accuracy:0.7061\n",
    "Epoch #200: Loss:0.5851, Accuracy:0.7014, Validation Loss:0.5752, Validation Accuracy:0.6880\n",
    "Epoch #201: Loss:0.5830, Accuracy:0.7018, Validation Loss:0.5791, Validation Accuracy:0.6864\n",
    "Epoch #202: Loss:0.5861, Accuracy:0.7031, Validation Loss:0.5726, Validation Accuracy:0.7061\n",
    "Epoch #203: Loss:0.5783, Accuracy:0.7064, Validation Loss:0.5718, Validation Accuracy:0.7028\n",
    "Epoch #204: Loss:0.5878, Accuracy:0.6977, Validation Loss:0.5787, Validation Accuracy:0.6880\n",
    "Epoch #205: Loss:0.5901, Accuracy:0.7035, Validation Loss:0.5700, Validation Accuracy:0.7028\n",
    "Epoch #206: Loss:0.5827, Accuracy:0.7055, Validation Loss:0.5732, Validation Accuracy:0.7094\n",
    "Epoch #207: Loss:0.5805, Accuracy:0.7072, Validation Loss:0.5678, Validation Accuracy:0.7028\n",
    "Epoch #208: Loss:0.5783, Accuracy:0.7076, Validation Loss:0.5726, Validation Accuracy:0.7143\n",
    "Epoch #209: Loss:0.5884, Accuracy:0.7023, Validation Loss:0.6092, Validation Accuracy:0.6847\n",
    "Epoch #210: Loss:0.6055, Accuracy:0.7035, Validation Loss:0.5882, Validation Accuracy:0.6897\n",
    "Epoch #211: Loss:0.5910, Accuracy:0.7023, Validation Loss:0.5681, Validation Accuracy:0.7044\n",
    "Epoch #212: Loss:0.5784, Accuracy:0.7080, Validation Loss:0.5778, Validation Accuracy:0.7061\n",
    "Epoch #213: Loss:0.5772, Accuracy:0.7142, Validation Loss:0.5677, Validation Accuracy:0.6995\n",
    "Epoch #214: Loss:0.5754, Accuracy:0.7088, Validation Loss:0.5688, Validation Accuracy:0.7094\n",
    "Epoch #215: Loss:0.5773, Accuracy:0.7043, Validation Loss:0.5768, Validation Accuracy:0.7044\n",
    "Epoch #216: Loss:0.5772, Accuracy:0.7076, Validation Loss:0.5809, Validation Accuracy:0.6995\n",
    "Epoch #217: Loss:0.5767, Accuracy:0.7105, Validation Loss:0.5844, Validation Accuracy:0.7028\n",
    "Epoch #218: Loss:0.5890, Accuracy:0.7047, Validation Loss:0.5661, Validation Accuracy:0.7110\n",
    "Epoch #219: Loss:0.6019, Accuracy:0.7060, Validation Loss:0.6028, Validation Accuracy:0.6913\n",
    "Epoch #220: Loss:0.5942, Accuracy:0.7035, Validation Loss:0.5663, Validation Accuracy:0.7094\n",
    "Epoch #221: Loss:0.5756, Accuracy:0.7109, Validation Loss:0.5674, Validation Accuracy:0.7061\n",
    "Epoch #222: Loss:0.5803, Accuracy:0.7047, Validation Loss:0.5663, Validation Accuracy:0.7126\n",
    "Epoch #223: Loss:0.5733, Accuracy:0.7097, Validation Loss:0.5670, Validation Accuracy:0.7307\n",
    "Epoch #224: Loss:0.5733, Accuracy:0.7092, Validation Loss:0.5988, Validation Accuracy:0.6864\n",
    "Epoch #225: Loss:0.5994, Accuracy:0.7010, Validation Loss:0.5645, Validation Accuracy:0.7209\n",
    "Epoch #226: Loss:0.6015, Accuracy:0.6965, Validation Loss:0.6208, Validation Accuracy:0.6732\n",
    "Epoch #227: Loss:0.6043, Accuracy:0.6986, Validation Loss:0.5761, Validation Accuracy:0.7028\n",
    "Epoch #228: Loss:0.5881, Accuracy:0.7031, Validation Loss:0.5677, Validation Accuracy:0.7143\n",
    "Epoch #229: Loss:0.5897, Accuracy:0.7064, Validation Loss:0.5845, Validation Accuracy:0.6913\n",
    "Epoch #230: Loss:0.5771, Accuracy:0.7166, Validation Loss:0.5659, Validation Accuracy:0.7225\n",
    "Epoch #231: Loss:0.5692, Accuracy:0.7117, Validation Loss:0.5710, Validation Accuracy:0.7044\n",
    "Epoch #232: Loss:0.5817, Accuracy:0.7047, Validation Loss:0.5631, Validation Accuracy:0.7241\n",
    "Epoch #233: Loss:0.5902, Accuracy:0.7006, Validation Loss:0.5686, Validation Accuracy:0.7094\n",
    "Epoch #234: Loss:0.5800, Accuracy:0.7080, Validation Loss:0.5723, Validation Accuracy:0.7044\n",
    "Epoch #235: Loss:0.5706, Accuracy:0.7121, Validation Loss:0.5652, Validation Accuracy:0.7110\n",
    "Epoch #236: Loss:0.5697, Accuracy:0.7105, Validation Loss:0.5611, Validation Accuracy:0.7225\n",
    "Epoch #237: Loss:0.5670, Accuracy:0.7092, Validation Loss:0.5588, Validation Accuracy:0.7192\n",
    "Epoch #238: Loss:0.5668, Accuracy:0.7150, Validation Loss:0.5617, Validation Accuracy:0.7192\n",
    "Epoch #239: Loss:0.5714, Accuracy:0.7101, Validation Loss:0.5594, Validation Accuracy:0.7126\n",
    "Epoch #240: Loss:0.5759, Accuracy:0.7051, Validation Loss:0.5827, Validation Accuracy:0.6946\n",
    "Epoch #241: Loss:0.6013, Accuracy:0.7064, Validation Loss:0.5953, Validation Accuracy:0.6946\n",
    "Epoch #242: Loss:0.5996, Accuracy:0.7018, Validation Loss:0.5688, Validation Accuracy:0.7225\n",
    "Epoch #243: Loss:0.5754, Accuracy:0.7121, Validation Loss:0.5705, Validation Accuracy:0.7061\n",
    "Epoch #244: Loss:0.5814, Accuracy:0.7055, Validation Loss:0.5629, Validation Accuracy:0.7241\n",
    "Epoch #245: Loss:0.5709, Accuracy:0.7146, Validation Loss:0.5576, Validation Accuracy:0.7176\n",
    "Epoch #246: Loss:0.5725, Accuracy:0.7088, Validation Loss:0.5754, Validation Accuracy:0.7126\n",
    "Epoch #247: Loss:0.5714, Accuracy:0.7125, Validation Loss:0.5593, Validation Accuracy:0.7192\n",
    "Epoch #248: Loss:0.5686, Accuracy:0.7113, Validation Loss:0.5612, Validation Accuracy:0.7176\n",
    "Epoch #249: Loss:0.5701, Accuracy:0.7097, Validation Loss:0.5563, Validation Accuracy:0.7192\n",
    "Epoch #250: Loss:0.5645, Accuracy:0.7088, Validation Loss:0.5659, Validation Accuracy:0.7061\n",
    "Epoch #251: Loss:0.5846, Accuracy:0.7027, Validation Loss:0.5586, Validation Accuracy:0.7176\n",
    "Epoch #252: Loss:0.5952, Accuracy:0.7080, Validation Loss:0.6231, Validation Accuracy:0.6831\n",
    "Epoch #253: Loss:0.5928, Accuracy:0.7060, Validation Loss:0.5834, Validation Accuracy:0.7061\n",
    "Epoch #254: Loss:0.5811, Accuracy:0.7129, Validation Loss:0.5700, Validation Accuracy:0.7061\n",
    "Epoch #255: Loss:0.5867, Accuracy:0.7125, Validation Loss:0.5556, Validation Accuracy:0.7323\n",
    "Epoch #256: Loss:0.5738, Accuracy:0.7138, Validation Loss:0.6115, Validation Accuracy:0.6946\n",
    "Epoch #257: Loss:0.5910, Accuracy:0.7101, Validation Loss:0.5741, Validation Accuracy:0.6962\n",
    "Epoch #258: Loss:0.5690, Accuracy:0.7154, Validation Loss:0.5627, Validation Accuracy:0.7291\n",
    "Epoch #259: Loss:0.5625, Accuracy:0.7191, Validation Loss:0.5547, Validation Accuracy:0.7241\n",
    "Epoch #260: Loss:0.5585, Accuracy:0.7257, Validation Loss:0.5585, Validation Accuracy:0.7258\n",
    "Epoch #261: Loss:0.5626, Accuracy:0.7203, Validation Loss:0.5610, Validation Accuracy:0.7225\n",
    "Epoch #262: Loss:0.5613, Accuracy:0.7220, Validation Loss:0.5757, Validation Accuracy:0.7143\n",
    "Epoch #263: Loss:0.5679, Accuracy:0.7129, Validation Loss:0.5588, Validation Accuracy:0.7274\n",
    "Epoch #264: Loss:0.5810, Accuracy:0.7166, Validation Loss:0.5734, Validation Accuracy:0.6995\n",
    "Epoch #265: Loss:0.5656, Accuracy:0.7211, Validation Loss:0.5899, Validation Accuracy:0.6995\n",
    "Epoch #266: Loss:0.5829, Accuracy:0.7142, Validation Loss:0.5821, Validation Accuracy:0.6979\n",
    "Epoch #267: Loss:0.5818, Accuracy:0.7129, Validation Loss:0.5902, Validation Accuracy:0.7028\n",
    "Epoch #268: Loss:0.5907, Accuracy:0.7117, Validation Loss:0.5557, Validation Accuracy:0.7291\n",
    "Epoch #269: Loss:0.5669, Accuracy:0.7236, Validation Loss:0.5644, Validation Accuracy:0.7225\n",
    "Epoch #270: Loss:0.5625, Accuracy:0.7232, Validation Loss:0.5817, Validation Accuracy:0.7094\n",
    "Epoch #271: Loss:0.5640, Accuracy:0.7203, Validation Loss:0.5513, Validation Accuracy:0.7356\n",
    "Epoch #272: Loss:0.5626, Accuracy:0.7257, Validation Loss:0.5606, Validation Accuracy:0.7143\n",
    "Epoch #273: Loss:0.5672, Accuracy:0.7154, Validation Loss:0.5896, Validation Accuracy:0.7011\n",
    "Epoch #274: Loss:0.5704, Accuracy:0.7207, Validation Loss:0.5835, Validation Accuracy:0.7110\n",
    "Epoch #275: Loss:0.5642, Accuracy:0.7294, Validation Loss:0.5886, Validation Accuracy:0.6979\n",
    "Epoch #276: Loss:0.5755, Accuracy:0.7211, Validation Loss:0.5576, Validation Accuracy:0.7143\n",
    "Epoch #277: Loss:0.5608, Accuracy:0.7228, Validation Loss:0.5585, Validation Accuracy:0.7241\n",
    "Epoch #278: Loss:0.5719, Accuracy:0.7199, Validation Loss:0.6037, Validation Accuracy:0.6995\n",
    "Epoch #279: Loss:0.5794, Accuracy:0.7146, Validation Loss:0.5891, Validation Accuracy:0.7159\n",
    "Epoch #280: Loss:0.5785, Accuracy:0.7113, Validation Loss:0.5627, Validation Accuracy:0.7209\n",
    "Epoch #281: Loss:0.5734, Accuracy:0.7170, Validation Loss:0.5630, Validation Accuracy:0.7110\n",
    "Epoch #282: Loss:0.5735, Accuracy:0.7216, Validation Loss:0.5731, Validation Accuracy:0.7061\n",
    "Epoch #283: Loss:0.5608, Accuracy:0.7261, Validation Loss:0.5528, Validation Accuracy:0.7225\n",
    "Epoch #284: Loss:0.5557, Accuracy:0.7273, Validation Loss:0.5515, Validation Accuracy:0.7143\n",
    "Epoch #285: Loss:0.5663, Accuracy:0.7216, Validation Loss:0.5509, Validation Accuracy:0.7340\n",
    "Epoch #286: Loss:0.5566, Accuracy:0.7326, Validation Loss:0.5587, Validation Accuracy:0.7291\n",
    "Epoch #287: Loss:0.5510, Accuracy:0.7273, Validation Loss:0.5579, Validation Accuracy:0.7323\n",
    "Epoch #288: Loss:0.5516, Accuracy:0.7294, Validation Loss:0.5475, Validation Accuracy:0.7323\n",
    "Epoch #289: Loss:0.5496, Accuracy:0.7310, Validation Loss:0.5523, Validation Accuracy:0.7176\n",
    "Epoch #290: Loss:0.5702, Accuracy:0.7236, Validation Loss:0.6138, Validation Accuracy:0.6962\n",
    "Epoch #291: Loss:0.5776, Accuracy:0.7257, Validation Loss:0.6671, Validation Accuracy:0.6683\n",
    "Epoch #292: Loss:0.6095, Accuracy:0.7039, Validation Loss:0.5854, Validation Accuracy:0.7077\n",
    "Epoch #293: Loss:0.5616, Accuracy:0.7195, Validation Loss:0.5568, Validation Accuracy:0.7094\n",
    "Epoch #294: Loss:0.5605, Accuracy:0.7302, Validation Loss:0.5658, Validation Accuracy:0.7061\n",
    "Epoch #295: Loss:0.5561, Accuracy:0.7310, Validation Loss:0.5820, Validation Accuracy:0.7044\n",
    "Epoch #296: Loss:0.5599, Accuracy:0.7261, Validation Loss:0.5511, Validation Accuracy:0.7291\n",
    "Epoch #297: Loss:0.5418, Accuracy:0.7405, Validation Loss:0.5448, Validation Accuracy:0.7438\n",
    "Epoch #298: Loss:0.5425, Accuracy:0.7335, Validation Loss:0.5455, Validation Accuracy:0.7422\n",
    "Epoch #299: Loss:0.5473, Accuracy:0.7244, Validation Loss:0.5536, Validation Accuracy:0.7126\n",
    "Epoch #300: Loss:0.5593, Accuracy:0.7339, Validation Loss:0.5529, Validation Accuracy:0.7209\n",
    "\n",
    "Test:\n",
    "Test Loss:0.55286872, Accuracy:0.7209\n",
    "Labels: ['02', '01', '05', '03', '04']\n",
    "Confusion Matrix:\n",
    "      02  01   05  03  04\n",
    "t:02  78  26    1   9   0\n",
    "t:01  36  85    1   2   2\n",
    "t:05   0   2  140   0   0\n",
    "t:03   8   1    1  49  56\n",
    "t:04   2   0    0  23  87\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.63      0.68      0.66       114\n",
    "          01       0.75      0.67      0.71       126\n",
    "          05       0.98      0.99      0.98       142\n",
    "          03       0.59      0.43      0.49       115\n",
    "          04       0.60      0.78      0.68       112\n",
    "\n",
    "    accuracy                           0.72       609\n",
    "   macro avg       0.71      0.71      0.70       609\n",
    "weighted avg       0.72      0.72      0.72       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 14:13:54 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 56 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6074929752177598, 1.6055408125048984, 1.6053323422746706, 1.6053963701909006, 1.6052191964119722, 1.6047499087839487, 1.604335065545707, 1.6037916636036338, 1.60294674730849, 1.6015780394887689, 1.5994640600505134, 1.5960629640150148, 1.5905993431072516, 1.5817036425147346, 1.568632161480257, 1.5501208438466139, 1.5262410086755487, 1.492573028714786, 1.4468751789509564, 1.386117544667474, 1.3408500187111214, 1.2961136437401983, 1.250067319384545, 1.2131009000079778, 1.1926103977146993, 1.1438639273588684, 1.085385200425322, 1.049744491017315, 1.0270792737187227, 1.0035079341803865, 1.0276291563201616, 0.9666584822148917, 0.9469218120982102, 0.941610726327536, 0.9202258020198991, 0.9082143280306473, 0.8974470531411947, 0.886583227340028, 0.8851916642220345, 0.8676148669668802, 0.8597055909473125, 0.8529350905387076, 0.8453043512327135, 0.8372615581662785, 0.8335467079785853, 0.8562783416856099, 0.81580070615402, 0.8147821619201372, 0.8451749434807814, 0.8117489715906591, 0.8249474360633562, 0.8276082524916613, 0.8479624029450816, 0.836240862097059, 0.8369916374068738, 0.777959308894397, 0.7876344333924292, 0.7688653223424514, 0.756936427897029, 0.7530965012282573, 0.746703116960322, 0.7541944957131823, 0.7278528046138181, 0.754826872513212, 0.7315663272524114, 0.7354507657694699, 0.7060260831428866, 0.7062193168990913, 0.7935132585918571, 0.7546344038300914, 0.6909410492539994, 0.6901347811390418, 0.6872054397178988, 0.7051205074258626, 0.6786578540340041, 0.6675855913772959, 0.670578220011957, 0.6624898187241139, 0.6770332767849877, 0.6572172893287709, 0.6857251850842255, 0.7306158699229824, 0.6558840790601396, 0.7075667339201239, 0.6502313453379914, 0.6495107020846337, 0.6725704651356527, 0.6782858882631574, 0.6608993316127357, 0.6950766363950395, 0.6602828250142742, 0.661712778319279, 0.6420099002191391, 0.6462801741849026, 0.6507048485510063, 0.6505517022950309, 0.6410281336170504, 0.6601757526593451, 0.6817106124998509, 0.7057692175624014, 0.646666734383024, 0.6357630226999668, 0.6593874306318599, 0.6661285481037961, 0.6353824146471195, 0.6402708619881929, 0.6293388532691793, 0.6358513558047941, 0.6297935783765195, 0.6291313704794459, 0.6237292892631443, 0.6272015150740424, 0.6246014534937728, 0.6210042680621343, 0.6230584810715787, 0.6545598485395434, 0.6758880519318855, 0.6193677317137005, 0.6275002856559941, 0.6183398536273411, 0.6308287728596204, 0.6387908099515881, 0.6299198900146046, 0.6265178516580554, 0.6278496142874406, 0.6209933929059698, 0.6110768600050451, 0.6130762859714051, 0.6106064426292144, 0.6100772782695313, 0.6075079754264092, 0.610425620928578, 0.6364973078807586, 0.6087529996933021, 0.6128686320018298, 0.6954379861773724, 0.671135245089852, 0.6097513024247143, 0.611117505185514, 0.6072483628449965, 0.6294455651579232, 0.6235123822841738, 0.6311502139556584, 0.6591621332959393, 0.6686996196095384, 0.639446018853994, 0.6665702156832652, 0.6222036540606143, 0.5986370703661187, 0.6007054801449204, 0.5990304836302947, 0.5992817985404693, 0.596362692577694, 0.5988688703828258, 0.5969368140098497, 0.6009578318235713, 0.5925949064675223, 0.5975081247257678, 0.5923210090800067, 0.6140377078150293, 0.5942351018462471, 0.5960639457788764, 0.596302710632581, 0.5935494819493913, 0.588321362125071, 0.5889770179155034, 0.5963648489151878, 0.5911978230687785, 0.5865025379876981, 0.602500431643331, 0.6328791584483117, 0.5846745953184044, 0.6028211885093664, 0.5831571878079319, 0.5854898205727388, 0.5843933814852108, 0.5838767748650268, 0.590000960333594, 0.5857202623571668, 0.5866569316348027, 0.5842491684386687, 0.5825296838099537, 0.579352575814587, 0.5813732939498569, 0.5791027323757291, 0.5849206973668586, 0.5890181653801052, 0.6007139100425545, 0.5798088899876293, 0.5769238578666411, 0.5772034784745309, 0.5995228452048278, 0.5758039604364749, 0.5801809394398738, 0.5755380660735915, 0.5783780350277968, 0.5800994346881735, 0.5783097278602017, 0.5746805104618198, 0.5752377274686284, 0.5791462946603647, 0.5726102401666062, 0.5718450698946497, 0.5786965648431105, 0.5700024886084307, 0.5732335435052224, 0.5678075457539269, 0.5726483205856361, 0.6091805709210915, 0.5882080160142558, 0.568113873549087, 0.5778463907136119, 0.5676984050982495, 0.5687580329346148, 0.5768119554801527, 0.5809104288446492, 0.5843941082899597, 0.5661182551352653, 0.6028477135550212, 0.5663383258583119, 0.5673844106678892, 0.5663427219700148, 0.5669574900017975, 0.5987771849326899, 0.5645019594573818, 0.620753137055289, 0.5760932001476413, 0.5676577131344963, 0.5845234860927601, 0.5658805882206496, 0.5710348497176992, 0.5631133852333858, 0.5685967991406891, 0.5723151426597182, 0.5652174613941675, 0.5610863571292269, 0.558827356605107, 0.5616775273004385, 0.5593547407042216, 0.5826848515344567, 0.5953307584393005, 0.5687570793581713, 0.5704995955054591, 0.562852936865661, 0.5576036702627423, 0.5753521802781643, 0.5593390937019843, 0.5612379595368171, 0.5563190570605799, 0.5659310584291449, 0.5585787805234662, 0.6230677277192302, 0.5833673832451769, 0.5699883274648382, 0.5555880061902828, 0.6115476811069182, 0.57414139100092, 0.5627246368890522, 0.5547451236956616, 0.5585327449886278, 0.5609596949101278, 0.575691499978255, 0.5587544753927315, 0.5734005729944639, 0.5898720799212777, 0.5820589234872013, 0.5902078686089351, 0.5556992399085723, 0.5644338571379337, 0.5816534243780991, 0.5512695762226343, 0.5606101903519999, 0.5896373229661012, 0.583509872699606, 0.58857198252858, 0.557595395151226, 0.5584677946684983, 0.6037361189258118, 0.5890781617614828, 0.562694378380705, 0.5630090013513425, 0.5730811015921469, 0.552841939656018, 0.5515347796022794, 0.5508503017852263, 0.558744446103796, 0.5579205297288441, 0.5475275520914293, 0.5522591408054621, 0.6138218294810779, 0.6670957459213307, 0.5854194781854627, 0.5567624962388589, 0.565815837952891, 0.5820219920969557, 0.5510607579560899, 0.5448171499816851, 0.5454900384145026, 0.5536211522537695, 0.5528687210310073], 'val_acc': [0.2068965516996697, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.23481116374137953, 0.24958948886453225, 0.2692939227535611, 0.2807881756271243, 0.28571428400150856, 0.30541871749904553, 0.3267651872192502, 0.3382594401906864, 0.3711001630780732, 0.4367816087549739, 0.4499178935937302, 0.482758616676863, 0.4778325077152409, 0.4712643675714095, 0.4926108327405206, 0.5057471230112273, 0.5090311953586898, 0.5188834118138393, 0.5254515565087643, 0.5041050862991947, 0.5287356290519727, 0.5435139540772524, 0.5385878457028682, 0.5517241349948451, 0.5599343155209459, 0.5615763516457406, 0.5615763513521216, 0.5665024598243789, 0.5632183875747894, 0.5763546767688933, 0.5812807852411505, 0.5697865321718413, 0.5878489296424565, 0.5845648574907399, 0.5779967123064501, 0.5894909658651243, 0.5878489298382025, 0.5829228206808343, 0.5927750383104596, 0.584564857588613, 0.5911330013048082, 0.566502459139268, 0.5697865313888575, 0.5697865314867304, 0.6091953992647882, 0.6091953990690422, 0.610837435487456, 0.6157635437639671, 0.6108374356832019, 0.6124794717101236, 0.6174055800845079, 0.628899833349563, 0.61083743529171, 0.6387520499025855, 0.6141215078349184, 0.635467977750869, 0.6387520498047126, 0.5779967119149583, 0.6009852182493225, 0.6338259416260743, 0.6256157609042275, 0.6502463029718947, 0.6338259413324553, 0.6502463029718947, 0.6469622307223052, 0.6387520500983315, 0.6469622307223052, 0.6486042668470999, 0.6469622306244323, 0.6453201944017645, 0.6305418688871199, 0.6502463029718947, 0.6469622308201782, 0.6403940861252533, 0.6338259415282013, 0.6453201945975104, 0.6535303751236112, 0.6551724113462789, 0.6453201943038915, 0.6600985197206631, 0.6650246281929204, 0.6600985197206631, 0.642036122152175, 0.6535303752214842, 0.6535303751236112, 0.6650246281929204, 0.6371100134841718, 0.6518883389988165, 0.6321839053055336, 0.655172411150533, 0.6617405559433309, 0.6535303751236112, 0.6436781584727157, 0.6617405559433309, 0.6633825920681257, 0.6699507365673046, 0.6732348088168941, 0.6617405559433309, 0.6568144472753278, 0.6715927726920994, 0.6683087004425099, 0.6650246280950475, 0.6699507364694317, 0.6650246280950475, 0.655172411150533, 0.6436781583748428, 0.6584564835958684, 0.6650246280950475, 0.6748768450395618, 0.6666666642198422, 0.6584564834001225, 0.6699507365673046, 0.6748768449416889, 0.6732348087190212, 0.6814449894408678, 0.6748768449416889, 0.6748768449416889, 0.6699507364694317, 0.6748768448438159, 0.6748768451374348, 0.6748768448438159, 0.6584564834001225, 0.6765188810664836, 0.6715927724963534, 0.6338259409409635, 0.6502463028740217, 0.6781609171912784, 0.6781609172891514, 0.6814449895387409, 0.6683087002467639, 0.6650246279971744, 0.6699507364694317, 0.6568144475689467, 0.6436781583748428, 0.6551724113462789, 0.6436781583748428, 0.6650246280950475, 0.6863710979131251, 0.6880131340379199, 0.6896551701627146, 0.6880131340379199, 0.6912972063853823, 0.6945812785370988, 0.6863710979131251, 0.6863710979131251, 0.6896551703584606, 0.6847290616904573, 0.6978653508845613, 0.6699507363715587, 0.6847290617883304, 0.6847290616904573, 0.6912972061896363, 0.6912972062875093, 0.6912972063853823, 0.7011494231341507, 0.6781609171912784, 0.6863710979131251, 0.6962233147597665, 0.6765188809686107, 0.6633825916766337, 0.6912972063853823, 0.6765188809686107, 0.7011494231341507, 0.706075531508535, 0.7027914592589455, 0.7027914592589455, 0.6896551701627146, 0.6945812786349718, 0.6978653508845613, 0.699507387009356, 0.7093596037581245, 0.692939242510177, 0.692939242510177, 0.7093596037581245, 0.6912972062875093, 0.6847290616904573, 0.6781609169955324, 0.706075531508535, 0.7027914593568185, 0.7110016398829192, 0.6830870254677897, 0.7044334954816133, 0.7027914592589455, 0.706075531508535, 0.6912972064832553, 0.706075531606408, 0.7077175676333297, 0.706075531508535, 0.6880131342336657, 0.6863710978152521, 0.706075531508535, 0.7027914594546915, 0.6880131342336657, 0.7027914592589455, 0.7093596037581245, 0.7027914594546915, 0.7142857122303817, 0.6847290615925844, 0.6896551700648416, 0.7044334954816133, 0.706075531508535, 0.699507387205102, 0.7093596039538704, 0.7044334953837402, 0.699507387009356, 0.7027914592589455, 0.7110016400786652, 0.6912972060917634, 0.7093596039538704, 0.706075531704281, 0.712643676105587, 0.7307060734783292, 0.6863710975216332, 0.7208538566316877, 0.6732348082296562, 0.7027914590631995, 0.7142857123282547, 0.6912972062875093, 0.7224958927564824, 0.7044334954816133, 0.7241379289791502, 0.7093596037581245, 0.7044334950901213, 0.7110016398829192, 0.7224958927564824, 0.719211820604766, 0.7192118205068929, 0.7126436760077139, 0.6945812786349718, 0.6945812782434798, 0.7224958928543554, 0.706075531312789, 0.7241379288812771, 0.7175697842842252, 0.712643676105587, 0.7192118205068929, 0.7175697842842252, 0.7192118205068929, 0.706075531606408, 0.7175697842842252, 0.6830870253699166, 0.7060755312149161, 0.706075531312789, 0.732348109505251, 0.694581278047734, 0.6962233145640205, 0.7290640373535344, 0.7241379288812771, 0.7257799651039449, 0.7224958927564824, 0.7142857121325087, 0.7274220011308667, 0.6995073869114831, 0.699507387107229, 0.6978653508845613, 0.7027914590631995, 0.7290640374514074, 0.7224958927564824, 0.7093596036602515, 0.7356321818527134, 0.7142857123282547, 0.7011494229384048, 0.7110016398829192, 0.6978653505909423, 0.7142857124261277, 0.7241379288812771, 0.6995073864221182, 0.7159277479636845, 0.7208538564359418, 0.7110016398829192, 0.706075531410662, 0.7224958928543554, 0.7142857122303817, 0.7339901457279187, 0.7290640371577884, 0.732348109505251, 0.7323481096031239, 0.7175697844799712, 0.6962233143682746, 0.668308700051018, 0.7077175676333297, 0.7093596037581245, 0.706075531606408, 0.7044334951879943, 0.7290640374514074, 0.7438423624766871, 0.7422003264497654, 0.7126436762034599, 0.7208538568274336], 'loss': [1.6097576981942021, 1.6065484354872968, 1.6057054222486837, 1.6055464577625909, 1.6056543945531825, 1.6052372412515128, 1.6047316217324572, 1.6043509339405035, 1.6036249349249463, 1.6026218593977315, 1.6010543502576542, 1.5983955089071693, 1.5943646901931605, 1.588342185186899, 1.578467622429928, 1.5652504342782179, 1.5466285280133665, 1.5207850741654696, 1.4832817761070674, 1.435549277201815, 1.386189838209681, 1.340442200901572, 1.3037616738548514, 1.2625420638912757, 1.2186733737373743, 1.1915529663313096, 1.1371499923709971, 1.0885158889837088, 1.0549120288609968, 1.0280009954127443, 1.010808366280072, 0.998944839461873, 0.9692438664622376, 0.9512059053845964, 0.9392149011212453, 0.9280234163301926, 0.9091495457371157, 0.8984034576455181, 0.9034862263491511, 0.8913291842296138, 0.8681910113632312, 0.8733875981836103, 0.8594313476854282, 0.8504621482482926, 0.84087242149964, 0.8376287044930507, 0.842964465539803, 0.8229947965737486, 0.8196947395923936, 0.837352114267173, 0.8315644369722637, 0.8363634773593174, 0.848783582338807, 0.8340987619433314, 0.8122296035656938, 0.795802437940907, 0.7817432315197813, 0.7736047119085794, 0.7704473187056899, 0.7678273540502701, 0.757642578050586, 0.7549924379501499, 0.7571932877603252, 0.7408398807415972, 0.7403565044275789, 0.7375022182963957, 0.7354492807290393, 0.7318141957572843, 0.7354796872246682, 0.7443084441905639, 0.7141475995463268, 0.702945930903942, 0.6983008076522874, 0.7023775510719424, 0.6938043445042761, 0.6845950050765239, 0.6815459094008381, 0.6778288980284266, 0.6733293009244931, 0.6795325671622885, 0.6826848901517583, 0.6875882861795367, 0.7034196063233596, 0.7179866123003636, 0.7418368552254945, 0.699854672150935, 0.6929744277891438, 0.68681685537773, 0.6730271392044835, 0.6701176905778888, 0.6830106547236198, 0.666932771435998, 0.6725769812076733, 0.6621868672801728, 0.6523456738714809, 0.6524614240599363, 0.6609432880638562, 0.6570533756602717, 0.6648241845978848, 0.7075599007048401, 0.7360320085862334, 0.7307344601629207, 0.6766631517811722, 0.6887494755231869, 0.6711078451154658, 0.6562731942601762, 0.6516219019889832, 0.645257404256895, 0.6431772840830825, 0.6397158534619842, 0.6428441136769446, 0.6369974056063736, 0.6460571655502554, 0.6347206599413736, 0.6324461935481986, 0.6382462364202652, 0.6597756229876492, 0.6575107781304471, 0.6459817695421849, 0.6422465206416481, 0.6302282794545073, 0.6332865997506363, 0.6487972785070447, 0.6304437949182561, 0.62591793062261, 0.6358726808422644, 0.6326694391101783, 0.6353287860108597, 0.6244900900970005, 0.6185472272994337, 0.6279022558513853, 0.6253745934556887, 0.6256975661557803, 0.6445276940382971, 0.6501924594080178, 0.6473955051610113, 0.6638582748798864, 0.6331485395558806, 0.6206328353353104, 0.6258063261513838, 0.6189160620162619, 0.6276420181536821, 0.6218395092893675, 0.6456372233876457, 0.6534805456715688, 0.6422973801957509, 0.6468430654224184, 0.6711358324703005, 0.6489478539392444, 0.6301843744528611, 0.6259768543302157, 0.6196666527822522, 0.618387775156777, 0.606742251996387, 0.6082487609352174, 0.6119556303876137, 0.6208638454854366, 0.618260251007041, 0.618270554434837, 0.6277989625196437, 0.6177408928743867, 0.6024616567016382, 0.6063605241706973, 0.6021095554441886, 0.5996465271748067, 0.5968949993288247, 0.5982967814380873, 0.6012021313212981, 0.5937716099760616, 0.599751620028298, 0.6041888558889071, 0.6337488045193087, 0.6227865895696244, 0.610967244896311, 0.601432533553004, 0.6041549585438362, 0.5962937600803571, 0.5958953249136281, 0.5991322148996702, 0.5970487303557582, 0.5935740927406405, 0.5933699947852619, 0.5900675224572481, 0.5893379025145967, 0.6042090687418865, 0.6009206438701011, 0.6108940728636003, 0.6038993923571075, 0.6130268455286045, 0.6009944862164022, 0.5876346807215492, 0.5907100242510959, 0.6065257335345603, 0.5887065760164045, 0.5869947320381963, 0.5895639925032425, 0.5881397203743091, 0.5823970505099522, 0.585304000808473, 0.5850997806819312, 0.5830319263851863, 0.5861284087326003, 0.5782728980943652, 0.587782522148665, 0.5901109391169381, 0.5826874774584291, 0.580483979390632, 0.5782622129765379, 0.5884325359146698, 0.6054745732146857, 0.5909986826673426, 0.5783611315966142, 0.5772276248285658, 0.5754037171663445, 0.5773142203658024, 0.5772224446341732, 0.5767336444688284, 0.5889965142312725, 0.6019217949873124, 0.5941665052878049, 0.5755795412239842, 0.5802710124844154, 0.5732599364903428, 0.5733420574199982, 0.5993939405838812, 0.601477124240609, 0.6043133108522858, 0.5881317957226011, 0.5896701368707896, 0.5771455419381786, 0.5691750620180087, 0.581693078727448, 0.5901677984966145, 0.5800291635906917, 0.5705528658273529, 0.5697057859119203, 0.5669670075361734, 0.5667516550244247, 0.5714384106150397, 0.5758934153423662, 0.6012959083492506, 0.5996090200892219, 0.5754418585089932, 0.5814003153013743, 0.570855554197848, 0.5724830057097167, 0.5713886027952974, 0.5685577446185588, 0.5701162512787069, 0.5645154348878645, 0.5846053077210146, 0.5951966413482259, 0.5927635669953035, 0.5811126546938071, 0.5866531476592626, 0.5738402598202841, 0.5909990962770685, 0.5689943247752023, 0.5624650167733493, 0.5585012352197322, 0.5625575962497469, 0.56131229153404, 0.5678720462493583, 0.5809865160644422, 0.56563057241009, 0.5829043165369445, 0.5817998005379397, 0.5907018096050443, 0.5668923618122782, 0.5624946515662959, 0.5639624519025032, 0.5626310592314546, 0.5671695963802769, 0.5704342399534503, 0.5641537965690331, 0.5754590941895205, 0.5608422799032082, 0.5719364220479186, 0.5794454303855034, 0.5785402175826948, 0.5733833362924, 0.5734816496866685, 0.5607753022495481, 0.555696193662757, 0.5662584450944983, 0.5565857446658783, 0.5510393752209705, 0.551635212746489, 0.5495999900956908, 0.5702218590086245, 0.5775980570967437, 0.609493725818775, 0.5616131892928842, 0.5604808435302985, 0.5561025933808125, 0.5599276851579639, 0.5417888128781955, 0.5424769750365976, 0.5473058517953454, 0.5593163194597625], 'acc': [0.2065708425439114, 0.2324435314848193, 0.23285420892542147, 0.23285421049203225, 0.2328542085337688, 0.2328542098861945, 0.23285420912124782, 0.23285421008202084, 0.2328542083195837, 0.23285420969036816, 0.23285420851541005, 0.23408624274040393, 0.2427104725240437, 0.2558521570855832, 0.26324435460249257, 0.26817248660680937, 0.2845995889429684, 0.29199178767155326, 0.3190965102316173, 0.3416837784054343, 0.3761807004773886, 0.4209445586561912, 0.4377823409970536, 0.4566735114282651, 0.4722792620898762, 0.47926078410608813, 0.4870636574786302, 0.5182751554728043, 0.5285420957287235, 0.5338809053511101, 0.5310061624162741, 0.5420944600624226, 0.5494866550335894, 0.5478439457362683, 0.5585215566829, 0.5667351086526436, 0.5749486612098662, 0.5774127286807222, 0.5728952740986489, 0.573305950150108, 0.5893223790172679, 0.5926078001576528, 0.5995893200564923, 0.5991786430259015, 0.6078028730053676, 0.606981517769228, 0.5995893206439713, 0.6102669393012656, 0.6139630382555467, 0.5921971268477626, 0.603696099370412, 0.6032854186191207, 0.590965092231116, 0.6036960974121486, 0.6065708401511583, 0.6234086240586314, 0.6242299769448548, 0.6254620143030704, 0.6299794655560957, 0.6381930190924502, 0.6468172482886109, 0.6435318296939685, 0.632032855408882, 0.6455852168051859, 0.6406570865633062, 0.6513347042414687, 0.6542094450222149, 0.6529774149095743, 0.6439425053537748, 0.6459958967731718, 0.6542094479596101, 0.6673511335002813, 0.6603696141889208, 0.6628336773515972, 0.6702258750643328, 0.6685831651795326, 0.6726899388144882, 0.6747433225232228, 0.6726899421435362, 0.6669404477798964, 0.6648870677918625, 0.6632443563404514, 0.6599589338292821, 0.6480492840802156, 0.656673513668029, 0.6566735138638553, 0.6673511323253232, 0.6599589276852305, 0.6657084210697386, 0.6731006179991689, 0.6587269027375098, 0.6731006197616061, 0.667761809355914, 0.6767967177367553, 0.6821355267716629, 0.6763860389437274, 0.672279258577241, 0.6759753625006156, 0.6698151970546103, 0.6537987693624085, 0.6505133472428919, 0.6418891155009887, 0.6644763886071818, 0.655852157648584, 0.6657084214613912, 0.6739219755852247, 0.6784394220649829, 0.6825462022356429, 0.682956880049539, 0.6845995930675608, 0.67843942899234, 0.6841889083263076, 0.6862423027565347, 0.687063657796848, 0.6845995920884291, 0.6817248497410722, 0.6755646864491567, 0.6694045215906304, 0.6809034935258008, 0.6747433304297116, 0.6862422956333513, 0.686242296025004, 0.6759753636755738, 0.6796714596924596, 0.6829568760595772, 0.6796714543317133, 0.6862423021690557, 0.6804928157119046, 0.6944558481905739, 0.6903490724015285, 0.6845995863360301, 0.6821355218025693, 0.690759750411251, 0.677618069643847, 0.6759753547899532, 0.6677618111183511, 0.6685831576646966, 0.6817248495452458, 0.6858316241593332, 0.6837782305124115, 0.6895277227219615, 0.6874743369815286, 0.6825462055646908, 0.6776180670246696, 0.6685831651795326, 0.6780287503951384, 0.6694045192407142, 0.6694045188490615, 0.671868587103223, 0.6841889160369701, 0.6845995861402038, 0.687885014403772, 0.6891170470621552, 0.6924024628417937, 0.6903490724015285, 0.6887063613417702, 0.6891170472579815, 0.6874743354149178, 0.6870636579926743, 0.6887063688566063, 0.6866529797871255, 0.6932238151405381, 0.6928131394807319, 0.6952772095707652, 0.6973305924961943, 0.6997946593795714, 0.6973305919087154, 0.6924024601002249, 0.7006160118741421, 0.6936344931502607, 0.6928131452331308, 0.683367558842567, 0.6841889127079221, 0.6948665341067852, 0.6891170403306245, 0.7026694090459381, 0.6989733045350844, 0.6989733079375673, 0.695277210549897, 0.6928131398723845, 0.6981519496905975, 0.6993839882972059, 0.7051334745585306, 0.6977412772374476, 0.6989733079375673, 0.697741271680875, 0.6845995926759082, 0.6907597523695145, 0.6874743288792134, 0.6891170387640136, 0.6993839809781962, 0.6973305919087154, 0.6932238153363645, 0.709240244007698, 0.7026694030977128, 0.6997946566380024, 0.7043121108284232, 0.7047227907964092, 0.7047227973321135, 0.7014373754084232, 0.7018480468824414, 0.7030800860765288, 0.7063655014645147, 0.6977412766499685, 0.7034907553964572, 0.7055441458367224, 0.7071868578756125, 0.7075975335354188, 0.7022587229339005, 0.7034907559839362, 0.702258731232042, 0.7080082127200994, 0.7141683740537514, 0.7088295656063228, 0.7043121108284232, 0.7075975335354188, 0.7104722835934383, 0.7047227971362872, 0.7059548232589659, 0.7034907559839362, 0.7108829562424145, 0.7047227973321135, 0.7096509198633307, 0.7092402418536082, 0.70102669066717, 0.6965092372600548, 0.6985626243712721, 0.7030800835307863, 0.7063655000937303, 0.716632440153823, 0.7117043075620272, 0.7047227888381457, 0.7006160152031901, 0.7080082103701832, 0.7121149883133185, 0.7104722832017856, 0.7092402420494346, 0.7149897328147653, 0.7100615972855743, 0.7051334662603891, 0.7063655000937303, 0.7018480540056248, 0.7121149865508813, 0.705544149630858, 0.7145790518676476, 0.7088295661938019, 0.7125256651480829, 0.7112936317063945, 0.7096509214299416, 0.7088295669771073, 0.7026694011394493, 0.7080082105660096, 0.7059548242380976, 0.7129363419828474, 0.7125256671063465, 0.7137576974148133, 0.7100615998313168, 0.7154004071037872, 0.7190965089954635, 0.7256673499054488, 0.7203285398914094, 0.7219712486012516, 0.7129363439411108, 0.7166324423079128, 0.721149895127549, 0.7141683825477193, 0.7129363423745001, 0.7117043104994224, 0.7236139586818782, 0.7232032840012035, 0.7203285383247986, 0.7256673508845806, 0.7154004078870926, 0.7207392165303476, 0.72936344670564, 0.7211498959108544, 0.7227926042290439, 0.7199178634482977, 0.7145790559800009, 0.7112936338604843, 0.7170431199259827, 0.7215605717664871, 0.7260780273276922, 0.727310060377728, 0.7215605723539662, 0.7326488700001147, 0.7273100615526861, 0.7293634494472089, 0.7310061593320091, 0.7236139620109261, 0.725667348338838, 0.7039014351686168, 0.7195071844594435, 0.730184803900043, 0.7310061597236618, 0.7260780290901294, 0.7404517443517885, 0.7334702258237333, 0.7244353154846286, 0.7338809034418031]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
