{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf16.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 08:54:19 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': 'Split', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 15 Label(s): ['eo', 'by', 'ek', 'my', 'mb', 'aa', 'sg', 'sk', 'ce', 'yd', 'ib', 'ck', 'eg', 'ds', 'eb'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F781385240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F7E5696EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6902, Accuracy:0.0815, Validation Loss:2.6789, Validation Accuracy:0.1236\n",
    "Epoch #2: Loss:2.6747, Accuracy:0.1138, Validation Loss:2.6679, Validation Accuracy:0.1018\n",
    "Epoch #3: Loss:2.6615, Accuracy:0.0742, Validation Loss:2.6530, Validation Accuracy:0.0603\n",
    "Epoch #4: Loss:2.6409, Accuracy:0.0989, Validation Loss:2.6232, Validation Accuracy:0.1030\n",
    "Epoch #5: Loss:2.6034, Accuracy:0.1090, Validation Loss:2.5839, Validation Accuracy:0.1186\n",
    "Epoch #6: Loss:2.5591, Accuracy:0.1367, Validation Loss:2.5411, Validation Accuracy:0.1544\n",
    "Epoch #7: Loss:2.5237, Accuracy:0.1599, Validation Loss:2.5133, Validation Accuracy:0.1568\n",
    "Epoch #8: Loss:2.5050, Accuracy:0.1679, Validation Loss:2.4953, Validation Accuracy:0.1765\n",
    "Epoch #9: Loss:2.4888, Accuracy:0.1752, Validation Loss:2.4853, Validation Accuracy:0.1732\n",
    "Epoch #10: Loss:2.4814, Accuracy:0.1727, Validation Loss:2.4772, Validation Accuracy:0.1790\n",
    "Epoch #11: Loss:2.4705, Accuracy:0.1755, Validation Loss:2.4696, Validation Accuracy:0.1753\n",
    "Epoch #12: Loss:2.4648, Accuracy:0.1768, Validation Loss:2.4697, Validation Accuracy:0.1687\n",
    "Epoch #13: Loss:2.4579, Accuracy:0.1774, Validation Loss:2.4576, Validation Accuracy:0.1786\n",
    "Epoch #14: Loss:2.4589, Accuracy:0.1743, Validation Loss:2.4575, Validation Accuracy:0.1798\n",
    "Epoch #15: Loss:2.4521, Accuracy:0.1801, Validation Loss:2.4526, Validation Accuracy:0.1819\n",
    "Epoch #16: Loss:2.4487, Accuracy:0.1770, Validation Loss:2.4517, Validation Accuracy:0.1839\n",
    "Epoch #17: Loss:2.4540, Accuracy:0.1764, Validation Loss:2.4540, Validation Accuracy:0.1810\n",
    "Epoch #18: Loss:2.4457, Accuracy:0.1797, Validation Loss:2.4484, Validation Accuracy:0.1810\n",
    "Epoch #19: Loss:2.4435, Accuracy:0.1774, Validation Loss:2.4448, Validation Accuracy:0.1835\n",
    "Epoch #20: Loss:2.4413, Accuracy:0.1807, Validation Loss:2.4463, Validation Accuracy:0.1847\n",
    "Epoch #21: Loss:2.4400, Accuracy:0.1791, Validation Loss:2.4427, Validation Accuracy:0.1843\n",
    "Epoch #22: Loss:2.4363, Accuracy:0.1840, Validation Loss:2.4450, Validation Accuracy:0.1749\n",
    "Epoch #23: Loss:2.4372, Accuracy:0.1793, Validation Loss:2.4420, Validation Accuracy:0.1856\n",
    "Epoch #24: Loss:2.4339, Accuracy:0.1825, Validation Loss:2.4408, Validation Accuracy:0.1814\n",
    "Epoch #25: Loss:2.4347, Accuracy:0.1815, Validation Loss:2.4386, Validation Accuracy:0.1864\n",
    "Epoch #26: Loss:2.4318, Accuracy:0.1811, Validation Loss:2.4424, Validation Accuracy:0.1843\n",
    "Epoch #27: Loss:2.4354, Accuracy:0.1803, Validation Loss:2.4382, Validation Accuracy:0.1802\n",
    "Epoch #28: Loss:2.4307, Accuracy:0.1780, Validation Loss:2.4605, Validation Accuracy:0.1675\n",
    "Epoch #29: Loss:2.4338, Accuracy:0.1781, Validation Loss:2.4408, Validation Accuracy:0.1728\n",
    "Epoch #30: Loss:2.4268, Accuracy:0.1830, Validation Loss:2.4382, Validation Accuracy:0.1810\n",
    "Epoch #31: Loss:2.4258, Accuracy:0.1819, Validation Loss:2.4350, Validation Accuracy:0.1884\n",
    "Epoch #32: Loss:2.4259, Accuracy:0.1837, Validation Loss:2.4490, Validation Accuracy:0.1798\n",
    "Epoch #33: Loss:2.4386, Accuracy:0.1798, Validation Loss:2.4394, Validation Accuracy:0.1888\n",
    "Epoch #34: Loss:2.4271, Accuracy:0.1800, Validation Loss:2.4358, Validation Accuracy:0.1856\n",
    "Epoch #35: Loss:2.4270, Accuracy:0.1802, Validation Loss:2.4367, Validation Accuracy:0.1741\n",
    "Epoch #36: Loss:2.4247, Accuracy:0.1813, Validation Loss:2.4377, Validation Accuracy:0.1769\n",
    "Epoch #37: Loss:2.4225, Accuracy:0.1845, Validation Loss:2.4331, Validation Accuracy:0.1851\n",
    "Epoch #38: Loss:2.4192, Accuracy:0.1852, Validation Loss:2.4319, Validation Accuracy:0.1872\n",
    "Epoch #39: Loss:2.4208, Accuracy:0.1840, Validation Loss:2.4425, Validation Accuracy:0.1761\n",
    "Epoch #40: Loss:2.4230, Accuracy:0.1803, Validation Loss:2.4356, Validation Accuracy:0.1778\n",
    "Epoch #41: Loss:2.4210, Accuracy:0.1825, Validation Loss:2.4482, Validation Accuracy:0.1761\n",
    "Epoch #42: Loss:2.4221, Accuracy:0.1861, Validation Loss:2.4375, Validation Accuracy:0.1757\n",
    "Epoch #43: Loss:2.4152, Accuracy:0.1862, Validation Loss:2.4343, Validation Accuracy:0.1814\n",
    "Epoch #44: Loss:2.4165, Accuracy:0.1837, Validation Loss:2.4329, Validation Accuracy:0.1790\n",
    "Epoch #45: Loss:2.4159, Accuracy:0.1860, Validation Loss:2.4397, Validation Accuracy:0.1757\n",
    "Epoch #46: Loss:2.4144, Accuracy:0.1852, Validation Loss:2.4310, Validation Accuracy:0.1876\n",
    "Epoch #47: Loss:2.4135, Accuracy:0.1890, Validation Loss:2.4317, Validation Accuracy:0.1884\n",
    "Epoch #48: Loss:2.4110, Accuracy:0.1874, Validation Loss:2.4307, Validation Accuracy:0.1814\n",
    "Epoch #49: Loss:2.4102, Accuracy:0.1885, Validation Loss:2.4295, Validation Accuracy:0.1810\n",
    "Epoch #50: Loss:2.4071, Accuracy:0.1860, Validation Loss:2.4279, Validation Accuracy:0.1794\n",
    "Epoch #51: Loss:2.4062, Accuracy:0.1880, Validation Loss:2.4275, Validation Accuracy:0.1827\n",
    "Epoch #52: Loss:2.4059, Accuracy:0.1884, Validation Loss:2.4274, Validation Accuracy:0.1876\n",
    "Epoch #53: Loss:2.4074, Accuracy:0.1893, Validation Loss:2.4277, Validation Accuracy:0.1794\n",
    "Epoch #54: Loss:2.4032, Accuracy:0.1933, Validation Loss:2.4284, Validation Accuracy:0.1798\n",
    "Epoch #55: Loss:2.4070, Accuracy:0.1898, Validation Loss:2.4300, Validation Accuracy:0.1839\n",
    "Epoch #56: Loss:2.4025, Accuracy:0.1933, Validation Loss:2.4334, Validation Accuracy:0.1802\n",
    "Epoch #57: Loss:2.4016, Accuracy:0.1927, Validation Loss:2.4253, Validation Accuracy:0.1860\n",
    "Epoch #58: Loss:2.4018, Accuracy:0.1944, Validation Loss:2.4431, Validation Accuracy:0.1843\n",
    "Epoch #59: Loss:2.4107, Accuracy:0.1858, Validation Loss:2.4288, Validation Accuracy:0.1905\n",
    "Epoch #60: Loss:2.4001, Accuracy:0.1926, Validation Loss:2.4265, Validation Accuracy:0.1823\n",
    "Epoch #61: Loss:2.4000, Accuracy:0.1941, Validation Loss:2.4250, Validation Accuracy:0.1884\n",
    "Epoch #62: Loss:2.3998, Accuracy:0.1979, Validation Loss:2.4214, Validation Accuracy:0.1929\n",
    "Epoch #63: Loss:2.3946, Accuracy:0.2002, Validation Loss:2.4227, Validation Accuracy:0.1888\n",
    "Epoch #64: Loss:2.3948, Accuracy:0.1982, Validation Loss:2.4345, Validation Accuracy:0.1819\n",
    "Epoch #65: Loss:2.4014, Accuracy:0.1950, Validation Loss:2.4189, Validation Accuracy:0.1839\n",
    "Epoch #66: Loss:2.3873, Accuracy:0.1982, Validation Loss:2.4212, Validation Accuracy:0.1860\n",
    "Epoch #67: Loss:2.3903, Accuracy:0.1980, Validation Loss:2.4247, Validation Accuracy:0.1827\n",
    "Epoch #68: Loss:2.3992, Accuracy:0.1944, Validation Loss:2.4144, Validation Accuracy:0.1868\n",
    "Epoch #69: Loss:2.3855, Accuracy:0.2000, Validation Loss:2.4175, Validation Accuracy:0.1933\n",
    "Epoch #70: Loss:2.3841, Accuracy:0.2008, Validation Loss:2.4119, Validation Accuracy:0.1938\n",
    "Epoch #71: Loss:2.3802, Accuracy:0.2049, Validation Loss:2.4126, Validation Accuracy:0.1950\n",
    "Epoch #72: Loss:2.3798, Accuracy:0.2038, Validation Loss:2.4150, Validation Accuracy:0.1876\n",
    "Epoch #73: Loss:2.3772, Accuracy:0.2046, Validation Loss:2.4075, Validation Accuracy:0.1970\n",
    "Epoch #74: Loss:2.3769, Accuracy:0.2053, Validation Loss:2.4112, Validation Accuracy:0.1983\n",
    "Epoch #75: Loss:2.3766, Accuracy:0.2049, Validation Loss:2.4095, Validation Accuracy:0.1958\n",
    "Epoch #76: Loss:2.3761, Accuracy:0.2039, Validation Loss:2.4045, Validation Accuracy:0.1954\n",
    "Epoch #77: Loss:2.3748, Accuracy:0.2038, Validation Loss:2.4087, Validation Accuracy:0.1933\n",
    "Epoch #78: Loss:2.3750, Accuracy:0.2066, Validation Loss:2.4139, Validation Accuracy:0.1946\n",
    "Epoch #79: Loss:2.3815, Accuracy:0.2047, Validation Loss:2.4076, Validation Accuracy:0.2003\n",
    "Epoch #80: Loss:2.3689, Accuracy:0.2126, Validation Loss:2.4054, Validation Accuracy:0.2040\n",
    "Epoch #81: Loss:2.3674, Accuracy:0.2120, Validation Loss:2.4040, Validation Accuracy:0.2061\n",
    "Epoch #82: Loss:2.3668, Accuracy:0.2117, Validation Loss:2.4151, Validation Accuracy:0.2016\n",
    "Epoch #83: Loss:2.3736, Accuracy:0.2083, Validation Loss:2.4039, Validation Accuracy:0.2073\n",
    "Epoch #84: Loss:2.3619, Accuracy:0.2146, Validation Loss:2.4020, Validation Accuracy:0.1958\n",
    "Epoch #85: Loss:2.3627, Accuracy:0.2117, Validation Loss:2.4038, Validation Accuracy:0.1999\n",
    "Epoch #86: Loss:2.3631, Accuracy:0.2104, Validation Loss:2.4072, Validation Accuracy:0.2007\n",
    "Epoch #87: Loss:2.3605, Accuracy:0.2111, Validation Loss:2.4015, Validation Accuracy:0.2024\n",
    "Epoch #88: Loss:2.3583, Accuracy:0.2134, Validation Loss:2.4079, Validation Accuracy:0.1970\n",
    "Epoch #89: Loss:2.3602, Accuracy:0.2144, Validation Loss:2.4073, Validation Accuracy:0.1946\n",
    "Epoch #90: Loss:2.3590, Accuracy:0.2161, Validation Loss:2.4012, Validation Accuracy:0.2016\n",
    "Epoch #91: Loss:2.3549, Accuracy:0.2175, Validation Loss:2.4013, Validation Accuracy:0.2032\n",
    "Epoch #92: Loss:2.3603, Accuracy:0.2113, Validation Loss:2.4100, Validation Accuracy:0.1999\n",
    "Epoch #93: Loss:2.3547, Accuracy:0.2188, Validation Loss:2.4008, Validation Accuracy:0.2040\n",
    "Epoch #94: Loss:2.3495, Accuracy:0.2202, Validation Loss:2.3984, Validation Accuracy:0.2028\n",
    "Epoch #95: Loss:2.3498, Accuracy:0.2187, Validation Loss:2.4033, Validation Accuracy:0.2048\n",
    "Epoch #96: Loss:2.3496, Accuracy:0.2206, Validation Loss:2.3996, Validation Accuracy:0.2053\n",
    "Epoch #97: Loss:2.3521, Accuracy:0.2231, Validation Loss:2.3987, Validation Accuracy:0.2048\n",
    "Epoch #98: Loss:2.3447, Accuracy:0.2236, Validation Loss:2.3979, Validation Accuracy:0.2081\n",
    "Epoch #99: Loss:2.3467, Accuracy:0.2198, Validation Loss:2.4002, Validation Accuracy:0.2085\n",
    "Epoch #100: Loss:2.3433, Accuracy:0.2231, Validation Loss:2.3948, Validation Accuracy:0.2069\n",
    "Epoch #101: Loss:2.3471, Accuracy:0.2196, Validation Loss:2.4072, Validation Accuracy:0.2053\n",
    "Epoch #102: Loss:2.3486, Accuracy:0.2219, Validation Loss:2.4014, Validation Accuracy:0.2089\n",
    "Epoch #103: Loss:2.3461, Accuracy:0.2214, Validation Loss:2.4026, Validation Accuracy:0.2016\n",
    "Epoch #104: Loss:2.3535, Accuracy:0.2196, Validation Loss:2.3998, Validation Accuracy:0.2024\n",
    "Epoch #105: Loss:2.3411, Accuracy:0.2283, Validation Loss:2.4006, Validation Accuracy:0.2098\n",
    "Epoch #106: Loss:2.3433, Accuracy:0.2262, Validation Loss:2.4033, Validation Accuracy:0.2106\n",
    "Epoch #107: Loss:2.3387, Accuracy:0.2257, Validation Loss:2.3994, Validation Accuracy:0.2085\n",
    "Epoch #108: Loss:2.3349, Accuracy:0.2282, Validation Loss:2.3948, Validation Accuracy:0.2024\n",
    "Epoch #109: Loss:2.3317, Accuracy:0.2277, Validation Loss:2.3967, Validation Accuracy:0.2098\n",
    "Epoch #110: Loss:2.3327, Accuracy:0.2269, Validation Loss:2.3961, Validation Accuracy:0.2077\n",
    "Epoch #111: Loss:2.3333, Accuracy:0.2286, Validation Loss:2.4016, Validation Accuracy:0.1999\n",
    "Epoch #112: Loss:2.3298, Accuracy:0.2280, Validation Loss:2.3927, Validation Accuracy:0.2069\n",
    "Epoch #113: Loss:2.3291, Accuracy:0.2296, Validation Loss:2.3992, Validation Accuracy:0.2028\n",
    "Epoch #114: Loss:2.3333, Accuracy:0.2268, Validation Loss:2.3949, Validation Accuracy:0.2011\n",
    "Epoch #115: Loss:2.3359, Accuracy:0.2268, Validation Loss:2.3978, Validation Accuracy:0.2011\n",
    "Epoch #116: Loss:2.3250, Accuracy:0.2293, Validation Loss:2.3941, Validation Accuracy:0.2053\n",
    "Epoch #117: Loss:2.3222, Accuracy:0.2309, Validation Loss:2.3921, Validation Accuracy:0.2003\n",
    "Epoch #118: Loss:2.3212, Accuracy:0.2340, Validation Loss:2.3977, Validation Accuracy:0.2036\n",
    "Epoch #119: Loss:2.3219, Accuracy:0.2316, Validation Loss:2.3952, Validation Accuracy:0.2077\n",
    "Epoch #120: Loss:2.3186, Accuracy:0.2335, Validation Loss:2.3886, Validation Accuracy:0.2085\n",
    "Epoch #121: Loss:2.3192, Accuracy:0.2345, Validation Loss:2.4011, Validation Accuracy:0.2065\n",
    "Epoch #122: Loss:2.3224, Accuracy:0.2339, Validation Loss:2.3926, Validation Accuracy:0.2024\n",
    "Epoch #123: Loss:2.3178, Accuracy:0.2315, Validation Loss:2.4008, Validation Accuracy:0.2048\n",
    "Epoch #124: Loss:2.3197, Accuracy:0.2345, Validation Loss:2.3990, Validation Accuracy:0.2098\n",
    "Epoch #125: Loss:2.3186, Accuracy:0.2303, Validation Loss:2.3990, Validation Accuracy:0.2057\n",
    "Epoch #126: Loss:2.3171, Accuracy:0.2354, Validation Loss:2.3949, Validation Accuracy:0.2065\n",
    "Epoch #127: Loss:2.3195, Accuracy:0.2287, Validation Loss:2.3962, Validation Accuracy:0.2085\n",
    "Epoch #128: Loss:2.3132, Accuracy:0.2350, Validation Loss:2.3945, Validation Accuracy:0.2028\n",
    "Epoch #129: Loss:2.3153, Accuracy:0.2374, Validation Loss:2.3936, Validation Accuracy:0.2069\n",
    "Epoch #130: Loss:2.3260, Accuracy:0.2297, Validation Loss:2.3936, Validation Accuracy:0.2073\n",
    "Epoch #131: Loss:2.3100, Accuracy:0.2353, Validation Loss:2.3966, Validation Accuracy:0.2098\n",
    "Epoch #132: Loss:2.3090, Accuracy:0.2359, Validation Loss:2.3898, Validation Accuracy:0.2073\n",
    "Epoch #133: Loss:2.3056, Accuracy:0.2371, Validation Loss:2.3940, Validation Accuracy:0.2073\n",
    "Epoch #134: Loss:2.3046, Accuracy:0.2369, Validation Loss:2.3977, Validation Accuracy:0.2028\n",
    "Epoch #135: Loss:2.3119, Accuracy:0.2337, Validation Loss:2.3930, Validation Accuracy:0.2057\n",
    "Epoch #136: Loss:2.3049, Accuracy:0.2332, Validation Loss:2.3940, Validation Accuracy:0.1995\n",
    "Epoch #137: Loss:2.3058, Accuracy:0.2366, Validation Loss:2.3910, Validation Accuracy:0.2044\n",
    "Epoch #138: Loss:2.3032, Accuracy:0.2410, Validation Loss:2.3891, Validation Accuracy:0.2044\n",
    "Epoch #139: Loss:2.3075, Accuracy:0.2376, Validation Loss:2.3914, Validation Accuracy:0.2028\n",
    "Epoch #140: Loss:2.2958, Accuracy:0.2384, Validation Loss:2.3911, Validation Accuracy:0.2036\n",
    "Epoch #141: Loss:2.3077, Accuracy:0.2374, Validation Loss:2.3930, Validation Accuracy:0.2073\n",
    "Epoch #142: Loss:2.3051, Accuracy:0.2391, Validation Loss:2.3908, Validation Accuracy:0.2016\n",
    "Epoch #143: Loss:2.2986, Accuracy:0.2408, Validation Loss:2.3861, Validation Accuracy:0.2110\n",
    "Epoch #144: Loss:2.2916, Accuracy:0.2415, Validation Loss:2.3977, Validation Accuracy:0.2089\n",
    "Epoch #145: Loss:2.3127, Accuracy:0.2371, Validation Loss:2.3910, Validation Accuracy:0.2003\n",
    "Epoch #146: Loss:2.2945, Accuracy:0.2402, Validation Loss:2.3908, Validation Accuracy:0.2094\n",
    "Epoch #147: Loss:2.2885, Accuracy:0.2408, Validation Loss:2.3870, Validation Accuracy:0.2057\n",
    "Epoch #148: Loss:2.2884, Accuracy:0.2427, Validation Loss:2.3916, Validation Accuracy:0.2061\n",
    "Epoch #149: Loss:2.2842, Accuracy:0.2435, Validation Loss:2.3898, Validation Accuracy:0.2053\n",
    "Epoch #150: Loss:2.2856, Accuracy:0.2420, Validation Loss:2.3954, Validation Accuracy:0.2085\n",
    "Epoch #151: Loss:2.2876, Accuracy:0.2469, Validation Loss:2.3932, Validation Accuracy:0.2089\n",
    "Epoch #152: Loss:2.2850, Accuracy:0.2443, Validation Loss:2.3911, Validation Accuracy:0.2065\n",
    "Epoch #153: Loss:2.2838, Accuracy:0.2418, Validation Loss:2.4047, Validation Accuracy:0.1966\n",
    "Epoch #154: Loss:2.2895, Accuracy:0.2375, Validation Loss:2.3927, Validation Accuracy:0.2016\n",
    "Epoch #155: Loss:2.2975, Accuracy:0.2380, Validation Loss:2.3820, Validation Accuracy:0.2036\n",
    "Epoch #156: Loss:2.2800, Accuracy:0.2443, Validation Loss:2.3812, Validation Accuracy:0.2053\n",
    "Epoch #157: Loss:2.2781, Accuracy:0.2467, Validation Loss:2.3830, Validation Accuracy:0.2053\n",
    "Epoch #158: Loss:2.2764, Accuracy:0.2479, Validation Loss:2.3858, Validation Accuracy:0.2081\n",
    "Epoch #159: Loss:2.2745, Accuracy:0.2494, Validation Loss:2.3902, Validation Accuracy:0.1991\n",
    "Epoch #160: Loss:2.2793, Accuracy:0.2491, Validation Loss:2.3890, Validation Accuracy:0.2036\n",
    "Epoch #161: Loss:2.2717, Accuracy:0.2464, Validation Loss:2.3829, Validation Accuracy:0.2085\n",
    "Epoch #162: Loss:2.2802, Accuracy:0.2414, Validation Loss:2.3882, Validation Accuracy:0.1999\n",
    "Epoch #163: Loss:2.2724, Accuracy:0.2477, Validation Loss:2.3842, Validation Accuracy:0.2016\n",
    "Epoch #164: Loss:2.2694, Accuracy:0.2529, Validation Loss:2.3819, Validation Accuracy:0.2089\n",
    "Epoch #165: Loss:2.2742, Accuracy:0.2501, Validation Loss:2.3813, Validation Accuracy:0.2089\n",
    "Epoch #166: Loss:2.2680, Accuracy:0.2516, Validation Loss:2.3841, Validation Accuracy:0.2057\n",
    "Epoch #167: Loss:2.2658, Accuracy:0.2494, Validation Loss:2.3874, Validation Accuracy:0.2040\n",
    "Epoch #168: Loss:2.2669, Accuracy:0.2482, Validation Loss:2.3857, Validation Accuracy:0.2102\n",
    "Epoch #169: Loss:2.2645, Accuracy:0.2487, Validation Loss:2.3836, Validation Accuracy:0.2069\n",
    "Epoch #170: Loss:2.2709, Accuracy:0.2436, Validation Loss:2.3825, Validation Accuracy:0.2089\n",
    "Epoch #171: Loss:2.2621, Accuracy:0.2514, Validation Loss:2.3794, Validation Accuracy:0.2057\n",
    "Epoch #172: Loss:2.2529, Accuracy:0.2562, Validation Loss:2.3853, Validation Accuracy:0.2126\n",
    "Epoch #173: Loss:2.2562, Accuracy:0.2553, Validation Loss:2.3840, Validation Accuracy:0.2110\n",
    "Epoch #174: Loss:2.2578, Accuracy:0.2541, Validation Loss:2.3882, Validation Accuracy:0.1995\n",
    "Epoch #175: Loss:2.2552, Accuracy:0.2534, Validation Loss:2.3870, Validation Accuracy:0.2032\n",
    "Epoch #176: Loss:2.2694, Accuracy:0.2465, Validation Loss:2.3919, Validation Accuracy:0.2073\n",
    "Epoch #177: Loss:2.2505, Accuracy:0.2561, Validation Loss:2.3815, Validation Accuracy:0.2057\n",
    "Epoch #178: Loss:2.2516, Accuracy:0.2559, Validation Loss:2.3811, Validation Accuracy:0.2053\n",
    "Epoch #179: Loss:2.2516, Accuracy:0.2518, Validation Loss:2.3803, Validation Accuracy:0.2098\n",
    "Epoch #180: Loss:2.2477, Accuracy:0.2541, Validation Loss:2.3815, Validation Accuracy:0.2114\n",
    "Epoch #181: Loss:2.2439, Accuracy:0.2565, Validation Loss:2.3834, Validation Accuracy:0.2020\n",
    "Epoch #182: Loss:2.2411, Accuracy:0.2587, Validation Loss:2.4009, Validation Accuracy:0.2048\n",
    "Epoch #183: Loss:2.2548, Accuracy:0.2508, Validation Loss:2.3932, Validation Accuracy:0.2024\n",
    "Epoch #184: Loss:2.2428, Accuracy:0.2574, Validation Loss:2.3844, Validation Accuracy:0.2061\n",
    "Epoch #185: Loss:2.2440, Accuracy:0.2514, Validation Loss:2.3925, Validation Accuracy:0.2053\n",
    "Epoch #186: Loss:2.2486, Accuracy:0.2545, Validation Loss:2.3855, Validation Accuracy:0.2044\n",
    "Epoch #187: Loss:2.2442, Accuracy:0.2551, Validation Loss:2.3839, Validation Accuracy:0.2135\n",
    "Epoch #188: Loss:2.2420, Accuracy:0.2536, Validation Loss:2.3876, Validation Accuracy:0.2011\n",
    "Epoch #189: Loss:2.2367, Accuracy:0.2543, Validation Loss:2.3848, Validation Accuracy:0.2143\n",
    "Epoch #190: Loss:2.2439, Accuracy:0.2560, Validation Loss:2.3817, Validation Accuracy:0.2024\n",
    "Epoch #191: Loss:2.2316, Accuracy:0.2609, Validation Loss:2.3840, Validation Accuracy:0.2151\n",
    "Epoch #192: Loss:2.2303, Accuracy:0.2572, Validation Loss:2.3806, Validation Accuracy:0.2135\n",
    "Epoch #193: Loss:2.2260, Accuracy:0.2595, Validation Loss:2.3819, Validation Accuracy:0.2118\n",
    "Epoch #194: Loss:2.2311, Accuracy:0.2595, Validation Loss:2.3953, Validation Accuracy:0.2053\n",
    "Epoch #195: Loss:2.2345, Accuracy:0.2579, Validation Loss:2.3836, Validation Accuracy:0.2069\n",
    "Epoch #196: Loss:2.2288, Accuracy:0.2605, Validation Loss:2.3807, Validation Accuracy:0.2163\n",
    "Epoch #197: Loss:2.2333, Accuracy:0.2591, Validation Loss:2.3898, Validation Accuracy:0.2048\n",
    "Epoch #198: Loss:2.2251, Accuracy:0.2612, Validation Loss:2.3894, Validation Accuracy:0.2077\n",
    "Epoch #199: Loss:2.2261, Accuracy:0.2598, Validation Loss:2.3818, Validation Accuracy:0.2118\n",
    "Epoch #200: Loss:2.2152, Accuracy:0.2696, Validation Loss:2.3780, Validation Accuracy:0.2122\n",
    "Epoch #201: Loss:2.2193, Accuracy:0.2622, Validation Loss:2.3771, Validation Accuracy:0.2065\n",
    "Epoch #202: Loss:2.2120, Accuracy:0.2670, Validation Loss:2.3870, Validation Accuracy:0.2110\n",
    "Epoch #203: Loss:2.2230, Accuracy:0.2557, Validation Loss:2.3902, Validation Accuracy:0.2122\n",
    "Epoch #204: Loss:2.2160, Accuracy:0.2645, Validation Loss:2.3753, Validation Accuracy:0.2089\n",
    "Epoch #205: Loss:2.2091, Accuracy:0.2652, Validation Loss:2.3776, Validation Accuracy:0.2114\n",
    "Epoch #206: Loss:2.2097, Accuracy:0.2715, Validation Loss:2.3761, Validation Accuracy:0.2135\n",
    "Epoch #207: Loss:2.2077, Accuracy:0.2677, Validation Loss:2.3846, Validation Accuracy:0.2135\n",
    "Epoch #208: Loss:2.2096, Accuracy:0.2654, Validation Loss:2.3767, Validation Accuracy:0.2110\n",
    "Epoch #209: Loss:2.2037, Accuracy:0.2682, Validation Loss:2.3805, Validation Accuracy:0.2122\n",
    "Epoch #210: Loss:2.2079, Accuracy:0.2650, Validation Loss:2.3777, Validation Accuracy:0.2110\n",
    "Epoch #211: Loss:2.2053, Accuracy:0.2678, Validation Loss:2.3814, Validation Accuracy:0.2155\n",
    "Epoch #212: Loss:2.2016, Accuracy:0.2706, Validation Loss:2.3852, Validation Accuracy:0.2102\n",
    "Epoch #213: Loss:2.1938, Accuracy:0.2742, Validation Loss:2.3762, Validation Accuracy:0.2114\n",
    "Epoch #214: Loss:2.1971, Accuracy:0.2686, Validation Loss:2.3742, Validation Accuracy:0.2114\n",
    "Epoch #215: Loss:2.1942, Accuracy:0.2749, Validation Loss:2.3880, Validation Accuracy:0.2131\n",
    "Epoch #216: Loss:2.2148, Accuracy:0.2619, Validation Loss:2.3816, Validation Accuracy:0.2126\n",
    "Epoch #217: Loss:2.1973, Accuracy:0.2709, Validation Loss:2.3743, Validation Accuracy:0.2184\n",
    "Epoch #218: Loss:2.1883, Accuracy:0.2773, Validation Loss:2.3814, Validation Accuracy:0.2200\n",
    "Epoch #219: Loss:2.1882, Accuracy:0.2751, Validation Loss:2.3791, Validation Accuracy:0.2163\n",
    "Epoch #220: Loss:2.1901, Accuracy:0.2749, Validation Loss:2.4003, Validation Accuracy:0.2262\n",
    "Epoch #221: Loss:2.2106, Accuracy:0.2645, Validation Loss:2.3811, Validation Accuracy:0.2180\n",
    "Epoch #222: Loss:2.1987, Accuracy:0.2738, Validation Loss:2.3761, Validation Accuracy:0.2139\n",
    "Epoch #223: Loss:2.1883, Accuracy:0.2711, Validation Loss:2.3759, Validation Accuracy:0.2184\n",
    "Epoch #224: Loss:2.1977, Accuracy:0.2740, Validation Loss:2.3736, Validation Accuracy:0.2192\n",
    "Epoch #225: Loss:2.1896, Accuracy:0.2768, Validation Loss:2.3772, Validation Accuracy:0.2225\n",
    "Epoch #226: Loss:2.1872, Accuracy:0.2761, Validation Loss:2.3796, Validation Accuracy:0.2135\n",
    "Epoch #227: Loss:2.1898, Accuracy:0.2795, Validation Loss:2.3772, Validation Accuracy:0.2221\n",
    "Epoch #228: Loss:2.1851, Accuracy:0.2786, Validation Loss:2.3780, Validation Accuracy:0.2188\n",
    "Epoch #229: Loss:2.1797, Accuracy:0.2824, Validation Loss:2.3708, Validation Accuracy:0.2307\n",
    "Epoch #230: Loss:2.1756, Accuracy:0.2802, Validation Loss:2.3757, Validation Accuracy:0.2184\n",
    "Epoch #231: Loss:2.1810, Accuracy:0.2803, Validation Loss:2.3773, Validation Accuracy:0.2196\n",
    "Epoch #232: Loss:2.1990, Accuracy:0.2704, Validation Loss:2.3768, Validation Accuracy:0.2118\n",
    "Epoch #233: Loss:2.1875, Accuracy:0.2735, Validation Loss:2.3716, Validation Accuracy:0.2163\n",
    "Epoch #234: Loss:2.1714, Accuracy:0.2797, Validation Loss:2.3720, Validation Accuracy:0.2217\n",
    "Epoch #235: Loss:2.1689, Accuracy:0.2840, Validation Loss:2.3791, Validation Accuracy:0.2196\n",
    "Epoch #236: Loss:2.1718, Accuracy:0.2789, Validation Loss:2.3808, Validation Accuracy:0.2254\n",
    "Epoch #237: Loss:2.1689, Accuracy:0.2860, Validation Loss:2.3758, Validation Accuracy:0.2167\n",
    "Epoch #238: Loss:2.1725, Accuracy:0.2810, Validation Loss:2.3751, Validation Accuracy:0.2237\n",
    "Epoch #239: Loss:2.1740, Accuracy:0.2836, Validation Loss:2.3768, Validation Accuracy:0.2233\n",
    "Epoch #240: Loss:2.1635, Accuracy:0.2845, Validation Loss:2.3768, Validation Accuracy:0.2266\n",
    "Epoch #241: Loss:2.1573, Accuracy:0.2875, Validation Loss:2.3807, Validation Accuracy:0.2254\n",
    "Epoch #242: Loss:2.1620, Accuracy:0.2847, Validation Loss:2.3895, Validation Accuracy:0.2245\n",
    "Epoch #243: Loss:2.1662, Accuracy:0.2861, Validation Loss:2.3947, Validation Accuracy:0.2196\n",
    "Epoch #244: Loss:2.1819, Accuracy:0.2729, Validation Loss:2.3805, Validation Accuracy:0.2155\n",
    "Epoch #245: Loss:2.1672, Accuracy:0.2809, Validation Loss:2.3878, Validation Accuracy:0.2118\n",
    "Epoch #246: Loss:2.1638, Accuracy:0.2862, Validation Loss:2.3807, Validation Accuracy:0.2122\n",
    "Epoch #247: Loss:2.1541, Accuracy:0.2875, Validation Loss:2.3752, Validation Accuracy:0.2176\n",
    "Epoch #248: Loss:2.1553, Accuracy:0.2896, Validation Loss:2.3830, Validation Accuracy:0.2225\n",
    "Epoch #249: Loss:2.1542, Accuracy:0.2884, Validation Loss:2.3902, Validation Accuracy:0.2184\n",
    "Epoch #250: Loss:2.1631, Accuracy:0.2853, Validation Loss:2.3870, Validation Accuracy:0.2196\n",
    "Epoch #251: Loss:2.1623, Accuracy:0.2882, Validation Loss:2.3908, Validation Accuracy:0.2262\n",
    "Epoch #252: Loss:2.1718, Accuracy:0.2770, Validation Loss:2.3892, Validation Accuracy:0.2213\n",
    "Epoch #253: Loss:2.1535, Accuracy:0.2861, Validation Loss:2.4106, Validation Accuracy:0.2254\n",
    "Epoch #254: Loss:2.1729, Accuracy:0.2785, Validation Loss:2.3987, Validation Accuracy:0.2241\n",
    "Epoch #255: Loss:2.1600, Accuracy:0.2859, Validation Loss:2.3745, Validation Accuracy:0.2245\n",
    "Epoch #256: Loss:2.1750, Accuracy:0.2822, Validation Loss:2.3771, Validation Accuracy:0.2254\n",
    "Epoch #257: Loss:2.1461, Accuracy:0.2879, Validation Loss:2.3831, Validation Accuracy:0.2184\n",
    "Epoch #258: Loss:2.1441, Accuracy:0.2900, Validation Loss:2.3770, Validation Accuracy:0.2266\n",
    "Epoch #259: Loss:2.1425, Accuracy:0.2911, Validation Loss:2.3810, Validation Accuracy:0.2172\n",
    "Epoch #260: Loss:2.1363, Accuracy:0.2950, Validation Loss:2.3810, Validation Accuracy:0.2266\n",
    "Epoch #261: Loss:2.1340, Accuracy:0.2964, Validation Loss:2.3819, Validation Accuracy:0.2225\n",
    "Epoch #262: Loss:2.1344, Accuracy:0.2941, Validation Loss:2.3871, Validation Accuracy:0.2225\n",
    "Epoch #263: Loss:2.1505, Accuracy:0.2866, Validation Loss:2.3912, Validation Accuracy:0.2282\n",
    "Epoch #264: Loss:2.1521, Accuracy:0.2905, Validation Loss:2.3807, Validation Accuracy:0.2213\n",
    "Epoch #265: Loss:2.1380, Accuracy:0.2972, Validation Loss:2.3818, Validation Accuracy:0.2167\n",
    "Epoch #266: Loss:2.1372, Accuracy:0.2886, Validation Loss:2.3826, Validation Accuracy:0.2217\n",
    "Epoch #267: Loss:2.1306, Accuracy:0.2917, Validation Loss:2.3798, Validation Accuracy:0.2167\n",
    "Epoch #268: Loss:2.1362, Accuracy:0.2938, Validation Loss:2.3779, Validation Accuracy:0.2192\n",
    "Epoch #269: Loss:2.1265, Accuracy:0.2958, Validation Loss:2.3806, Validation Accuracy:0.2241\n",
    "Epoch #270: Loss:2.1263, Accuracy:0.3028, Validation Loss:2.3794, Validation Accuracy:0.2225\n",
    "Epoch #271: Loss:2.1226, Accuracy:0.2987, Validation Loss:2.3899, Validation Accuracy:0.2200\n",
    "Epoch #272: Loss:2.1373, Accuracy:0.2920, Validation Loss:2.3830, Validation Accuracy:0.2233\n",
    "Epoch #273: Loss:2.1230, Accuracy:0.2964, Validation Loss:2.3913, Validation Accuracy:0.2287\n",
    "Epoch #274: Loss:2.1313, Accuracy:0.2926, Validation Loss:2.3855, Validation Accuracy:0.2213\n",
    "Epoch #275: Loss:2.1279, Accuracy:0.2940, Validation Loss:2.3916, Validation Accuracy:0.2254\n",
    "Epoch #276: Loss:2.1230, Accuracy:0.2953, Validation Loss:2.3851, Validation Accuracy:0.2221\n",
    "Epoch #277: Loss:2.1203, Accuracy:0.2978, Validation Loss:2.3838, Validation Accuracy:0.2266\n",
    "Epoch #278: Loss:2.1235, Accuracy:0.2969, Validation Loss:2.3884, Validation Accuracy:0.2233\n",
    "Epoch #279: Loss:2.1148, Accuracy:0.2989, Validation Loss:2.3917, Validation Accuracy:0.2307\n",
    "Epoch #280: Loss:2.1085, Accuracy:0.3047, Validation Loss:2.3849, Validation Accuracy:0.2291\n",
    "Epoch #281: Loss:2.1110, Accuracy:0.3044, Validation Loss:2.3915, Validation Accuracy:0.2307\n",
    "Epoch #282: Loss:2.1095, Accuracy:0.3026, Validation Loss:2.3871, Validation Accuracy:0.2332\n",
    "Epoch #283: Loss:2.1187, Accuracy:0.2991, Validation Loss:2.3832, Validation Accuracy:0.2229\n",
    "Epoch #284: Loss:2.1152, Accuracy:0.2985, Validation Loss:2.3941, Validation Accuracy:0.2291\n",
    "Epoch #285: Loss:2.1125, Accuracy:0.3055, Validation Loss:2.3917, Validation Accuracy:0.2303\n",
    "Epoch #286: Loss:2.1110, Accuracy:0.3026, Validation Loss:2.3936, Validation Accuracy:0.2245\n",
    "Epoch #287: Loss:2.1140, Accuracy:0.2972, Validation Loss:2.3934, Validation Accuracy:0.2291\n",
    "Epoch #288: Loss:2.1076, Accuracy:0.3041, Validation Loss:2.3913, Validation Accuracy:0.2274\n",
    "Epoch #289: Loss:2.1050, Accuracy:0.3042, Validation Loss:2.3975, Validation Accuracy:0.2241\n",
    "Epoch #290: Loss:2.1277, Accuracy:0.2924, Validation Loss:2.4119, Validation Accuracy:0.2245\n",
    "Epoch #291: Loss:2.1378, Accuracy:0.2937, Validation Loss:2.4047, Validation Accuracy:0.2250\n",
    "Epoch #292: Loss:2.1102, Accuracy:0.2977, Validation Loss:2.3879, Validation Accuracy:0.2225\n",
    "Epoch #293: Loss:2.1428, Accuracy:0.2874, Validation Loss:2.3957, Validation Accuracy:0.2209\n",
    "Epoch #294: Loss:2.1188, Accuracy:0.2966, Validation Loss:2.3961, Validation Accuracy:0.2237\n",
    "Epoch #295: Loss:2.1014, Accuracy:0.3046, Validation Loss:2.3853, Validation Accuracy:0.2270\n",
    "Epoch #296: Loss:2.1090, Accuracy:0.3007, Validation Loss:2.3923, Validation Accuracy:0.2323\n",
    "Epoch #297: Loss:2.1025, Accuracy:0.3039, Validation Loss:2.4051, Validation Accuracy:0.2307\n",
    "Epoch #298: Loss:2.1068, Accuracy:0.3010, Validation Loss:2.3919, Validation Accuracy:0.2274\n",
    "Epoch #299: Loss:2.0949, Accuracy:0.3087, Validation Loss:2.3930, Validation Accuracy:0.2287\n",
    "Epoch #300: Loss:2.1050, Accuracy:0.3013, Validation Loss:2.3950, Validation Accuracy:0.2278\n",
    "\n",
    "Test:\n",
    "Test Loss:2.39496017, Accuracy:0.2278\n",
    "Labels: ['eo', 'by', 'ek', 'my', 'mb', 'aa', 'sg', 'sk', 'ce', 'yd', 'ib', 'ck', 'eg', 'ds', 'eb']\n",
    "Confusion Matrix:\n",
    "      eo  by  ek  my  mb  aa  sg  sk  ce   yd  ib  ck  eg  ds  eb\n",
    "t:eo  17   9   9   0  13   1  27   6   1   17  12   2   8   2  11\n",
    "t:by  12  21   8   2  21   6  24   2   0   18   9   1  17   5  16\n",
    "t:ek   9  14  22   0  19   6  12   8   4   35  10   3  24   6  19\n",
    "t:my   3   8   4   2   9   5   2   4   0   12  17   0   3  10   1\n",
    "t:mb   8  12   7   2  55   6  17  11   0   38  14   1  12   6  18\n",
    "t:aa   2   5   6   1  13   8   7   7   0    8   6   1  25  27  21\n",
    "t:sg   5   9   6   5  31   0  47   3   0   45  28   1   7   3  13\n",
    "t:sk   4   4  13   0  13   5   8  14   0   10   5   1  22  14  17\n",
    "t:ce  14   4   8   0  19   1   5   3   0   14   5   0  16  10  10\n",
    "t:yd   6   6   6   0  17   0  27   5   0  132  41   0   5   0   4\n",
    "t:ib   5   2   2   0  19   1  13   5   0   69  79   2  11   4   5\n",
    "t:ck   9  10   3   0   9   8   6   1   0    9   1   7  13   8   7\n",
    "t:eg   5   7   4   0  13  17   5  21   0    7   3   0  70  20  26\n",
    "t:ds   4   7   4   2  11  12   6   7   0    3   4   1  15  38  12\n",
    "t:eb  10   9  14   0  20   7  16  11   1   26  13   0  19  12  43\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eo       0.15      0.13      0.14       135\n",
    "          by       0.17      0.13      0.15       162\n",
    "          ek       0.19      0.12      0.14       191\n",
    "          my       0.14      0.03      0.04        80\n",
    "          mb       0.20      0.27      0.22       207\n",
    "          aa       0.10      0.06      0.07       137\n",
    "          sg       0.21      0.23      0.22       203\n",
    "          sk       0.13      0.11      0.12       130\n",
    "          ce       0.00      0.00      0.00       109\n",
    "          yd       0.30      0.53      0.38       249\n",
    "          ib       0.32      0.36      0.34       217\n",
    "          ck       0.35      0.08      0.13        91\n",
    "          eg       0.26      0.35      0.30       198\n",
    "          ds       0.23      0.30      0.26       126\n",
    "          eb       0.19      0.21      0.20       201\n",
    "\n",
    "    accuracy                           0.23      2436\n",
    "   macro avg       0.20      0.19      0.18      2436\n",
    "weighted avg       0.21      0.23      0.21      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 09:56:34 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 15 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.678871647281991, 2.667862983759988, 2.6529590480629053, 2.6231897527165406, 2.583917407958183, 2.541091514925651, 2.5132777001861673, 2.495276532932651, 2.4852803097961376, 2.4772172933337333, 2.469638223914286, 2.469664533345766, 2.4575688827213984, 2.457452746251925, 2.4525531360081265, 2.4517217077840923, 2.4540028940084926, 2.448412560868537, 2.4447862244591922, 2.446299423333655, 2.442684429228208, 2.445047354659032, 2.4420305739091144, 2.4407632288282923, 2.438583288678199, 2.442353528512914, 2.4381723591846787, 2.46045880364667, 2.4407976957769035, 2.4381630534217473, 2.4350031878560636, 2.448978702031529, 2.4394309575530304, 2.435806067314837, 2.4367385998931033, 2.437749041320851, 2.433134630200115, 2.4318771667668386, 2.442455383748648, 2.4355804841898148, 2.448206351112654, 2.437454881339238, 2.434313656269819, 2.4329473056229465, 2.4396535320626493, 2.4310056544681293, 2.4316659901529696, 2.4307153976609555, 2.429487363458267, 2.4279172138627527, 2.427477991248195, 2.4274071817131855, 2.427736341072421, 2.4284082261603848, 2.430043749034111, 2.4333547019018917, 2.4253328108826686, 2.4430831105055284, 2.4288208656906103, 2.4264785742329065, 2.4249755756804117, 2.4214409493851936, 2.422669017647679, 2.4345096061969627, 2.418854324688465, 2.4211996040125, 2.424677789700638, 2.4144493993279967, 2.417493891833451, 2.4118870717942813, 2.412581521693513, 2.4150063490436975, 2.407489732372741, 2.411236331967885, 2.4094645287994485, 2.4045382426877326, 2.4086891477331154, 2.4138771865168227, 2.4075609690254347, 2.4054403469480317, 2.403981942065635, 2.415095914565088, 2.40387781069588, 2.4020216617678187, 2.403834319075536, 2.4072432146088047, 2.401470169449479, 2.407932157782694, 2.4072956067979434, 2.401248867288599, 2.401261286195276, 2.410028928606381, 2.4008036211793646, 2.39837520658872, 2.403269789293286, 2.399555512837001, 2.3987107879814062, 2.397925801660823, 2.400212541589596, 2.394780411900362, 2.4071548724996634, 2.4014448696756596, 2.402624035899471, 2.3998462417834303, 2.400573174158732, 2.4032807087859105, 2.399374914482505, 2.394840276887264, 2.396675590223867, 2.3960580931508484, 2.4015790550971072, 2.39268487584219, 2.399186397420949, 2.3948881853194464, 2.3978003281090645, 2.3941343132106736, 2.3921230500946296, 2.3976541285835853, 2.395200117076755, 2.3885903922207836, 2.4010594995151013, 2.3926387905878777, 2.400755295808288, 2.3989544318031597, 2.3990144698294906, 2.39494012572691, 2.3961501372075826, 2.394489507957045, 2.3935540709002265, 2.393577312601024, 2.3966449622450203, 2.389765106398484, 2.393979150477693, 2.397739842215978, 2.3929678191888115, 2.394030408123248, 2.3909749628483565, 2.389080987579521, 2.3914136874851923, 2.3911188807589276, 2.392987423146691, 2.390797690217718, 2.3860895711995895, 2.3976967581387223, 2.390981297970601, 2.3908274545653896, 2.38700023895414, 2.391647907313455, 2.389827037679738, 2.3953702062221582, 2.3931565445240692, 2.3911223881350363, 2.404673653870381, 2.3926895909708707, 2.381968544426027, 2.381205544683146, 2.383022418750331, 2.3858382095061303, 2.3902458510375375, 2.3890448900670647, 2.3828744884390747, 2.388152037935304, 2.3842129914827144, 2.3818547659123865, 2.3813391295559887, 2.384107622607001, 2.38735313916637, 2.3856840951885103, 2.383640409886152, 2.3824730369649303, 2.3793888882854692, 2.385281625443883, 2.383997725344252, 2.3881887935456776, 2.386976480484009, 2.3919051519559913, 2.3815059896760387, 2.3810929171557498, 2.3803332983370877, 2.3814977624733458, 2.383419534842956, 2.4009251120838235, 2.3931844751235887, 2.384442980066309, 2.392475988281576, 2.385537516307361, 2.3838845481622, 2.3875640832340386, 2.3847871143829646, 2.381665328844819, 2.3839682608793913, 2.380595708324013, 2.381866220965957, 2.395267548819481, 2.383643933704921, 2.3807308223642933, 2.389754812509947, 2.389371189577826, 2.381840847200165, 2.378023240758085, 2.377055312220882, 2.3869508119248013, 2.3902330476857956, 2.3753447407376393, 2.377629976554457, 2.3761299956216795, 2.384585055615906, 2.3767044865243343, 2.3804631405471777, 2.377676586407942, 2.3813773920187615, 2.385154357293165, 2.376211485056259, 2.374222642682456, 2.387993619947011, 2.3815859158833823, 2.3743012362513047, 2.381375624432744, 2.379077823682763, 2.400287096136309, 2.3811455955254814, 2.376065727133665, 2.375909848361963, 2.3735955971215157, 2.3772092358819368, 2.3796244082583975, 2.377214939527715, 2.3780352110150216, 2.3708031110967127, 2.375698378912138, 2.3773040133352548, 2.3768382197726146, 2.3715656320449754, 2.372008201132463, 2.379119277587665, 2.3807944315799157, 2.3758452173524303, 2.3751229366841184, 2.376804880712224, 2.376824906698393, 2.380692476513742, 2.3894719153593718, 2.394690763382685, 2.380474680554495, 2.3878007541932105, 2.3806645592249476, 2.37516853648845, 2.3829703139162612, 2.39015095731112, 2.387003228777931, 2.390788621307398, 2.3891618251800537, 2.4105606267017685, 2.3986915113107714, 2.3744912918761054, 2.377109622720427, 2.383145637308631, 2.377037179098145, 2.38101584883942, 2.381009115961385, 2.3818537104501707, 2.3871348385740383, 2.391165393522416, 2.380740195463835, 2.381826946496572, 2.3826317078570036, 2.3797699919670867, 2.377898049080509, 2.380570569061881, 2.379433429104158, 2.3898810968414708, 2.382990295663843, 2.3913215972324116, 2.3854636852377156, 2.3916246761829396, 2.3850914731205783, 2.3838416978056207, 2.388423725889234, 2.391682187128928, 2.3849229342831766, 2.391515389647586, 2.387100261616198, 2.383236943794589, 2.3941372212126533, 2.3917451077102636, 2.393565877905033, 2.3933565640097183, 2.3913383198097615, 2.3974998510138352, 2.4118548946819085, 2.4046846234739707, 2.3879389293088114, 2.3956768242596405, 2.3960848218701742, 2.3852777723804093, 2.392290662270657, 2.405070221091335, 2.3919050936236954, 2.3930273882078223, 2.3949603936550847], 'val_acc': [0.12356321724079708, 0.10180623970057186, 0.06034482710907612, 0.10303776674523142, 0.11863710891534934, 0.15435139494772224, 0.1568144491593826, 0.17651888277926078, 0.17323481057860776, 0.1789819370153894, 0.17528735580800595, 0.16871921118648572, 0.17857142798419068, 0.1798029550533185, 0.1818555002337802, 0.18390804529190063, 0.18103448202457334, 0.18103448212244633, 0.18349753640751143, 0.18472906342770273, 0.18431855432309932, 0.17487684660552955, 0.18555008144116364, 0.18144499108024026, 0.18637109952802924, 0.18431855434756755, 0.18021346410898545, 0.16748768404395317, 0.17282430142506786, 0.18103448214691456, 0.18842364473295917, 0.17980295490650905, 0.18883415364181663, 0.1855500814901001, 0.1740558285186639, 0.1769293917859912, 0.1851395723854967, 0.18719211759042662, 0.17610837372359384, 0.17775040982392035, 0.17610837372359384, 0.17569786457005393, 0.18144499105577203, 0.1789819368685799, 0.17569786457005393, 0.18760262654822057, 0.18842364458614969, 0.18144499108024026, 0.1810344820001051, 0.17939244604658808, 0.1826765181004316, 0.1876026266216253, 0.17939244597318332, 0.1798029548820408, 0.1839080452674324, 0.18021346410898545, 0.18596059042342583, 0.18431855427416283, 0.19047618971767488, 0.18226600914263763, 0.18842364461061795, 0.19293924405167648, 0.18883415371522136, 0.18185549998909772, 0.1839080452184959, 0.18596059049683056, 0.1826765182227728, 0.18678160863263266, 0.1933497530094704, 0.19376026211407385, 0.19499178923213814, 0.18760262647481582, 0.19704433438813157, 0.19827586148172763, 0.19581280724559902, 0.19540229821440033, 0.19334975308287516, 0.19458128022540772, 0.20032840651537984, 0.20402298791850926, 0.2060755329766297, 0.20155993375578538, 0.20730706019256698, 0.19581280722113079, 0.19991789736183993, 0.20073891557104678, 0.20238095159796854, 0.19704433409451264, 0.19458128002966174, 0.2015599336089759, 0.20320196978270713, 0.1999178973863082, 0.20402298774723154, 0.2027914607270402, 0.20484400583409715, 0.2052545148408276, 0.20484400593197014, 0.2081280780592184, 0.20853858711488532, 0.20689655103902707, 0.2052545148408276, 0.20894909617055227, 0.2015599336089759, 0.20238095176924625, 0.20977011420848138, 0.21059113239321997, 0.20853858711488532, 0.2023809516224368, 0.20977011435529086, 0.20771756912589268, 0.19991789745971292, 0.20689655094115408, 0.20279146080044494, 0.2011494245777772, 0.20114942470011843, 0.2052545148897641, 0.2003284065887846, 0.20361247881390582, 0.20771756919929743, 0.20853858723722654, 0.2064860420812331, 0.20238095174477802, 0.20484400588303364, 0.20977011435529086, 0.20566502406777223, 0.2064860420812331, 0.20853858728616304, 0.2027914607759767, 0.20689655099009058, 0.20730706002128926, 0.20977011428188613, 0.20730706007022576, 0.20730706021703524, 0.20279146058023073, 0.20566502401883574, 0.1995073883551095, 0.20443349672949374, 0.20443349677843023, 0.2027914607270402, 0.20361247871603286, 0.20730706004575752, 0.20155993358450766, 0.21100164142441866, 0.20894909626842523, 0.20032840668665755, 0.2093596053485604, 0.20566502392096278, 0.2060755329276932, 0.20525451498763705, 0.20853858718829008, 0.20894909631736172, 0.2064860421301696, 0.19663382535693288, 0.20155993363344415, 0.20361247883837408, 0.20525451498763705, 0.20525451493870056, 0.20812807820602786, 0.19909687947072027, 0.20361247878943758, 0.2085385872127583, 0.19991789760652237, 0.20155993363344415, 0.20894909639076648, 0.20894909639076648, 0.20566502396989925, 0.2040229879429775, 0.21018062343542604, 0.2068965511124318, 0.208949096243957, 0.20566502406777223, 0.2126436775981499, 0.21100164147335515, 0.19950738869766493, 0.2032019699295166, 0.2073070603638447, 0.20566502411670873, 0.20525451503657355, 0.20977011452656857, 0.21141215072476804, 0.20197044271357933, 0.20484400602984312, 0.20238095174477802, 0.20607553312343918, 0.20525451518338303, 0.2044334969986444, 0.21346469575842025, 0.20114942470011843, 0.21428571394315885, 0.20238095179371451, 0.21510673200555622, 0.21346469583182498, 0.2118226596336255, 0.20525451503657355, 0.20689655113690006, 0.2163382590502158, 0.20484400602984312, 0.20771756937057514, 0.21182265958468902, 0.2122331686648242, 0.2064860422280426, 0.21100164166910113, 0.21223316878716542, 0.20894909643970294, 0.21141215060242682, 0.21346469575842025, 0.21346469590522973, 0.2110016415956964, 0.21223316878716542, 0.2110016415956964, 0.21551724093888194, 0.210180623533299, 0.21141215055349033, 0.21141215050455384, 0.21305418670275333, 0.21264367779389587, 0.2183908043040822, 0.22003284030653572, 0.21633825909915227, 0.22619047577451604, 0.21798029517501055, 0.21387520493642842, 0.21839080423067747, 0.21921182229307484, 0.22249589444479137, 0.21346469583182498, 0.22208538556040214, 0.21880131333528088, 0.2307060752889793, 0.21839080423067747, 0.2196223312998053, 0.21182265958468902, 0.2163382590502158, 0.22167487645579873, 0.2196223312998053, 0.22536945785892812, 0.21674876815481922, 0.22372742178306987, 0.2233169127029347, 0.22660098480571472, 0.2253694577855234, 0.2245484396007848, 0.219622331568956, 0.21551724088994545, 0.2118226597314985, 0.2122331687137607, 0.21756978619274836, 0.2224958945181961, 0.21839080442642345, 0.2196223313487418, 0.2261904758234525, 0.22126436752247303, 0.2253694577365869, 0.2241379308387368, 0.22454843979653075, 0.22536945776105513, 0.21839080440195519, 0.22660098483018296, 0.21715927694133547, 0.22660098480571472, 0.2224958945671326, 0.22249589459160082, 0.22824302102838243, 0.22126436742460004, 0.21674876820375572, 0.22167487660260818, 0.21674876815481922, 0.2192118223175431, 0.22413793064299084, 0.22249589464053732, 0.22003284052674993, 0.22331691267846646, 0.22865353003511288, 0.22126436752247303, 0.22536945783445989, 0.22208538568274336, 0.22660098497699244, 0.22331691272740295, 0.2307060752889793, 0.22906403904184333, 0.23070607516663807, 0.2331691293048937, 0.22290640359833128, 0.22906403904184333, 0.23029556611097113, 0.2245484396497213, 0.22906403911524806, 0.22742200294151682, 0.22413793078980032, 0.22454843979653075, 0.2249589488032612, 0.22249589464053732, 0.22085385866255206, 0.2237274218075381, 0.22701149393478637, 0.2323481111690916, 0.23070607526451103, 0.22742200274577087, 0.2286535302063906, 0.227832512021652], 'loss': [2.690208822011458, 2.674708227651075, 2.66154657203314, 2.640949081738137, 2.6034035055054776, 2.5590618083609202, 2.523721024436872, 2.5049656456745626, 2.488752618805339, 2.4813919859500393, 2.470491115855975, 2.4648343221362854, 2.45786050575225, 2.458874377773528, 2.4520988449668493, 2.448745091005517, 2.4540352770435248, 2.4456726645052553, 2.4435248679448938, 2.441268400000351, 2.440025853621152, 2.4363274551759755, 2.4371621650836794, 2.433934842977191, 2.4346974062479005, 2.431827212554963, 2.435363865290334, 2.430717595940498, 2.4337619636582644, 2.4267876651497593, 2.425767905805145, 2.42587291239713, 2.4385804087229577, 2.427127585812516, 2.427036466539763, 2.4246967696556077, 2.4225354146663656, 2.4192161807778922, 2.4208286647679134, 2.4230490807879876, 2.4210265458242115, 2.422144191221045, 2.4152492915825188, 2.416542374084128, 2.415903086583962, 2.414422494919638, 2.4134990908526786, 2.411043271293875, 2.4102051273753267, 2.4070975890149815, 2.4062264276970584, 2.405904785661482, 2.4074073015052435, 2.403235463684834, 2.4069912167300433, 2.4025166293189266, 2.4016319839861358, 2.4017522697331235, 2.4106857168601032, 2.400055945237804, 2.4000466116644765, 2.399775798413788, 2.3945945055333007, 2.394813293695939, 2.401385798150754, 2.387294943964212, 2.3902573975204686, 2.39924812855417, 2.3855485725207006, 2.384122286195383, 2.3802281346409226, 2.379804291911194, 2.377183523413092, 2.3769240714196553, 2.3766188653832345, 2.3761354986891856, 2.374847935553204, 2.3750064826354356, 2.381470995419324, 2.3689052855943995, 2.367421385982443, 2.3668162875596503, 2.3735725286315352, 2.3619046502044805, 2.3627162470220298, 2.3631440677681987, 2.360452753121848, 2.3582700891905986, 2.3601832457391634, 2.3590411048160687, 2.3548696161540383, 2.3603424641141166, 2.354650468552137, 2.349460673381171, 2.349807397638748, 2.349625661779478, 2.352074278943103, 2.3446846146358356, 2.346664440656345, 2.3432852084142226, 2.3470570926548766, 2.3485506371061415, 2.3460872933115557, 2.3535159378815482, 2.3410631027065016, 2.3432728784040258, 2.3386685017932365, 2.334945523714383, 2.331687203520867, 2.332656292748892, 2.333293600199893, 2.3298258226265407, 2.3290863394492463, 2.333312080332386, 2.335919464687058, 2.3249918770251576, 2.3221669055108416, 2.3211754211410116, 2.3218705148912306, 2.3186350374985523, 2.3191648038750556, 2.3224360593290543, 2.3177671185264352, 2.319676119884671, 2.318581456080599, 2.3170652131035587, 2.3195000551564493, 2.3132472405443445, 2.3152797271094037, 2.325967466806729, 2.3099958431549386, 2.3089999750164747, 2.305594565393499, 2.3046280236214827, 2.3118619931551954, 2.3048531759446163, 2.3058463271883234, 2.303201453837526, 2.307477528554458, 2.295825777014668, 2.3077296965910423, 2.305054618788451, 2.298630340094439, 2.291607065856824, 2.3126675540172097, 2.294486205337963, 2.2885159362268155, 2.2884499335435873, 2.284188470409636, 2.2855572288286026, 2.2875576496124266, 2.284980632539158, 2.283753597662924, 2.289479330579848, 2.2974747171147403, 2.2799995468382472, 2.2780592423934465, 2.2764008775628812, 2.274543148193516, 2.2793235273576613, 2.271720691285339, 2.2802137128136732, 2.2724357604001337, 2.2693910746603776, 2.274176375626049, 2.2679766459141915, 2.265819404404266, 2.2668914032178247, 2.2645326231539373, 2.2709486085041837, 2.2620834659012434, 2.2528924948870523, 2.2561926246913306, 2.2578299665353136, 2.255248272541367, 2.2694406856991183, 2.250508505509864, 2.251563398940852, 2.2516300857434284, 2.2477466247409765, 2.2438944809735433, 2.2410954386301842, 2.254784861629259, 2.242793823022862, 2.243987260364164, 2.24862640036205, 2.2441528632655525, 2.242043621290391, 2.236711382621123, 2.243896832554247, 2.231577094820246, 2.2303187191119185, 2.2260409266062586, 2.231118850688425, 2.234468834796725, 2.228762018900877, 2.2332628444969287, 2.2251097380503, 2.226071330996754, 2.2152316956549454, 2.21928545258618, 2.2119731181945643, 2.2230042953021227, 2.2160250662778194, 2.2091367814575134, 2.2097450530504545, 2.2077381401825735, 2.209599905973587, 2.2036511753129275, 2.207870301818456, 2.2053281826649846, 2.201556195860281, 2.193811960925312, 2.1971413618240514, 2.194186862831977, 2.21480690742665, 2.197275943530903, 2.188330066375419, 2.188237221236102, 2.190057938593369, 2.2106048218768235, 2.198708838212172, 2.1882615556462346, 2.1976700002407883, 2.1895631634479185, 2.187157283916121, 2.1897537068909445, 2.185071350759549, 2.1797048529560317, 2.1756125083939004, 2.180979212402563, 2.1990388353257697, 2.187511985992259, 2.1714055422639946, 2.1689311811811383, 2.1717835818962397, 2.1688899397605255, 2.1724792746792585, 2.1740332326360305, 2.163541787067233, 2.1573122029431793, 2.1620244921110494, 2.166192027479716, 2.181933543520542, 2.1672202054969585, 2.16380874009103, 2.154089284579612, 2.155261515494, 2.154168349859406, 2.163072959010851, 2.16228016485179, 2.17181573653368, 2.153467186373607, 2.172870062508867, 2.1599760867242206, 2.174983825037367, 2.14606044649833, 2.14407800291598, 2.1424623024292306, 2.1362826425681614, 2.133974293957501, 2.1344124251567362, 2.150526624440657, 2.152084870211153, 2.1380198449814345, 2.137171511483633, 2.1306377030985555, 2.136152103451488, 2.1264985873713873, 2.1263353411421884, 2.122615679333587, 2.1373190634549277, 2.1229990544994752, 2.1312750306217576, 2.1278821082575363, 2.123035837737442, 2.1203291378961207, 2.1234914066855177, 2.114800059917771, 2.1085103884859495, 2.110969049289241, 2.109501754478752, 2.1187146647999664, 2.1152242837745305, 2.1124590548646522, 2.1109503560976814, 2.114023106788463, 2.107618343267108, 2.1049997516726076, 2.12767863655482, 2.137809567089198, 2.110230078099934, 2.1428211454003745, 2.118786121834475, 2.101391046198976, 2.1089904153616277, 2.1025474074440083, 2.1067871959792024, 2.094861066561705, 2.1049745532276694], 'acc': [0.08151950718685831, 0.11375770021757796, 0.07422997946917888, 0.0988706365533678, 0.10903490760059573, 0.13665297740049187, 0.15985626283673535, 0.16786447638909674, 0.17515400411901533, 0.17268993840141708, 0.17546201232032854, 0.17679671458211524, 0.17741273100921995, 0.17433264887369634, 0.18008213552973354, 0.17700205338809036, 0.17638603696404542, 0.17967145791166372, 0.17741273101227975, 0.18069815196295783, 0.17905544148455899, 0.1839835728952772, 0.1792607802935939, 0.18254620123815243, 0.18151950719297788, 0.1811088295718483, 0.18028747433264888, 0.17802874743632469, 0.1781314168439019, 0.18295687885622225, 0.18193018480492812, 0.1836755646878444, 0.17977412731618117, 0.1799794661221563, 0.18018480493425099, 0.1813141683778234, 0.1844969199178645, 0.18521560574948664, 0.18398357290751635, 0.18028747433876843, 0.18254620123203286, 0.18613963039014375, 0.18624229980078078, 0.18367556468172486, 0.18603696098868608, 0.18521560574948664, 0.18901437371663243, 0.18737166325047275, 0.18850102669404517, 0.18603696099174585, 0.1879876796714579, 0.1883983572925875, 0.1893223819332446, 0.1933264887063655, 0.18983572895889164, 0.1933264887094253, 0.19271047228538035, 0.19435318275154004, 0.18583162217659138, 0.1926078028808629, 0.19414784394556492, 0.1979466119157705, 0.2002053388090349, 0.1981519507248054, 0.19496919918170455, 0.19815195071868583, 0.19804928131416838, 0.19435318273930088, 0.2, 0.20082135523613964, 0.20492813141989757, 0.2037987679671458, 0.204620123209405, 0.20533880904102716, 0.20492813142295735, 0.20390143737166325, 0.20379876797938493, 0.20657084189217684, 0.20472279259556372, 0.21262833675564682, 0.2120123203285421, 0.21170431211804952, 0.20831622177203332, 0.21457905544453823, 0.2117043121211093, 0.21036960985626282, 0.21108829569400459, 0.21344969199178646, 0.2143737166446827, 0.21611909651535982, 0.21745379877102694, 0.21129363450915906, 0.2187885010389332, 0.2202258726899384, 0.21868583162829616, 0.22063655032024737, 0.22310061601948689, 0.22361396304513395, 0.21981519507492836, 0.2231006160225467, 0.21960985626589347, 0.22186858316833724, 0.22135523615186953, 0.21960985626283366, 0.22833675563457811, 0.22618069815195072, 0.22566735113242323, 0.2282340862422998, 0.22772073923195169, 0.22689938398969248, 0.2286447638664892, 0.22802874743938445, 0.2295687885040865, 0.226796714585175, 0.22679671459129458, 0.22926078029971347, 0.23090349076587316, 0.23398357290751634, 0.23162217659137577, 0.23347022588492908, 0.23449691991786448, 0.23388090349075977, 0.23151950717461917, 0.23449691993010363, 0.23028747433876845, 0.2354209445707607, 0.22874743325264793, 0.23501026694657132, 0.23737166325659234, 0.22967145788106585, 0.23531827515706388, 0.23593429159334797, 0.23706365504303997, 0.23685831623400505, 0.23367556468172485, 0.23316221766525716, 0.2365503079959744, 0.24096509240246405, 0.23757700204114895, 0.2383983572956473, 0.23737166325659234, 0.2391170431211499, 0.24075975359954874, 0.24147843942505134, 0.23706365504303997, 0.24024640657696147, 0.24075975359648893, 0.24271047228232057, 0.2435318275184602, 0.2419917864476386, 0.24691991786753617, 0.24425051334702258, 0.2417864476416635, 0.2374743326549902, 0.2379876796745177, 0.24425051335008238, 0.24671457905850128, 0.24794661192189008, 0.24938398354841698, 0.24907597535934292, 0.24640657085412826, 0.24137577002665345, 0.24774127310061603, 0.2528747433264887, 0.2501026694045175, 0.2516427104845184, 0.2493839835728953, 0.24815195071868584, 0.24866529775351226, 0.24363449692603745, 0.2514373716510052, 0.2561601642771668, 0.2553388090410272, 0.2541067761929373, 0.253388090349076, 0.24650924024640658, 0.25605749487264934, 0.25585215605749484, 0.25184804928437393, 0.254106776168459, 0.2564681724907192, 0.25872689939622273, 0.25082135523613963, 0.25739219712831646, 0.25143737166324437, 0.25451745378652885, 0.2551334702319923, 0.25359342916423044, 0.25431211498973305, 0.2559548254620123, 0.26088295686661095, 0.2571868583284609, 0.2595482546201232, 0.25954825463236236, 0.25790554414784395, 0.2604722792638401, 0.2591375770142925, 0.2611909650801633, 0.25975359344139726, 0.26960985627507283, 0.26221765914981615, 0.26704312113765816, 0.25574948665909697, 0.26447638604002077, 0.26519507187470276, 0.27145790555026744, 0.26765913760148036, 0.2654004106806779, 0.2681724846057089, 0.2649897330717874, 0.2677618069570412, 0.270636550311068, 0.27422997947835825, 0.26858316221765915, 0.27494866529774126, 0.2619096509270844, 0.2709445585093214, 0.2773100616077623, 0.27505133470225873, 0.2749486653099804, 0.26447638604920015, 0.27381930184804926, 0.2711498973428346, 0.274024640660144, 0.2767967145790554, 0.2760780287596724, 0.27946611909650926, 0.2786447638726088, 0.2824435318275154, 0.2801848049281314, 0.28028747433570866, 0.27043121151121247, 0.27351129361001864, 0.27967145790554415, 0.2839835728952772, 0.2788501026694045, 0.28603696099174586, 0.2810061601765102, 0.28357289528332696, 0.2844969199301036, 0.28747433265499023, 0.28470225872689936, 0.2861396303779046, 0.27289527719515305, 0.2809034907719927, 0.286242299797721, 0.28747433265499023, 0.28963039014373715, 0.28839835728952773, 0.28531827514176494, 0.2881930184835526, 0.2770020534003295, 0.2861396303779046, 0.2785420944680913, 0.2859342915841686, 0.2822381930307196, 0.2878850102791796, 0.29004106777404615, 0.2910677618100413, 0.2949691991664056, 0.2964065708418891, 0.29414784394250515, 0.2865503080204527, 0.2904517453859964, 0.2972279260902679, 0.2886036961016224, 0.29168377823714603, 0.2938398357167136, 0.2957905544147844, 0.3027720739342104, 0.29866529772903394, 0.2919917864231603, 0.2964065708480087, 0.2926078028747433, 0.2940451745441073, 0.295277207379958, 0.2978439425173726, 0.2969199178644764, 0.29887063656254714, 0.30472279262004204, 0.30441478438201136, 0.3025667351190559, 0.2990759753654625, 0.29845995894447735, 0.30554414785006206, 0.30256673511599613, 0.2972279260780287, 0.3041067761929373, 0.30420944559133517, 0.2924024640657084, 0.2937371663305549, 0.29774127311285514, 0.2873716632443532, 0.29661190965092404, 0.304620123209405, 0.30071868583774175, 0.30390143737166325, 0.30102669405741367, 0.3087268993717444, 0.3013347022587269]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
