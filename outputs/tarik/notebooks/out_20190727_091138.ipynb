{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf35.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 09:11:38 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'Front', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '04', '03', '01', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000029A290CBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000029A24896EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6069, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6008, Accuracy:0.2329, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #18: Loss:1.5981, Accuracy:0.2456, Validation Loss:1.5986, Validation Accuracy:0.2430\n",
    "Epoch #19: Loss:1.5948, Accuracy:0.2517, Validation Loss:1.5944, Validation Accuracy:0.2397\n",
    "Epoch #20: Loss:1.5879, Accuracy:0.2657, Validation Loss:1.5888, Validation Accuracy:0.2512\n",
    "Epoch #21: Loss:1.5787, Accuracy:0.2698, Validation Loss:1.5816, Validation Accuracy:0.2562\n",
    "Epoch #22: Loss:1.5707, Accuracy:0.2739, Validation Loss:1.5744, Validation Accuracy:0.2611\n",
    "Epoch #23: Loss:1.5634, Accuracy:0.2772, Validation Loss:1.5696, Validation Accuracy:0.2677\n",
    "Epoch #24: Loss:1.5561, Accuracy:0.2789, Validation Loss:1.5674, Validation Accuracy:0.2742\n",
    "Epoch #25: Loss:1.5530, Accuracy:0.2780, Validation Loss:1.5636, Validation Accuracy:0.2726\n",
    "Epoch #26: Loss:1.5485, Accuracy:0.2830, Validation Loss:1.5606, Validation Accuracy:0.2759\n",
    "Epoch #27: Loss:1.5486, Accuracy:0.2825, Validation Loss:1.5572, Validation Accuracy:0.2677\n",
    "Epoch #28: Loss:1.5456, Accuracy:0.2817, Validation Loss:1.5561, Validation Accuracy:0.2742\n",
    "Epoch #29: Loss:1.5439, Accuracy:0.2817, Validation Loss:1.5544, Validation Accuracy:0.2693\n",
    "Epoch #30: Loss:1.5431, Accuracy:0.2916, Validation Loss:1.5527, Validation Accuracy:0.2775\n",
    "Epoch #31: Loss:1.5410, Accuracy:0.2871, Validation Loss:1.5518, Validation Accuracy:0.2742\n",
    "Epoch #32: Loss:1.5398, Accuracy:0.2924, Validation Loss:1.5509, Validation Accuracy:0.2660\n",
    "Epoch #33: Loss:1.5381, Accuracy:0.2924, Validation Loss:1.5504, Validation Accuracy:0.2693\n",
    "Epoch #34: Loss:1.5369, Accuracy:0.2932, Validation Loss:1.5497, Validation Accuracy:0.2578\n",
    "Epoch #35: Loss:1.5355, Accuracy:0.2903, Validation Loss:1.5500, Validation Accuracy:0.2726\n",
    "Epoch #36: Loss:1.5346, Accuracy:0.2957, Validation Loss:1.5496, Validation Accuracy:0.2627\n",
    "Epoch #37: Loss:1.5342, Accuracy:0.2920, Validation Loss:1.5494, Validation Accuracy:0.2726\n",
    "Epoch #38: Loss:1.5342, Accuracy:0.2936, Validation Loss:1.5514, Validation Accuracy:0.2709\n",
    "Epoch #39: Loss:1.5322, Accuracy:0.2875, Validation Loss:1.5506, Validation Accuracy:0.2824\n",
    "Epoch #40: Loss:1.5354, Accuracy:0.2965, Validation Loss:1.5509, Validation Accuracy:0.2677\n",
    "Epoch #41: Loss:1.5329, Accuracy:0.2986, Validation Loss:1.5470, Validation Accuracy:0.2693\n",
    "Epoch #42: Loss:1.5321, Accuracy:0.2994, Validation Loss:1.5470, Validation Accuracy:0.2512\n",
    "Epoch #43: Loss:1.5306, Accuracy:0.2994, Validation Loss:1.5457, Validation Accuracy:0.2644\n",
    "Epoch #44: Loss:1.5302, Accuracy:0.2945, Validation Loss:1.5452, Validation Accuracy:0.2611\n",
    "Epoch #45: Loss:1.5298, Accuracy:0.2994, Validation Loss:1.5454, Validation Accuracy:0.2578\n",
    "Epoch #46: Loss:1.5301, Accuracy:0.2949, Validation Loss:1.5454, Validation Accuracy:0.2693\n",
    "Epoch #47: Loss:1.5269, Accuracy:0.2961, Validation Loss:1.5450, Validation Accuracy:0.2677\n",
    "Epoch #48: Loss:1.5259, Accuracy:0.2912, Validation Loss:1.5462, Validation Accuracy:0.2594\n",
    "Epoch #49: Loss:1.5299, Accuracy:0.2949, Validation Loss:1.5456, Validation Accuracy:0.2775\n",
    "Epoch #50: Loss:1.5243, Accuracy:0.3031, Validation Loss:1.5431, Validation Accuracy:0.2627\n",
    "Epoch #51: Loss:1.5249, Accuracy:0.2932, Validation Loss:1.5420, Validation Accuracy:0.2693\n",
    "Epoch #52: Loss:1.5238, Accuracy:0.3035, Validation Loss:1.5402, Validation Accuracy:0.2824\n",
    "Epoch #53: Loss:1.5218, Accuracy:0.2945, Validation Loss:1.5393, Validation Accuracy:0.2841\n",
    "Epoch #54: Loss:1.5215, Accuracy:0.3043, Validation Loss:1.5411, Validation Accuracy:0.2956\n",
    "Epoch #55: Loss:1.5221, Accuracy:0.2953, Validation Loss:1.5405, Validation Accuracy:0.2726\n",
    "Epoch #56: Loss:1.5217, Accuracy:0.2936, Validation Loss:1.5381, Validation Accuracy:0.2890\n",
    "Epoch #57: Loss:1.5201, Accuracy:0.3014, Validation Loss:1.5369, Validation Accuracy:0.2956\n",
    "Epoch #58: Loss:1.5197, Accuracy:0.3006, Validation Loss:1.5364, Validation Accuracy:0.2939\n",
    "Epoch #59: Loss:1.5162, Accuracy:0.3055, Validation Loss:1.5413, Validation Accuracy:0.2775\n",
    "Epoch #60: Loss:1.5196, Accuracy:0.3072, Validation Loss:1.5377, Validation Accuracy:0.2742\n",
    "Epoch #61: Loss:1.5176, Accuracy:0.3014, Validation Loss:1.5353, Validation Accuracy:0.2923\n",
    "Epoch #62: Loss:1.5134, Accuracy:0.3097, Validation Loss:1.5317, Validation Accuracy:0.3038\n",
    "Epoch #63: Loss:1.5106, Accuracy:0.3072, Validation Loss:1.5315, Validation Accuracy:0.3005\n",
    "Epoch #64: Loss:1.5094, Accuracy:0.3055, Validation Loss:1.5318, Validation Accuracy:0.2841\n",
    "Epoch #65: Loss:1.5096, Accuracy:0.3175, Validation Loss:1.5461, Validation Accuracy:0.2841\n",
    "Epoch #66: Loss:1.5141, Accuracy:0.2994, Validation Loss:1.5307, Validation Accuracy:0.2874\n",
    "Epoch #67: Loss:1.5095, Accuracy:0.3109, Validation Loss:1.5286, Validation Accuracy:0.3005\n",
    "Epoch #68: Loss:1.5055, Accuracy:0.3080, Validation Loss:1.5262, Validation Accuracy:0.2972\n",
    "Epoch #69: Loss:1.5036, Accuracy:0.3080, Validation Loss:1.5242, Validation Accuracy:0.3153\n",
    "Epoch #70: Loss:1.5028, Accuracy:0.3076, Validation Loss:1.5248, Validation Accuracy:0.3038\n",
    "Epoch #71: Loss:1.5000, Accuracy:0.3125, Validation Loss:1.5203, Validation Accuracy:0.3038\n",
    "Epoch #72: Loss:1.5001, Accuracy:0.3154, Validation Loss:1.5189, Validation Accuracy:0.3103\n",
    "Epoch #73: Loss:1.4968, Accuracy:0.3166, Validation Loss:1.5173, Validation Accuracy:0.3169\n",
    "Epoch #74: Loss:1.4949, Accuracy:0.3150, Validation Loss:1.5149, Validation Accuracy:0.3186\n",
    "Epoch #75: Loss:1.4940, Accuracy:0.3179, Validation Loss:1.5123, Validation Accuracy:0.3218\n",
    "Epoch #76: Loss:1.4932, Accuracy:0.3257, Validation Loss:1.5193, Validation Accuracy:0.3153\n",
    "Epoch #77: Loss:1.4913, Accuracy:0.3220, Validation Loss:1.5071, Validation Accuracy:0.3153\n",
    "Epoch #78: Loss:1.4908, Accuracy:0.3273, Validation Loss:1.5050, Validation Accuracy:0.3415\n",
    "Epoch #79: Loss:1.4876, Accuracy:0.3285, Validation Loss:1.5036, Validation Accuracy:0.3284\n",
    "Epoch #80: Loss:1.4806, Accuracy:0.3343, Validation Loss:1.5122, Validation Accuracy:0.3071\n",
    "Epoch #81: Loss:1.4874, Accuracy:0.3269, Validation Loss:1.4997, Validation Accuracy:0.3284\n",
    "Epoch #82: Loss:1.4769, Accuracy:0.3298, Validation Loss:1.5009, Validation Accuracy:0.3284\n",
    "Epoch #83: Loss:1.4780, Accuracy:0.3363, Validation Loss:1.5034, Validation Accuracy:0.3251\n",
    "Epoch #84: Loss:1.4931, Accuracy:0.3359, Validation Loss:1.4956, Validation Accuracy:0.3514\n",
    "Epoch #85: Loss:1.4754, Accuracy:0.3462, Validation Loss:1.4912, Validation Accuracy:0.3498\n",
    "Epoch #86: Loss:1.4681, Accuracy:0.3450, Validation Loss:1.4877, Validation Accuracy:0.3383\n",
    "Epoch #87: Loss:1.4635, Accuracy:0.3487, Validation Loss:1.4828, Validation Accuracy:0.3448\n",
    "Epoch #88: Loss:1.4573, Accuracy:0.3515, Validation Loss:1.4801, Validation Accuracy:0.3383\n",
    "Epoch #89: Loss:1.4525, Accuracy:0.3491, Validation Loss:1.4817, Validation Accuracy:0.3465\n",
    "Epoch #90: Loss:1.4546, Accuracy:0.3561, Validation Loss:1.4746, Validation Accuracy:0.3580\n",
    "Epoch #91: Loss:1.4645, Accuracy:0.3372, Validation Loss:1.4920, Validation Accuracy:0.3481\n",
    "Epoch #92: Loss:1.4825, Accuracy:0.3478, Validation Loss:1.4696, Validation Accuracy:0.3498\n",
    "Epoch #93: Loss:1.4558, Accuracy:0.3540, Validation Loss:1.4794, Validation Accuracy:0.3530\n",
    "Epoch #94: Loss:1.4600, Accuracy:0.3573, Validation Loss:1.4786, Validation Accuracy:0.3547\n",
    "Epoch #95: Loss:1.4579, Accuracy:0.3520, Validation Loss:1.4881, Validation Accuracy:0.3448\n",
    "Epoch #96: Loss:1.4499, Accuracy:0.3581, Validation Loss:1.4709, Validation Accuracy:0.3596\n",
    "Epoch #97: Loss:1.4366, Accuracy:0.3622, Validation Loss:1.4611, Validation Accuracy:0.3711\n",
    "Epoch #98: Loss:1.4329, Accuracy:0.3659, Validation Loss:1.4593, Validation Accuracy:0.3678\n",
    "Epoch #99: Loss:1.4299, Accuracy:0.3684, Validation Loss:1.4551, Validation Accuracy:0.3678\n",
    "Epoch #100: Loss:1.4293, Accuracy:0.3713, Validation Loss:1.4560, Validation Accuracy:0.3645\n",
    "Epoch #101: Loss:1.4338, Accuracy:0.3696, Validation Loss:1.4592, Validation Accuracy:0.3596\n",
    "Epoch #102: Loss:1.4277, Accuracy:0.3770, Validation Loss:1.4492, Validation Accuracy:0.3760\n",
    "Epoch #103: Loss:1.4200, Accuracy:0.3725, Validation Loss:1.4451, Validation Accuracy:0.3711\n",
    "Epoch #104: Loss:1.4247, Accuracy:0.3749, Validation Loss:1.4420, Validation Accuracy:0.3941\n",
    "Epoch #105: Loss:1.4228, Accuracy:0.3778, Validation Loss:1.4482, Validation Accuracy:0.3810\n",
    "Epoch #106: Loss:1.4301, Accuracy:0.3778, Validation Loss:1.4360, Validation Accuracy:0.3859\n",
    "Epoch #107: Loss:1.4374, Accuracy:0.3622, Validation Loss:1.4498, Validation Accuracy:0.3645\n",
    "Epoch #108: Loss:1.4243, Accuracy:0.3708, Validation Loss:1.4356, Validation Accuracy:0.3892\n",
    "Epoch #109: Loss:1.4206, Accuracy:0.3778, Validation Loss:1.4340, Validation Accuracy:0.3842\n",
    "Epoch #110: Loss:1.4131, Accuracy:0.3725, Validation Loss:1.4330, Validation Accuracy:0.3941\n",
    "Epoch #111: Loss:1.4085, Accuracy:0.3733, Validation Loss:1.4399, Validation Accuracy:0.3793\n",
    "Epoch #112: Loss:1.4169, Accuracy:0.3766, Validation Loss:1.4294, Validation Accuracy:0.3875\n",
    "Epoch #113: Loss:1.4051, Accuracy:0.3873, Validation Loss:1.4277, Validation Accuracy:0.4007\n",
    "Epoch #114: Loss:1.4042, Accuracy:0.3819, Validation Loss:1.4284, Validation Accuracy:0.3810\n",
    "Epoch #115: Loss:1.4053, Accuracy:0.3803, Validation Loss:1.4238, Validation Accuracy:0.3941\n",
    "Epoch #116: Loss:1.3964, Accuracy:0.3893, Validation Loss:1.4248, Validation Accuracy:0.3892\n",
    "Epoch #117: Loss:1.3968, Accuracy:0.3873, Validation Loss:1.4213, Validation Accuracy:0.3941\n",
    "Epoch #118: Loss:1.4004, Accuracy:0.3914, Validation Loss:1.4214, Validation Accuracy:0.3908\n",
    "Epoch #119: Loss:1.3954, Accuracy:0.3815, Validation Loss:1.4275, Validation Accuracy:0.3777\n",
    "Epoch #120: Loss:1.3956, Accuracy:0.3852, Validation Loss:1.4201, Validation Accuracy:0.3957\n",
    "Epoch #121: Loss:1.3968, Accuracy:0.3803, Validation Loss:1.4270, Validation Accuracy:0.3793\n",
    "Epoch #122: Loss:1.4014, Accuracy:0.3889, Validation Loss:1.4157, Validation Accuracy:0.4039\n",
    "Epoch #123: Loss:1.3901, Accuracy:0.3922, Validation Loss:1.4190, Validation Accuracy:0.3892\n",
    "Epoch #124: Loss:1.3915, Accuracy:0.3918, Validation Loss:1.4166, Validation Accuracy:0.4105\n",
    "Epoch #125: Loss:1.3883, Accuracy:0.3922, Validation Loss:1.4152, Validation Accuracy:0.3892\n",
    "Epoch #126: Loss:1.3841, Accuracy:0.3918, Validation Loss:1.4151, Validation Accuracy:0.4056\n",
    "Epoch #127: Loss:1.3888, Accuracy:0.4004, Validation Loss:1.4257, Validation Accuracy:0.3810\n",
    "Epoch #128: Loss:1.4162, Accuracy:0.3770, Validation Loss:1.4359, Validation Accuracy:0.3744\n",
    "Epoch #129: Loss:1.4195, Accuracy:0.3762, Validation Loss:1.4225, Validation Accuracy:0.3777\n",
    "Epoch #130: Loss:1.4196, Accuracy:0.3807, Validation Loss:1.4240, Validation Accuracy:0.3793\n",
    "Epoch #131: Loss:1.3984, Accuracy:0.3864, Validation Loss:1.4346, Validation Accuracy:0.3744\n",
    "Epoch #132: Loss:1.3894, Accuracy:0.3971, Validation Loss:1.4130, Validation Accuracy:0.3974\n",
    "Epoch #133: Loss:1.3802, Accuracy:0.3926, Validation Loss:1.4216, Validation Accuracy:0.3924\n",
    "Epoch #134: Loss:1.3809, Accuracy:0.4000, Validation Loss:1.4172, Validation Accuracy:0.3941\n",
    "Epoch #135: Loss:1.3800, Accuracy:0.4037, Validation Loss:1.4139, Validation Accuracy:0.3908\n",
    "Epoch #136: Loss:1.3767, Accuracy:0.3955, Validation Loss:1.4118, Validation Accuracy:0.3941\n",
    "Epoch #137: Loss:1.3778, Accuracy:0.4029, Validation Loss:1.3997, Validation Accuracy:0.3974\n",
    "Epoch #138: Loss:1.3698, Accuracy:0.4136, Validation Loss:1.3988, Validation Accuracy:0.4007\n",
    "Epoch #139: Loss:1.3693, Accuracy:0.3967, Validation Loss:1.3984, Validation Accuracy:0.3908\n",
    "Epoch #140: Loss:1.3690, Accuracy:0.4053, Validation Loss:1.4013, Validation Accuracy:0.4039\n",
    "Epoch #141: Loss:1.3763, Accuracy:0.3922, Validation Loss:1.4074, Validation Accuracy:0.3892\n",
    "Epoch #142: Loss:1.3779, Accuracy:0.3951, Validation Loss:1.3994, Validation Accuracy:0.3974\n",
    "Epoch #143: Loss:1.3723, Accuracy:0.4000, Validation Loss:1.3945, Validation Accuracy:0.3974\n",
    "Epoch #144: Loss:1.3704, Accuracy:0.4099, Validation Loss:1.4053, Validation Accuracy:0.4072\n",
    "Epoch #145: Loss:1.3660, Accuracy:0.4049, Validation Loss:1.3939, Validation Accuracy:0.4056\n",
    "Epoch #146: Loss:1.3630, Accuracy:0.4074, Validation Loss:1.3917, Validation Accuracy:0.3990\n",
    "Epoch #147: Loss:1.3599, Accuracy:0.4090, Validation Loss:1.3916, Validation Accuracy:0.4105\n",
    "Epoch #148: Loss:1.3692, Accuracy:0.4057, Validation Loss:1.3915, Validation Accuracy:0.4039\n",
    "Epoch #149: Loss:1.3581, Accuracy:0.4057, Validation Loss:1.3971, Validation Accuracy:0.4154\n",
    "Epoch #150: Loss:1.3588, Accuracy:0.4029, Validation Loss:1.3880, Validation Accuracy:0.3974\n",
    "Epoch #151: Loss:1.3582, Accuracy:0.4164, Validation Loss:1.3915, Validation Accuracy:0.4187\n",
    "Epoch #152: Loss:1.3599, Accuracy:0.4090, Validation Loss:1.3878, Validation Accuracy:0.4039\n",
    "Epoch #153: Loss:1.3594, Accuracy:0.4119, Validation Loss:1.3978, Validation Accuracy:0.4039\n",
    "Epoch #154: Loss:1.3557, Accuracy:0.4140, Validation Loss:1.4075, Validation Accuracy:0.3859\n",
    "Epoch #155: Loss:1.3648, Accuracy:0.4086, Validation Loss:1.4012, Validation Accuracy:0.3941\n",
    "Epoch #156: Loss:1.3622, Accuracy:0.4070, Validation Loss:1.3897, Validation Accuracy:0.4023\n",
    "Epoch #157: Loss:1.3547, Accuracy:0.4107, Validation Loss:1.3922, Validation Accuracy:0.4023\n",
    "Epoch #158: Loss:1.3561, Accuracy:0.4066, Validation Loss:1.3821, Validation Accuracy:0.3957\n",
    "Epoch #159: Loss:1.3467, Accuracy:0.4111, Validation Loss:1.3829, Validation Accuracy:0.4089\n",
    "Epoch #160: Loss:1.3471, Accuracy:0.4123, Validation Loss:1.3840, Validation Accuracy:0.4105\n",
    "Epoch #161: Loss:1.3500, Accuracy:0.4066, Validation Loss:1.3877, Validation Accuracy:0.4023\n",
    "Epoch #162: Loss:1.3641, Accuracy:0.4074, Validation Loss:1.4547, Validation Accuracy:0.3924\n",
    "Epoch #163: Loss:1.3900, Accuracy:0.4004, Validation Loss:1.4551, Validation Accuracy:0.3859\n",
    "Epoch #164: Loss:1.3673, Accuracy:0.4136, Validation Loss:1.4042, Validation Accuracy:0.4056\n",
    "Epoch #165: Loss:1.3515, Accuracy:0.4152, Validation Loss:1.3859, Validation Accuracy:0.4105\n",
    "Epoch #166: Loss:1.3505, Accuracy:0.4107, Validation Loss:1.3858, Validation Accuracy:0.4089\n",
    "Epoch #167: Loss:1.3454, Accuracy:0.4242, Validation Loss:1.3783, Validation Accuracy:0.4122\n",
    "Epoch #168: Loss:1.3488, Accuracy:0.4123, Validation Loss:1.3799, Validation Accuracy:0.4122\n",
    "Epoch #169: Loss:1.3465, Accuracy:0.4136, Validation Loss:1.3835, Validation Accuracy:0.3908\n",
    "Epoch #170: Loss:1.3494, Accuracy:0.4172, Validation Loss:1.3830, Validation Accuracy:0.4154\n",
    "Epoch #171: Loss:1.3454, Accuracy:0.4148, Validation Loss:1.3891, Validation Accuracy:0.3892\n",
    "Epoch #172: Loss:1.3476, Accuracy:0.4066, Validation Loss:1.3821, Validation Accuracy:0.4122\n",
    "Epoch #173: Loss:1.3401, Accuracy:0.4119, Validation Loss:1.3890, Validation Accuracy:0.4138\n",
    "Epoch #174: Loss:1.3384, Accuracy:0.4086, Validation Loss:1.3856, Validation Accuracy:0.4105\n",
    "Epoch #175: Loss:1.3431, Accuracy:0.4115, Validation Loss:1.3888, Validation Accuracy:0.4171\n",
    "Epoch #176: Loss:1.3553, Accuracy:0.4008, Validation Loss:1.3827, Validation Accuracy:0.4154\n",
    "Epoch #177: Loss:1.3447, Accuracy:0.4131, Validation Loss:1.3781, Validation Accuracy:0.4072\n",
    "Epoch #178: Loss:1.3477, Accuracy:0.4201, Validation Loss:1.3768, Validation Accuracy:0.4171\n",
    "Epoch #179: Loss:1.3481, Accuracy:0.4172, Validation Loss:1.3855, Validation Accuracy:0.4204\n",
    "Epoch #180: Loss:1.3611, Accuracy:0.4037, Validation Loss:1.3804, Validation Accuracy:0.4089\n",
    "Epoch #181: Loss:1.3473, Accuracy:0.4115, Validation Loss:1.3794, Validation Accuracy:0.4204\n",
    "Epoch #182: Loss:1.3358, Accuracy:0.4193, Validation Loss:1.3891, Validation Accuracy:0.4072\n",
    "Epoch #183: Loss:1.3369, Accuracy:0.4205, Validation Loss:1.4093, Validation Accuracy:0.4154\n",
    "Epoch #184: Loss:1.3505, Accuracy:0.4078, Validation Loss:1.3912, Validation Accuracy:0.4138\n",
    "Epoch #185: Loss:1.3360, Accuracy:0.4234, Validation Loss:1.3842, Validation Accuracy:0.4187\n",
    "Epoch #186: Loss:1.3314, Accuracy:0.4177, Validation Loss:1.3800, Validation Accuracy:0.4039\n",
    "Epoch #187: Loss:1.3304, Accuracy:0.4181, Validation Loss:1.3783, Validation Accuracy:0.4236\n",
    "Epoch #188: Loss:1.3294, Accuracy:0.4197, Validation Loss:1.3803, Validation Accuracy:0.4122\n",
    "Epoch #189: Loss:1.3247, Accuracy:0.4193, Validation Loss:1.3778, Validation Accuracy:0.4072\n",
    "Epoch #190: Loss:1.3250, Accuracy:0.4177, Validation Loss:1.3757, Validation Accuracy:0.4089\n",
    "Epoch #191: Loss:1.3230, Accuracy:0.4177, Validation Loss:1.3800, Validation Accuracy:0.4187\n",
    "Epoch #192: Loss:1.3385, Accuracy:0.4185, Validation Loss:1.3967, Validation Accuracy:0.4007\n",
    "Epoch #193: Loss:1.3374, Accuracy:0.4115, Validation Loss:1.4181, Validation Accuracy:0.3957\n",
    "Epoch #194: Loss:1.3508, Accuracy:0.4025, Validation Loss:1.4169, Validation Accuracy:0.4023\n",
    "Epoch #195: Loss:1.3469, Accuracy:0.4094, Validation Loss:1.3987, Validation Accuracy:0.3990\n",
    "Epoch #196: Loss:1.3305, Accuracy:0.4214, Validation Loss:1.4098, Validation Accuracy:0.3941\n",
    "Epoch #197: Loss:1.3398, Accuracy:0.4185, Validation Loss:1.3942, Validation Accuracy:0.3990\n",
    "Epoch #198: Loss:1.3281, Accuracy:0.4189, Validation Loss:1.3760, Validation Accuracy:0.4105\n",
    "Epoch #199: Loss:1.3256, Accuracy:0.4168, Validation Loss:1.3737, Validation Accuracy:0.3941\n",
    "Epoch #200: Loss:1.3277, Accuracy:0.4177, Validation Loss:1.3755, Validation Accuracy:0.4122\n",
    "Epoch #201: Loss:1.3208, Accuracy:0.4168, Validation Loss:1.3835, Validation Accuracy:0.3892\n",
    "Epoch #202: Loss:1.3318, Accuracy:0.4189, Validation Loss:1.4481, Validation Accuracy:0.3514\n",
    "Epoch #203: Loss:1.3750, Accuracy:0.4021, Validation Loss:1.3968, Validation Accuracy:0.3859\n",
    "Epoch #204: Loss:1.3469, Accuracy:0.4115, Validation Loss:1.3826, Validation Accuracy:0.3908\n",
    "Epoch #205: Loss:1.3478, Accuracy:0.4136, Validation Loss:1.3829, Validation Accuracy:0.3974\n",
    "Epoch #206: Loss:1.3380, Accuracy:0.4127, Validation Loss:1.3731, Validation Accuracy:0.4138\n",
    "Epoch #207: Loss:1.3275, Accuracy:0.4218, Validation Loss:1.3722, Validation Accuracy:0.4154\n",
    "Epoch #208: Loss:1.3190, Accuracy:0.4251, Validation Loss:1.3907, Validation Accuracy:0.4089\n",
    "Epoch #209: Loss:1.3287, Accuracy:0.4189, Validation Loss:1.3886, Validation Accuracy:0.3990\n",
    "Epoch #210: Loss:1.3258, Accuracy:0.4164, Validation Loss:1.3767, Validation Accuracy:0.4154\n",
    "Epoch #211: Loss:1.3176, Accuracy:0.4300, Validation Loss:1.3738, Validation Accuracy:0.4039\n",
    "Epoch #212: Loss:1.3149, Accuracy:0.4234, Validation Loss:1.3772, Validation Accuracy:0.4039\n",
    "Epoch #213: Loss:1.3193, Accuracy:0.4177, Validation Loss:1.3835, Validation Accuracy:0.4072\n",
    "Epoch #214: Loss:1.3285, Accuracy:0.4267, Validation Loss:1.3775, Validation Accuracy:0.3941\n",
    "Epoch #215: Loss:1.3197, Accuracy:0.4172, Validation Loss:1.3740, Validation Accuracy:0.4056\n",
    "Epoch #216: Loss:1.3153, Accuracy:0.4242, Validation Loss:1.3733, Validation Accuracy:0.3974\n",
    "Epoch #217: Loss:1.3295, Accuracy:0.4263, Validation Loss:1.3799, Validation Accuracy:0.4039\n",
    "Epoch #218: Loss:1.3298, Accuracy:0.4140, Validation Loss:1.3818, Validation Accuracy:0.3924\n",
    "Epoch #219: Loss:1.3440, Accuracy:0.4251, Validation Loss:1.3786, Validation Accuracy:0.4039\n",
    "Epoch #220: Loss:1.3376, Accuracy:0.4127, Validation Loss:1.3759, Validation Accuracy:0.4056\n",
    "Epoch #221: Loss:1.3217, Accuracy:0.4279, Validation Loss:1.3795, Validation Accuracy:0.4023\n",
    "Epoch #222: Loss:1.3290, Accuracy:0.4136, Validation Loss:1.4026, Validation Accuracy:0.4154\n",
    "Epoch #223: Loss:1.3296, Accuracy:0.4181, Validation Loss:1.3838, Validation Accuracy:0.4023\n",
    "Epoch #224: Loss:1.3290, Accuracy:0.4205, Validation Loss:1.3817, Validation Accuracy:0.4122\n",
    "Epoch #225: Loss:1.3192, Accuracy:0.4234, Validation Loss:1.3854, Validation Accuracy:0.4007\n",
    "Epoch #226: Loss:1.3213, Accuracy:0.4234, Validation Loss:1.3746, Validation Accuracy:0.4072\n",
    "Epoch #227: Loss:1.3242, Accuracy:0.4214, Validation Loss:1.3892, Validation Accuracy:0.4105\n",
    "Epoch #228: Loss:1.3333, Accuracy:0.4144, Validation Loss:1.4228, Validation Accuracy:0.3892\n",
    "Epoch #229: Loss:1.3558, Accuracy:0.4156, Validation Loss:1.4753, Validation Accuracy:0.3678\n",
    "Epoch #230: Loss:1.3776, Accuracy:0.4062, Validation Loss:1.3724, Validation Accuracy:0.4007\n",
    "Epoch #231: Loss:1.3544, Accuracy:0.4152, Validation Loss:1.3978, Validation Accuracy:0.4105\n",
    "Epoch #232: Loss:1.3314, Accuracy:0.4164, Validation Loss:1.3778, Validation Accuracy:0.3990\n",
    "Epoch #233: Loss:1.3218, Accuracy:0.4205, Validation Loss:1.3829, Validation Accuracy:0.4089\n",
    "Epoch #234: Loss:1.3214, Accuracy:0.4140, Validation Loss:1.3907, Validation Accuracy:0.4056\n",
    "Epoch #235: Loss:1.3263, Accuracy:0.4205, Validation Loss:1.3920, Validation Accuracy:0.4039\n",
    "Epoch #236: Loss:1.3267, Accuracy:0.4136, Validation Loss:1.3801, Validation Accuracy:0.4187\n",
    "Epoch #237: Loss:1.3141, Accuracy:0.4263, Validation Loss:1.3778, Validation Accuracy:0.3892\n",
    "Epoch #238: Loss:1.3166, Accuracy:0.4308, Validation Loss:1.3739, Validation Accuracy:0.4056\n",
    "Epoch #239: Loss:1.3154, Accuracy:0.4242, Validation Loss:1.3878, Validation Accuracy:0.3892\n",
    "Epoch #240: Loss:1.3217, Accuracy:0.4201, Validation Loss:1.3724, Validation Accuracy:0.4007\n",
    "Epoch #241: Loss:1.3150, Accuracy:0.4292, Validation Loss:1.3747, Validation Accuracy:0.4089\n",
    "Epoch #242: Loss:1.3085, Accuracy:0.4263, Validation Loss:1.3759, Validation Accuracy:0.4138\n",
    "Epoch #243: Loss:1.3085, Accuracy:0.4271, Validation Loss:1.3823, Validation Accuracy:0.4007\n",
    "Epoch #244: Loss:1.3126, Accuracy:0.4259, Validation Loss:1.3793, Validation Accuracy:0.4154\n",
    "Epoch #245: Loss:1.3117, Accuracy:0.4271, Validation Loss:1.3736, Validation Accuracy:0.4122\n",
    "Epoch #246: Loss:1.3064, Accuracy:0.4312, Validation Loss:1.3746, Validation Accuracy:0.4253\n",
    "Epoch #247: Loss:1.3103, Accuracy:0.4271, Validation Loss:1.3759, Validation Accuracy:0.3974\n",
    "Epoch #248: Loss:1.3162, Accuracy:0.4263, Validation Loss:1.3715, Validation Accuracy:0.4154\n",
    "Epoch #249: Loss:1.3058, Accuracy:0.4234, Validation Loss:1.3770, Validation Accuracy:0.4023\n",
    "Epoch #250: Loss:1.3080, Accuracy:0.4304, Validation Loss:1.3734, Validation Accuracy:0.3957\n",
    "Epoch #251: Loss:1.3108, Accuracy:0.4316, Validation Loss:1.3780, Validation Accuracy:0.4007\n",
    "Epoch #252: Loss:1.3104, Accuracy:0.4296, Validation Loss:1.3699, Validation Accuracy:0.4122\n",
    "Epoch #253: Loss:1.3051, Accuracy:0.4312, Validation Loss:1.3717, Validation Accuracy:0.4154\n",
    "Epoch #254: Loss:1.3125, Accuracy:0.4357, Validation Loss:1.3709, Validation Accuracy:0.4154\n",
    "Epoch #255: Loss:1.3014, Accuracy:0.4287, Validation Loss:1.3772, Validation Accuracy:0.4187\n",
    "Epoch #256: Loss:1.3041, Accuracy:0.4337, Validation Loss:1.3691, Validation Accuracy:0.4089\n",
    "Epoch #257: Loss:1.3004, Accuracy:0.4275, Validation Loss:1.3790, Validation Accuracy:0.4187\n",
    "Epoch #258: Loss:1.3061, Accuracy:0.4259, Validation Loss:1.3700, Validation Accuracy:0.4056\n",
    "Epoch #259: Loss:1.3093, Accuracy:0.4341, Validation Loss:1.3763, Validation Accuracy:0.4056\n",
    "Epoch #260: Loss:1.3052, Accuracy:0.4271, Validation Loss:1.3777, Validation Accuracy:0.3990\n",
    "Epoch #261: Loss:1.3110, Accuracy:0.4353, Validation Loss:1.3707, Validation Accuracy:0.4171\n",
    "Epoch #262: Loss:1.3085, Accuracy:0.4255, Validation Loss:1.3716, Validation Accuracy:0.4138\n",
    "Epoch #263: Loss:1.3015, Accuracy:0.4398, Validation Loss:1.3693, Validation Accuracy:0.4056\n",
    "Epoch #264: Loss:1.3016, Accuracy:0.4324, Validation Loss:1.3826, Validation Accuracy:0.4007\n",
    "Epoch #265: Loss:1.3125, Accuracy:0.4292, Validation Loss:1.3786, Validation Accuracy:0.3941\n",
    "Epoch #266: Loss:1.3222, Accuracy:0.4312, Validation Loss:1.3787, Validation Accuracy:0.4023\n",
    "Epoch #267: Loss:1.3132, Accuracy:0.4308, Validation Loss:1.3680, Validation Accuracy:0.4105\n",
    "Epoch #268: Loss:1.3004, Accuracy:0.4353, Validation Loss:1.3696, Validation Accuracy:0.4154\n",
    "Epoch #269: Loss:1.3035, Accuracy:0.4329, Validation Loss:1.3681, Validation Accuracy:0.4089\n",
    "Epoch #270: Loss:1.3036, Accuracy:0.4366, Validation Loss:1.3701, Validation Accuracy:0.4072\n",
    "Epoch #271: Loss:1.2998, Accuracy:0.4246, Validation Loss:1.3752, Validation Accuracy:0.4007\n",
    "Epoch #272: Loss:1.3065, Accuracy:0.4279, Validation Loss:1.3710, Validation Accuracy:0.4154\n",
    "Epoch #273: Loss:1.2979, Accuracy:0.4296, Validation Loss:1.3703, Validation Accuracy:0.4187\n",
    "Epoch #274: Loss:1.2967, Accuracy:0.4378, Validation Loss:1.3714, Validation Accuracy:0.4007\n",
    "Epoch #275: Loss:1.2983, Accuracy:0.4378, Validation Loss:1.3717, Validation Accuracy:0.4056\n",
    "Epoch #276: Loss:1.2979, Accuracy:0.4341, Validation Loss:1.3724, Validation Accuracy:0.4105\n",
    "Epoch #277: Loss:1.2976, Accuracy:0.4415, Validation Loss:1.3742, Validation Accuracy:0.4007\n",
    "Epoch #278: Loss:1.2987, Accuracy:0.4320, Validation Loss:1.3718, Validation Accuracy:0.4171\n",
    "Epoch #279: Loss:1.2956, Accuracy:0.4329, Validation Loss:1.3698, Validation Accuracy:0.4089\n",
    "Epoch #280: Loss:1.2990, Accuracy:0.4370, Validation Loss:1.3770, Validation Accuracy:0.4023\n",
    "Epoch #281: Loss:1.3013, Accuracy:0.4341, Validation Loss:1.3696, Validation Accuracy:0.4187\n",
    "Epoch #282: Loss:1.2974, Accuracy:0.4398, Validation Loss:1.3727, Validation Accuracy:0.4089\n",
    "Epoch #283: Loss:1.2967, Accuracy:0.4341, Validation Loss:1.3697, Validation Accuracy:0.4171\n",
    "Epoch #284: Loss:1.2987, Accuracy:0.4349, Validation Loss:1.3796, Validation Accuracy:0.3924\n",
    "Epoch #285: Loss:1.2993, Accuracy:0.4378, Validation Loss:1.3682, Validation Accuracy:0.4187\n",
    "Epoch #286: Loss:1.3004, Accuracy:0.4279, Validation Loss:1.3698, Validation Accuracy:0.4171\n",
    "Epoch #287: Loss:1.3151, Accuracy:0.4308, Validation Loss:1.3975, Validation Accuracy:0.4023\n",
    "Epoch #288: Loss:1.3234, Accuracy:0.4279, Validation Loss:1.3936, Validation Accuracy:0.4204\n",
    "Epoch #289: Loss:1.3214, Accuracy:0.4316, Validation Loss:1.3907, Validation Accuracy:0.4039\n",
    "Epoch #290: Loss:1.3116, Accuracy:0.4316, Validation Loss:1.3746, Validation Accuracy:0.4171\n",
    "Epoch #291: Loss:1.2946, Accuracy:0.4366, Validation Loss:1.3692, Validation Accuracy:0.4187\n",
    "Epoch #292: Loss:1.2956, Accuracy:0.4378, Validation Loss:1.3695, Validation Accuracy:0.4138\n",
    "Epoch #293: Loss:1.2968, Accuracy:0.4287, Validation Loss:1.3689, Validation Accuracy:0.4154\n",
    "Epoch #294: Loss:1.2966, Accuracy:0.4460, Validation Loss:1.3687, Validation Accuracy:0.4138\n",
    "Epoch #295: Loss:1.2958, Accuracy:0.4390, Validation Loss:1.3698, Validation Accuracy:0.4171\n",
    "Epoch #296: Loss:1.2907, Accuracy:0.4415, Validation Loss:1.3691, Validation Accuracy:0.4122\n",
    "Epoch #297: Loss:1.2917, Accuracy:0.4398, Validation Loss:1.3706, Validation Accuracy:0.4138\n",
    "Epoch #298: Loss:1.2937, Accuracy:0.4398, Validation Loss:1.3701, Validation Accuracy:0.4187\n",
    "Epoch #299: Loss:1.2909, Accuracy:0.4419, Validation Loss:1.3685, Validation Accuracy:0.4171\n",
    "Epoch #300: Loss:1.2912, Accuracy:0.4452, Validation Loss:1.3681, Validation Accuracy:0.4105\n",
    "\n",
    "Test:\n",
    "Test Loss:1.36809957, Accuracy:0.4105\n",
    "Labels: ['02', '04', '03', '01', '05']\n",
    "Confusion Matrix:\n",
    "      02  04  03  01   05\n",
    "t:02  17  20  10  49   18\n",
    "t:04  11  28  25  26   22\n",
    "t:03   9  27  23  44   12\n",
    "t:01  13   9  13  75   16\n",
    "t:05   9   7   6  13  107\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.29      0.15      0.20       114\n",
    "          04       0.31      0.25      0.28       112\n",
    "          03       0.30      0.20      0.24       115\n",
    "          01       0.36      0.60      0.45       126\n",
    "          05       0.61      0.75      0.68       142\n",
    "\n",
    "    accuracy                           0.41       609\n",
    "   macro avg       0.37      0.39      0.37       609\n",
    "weighted avg       0.38      0.41      0.38       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 09:27:59 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 16 minutes, 20 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6056561978971233, 1.6053474389860782, 1.605445146560669, 1.6055160075768657, 1.605313538917767, 1.6052862018199976, 1.6052570041568799, 1.6052168732989207, 1.605141507189458, 1.60503497088484, 1.6049129659514905, 1.604735323360988, 1.6044779730156333, 1.6040583240183313, 1.6034147766814835, 1.6024232550794855, 1.6007924919645187, 1.5986328596747763, 1.5944334325336276, 1.5888434781621046, 1.5816333104041214, 1.5743962947175225, 1.5695606693258426, 1.5674391834215187, 1.5635901223654034, 1.560617105714206, 1.5571689732947764, 1.5561306770211958, 1.5543658574813692, 1.5526995899642042, 1.5517805707082764, 1.550918780524155, 1.550396895760973, 1.5497212206397346, 1.550036882532054, 1.549608173628746, 1.54935049775786, 1.5514226378674185, 1.5506036404905648, 1.550944082059688, 1.5470352092595718, 1.5469546527502376, 1.5456960841352716, 1.5452392770739025, 1.54539073061669, 1.5454115041566796, 1.5449522923561936, 1.546209146823789, 1.5456476143036766, 1.5431051804318607, 1.541964985858435, 1.5402488808326533, 1.5392997677886036, 1.5410723764516645, 1.5405378897593331, 1.5381499260712923, 1.5369358567768716, 1.5363666547343062, 1.541294524626583, 1.537734041856036, 1.5352866702479095, 1.5316779231790252, 1.5315080986821592, 1.531820770359196, 1.5461033980051677, 1.530736320515963, 1.5286279903061089, 1.526244039410245, 1.5241745391307011, 1.5247731130502886, 1.520289327906466, 1.5189253381516155, 1.5172838164472031, 1.5148604875323417, 1.5123008683397265, 1.5193192322657418, 1.5070630975544745, 1.505021718531015, 1.5036042079158214, 1.5121660929399563, 1.4996937540755875, 1.5008564148043178, 1.503445326596841, 1.4956008405325252, 1.4912404497268752, 1.487675754503272, 1.4828441509080834, 1.4800886757463854, 1.4816892522896452, 1.4745898661746573, 1.4919520456019686, 1.4695908958688746, 1.479412176143164, 1.478613438668901, 1.488067119775343, 1.4709002296325608, 1.4610631366081426, 1.4592991642568303, 1.455061439810128, 1.4559626638008456, 1.4591759453070379, 1.4491615921797227, 1.4451156461180137, 1.4420106317022163, 1.4482011268487314, 1.435990811177271, 1.4498016200042123, 1.4355827830303673, 1.4339958822785928, 1.4330352797296835, 1.4398926668762182, 1.4294058270446577, 1.4277338995330635, 1.4283659013817072, 1.4237765431991352, 1.42484964251714, 1.421271634806553, 1.4213652377841117, 1.4274532763633039, 1.4201065368448769, 1.42697258163947, 1.4157440754384634, 1.419021487040277, 1.416601320009905, 1.4152132713148746, 1.415118517742564, 1.4257413867267676, 1.435949473741215, 1.4224726539135761, 1.4239575543818608, 1.4346437963163128, 1.4130245026305, 1.4215523290320962, 1.4172080420508173, 1.4139441470990235, 1.4117948454980584, 1.3996638085063065, 1.398800395196686, 1.3984370057414515, 1.4012627507665474, 1.4074462583695335, 1.399412731623219, 1.3944746504472003, 1.4052658470589148, 1.3939411019652543, 1.3917442125639892, 1.3915913703993623, 1.391518249887551, 1.3971416034134738, 1.3879898310882117, 1.3915127623453125, 1.387835530420438, 1.3978045959582275, 1.4075046468446604, 1.4011715069192971, 1.38972390500587, 1.3922408259365162, 1.3821159564019816, 1.3829342518338232, 1.3839903615769886, 1.3877391041989005, 1.454686399555363, 1.4550569754320217, 1.4041985754896267, 1.3858944502565858, 1.3858491244965978, 1.3782744940082818, 1.3799430535148909, 1.3835285808065254, 1.3829600444959693, 1.3891407680041685, 1.3821233761525897, 1.3889956120004012, 1.3856177944659405, 1.3888496448253762, 1.3827366030274941, 1.3781222919329439, 1.3767794243416371, 1.3855041839023334, 1.3804083314826727, 1.3793722160148307, 1.3891066551599989, 1.409310558746601, 1.3912383656587899, 1.384161256804255, 1.3800259350947364, 1.3783277183134959, 1.3802877533416247, 1.377841816551384, 1.3757107917506903, 1.3800209210619747, 1.3967397077917465, 1.4180966380781728, 1.4168790281308303, 1.3987304191479737, 1.4097734781713125, 1.3942377588823316, 1.3759661114274575, 1.3737190351110373, 1.3755056754317385, 1.3835369559931638, 1.4481004975699439, 1.3968227036872325, 1.3826205750012828, 1.3829047603560198, 1.3730531048109182, 1.3722018322529659, 1.3906570782606629, 1.3886368000644378, 1.3767408582768808, 1.3737992085455282, 1.3771831360943798, 1.3835262126719032, 1.377484152078237, 1.374028176118196, 1.3733392775743858, 1.3798901230243628, 1.3817899763486263, 1.3786331652029002, 1.37589512787429, 1.379493441683514, 1.4025679212094135, 1.3838317071275759, 1.381711331885828, 1.3854191925725325, 1.3745980848036767, 1.3892417950387463, 1.422829017067582, 1.4752996010929103, 1.3723568413253684, 1.3978003144068476, 1.377825063046172, 1.382946814222289, 1.3907142529150927, 1.3920076175276281, 1.3801244976876796, 1.377833405738981, 1.3738905690573706, 1.3878220360854576, 1.3724278608957927, 1.3747040103809, 1.3759273447230924, 1.3823081519216152, 1.379345855885147, 1.3736014808535773, 1.3746173076441723, 1.3758562938333145, 1.3715222525870663, 1.3769508882109167, 1.373364703604349, 1.377951892138702, 1.3699138094051717, 1.37172880196219, 1.3709434139708971, 1.377245155461316, 1.3691291824741707, 1.378998650510127, 1.370021323852351, 1.3763131780185918, 1.377687116561852, 1.3706945993238677, 1.3716172819654342, 1.3693355824951272, 1.3826205043369912, 1.3785559077959735, 1.3786690986802426, 1.3680310862013467, 1.3695620167235827, 1.3681280986820339, 1.3700898259339858, 1.3752175005785938, 1.3709743392878566, 1.37030184386399, 1.3714142429026086, 1.3716939229683336, 1.372356194776463, 1.3742024305418794, 1.3717701225640935, 1.3697532225516433, 1.3770271220622197, 1.3695986458820661, 1.3727456037634111, 1.369659824324359, 1.3796001130528441, 1.3682468221301125, 1.3697702428585985, 1.3974694763302606, 1.3936120144447866, 1.390666517326593, 1.374605728291917, 1.369221270573746, 1.3695202985616348, 1.3689456984327344, 1.36871728932329, 1.3697831556323323, 1.3691257665114254, 1.3706490725327791, 1.3700897308014492, 1.3685046197549855, 1.3680995203591333], 'val_acc': [0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23481116533181545, 0.2430213459557892, 0.23973727360832672, 0.25123152677550886, 0.2561576351498931, 0.2610837436221503, 0.2676518881213293, 0.27422003271838125, 0.2725779965935865, 0.27586206874530306, 0.2676518881213293, 0.27422003281625423, 0.269293924343997, 0.2775041050658437, 0.27422003271838125, 0.26600984991673376, 0.26929392216632325, 0.2577996715683068, 0.2725779965935865, 0.26272577994269103, 0.27257799649571357, 0.27093596046879176, 0.282430213342355, 0.2676518881213293, 0.26929392444187, 0.25123152697125484, 0.2643678160674858, 0.2610837438178963, 0.2577996714704338, 0.26929392444187, 0.26765188831707526, 0.25944170541755474, 0.2775041050658437, 0.2627257776671442, 0.269293924343997, 0.28243021344022795, 0.28407224728947594, 0.2955665005545311, 0.27257799441591274, 0.28899835615322506, 0.2955665006524041, 0.2939244642339904, 0.27750410279029697, 0.2742200326205083, 0.29228242850068753, 0.3037766813742508, 0.3004926090267883, 0.28407224956502275, 0.28407224738734893, 0.28735631953906543, 0.3004926093204072, 0.2972085363857069, 0.315270934247814, 0.3037766813742508, 0.3037766811785048, 0.3103448257755568, 0.3169129704704817, 0.3185550064974035, 0.32183907874699297, 0.315270934149941, 0.315270934149941, 0.3415435123424029, 0.32840722324617194, 0.3070607535259673, 0.32840722324617194, 0.32840722324617194, 0.3251231512902014, 0.3513957294826633, 0.34975369316212257, 0.3382594400928134, 0.3448275849834843, 0.33825944028855937, 0.3464696205210412, 0.3579638736882233, 0.3481116573309468, 0.3497536930642496, 0.35303776550958504, 0.35467980202587174, 0.3448275851792303, 0.35960590991089103, 0.3711001627844543, 0.3678160905348648, 0.3678160904369918, 0.3645320185788942, 0.35960591040025597, 0.3760262712567115, 0.3711001627844543, 0.39408866911881857, 0.38095237992471465, 0.3858784882990989, 0.36453201818740233, 0.38916256025506946, 0.38423645217430413, 0.39408866882519966, 0.3793103438977929, 0.3875205242281477, 0.4006568135201246, 0.3809523797289687, 0.3940886690209456, 0.38916256035294244, 0.39408866882519966, 0.39080459696710207, 0.37766830738150625, 0.3957307049499944, 0.3793103437999199, 0.40394088547609514, 0.38916256064656135, 0.4105090300731471, 0.38916256064656135, 0.40558292179663585, 0.3809523797289687, 0.37438423542553567, 0.37766830747937924, 0.3793103437999199, 0.37438423542553567, 0.3973727412705351, 0.39244663289615084, 0.3940886690209456, 0.3908045967713561, 0.3940886690209456, 0.39737274156415403, 0.4006568139116165, 0.3908045967713561, 0.403940886063333, 0.38916256054868836, 0.39737274136840806, 0.39737274146628104, 0.40722495860654145, 0.40558292209025476, 0.3990147776889488, 0.410509030562512, 0.403940886063333, 0.4154351388390233, 0.39737274146628104, 0.4187192112843587, 0.4039408857697141, 0.403940886063333, 0.38587848859271784, 0.39408866941243753, 0.40229884974279234, 0.40229884993853826, 0.3957307052436133, 0.40886699443771723, 0.41050903046463905, 0.40229884974279234, 0.3924466329940238, 0.3858784882990989, 0.40558292199238183, 0.41050903066038497, 0.40886699443771723, 0.4121510665894338, 0.4121510664915608, 0.3908045968692291, 0.4154351388390233, 0.38916256074443434, 0.4121510665894338, 0.41379310290997445, 0.41050903046463905, 0.41707717515956394, 0.41543513893689626, 0.40722495811717657, 0.41707717486594503, 0.4203612475070264, 0.40886699443771723, 0.4203612472134075, 0.4072249580193036, 0.4154351385454043, 0.41379310271422853, 0.41871921108861276, 0.403940886063333, 0.42364531956087, 0.4121510664915608, 0.40722495811717657, 0.40886699414409833, 0.41871921108861276, 0.4006568134222516, 0.3957307049499944, 0.4022988498406653, 0.3990147772974569, 0.3940886690209456, 0.3990147772974569, 0.41050903036676606, 0.39408866911881857, 0.4121510664915608, 0.38916256074443434, 0.35139572967840926, 0.38587848849484485, 0.39080459696710207, 0.39737274136840806, 0.41379310261635555, 0.4154351387411503, 0.4088669943398443, 0.3990147774932028, 0.4154351387411503, 0.4039408856718411, 0.40394088596546, 0.40722495811717657, 0.39408866911881857, 0.40558292189450884, 0.3973727412705351, 0.4039408858675871, 0.39244663289615084, 0.40394088596546, 0.40558292209025476, 0.4022988498406653, 0.4154351385454043, 0.40229884974279234, 0.4121510664915608, 0.40065681332437864, 0.4072249582150495, 0.41050903046463905, 0.38916256025506946, 0.36781609024124584, 0.4006568134222516, 0.41050903075825795, 0.3990147775910758, 0.4088669943398443, 0.40558292209025476, 0.4039408856718411, 0.4187192109907398, 0.3891625608423073, 0.40558292209025476, 0.3891625608423073, 0.40065681371587053, 0.4088669943398443, 0.41379310251848256, 0.4006568135201246, 0.4154351388390233, 0.4121510663936878, 0.42528735558779174, 0.39737274156415403, 0.41543513893689626, 0.4022988498406653, 0.3957307054393593, 0.40065681371587053, 0.4121510665894338, 0.41543513893689626, 0.4154351387411503, 0.4187192109907398, 0.40886699414409833, 0.41871921108861276, 0.40558292189450884, 0.40558292218812775, 0.3990147775910758, 0.417077175061691, 0.41379310261635555, 0.40558292189450884, 0.40065681371587053, 0.39408866911881857, 0.40229885003641125, 0.41050903026889307, 0.4154351387411503, 0.4088669942419713, 0.40722495811717657, 0.4006568138137435, 0.4154351387411503, 0.41871921108861276, 0.40065681361799754, 0.40558292199238183, 0.41050903046463905, 0.4006568138137435, 0.417077174963818, 0.40886699414409833, 0.4022988498406653, 0.41871921108861276, 0.40886699443771723, 0.41707717486594503, 0.3924466330918968, 0.4187192109907398, 0.41707717476807205, 0.40229884954704637, 0.4203612471155345, 0.4039408857697141, 0.417077175061691, 0.41871921108861276, 0.41379310261635555, 0.4154351388390233, 0.41379310242060957, 0.417077174963818, 0.4121510664915608, 0.41379310271422853, 0.4187192108928668, 0.41707717486594503, 0.41050903026889307], 'loss': [1.6069486539221887, 1.6056623052522632, 1.6055643945259235, 1.6057458333655794, 1.6058246930277078, 1.6056740073942306, 1.605490964100346, 1.605363347398182, 1.6053208650749566, 1.6052393537282454, 1.6050680953129606, 1.6048192462392412, 1.6046457458080943, 1.604189451274441, 1.6035569129293705, 1.602392375738469, 1.6007854322143649, 1.5980866969733267, 1.594759253799548, 1.5878659844643281, 1.5787252562246774, 1.5706788450785487, 1.5634308967746993, 1.5560659961289205, 1.5529918935998999, 1.5485196612943615, 1.5486249883072087, 1.5455637931823731, 1.5438575914508263, 1.5431207779741385, 1.5409929418465929, 1.5397832819078983, 1.5380743106043069, 1.536898542821285, 1.5355093205489172, 1.5346470580208718, 1.5342125148009471, 1.5342338734338905, 1.5322330055295563, 1.5353722681010284, 1.532865567961268, 1.5320765258839977, 1.530577917412321, 1.5302236484551086, 1.5297868076535954, 1.5301360012324683, 1.5268770009830013, 1.5259043725853827, 1.5298627382920753, 1.5243142019796665, 1.524923017137594, 1.523817654312024, 1.5218293798532818, 1.5214783416391644, 1.5221293615364686, 1.5216962515695873, 1.520110176035511, 1.5197320683046533, 1.5162061838643506, 1.5195667840616904, 1.5176284147239074, 1.513433320468456, 1.510599379177211, 1.5093996578173472, 1.5096342129384224, 1.5141208567413706, 1.509522144065011, 1.5054942468842931, 1.5035982346877426, 1.5028365595874356, 1.5000164900961843, 1.5001347055670173, 1.4967890995483868, 1.4948738531899892, 1.4939881278748874, 1.4931890510190928, 1.491286830295038, 1.4907505584203733, 1.4875931460754583, 1.4806452439795774, 1.4873783133113163, 1.4769043657079615, 1.4779593920071268, 1.4931464098806988, 1.4754482955168895, 1.4680922907725498, 1.4634928116808192, 1.4573096880433005, 1.4524762149219395, 1.4546286897248066, 1.464459423656581, 1.482511532282193, 1.4557927094935392, 1.459970647353656, 1.4579056705537519, 1.449884369681748, 1.436606148723704, 1.4328531405274627, 1.429855288764045, 1.4292771720298751, 1.4338249080724539, 1.427680986568913, 1.419987388166314, 1.4247016385840194, 1.422783665147895, 1.430076053695757, 1.4374080791610466, 1.4242564329131673, 1.420633768349947, 1.413107693611474, 1.408457088225676, 1.4168666077345549, 1.4050734216917222, 1.4041505456215546, 1.4052667209499916, 1.3964138397201131, 1.3968085939634507, 1.400434695181171, 1.3953898053394451, 1.3956402789395936, 1.3968276751849196, 1.4014271113906798, 1.3900731484258444, 1.391486166585887, 1.3883299282199304, 1.384067339965695, 1.3887884794074652, 1.416234196040175, 1.4194526000678906, 1.4195986815301789, 1.398438082877126, 1.3894027182698494, 1.3801976474158817, 1.3808875542646561, 1.3800085339947648, 1.376711441457149, 1.3777794738569789, 1.3697702036745985, 1.3693038311337544, 1.3689680252231857, 1.37628507452579, 1.3778917591185051, 1.3723235939562444, 1.3704043180301204, 1.3659866294332108, 1.3629702805493646, 1.3599473764274643, 1.3692393323479246, 1.3580546604289656, 1.358792120475299, 1.358205177309087, 1.3598857716613237, 1.3594271921279248, 1.3556911391644018, 1.3647546432835855, 1.3621944493581626, 1.3547121224706913, 1.3561146113906797, 1.3467312844626957, 1.3471013912185261, 1.349983514848431, 1.3640519435400835, 1.3899546875356403, 1.3673465443832429, 1.3514694893384616, 1.350510040690522, 1.3454287489336865, 1.3487539893058291, 1.346544904826358, 1.3493623976834745, 1.3453638796444056, 1.3476325375343494, 1.3400753780067334, 1.3383949795298018, 1.3431170593296968, 1.3552767148497658, 1.3446841312874513, 1.3477240504670192, 1.3480799433142252, 1.3611100302584607, 1.347345912334121, 1.3357554246757555, 1.3368624514378071, 1.350505694863243, 1.336017151241185, 1.331352763841774, 1.3303795061072285, 1.3294224596121473, 1.3246773078456306, 1.3250277807090807, 1.3230481720068616, 1.3385068241820444, 1.3373968861186285, 1.3507764117673682, 1.3468891992216483, 1.3304860729946004, 1.33980317326297, 1.3281350273371233, 1.3255891545842071, 1.3276752863576524, 1.3207695259450642, 1.3317835677575771, 1.374951367015956, 1.3469192318358216, 1.3477591010824121, 1.3379774044671342, 1.3274793712999786, 1.3190205528016452, 1.3286638420954866, 1.3257546722521771, 1.3175690435530958, 1.3149283720482545, 1.3192962787968912, 1.3285327647500949, 1.319709069919782, 1.3153081500799504, 1.329516820790097, 1.329766452777557, 1.3440014279109007, 1.3375717573342136, 1.3216622642423093, 1.3290191794812556, 1.329624207406563, 1.3290194776268711, 1.3192345290213394, 1.3213281892408335, 1.3242430951316253, 1.33325391760597, 1.3558428435844563, 1.377640373799835, 1.354441750808418, 1.3314198716220425, 1.321825500337495, 1.3214345114921398, 1.3262970110718963, 1.3267051709995623, 1.314053579424441, 1.3166332533716911, 1.3153889526331939, 1.321711958752031, 1.3149576897004303, 1.308494922708437, 1.3084659471159354, 1.3125900847221548, 1.3117421258401576, 1.3063866747478194, 1.3102855206025454, 1.3161606122336105, 1.305848287312157, 1.3079946806298635, 1.3108294952576656, 1.3104033685562793, 1.3050933646959935, 1.3124844783140648, 1.3013986915043982, 1.3040842078304877, 1.3004076558706452, 1.3061420905271839, 1.3092679644022633, 1.3051985940894062, 1.3110386083258252, 1.3084943190737182, 1.3014841854205121, 1.3015512135973701, 1.3125144670141795, 1.3222007710830876, 1.3131750649740075, 1.3003601534900235, 1.3035095426825771, 1.3035888605783608, 1.2998443218227285, 1.306546568919501, 1.2979314944582554, 1.2967267281220924, 1.2983270897268024, 1.2978515843835945, 1.2976457964957862, 1.2986501241366721, 1.2956435708784224, 1.298972560003308, 1.301280766639866, 1.2973764981087719, 1.2966894847901205, 1.2987499456875622, 1.2992621963273818, 1.3003853951146715, 1.3151093493252075, 1.32343838376431, 1.3213869768980837, 1.311614252899217, 1.294577102200941, 1.2956339595254196, 1.2967612565665274, 1.2965965812945512, 1.2958020984269756, 1.290715330239439, 1.2916919649504048, 1.2936963737867697, 1.2908849136540532, 1.2911569670730059], 'acc': [0.23285421011873828, 0.23285420969036816, 0.23285421049203225, 0.23285421008202084, 0.23285421051039099, 0.23285420892542147, 0.2328542102778472, 0.23285421029620593, 0.23285421010037957, 0.23285420953125924, 0.23285420929871545, 0.2328542083195837, 0.2328542087112364, 0.23285420970872686, 0.23285421010037957, 0.23285420851541005, 0.23285420990455322, 0.24558521643801146, 0.25174538012157965, 0.2657084167493197, 0.26981519312584423, 0.27392197188900236, 0.27720739005527456, 0.2788501011150329, 0.27802874627054597, 0.282956876708252, 0.2825462004609666, 0.2817248472198079, 0.28172484741563425, 0.2915811084501552, 0.2870636570380209, 0.2924024629029895, 0.29240246270716314, 0.2932238216640034, 0.2903490742251614, 0.2956878863932905, 0.2919917884548587, 0.2936344975563535, 0.2874743338727853, 0.296509242449453, 0.29856262658655763, 0.2993839846009836, 0.2993839820185236, 0.2944558541632776, 0.29938398241017633, 0.29486652978636646, 0.29609856346059876, 0.29117043024460637, 0.294866530998042, 0.3030800835185472, 0.29322382052576274, 0.3034907580033954, 0.2944558507975122, 0.30431211519779855, 0.2952772066211309, 0.29363449775217987, 0.3014373712838308, 0.3006160174184756, 0.3055441476603553, 0.3071868598950717, 0.3014373726913327, 0.3096509216869636, 0.30718685969924536, 0.3055441490678572, 0.31745379897603265, 0.2993839826060027, 0.31088295728274196, 0.3080082133687742, 0.3080082157554078, 0.30759753379244087, 0.31252566873415294, 0.31540041127733626, 0.31663244432737203, 0.31498973467511565, 0.3178644787481923, 0.32566735212073433, 0.3219712537539324, 0.32731006200553453, 0.32854209662218115, 0.334291580141937, 0.32689938200083113, 0.3297741253640373, 0.33634496901559147, 0.3359342919850007, 0.34620123282839876, 0.3449691993867103, 0.3486652973251421, 0.3515400406516308, 0.34907597376825383, 0.356057496053727, 0.3371663228809466, 0.3478439405223917, 0.3540041081592043, 0.35728952890793647, 0.3519507174863952, 0.3581108846948377, 0.36221766091225327, 0.36591375947488164, 0.36837782338414593, 0.3712525675306575, 0.36960985604252905, 0.3770020529719594, 0.3724846005439758, 0.3749486640983049, 0.37782340859975166, 0.3778234089914044, 0.36221765813396695, 0.37084188713430133, 0.377823410790559, 0.3724845991731914, 0.3733059528427202, 0.3765913743747578, 0.3872689945986628, 0.3819301867387133, 0.3802874721173633, 0.38932238366814365, 0.3872689920529203, 0.39137577234597176, 0.38151950814151175, 0.3852156074874455, 0.38028747293738613, 0.3889117042876367, 0.3921971246447162, 0.39178644542331814, 0.39219712542802154, 0.39178644898490983, 0.4004106767735687, 0.3770020557502457, 0.3761806971483407, 0.38069815133876134, 0.38644764053748126, 0.3971252584114701, 0.39260780281354757, 0.40000000232543786, 0.40369609775484466, 0.39548254735171184, 0.4028747448686212, 0.41355235976849736, 0.396714580597574, 0.4053388083862328, 0.3921971256238479, 0.3950718669553557, 0.3999999995471516, 0.40985626163423916, 0.40492813374227565, 0.40739219905904184, 0.40903490675303483, 0.4057494858084763, 0.4057494846335182, 0.40287474209033486, 0.4164271034866388, 0.4090349054189678, 0.41190965324946255, 0.4139630409481589, 0.4086242285474859, 0.40698151905433844, 0.4106776203952531, 0.4065708404571369, 0.4110882948433839, 0.41232032730594065, 0.4065708428437705, 0.4073921949099711, 0.40041067974768135, 0.41355236274261004, 0.4151950696532976, 0.41067761667455244, 0.4242299806165989, 0.4123203298884006, 0.4135523629384364, 0.4172484609135857, 0.4147843944218614, 0.40657084319870573, 0.41190965187867806, 0.4086242319132513, 0.41149897304893274, 0.40082135341250674, 0.41314168512454025, 0.4201232036158779, 0.41724845813529937, 0.40369609752230085, 0.41149897191069207, 0.4193018468131275, 0.42053388084229504, 0.4078028733113463, 0.42340862338547836, 0.41765913575336916, 0.4180698143505707, 0.4197125272094836, 0.41930184994634906, 0.4176591369283273, 0.4176591397066136, 0.4184804917728142, 0.41149897246145367, 0.4024640648639178, 0.40944558535023634, 0.42135523686174003, 0.4184804917728142, 0.41889116958671035, 0.41683778051722953, 0.41765913872748184, 0.4168377807130559, 0.41889117256082303, 0.40205338724584794, 0.4114989712864956, 0.4135523637217418, 0.4127310078981231, 0.4217659144798099, 0.4250513326827995, 0.41889116997836306, 0.41642710426994417, 0.4299794652745954, 0.42340862299382565, 0.41765913673250094, 0.4266940443300369, 0.41724845813529937, 0.42422998081242524, 0.4262833696860797, 0.41396303898989545, 0.42505133562019476, 0.4127310047281841, 0.42792607659676724, 0.41355236016015007, 0.4180698149380498, 0.4205338786882052, 0.4234086251479155, 0.4234086222105202, 0.4213552384650683, 0.41437371598376876, 0.4156057480546728, 0.406160164834048, 0.415195073606542, 0.41642710387829146, 0.4205338830331023, 0.41396304055650623, 0.42053388299638483, 0.4135523625467837, 0.4262833665161407, 0.4308008236439566, 0.42422997979657606, 0.4201232020492671, 0.42915811043010843, 0.42628336909860065, 0.4271047213606276, 0.42587268831059183, 0.4271047207731486, 0.43121149891211025, 0.42710472296395585, 0.42628336608777057, 0.4234086245971539, 0.43039014406762327, 0.4316221777418556, 0.4295687905939208, 0.4312114977371521, 0.4357289521233991, 0.4287474330445824, 0.4336755665787926, 0.4275153997620028, 0.4258726899139201, 0.4340862420427726, 0.42710472453056664, 0.4353182739178503, 0.4254620104966957, 0.4398357267007691, 0.43244353235379873, 0.42915811160506656, 0.4312114981288048, 0.43080082109821405, 0.43531827450532934, 0.43285420879691044, 0.4365503061845807, 0.4246406566313405, 0.4279260795341625, 0.4295687878523519, 0.4377823413887063, 0.4377823409970536, 0.43408624321773065, 0.44147844057553115, 0.4320328535607708, 0.43285420938438945, 0.4369609861525667, 0.4340862438052097, 0.43983572928322906, 0.4340862443926888, 0.43490759669143314, 0.4377823400179219, 0.42792607640094094, 0.43080081949488586, 0.42792608031746787, 0.4316221777051381, 0.43162217852516094, 0.4365503081428442, 0.43778234260038185, 0.42874743281203864, 0.4459958949250607, 0.4390143736554367, 0.4414784389722029, 0.43983572990742553, 0.43983573104566615, 0.44188911577024986, 0.4451745373390049]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
