{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf17.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 09:56:40 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': 'Split', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 5 Label(s): ['03', '04', '01', '02', '05'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000024FD0B11E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000024FCE1C6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6109, Accuracy:0.2093, Validation Loss:1.6063, Validation Accuracy:0.2328\n",
    "Epoch #2: Loss:1.6056, Accuracy:0.2330, Validation Loss:1.6053, Validation Accuracy:0.2328\n",
    "Epoch #3: Loss:1.6054, Accuracy:0.2330, Validation Loss:1.6052, Validation Accuracy:0.2328\n",
    "Epoch #4: Loss:1.6052, Accuracy:0.2354, Validation Loss:1.6046, Validation Accuracy:0.2328\n",
    "Epoch #5: Loss:1.6039, Accuracy:0.2330, Validation Loss:1.6036, Validation Accuracy:0.2328\n",
    "Epoch #6: Loss:1.6035, Accuracy:0.2330, Validation Loss:1.6029, Validation Accuracy:0.2328\n",
    "Epoch #7: Loss:1.6031, Accuracy:0.2379, Validation Loss:1.6021, Validation Accuracy:0.2336\n",
    "Epoch #8: Loss:1.6019, Accuracy:0.2406, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #9: Loss:1.6009, Accuracy:0.2429, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #10: Loss:1.6002, Accuracy:0.2435, Validation Loss:1.5999, Validation Accuracy:0.2422\n",
    "Epoch #11: Loss:1.6009, Accuracy:0.2403, Validation Loss:1.5998, Validation Accuracy:0.2430\n",
    "Epoch #12: Loss:1.5996, Accuracy:0.2447, Validation Loss:1.5991, Validation Accuracy:0.2430\n",
    "Epoch #13: Loss:1.5989, Accuracy:0.2448, Validation Loss:1.5988, Validation Accuracy:0.2422\n",
    "Epoch #14: Loss:1.5997, Accuracy:0.2448, Validation Loss:1.5999, Validation Accuracy:0.2434\n",
    "Epoch #15: Loss:1.5994, Accuracy:0.2429, Validation Loss:1.5992, Validation Accuracy:0.2438\n",
    "Epoch #16: Loss:1.5992, Accuracy:0.2445, Validation Loss:1.5995, Validation Accuracy:0.2434\n",
    "Epoch #17: Loss:1.5986, Accuracy:0.2449, Validation Loss:1.5987, Validation Accuracy:0.2471\n",
    "Epoch #18: Loss:1.5979, Accuracy:0.2436, Validation Loss:1.5981, Validation Accuracy:0.2467\n",
    "Epoch #19: Loss:1.5977, Accuracy:0.2452, Validation Loss:1.5984, Validation Accuracy:0.2467\n",
    "Epoch #20: Loss:1.5980, Accuracy:0.2407, Validation Loss:1.5982, Validation Accuracy:0.2418\n",
    "Epoch #21: Loss:1.5975, Accuracy:0.2439, Validation Loss:1.5988, Validation Accuracy:0.2484\n",
    "Epoch #22: Loss:1.5969, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2422\n",
    "Epoch #23: Loss:1.5970, Accuracy:0.2445, Validation Loss:1.5980, Validation Accuracy:0.2479\n",
    "Epoch #24: Loss:1.5963, Accuracy:0.2414, Validation Loss:1.5982, Validation Accuracy:0.2479\n",
    "Epoch #25: Loss:1.5963, Accuracy:0.2441, Validation Loss:1.5985, Validation Accuracy:0.2443\n",
    "Epoch #26: Loss:1.5957, Accuracy:0.2446, Validation Loss:1.5981, Validation Accuracy:0.2434\n",
    "Epoch #27: Loss:1.5957, Accuracy:0.2421, Validation Loss:1.5978, Validation Accuracy:0.2430\n",
    "Epoch #28: Loss:1.5957, Accuracy:0.2428, Validation Loss:1.5976, Validation Accuracy:0.2434\n",
    "Epoch #29: Loss:1.5946, Accuracy:0.2443, Validation Loss:1.5967, Validation Accuracy:0.2504\n",
    "Epoch #30: Loss:1.5943, Accuracy:0.2432, Validation Loss:1.5973, Validation Accuracy:0.2443\n",
    "Epoch #31: Loss:1.5951, Accuracy:0.2429, Validation Loss:1.5979, Validation Accuracy:0.2525\n",
    "Epoch #32: Loss:1.5991, Accuracy:0.2348, Validation Loss:1.5981, Validation Accuracy:0.2414\n",
    "Epoch #33: Loss:1.5954, Accuracy:0.2440, Validation Loss:1.5968, Validation Accuracy:0.2537\n",
    "Epoch #34: Loss:1.5939, Accuracy:0.2437, Validation Loss:1.5965, Validation Accuracy:0.2521\n",
    "Epoch #35: Loss:1.5924, Accuracy:0.2436, Validation Loss:1.5954, Validation Accuracy:0.2533\n",
    "Epoch #36: Loss:1.5921, Accuracy:0.2418, Validation Loss:1.5968, Validation Accuracy:0.2533\n",
    "Epoch #37: Loss:1.5920, Accuracy:0.2490, Validation Loss:1.5940, Validation Accuracy:0.2455\n",
    "Epoch #38: Loss:1.5911, Accuracy:0.2437, Validation Loss:1.5952, Validation Accuracy:0.2488\n",
    "Epoch #39: Loss:1.5893, Accuracy:0.2496, Validation Loss:1.5944, Validation Accuracy:0.2385\n",
    "Epoch #40: Loss:1.5889, Accuracy:0.2530, Validation Loss:1.5927, Validation Accuracy:0.2607\n",
    "Epoch #41: Loss:1.5866, Accuracy:0.2542, Validation Loss:1.5932, Validation Accuracy:0.2619\n",
    "Epoch #42: Loss:1.5874, Accuracy:0.2512, Validation Loss:1.5997, Validation Accuracy:0.2508\n",
    "Epoch #43: Loss:1.5897, Accuracy:0.2529, Validation Loss:1.5927, Validation Accuracy:0.2557\n",
    "Epoch #44: Loss:1.5870, Accuracy:0.2549, Validation Loss:1.5945, Validation Accuracy:0.2640\n",
    "Epoch #45: Loss:1.5851, Accuracy:0.2607, Validation Loss:1.5950, Validation Accuracy:0.2537\n",
    "Epoch #46: Loss:1.5865, Accuracy:0.2607, Validation Loss:1.5907, Validation Accuracy:0.2672\n",
    "Epoch #47: Loss:1.5848, Accuracy:0.2540, Validation Loss:1.5917, Validation Accuracy:0.2623\n",
    "Epoch #48: Loss:1.5818, Accuracy:0.2653, Validation Loss:1.5924, Validation Accuracy:0.2631\n",
    "Epoch #49: Loss:1.5829, Accuracy:0.2637, Validation Loss:1.5926, Validation Accuracy:0.2590\n",
    "Epoch #50: Loss:1.5820, Accuracy:0.2657, Validation Loss:1.5915, Validation Accuracy:0.2553\n",
    "Epoch #51: Loss:1.5794, Accuracy:0.2674, Validation Loss:1.5904, Validation Accuracy:0.2586\n",
    "Epoch #52: Loss:1.5797, Accuracy:0.2652, Validation Loss:1.5920, Validation Accuracy:0.2467\n",
    "Epoch #53: Loss:1.5785, Accuracy:0.2659, Validation Loss:1.5954, Validation Accuracy:0.2512\n",
    "Epoch #54: Loss:1.5873, Accuracy:0.2569, Validation Loss:1.5902, Validation Accuracy:0.2718\n",
    "Epoch #55: Loss:1.5786, Accuracy:0.2715, Validation Loss:1.5883, Validation Accuracy:0.2705\n",
    "Epoch #56: Loss:1.5777, Accuracy:0.2702, Validation Loss:1.5897, Validation Accuracy:0.2652\n",
    "Epoch #57: Loss:1.5770, Accuracy:0.2744, Validation Loss:1.5886, Validation Accuracy:0.2619\n",
    "Epoch #58: Loss:1.5775, Accuracy:0.2633, Validation Loss:1.5906, Validation Accuracy:0.2599\n",
    "Epoch #59: Loss:1.5751, Accuracy:0.2686, Validation Loss:1.5933, Validation Accuracy:0.2582\n",
    "Epoch #60: Loss:1.5769, Accuracy:0.2700, Validation Loss:1.5951, Validation Accuracy:0.2718\n",
    "Epoch #61: Loss:1.5794, Accuracy:0.2727, Validation Loss:1.5905, Validation Accuracy:0.2668\n",
    "Epoch #62: Loss:1.5734, Accuracy:0.2796, Validation Loss:1.5864, Validation Accuracy:0.2718\n",
    "Epoch #63: Loss:1.5726, Accuracy:0.2776, Validation Loss:1.5886, Validation Accuracy:0.2611\n",
    "Epoch #64: Loss:1.5713, Accuracy:0.2742, Validation Loss:1.5879, Validation Accuracy:0.2755\n",
    "Epoch #65: Loss:1.5701, Accuracy:0.2773, Validation Loss:1.5888, Validation Accuracy:0.2693\n",
    "Epoch #66: Loss:1.5695, Accuracy:0.2803, Validation Loss:1.5870, Validation Accuracy:0.2734\n",
    "Epoch #67: Loss:1.5676, Accuracy:0.2809, Validation Loss:1.5875, Validation Accuracy:0.2726\n",
    "Epoch #68: Loss:1.5695, Accuracy:0.2780, Validation Loss:1.5917, Validation Accuracy:0.2705\n",
    "Epoch #69: Loss:1.5682, Accuracy:0.2809, Validation Loss:1.5859, Validation Accuracy:0.2738\n",
    "Epoch #70: Loss:1.5655, Accuracy:0.2869, Validation Loss:1.5876, Validation Accuracy:0.2726\n",
    "Epoch #71: Loss:1.5674, Accuracy:0.2792, Validation Loss:1.5863, Validation Accuracy:0.2816\n",
    "Epoch #72: Loss:1.5647, Accuracy:0.2844, Validation Loss:1.5858, Validation Accuracy:0.2726\n",
    "Epoch #73: Loss:1.5631, Accuracy:0.2850, Validation Loss:1.5892, Validation Accuracy:0.2656\n",
    "Epoch #74: Loss:1.5670, Accuracy:0.2801, Validation Loss:1.5859, Validation Accuracy:0.2746\n",
    "Epoch #75: Loss:1.5681, Accuracy:0.2773, Validation Loss:1.5960, Validation Accuracy:0.2656\n",
    "Epoch #76: Loss:1.5683, Accuracy:0.2819, Validation Loss:1.5886, Validation Accuracy:0.2722\n",
    "Epoch #77: Loss:1.5634, Accuracy:0.2809, Validation Loss:1.5870, Validation Accuracy:0.2672\n",
    "Epoch #78: Loss:1.5616, Accuracy:0.2835, Validation Loss:1.5877, Validation Accuracy:0.2837\n",
    "Epoch #79: Loss:1.5596, Accuracy:0.2887, Validation Loss:1.5856, Validation Accuracy:0.2693\n",
    "Epoch #80: Loss:1.5648, Accuracy:0.2868, Validation Loss:1.5900, Validation Accuracy:0.2730\n",
    "Epoch #81: Loss:1.5619, Accuracy:0.2884, Validation Loss:1.5851, Validation Accuracy:0.2808\n",
    "Epoch #82: Loss:1.5583, Accuracy:0.2857, Validation Loss:1.5837, Validation Accuracy:0.2837\n",
    "Epoch #83: Loss:1.5568, Accuracy:0.2878, Validation Loss:1.5864, Validation Accuracy:0.2853\n",
    "Epoch #84: Loss:1.5565, Accuracy:0.2906, Validation Loss:1.5865, Validation Accuracy:0.2746\n",
    "Epoch #85: Loss:1.5586, Accuracy:0.2888, Validation Loss:1.5936, Validation Accuracy:0.2775\n",
    "Epoch #86: Loss:1.5600, Accuracy:0.2842, Validation Loss:1.5858, Validation Accuracy:0.2816\n",
    "Epoch #87: Loss:1.5607, Accuracy:0.2835, Validation Loss:1.5873, Validation Accuracy:0.2763\n",
    "Epoch #88: Loss:1.5549, Accuracy:0.2953, Validation Loss:1.5894, Validation Accuracy:0.2685\n",
    "Epoch #89: Loss:1.5544, Accuracy:0.2917, Validation Loss:1.5822, Validation Accuracy:0.2816\n",
    "Epoch #90: Loss:1.5524, Accuracy:0.2972, Validation Loss:1.5822, Validation Accuracy:0.2763\n",
    "Epoch #91: Loss:1.5515, Accuracy:0.2920, Validation Loss:1.5834, Validation Accuracy:0.2857\n",
    "Epoch #92: Loss:1.5485, Accuracy:0.3048, Validation Loss:1.5832, Validation Accuracy:0.2808\n",
    "Epoch #93: Loss:1.5493, Accuracy:0.2971, Validation Loss:1.5825, Validation Accuracy:0.2902\n",
    "Epoch #94: Loss:1.5478, Accuracy:0.2988, Validation Loss:1.5816, Validation Accuracy:0.2755\n",
    "Epoch #95: Loss:1.5502, Accuracy:0.2954, Validation Loss:1.5832, Validation Accuracy:0.2755\n",
    "Epoch #96: Loss:1.5450, Accuracy:0.3022, Validation Loss:1.5829, Validation Accuracy:0.2878\n",
    "Epoch #97: Loss:1.5465, Accuracy:0.2990, Validation Loss:1.5787, Validation Accuracy:0.2833\n",
    "Epoch #98: Loss:1.5545, Accuracy:0.2913, Validation Loss:1.5799, Validation Accuracy:0.2787\n",
    "Epoch #99: Loss:1.5532, Accuracy:0.2925, Validation Loss:1.5798, Validation Accuracy:0.2911\n",
    "Epoch #100: Loss:1.5452, Accuracy:0.3046, Validation Loss:1.5874, Validation Accuracy:0.2816\n",
    "Epoch #101: Loss:1.5414, Accuracy:0.3085, Validation Loss:1.5819, Validation Accuracy:0.2902\n",
    "Epoch #102: Loss:1.5418, Accuracy:0.3076, Validation Loss:1.5812, Validation Accuracy:0.2767\n",
    "Epoch #103: Loss:1.5428, Accuracy:0.3004, Validation Loss:1.5784, Validation Accuracy:0.2911\n",
    "Epoch #104: Loss:1.5423, Accuracy:0.3018, Validation Loss:1.5793, Validation Accuracy:0.2849\n",
    "Epoch #105: Loss:1.5408, Accuracy:0.3082, Validation Loss:1.5809, Validation Accuracy:0.2783\n",
    "Epoch #106: Loss:1.5448, Accuracy:0.2921, Validation Loss:1.5824, Validation Accuracy:0.2902\n",
    "Epoch #107: Loss:1.5362, Accuracy:0.3056, Validation Loss:1.5821, Validation Accuracy:0.2841\n",
    "Epoch #108: Loss:1.5349, Accuracy:0.3111, Validation Loss:1.5788, Validation Accuracy:0.2767\n",
    "Epoch #109: Loss:1.5338, Accuracy:0.3078, Validation Loss:1.5777, Validation Accuracy:0.2816\n",
    "Epoch #110: Loss:1.5309, Accuracy:0.3099, Validation Loss:1.5797, Validation Accuracy:0.2841\n",
    "Epoch #111: Loss:1.5305, Accuracy:0.3118, Validation Loss:1.5793, Validation Accuracy:0.2783\n",
    "Epoch #112: Loss:1.5358, Accuracy:0.3063, Validation Loss:1.5807, Validation Accuracy:0.2783\n",
    "Epoch #113: Loss:1.5291, Accuracy:0.3082, Validation Loss:1.5791, Validation Accuracy:0.2853\n",
    "Epoch #114: Loss:1.5259, Accuracy:0.3140, Validation Loss:1.5758, Validation Accuracy:0.2759\n",
    "Epoch #115: Loss:1.5244, Accuracy:0.3146, Validation Loss:1.5836, Validation Accuracy:0.2701\n",
    "Epoch #116: Loss:1.5301, Accuracy:0.3103, Validation Loss:1.5794, Validation Accuracy:0.2800\n",
    "Epoch #117: Loss:1.5268, Accuracy:0.3118, Validation Loss:1.5752, Validation Accuracy:0.2791\n",
    "Epoch #118: Loss:1.5374, Accuracy:0.3060, Validation Loss:1.5808, Validation Accuracy:0.2828\n",
    "Epoch #119: Loss:1.5291, Accuracy:0.3115, Validation Loss:1.5786, Validation Accuracy:0.2726\n",
    "Epoch #120: Loss:1.5275, Accuracy:0.3078, Validation Loss:1.5764, Validation Accuracy:0.2755\n",
    "Epoch #121: Loss:1.5179, Accuracy:0.3192, Validation Loss:1.5706, Validation Accuracy:0.2906\n",
    "Epoch #122: Loss:1.5150, Accuracy:0.3213, Validation Loss:1.5712, Validation Accuracy:0.2828\n",
    "Epoch #123: Loss:1.5205, Accuracy:0.3138, Validation Loss:1.5730, Validation Accuracy:0.2804\n",
    "Epoch #124: Loss:1.5177, Accuracy:0.3231, Validation Loss:1.5743, Validation Accuracy:0.2828\n",
    "Epoch #125: Loss:1.5178, Accuracy:0.3255, Validation Loss:1.5686, Validation Accuracy:0.2931\n",
    "Epoch #126: Loss:1.5149, Accuracy:0.3272, Validation Loss:1.5740, Validation Accuracy:0.2882\n",
    "Epoch #127: Loss:1.5118, Accuracy:0.3305, Validation Loss:1.5711, Validation Accuracy:0.2935\n",
    "Epoch #128: Loss:1.5125, Accuracy:0.3261, Validation Loss:1.5726, Validation Accuracy:0.2898\n",
    "Epoch #129: Loss:1.5116, Accuracy:0.3268, Validation Loss:1.5705, Validation Accuracy:0.2927\n",
    "Epoch #130: Loss:1.5074, Accuracy:0.3350, Validation Loss:1.5742, Validation Accuracy:0.2890\n",
    "Epoch #131: Loss:1.5133, Accuracy:0.3262, Validation Loss:1.5766, Validation Accuracy:0.2943\n",
    "Epoch #132: Loss:1.5219, Accuracy:0.3202, Validation Loss:1.5703, Validation Accuracy:0.2898\n",
    "Epoch #133: Loss:1.5119, Accuracy:0.3225, Validation Loss:1.5677, Validation Accuracy:0.2906\n",
    "Epoch #134: Loss:1.5072, Accuracy:0.3270, Validation Loss:1.5698, Validation Accuracy:0.2837\n",
    "Epoch #135: Loss:1.5108, Accuracy:0.3219, Validation Loss:1.5680, Validation Accuracy:0.2894\n",
    "Epoch #136: Loss:1.5032, Accuracy:0.3328, Validation Loss:1.5685, Validation Accuracy:0.2882\n",
    "Epoch #137: Loss:1.5000, Accuracy:0.3294, Validation Loss:1.5678, Validation Accuracy:0.2964\n",
    "Epoch #138: Loss:1.5043, Accuracy:0.3299, Validation Loss:1.5706, Validation Accuracy:0.2861\n",
    "Epoch #139: Loss:1.5010, Accuracy:0.3329, Validation Loss:1.5671, Validation Accuracy:0.2845\n",
    "Epoch #140: Loss:1.4947, Accuracy:0.3360, Validation Loss:1.5681, Validation Accuracy:0.2853\n",
    "Epoch #141: Loss:1.4989, Accuracy:0.3340, Validation Loss:1.5641, Validation Accuracy:0.2902\n",
    "Epoch #142: Loss:1.4942, Accuracy:0.3382, Validation Loss:1.5655, Validation Accuracy:0.2947\n",
    "Epoch #143: Loss:1.4925, Accuracy:0.3386, Validation Loss:1.5678, Validation Accuracy:0.2943\n",
    "Epoch #144: Loss:1.4995, Accuracy:0.3290, Validation Loss:1.5706, Validation Accuracy:0.3005\n",
    "Epoch #145: Loss:1.4917, Accuracy:0.3399, Validation Loss:1.5679, Validation Accuracy:0.2993\n",
    "Epoch #146: Loss:1.4904, Accuracy:0.3420, Validation Loss:1.5665, Validation Accuracy:0.2968\n",
    "Epoch #147: Loss:1.4925, Accuracy:0.3384, Validation Loss:1.5681, Validation Accuracy:0.2915\n",
    "Epoch #148: Loss:1.4904, Accuracy:0.3440, Validation Loss:1.5624, Validation Accuracy:0.2947\n",
    "Epoch #149: Loss:1.4909, Accuracy:0.3384, Validation Loss:1.5681, Validation Accuracy:0.2927\n",
    "Epoch #150: Loss:1.4956, Accuracy:0.3351, Validation Loss:1.5752, Validation Accuracy:0.2820\n",
    "Epoch #151: Loss:1.4957, Accuracy:0.3405, Validation Loss:1.5712, Validation Accuracy:0.2878\n",
    "Epoch #152: Loss:1.5003, Accuracy:0.3366, Validation Loss:1.5777, Validation Accuracy:0.2804\n",
    "Epoch #153: Loss:1.4911, Accuracy:0.3379, Validation Loss:1.5675, Validation Accuracy:0.2947\n",
    "Epoch #154: Loss:1.4915, Accuracy:0.3373, Validation Loss:1.5617, Validation Accuracy:0.2952\n",
    "Epoch #155: Loss:1.4982, Accuracy:0.3305, Validation Loss:1.5647, Validation Accuracy:0.2972\n",
    "Epoch #156: Loss:1.4855, Accuracy:0.3431, Validation Loss:1.5678, Validation Accuracy:0.2919\n",
    "Epoch #157: Loss:1.4863, Accuracy:0.3456, Validation Loss:1.5621, Validation Accuracy:0.3017\n",
    "Epoch #158: Loss:1.4801, Accuracy:0.3385, Validation Loss:1.5717, Validation Accuracy:0.2911\n",
    "Epoch #159: Loss:1.4836, Accuracy:0.3441, Validation Loss:1.5861, Validation Accuracy:0.2861\n",
    "Epoch #160: Loss:1.5028, Accuracy:0.3264, Validation Loss:1.5640, Validation Accuracy:0.2943\n",
    "Epoch #161: Loss:1.4811, Accuracy:0.3447, Validation Loss:1.5659, Validation Accuracy:0.2894\n",
    "Epoch #162: Loss:1.4779, Accuracy:0.3460, Validation Loss:1.5660, Validation Accuracy:0.2894\n",
    "Epoch #163: Loss:1.4761, Accuracy:0.3446, Validation Loss:1.5648, Validation Accuracy:0.2874\n",
    "Epoch #164: Loss:1.4830, Accuracy:0.3392, Validation Loss:1.5606, Validation Accuracy:0.2902\n",
    "Epoch #165: Loss:1.4832, Accuracy:0.3425, Validation Loss:1.5623, Validation Accuracy:0.2878\n",
    "Epoch #166: Loss:1.4758, Accuracy:0.3511, Validation Loss:1.5646, Validation Accuracy:0.2927\n",
    "Epoch #167: Loss:1.4711, Accuracy:0.3513, Validation Loss:1.5654, Validation Accuracy:0.2902\n",
    "Epoch #168: Loss:1.4720, Accuracy:0.3538, Validation Loss:1.5629, Validation Accuracy:0.2886\n",
    "Epoch #169: Loss:1.4744, Accuracy:0.3494, Validation Loss:1.5666, Validation Accuracy:0.2878\n",
    "Epoch #170: Loss:1.4716, Accuracy:0.3576, Validation Loss:1.5682, Validation Accuracy:0.2886\n",
    "Epoch #171: Loss:1.4795, Accuracy:0.3461, Validation Loss:1.5737, Validation Accuracy:0.2841\n",
    "Epoch #172: Loss:1.4784, Accuracy:0.3423, Validation Loss:1.5651, Validation Accuracy:0.3001\n",
    "Epoch #173: Loss:1.4768, Accuracy:0.3465, Validation Loss:1.5615, Validation Accuracy:0.2980\n",
    "Epoch #174: Loss:1.4717, Accuracy:0.3511, Validation Loss:1.5623, Validation Accuracy:0.2939\n",
    "Epoch #175: Loss:1.4822, Accuracy:0.3426, Validation Loss:1.5738, Validation Accuracy:0.2857\n",
    "Epoch #176: Loss:1.4748, Accuracy:0.3445, Validation Loss:1.5657, Validation Accuracy:0.2923\n",
    "Epoch #177: Loss:1.4732, Accuracy:0.3511, Validation Loss:1.5640, Validation Accuracy:0.2931\n",
    "Epoch #178: Loss:1.4649, Accuracy:0.3565, Validation Loss:1.5694, Validation Accuracy:0.2964\n",
    "Epoch #179: Loss:1.4630, Accuracy:0.3579, Validation Loss:1.5691, Validation Accuracy:0.2927\n",
    "Epoch #180: Loss:1.4692, Accuracy:0.3527, Validation Loss:1.5714, Validation Accuracy:0.2923\n",
    "Epoch #181: Loss:1.4681, Accuracy:0.3590, Validation Loss:1.5824, Validation Accuracy:0.2911\n",
    "Epoch #182: Loss:1.4778, Accuracy:0.3468, Validation Loss:1.5644, Validation Accuracy:0.2923\n",
    "Epoch #183: Loss:1.4580, Accuracy:0.3608, Validation Loss:1.5694, Validation Accuracy:0.2857\n",
    "Epoch #184: Loss:1.4602, Accuracy:0.3564, Validation Loss:1.5692, Validation Accuracy:0.2898\n",
    "Epoch #185: Loss:1.4658, Accuracy:0.3549, Validation Loss:1.5712, Validation Accuracy:0.2845\n",
    "Epoch #186: Loss:1.4651, Accuracy:0.3595, Validation Loss:1.5642, Validation Accuracy:0.2939\n",
    "Epoch #187: Loss:1.4554, Accuracy:0.3601, Validation Loss:1.5662, Validation Accuracy:0.2984\n",
    "Epoch #188: Loss:1.4518, Accuracy:0.3638, Validation Loss:1.5699, Validation Accuracy:0.2923\n",
    "Epoch #189: Loss:1.4511, Accuracy:0.3649, Validation Loss:1.5730, Validation Accuracy:0.2865\n",
    "Epoch #190: Loss:1.4539, Accuracy:0.3661, Validation Loss:1.5722, Validation Accuracy:0.2894\n",
    "Epoch #191: Loss:1.4506, Accuracy:0.3639, Validation Loss:1.5760, Validation Accuracy:0.2824\n",
    "Epoch #192: Loss:1.4598, Accuracy:0.3602, Validation Loss:1.5711, Validation Accuracy:0.2869\n",
    "Epoch #193: Loss:1.4502, Accuracy:0.3653, Validation Loss:1.5744, Validation Accuracy:0.2841\n",
    "Epoch #194: Loss:1.4522, Accuracy:0.3626, Validation Loss:1.5680, Validation Accuracy:0.2915\n",
    "Epoch #195: Loss:1.4456, Accuracy:0.3637, Validation Loss:1.5653, Validation Accuracy:0.2964\n",
    "Epoch #196: Loss:1.4478, Accuracy:0.3666, Validation Loss:1.5694, Validation Accuracy:0.2911\n",
    "Epoch #197: Loss:1.4453, Accuracy:0.3707, Validation Loss:1.5686, Validation Accuracy:0.2939\n",
    "Epoch #198: Loss:1.4491, Accuracy:0.3676, Validation Loss:1.5689, Validation Accuracy:0.2894\n",
    "Epoch #199: Loss:1.4525, Accuracy:0.3584, Validation Loss:1.5735, Validation Accuracy:0.2804\n",
    "Epoch #200: Loss:1.4606, Accuracy:0.3611, Validation Loss:1.5687, Validation Accuracy:0.2898\n",
    "Epoch #201: Loss:1.4526, Accuracy:0.3646, Validation Loss:1.5703, Validation Accuracy:0.2906\n",
    "Epoch #202: Loss:1.4440, Accuracy:0.3687, Validation Loss:1.5700, Validation Accuracy:0.2915\n",
    "Epoch #203: Loss:1.4404, Accuracy:0.3716, Validation Loss:1.5685, Validation Accuracy:0.3001\n",
    "Epoch #204: Loss:1.4582, Accuracy:0.3582, Validation Loss:1.5684, Validation Accuracy:0.2919\n",
    "Epoch #205: Loss:1.4535, Accuracy:0.3630, Validation Loss:1.5732, Validation Accuracy:0.2874\n",
    "Epoch #206: Loss:1.4461, Accuracy:0.3626, Validation Loss:1.5730, Validation Accuracy:0.2906\n",
    "Epoch #207: Loss:1.4389, Accuracy:0.3681, Validation Loss:1.5750, Validation Accuracy:0.2915\n",
    "Epoch #208: Loss:1.4392, Accuracy:0.3700, Validation Loss:1.5689, Validation Accuracy:0.2989\n",
    "Epoch #209: Loss:1.4424, Accuracy:0.3652, Validation Loss:1.5795, Validation Accuracy:0.2861\n",
    "Epoch #210: Loss:1.4439, Accuracy:0.3637, Validation Loss:1.5697, Validation Accuracy:0.2984\n",
    "Epoch #211: Loss:1.4417, Accuracy:0.3732, Validation Loss:1.5762, Validation Accuracy:0.2898\n",
    "Epoch #212: Loss:1.4464, Accuracy:0.3681, Validation Loss:1.5734, Validation Accuracy:0.2894\n",
    "Epoch #213: Loss:1.4514, Accuracy:0.3602, Validation Loss:1.5732, Validation Accuracy:0.2939\n",
    "Epoch #214: Loss:1.4349, Accuracy:0.3699, Validation Loss:1.5704, Validation Accuracy:0.2968\n",
    "Epoch #215: Loss:1.4338, Accuracy:0.3748, Validation Loss:1.5734, Validation Accuracy:0.2935\n",
    "Epoch #216: Loss:1.4414, Accuracy:0.3665, Validation Loss:1.5796, Validation Accuracy:0.2960\n",
    "Epoch #217: Loss:1.4474, Accuracy:0.3675, Validation Loss:1.5773, Validation Accuracy:0.2923\n",
    "Epoch #218: Loss:1.4388, Accuracy:0.3722, Validation Loss:1.5724, Validation Accuracy:0.2993\n",
    "Epoch #219: Loss:1.4349, Accuracy:0.3738, Validation Loss:1.5732, Validation Accuracy:0.3025\n",
    "Epoch #220: Loss:1.4392, Accuracy:0.3699, Validation Loss:1.5697, Validation Accuracy:0.2964\n",
    "Epoch #221: Loss:1.4329, Accuracy:0.3764, Validation Loss:1.5733, Validation Accuracy:0.2911\n",
    "Epoch #222: Loss:1.4316, Accuracy:0.3737, Validation Loss:1.5751, Validation Accuracy:0.2989\n",
    "Epoch #223: Loss:1.4270, Accuracy:0.3805, Validation Loss:1.5741, Validation Accuracy:0.2976\n",
    "Epoch #224: Loss:1.4333, Accuracy:0.3719, Validation Loss:1.5804, Validation Accuracy:0.2906\n",
    "Epoch #225: Loss:1.4649, Accuracy:0.3533, Validation Loss:1.5794, Validation Accuracy:0.2931\n",
    "Epoch #226: Loss:1.4371, Accuracy:0.3711, Validation Loss:1.5683, Validation Accuracy:0.2964\n",
    "Epoch #227: Loss:1.4287, Accuracy:0.3796, Validation Loss:1.5737, Validation Accuracy:0.2931\n",
    "Epoch #228: Loss:1.4463, Accuracy:0.3606, Validation Loss:1.5781, Validation Accuracy:0.2935\n",
    "Epoch #229: Loss:1.4393, Accuracy:0.3684, Validation Loss:1.5764, Validation Accuracy:0.2989\n",
    "Epoch #230: Loss:1.4226, Accuracy:0.3796, Validation Loss:1.5814, Validation Accuracy:0.2878\n",
    "Epoch #231: Loss:1.4275, Accuracy:0.3783, Validation Loss:1.5767, Validation Accuracy:0.2906\n",
    "Epoch #232: Loss:1.4269, Accuracy:0.3764, Validation Loss:1.5867, Validation Accuracy:0.2902\n",
    "Epoch #233: Loss:1.4248, Accuracy:0.3804, Validation Loss:1.5806, Validation Accuracy:0.2943\n",
    "Epoch #234: Loss:1.4200, Accuracy:0.3871, Validation Loss:1.5797, Validation Accuracy:0.3001\n",
    "Epoch #235: Loss:1.4203, Accuracy:0.3802, Validation Loss:1.5808, Validation Accuracy:0.2906\n",
    "Epoch #236: Loss:1.4363, Accuracy:0.3708, Validation Loss:1.5794, Validation Accuracy:0.2993\n",
    "Epoch #237: Loss:1.4179, Accuracy:0.3846, Validation Loss:1.5804, Validation Accuracy:0.2960\n",
    "Epoch #238: Loss:1.4204, Accuracy:0.3821, Validation Loss:1.5820, Validation Accuracy:0.2984\n",
    "Epoch #239: Loss:1.4155, Accuracy:0.3833, Validation Loss:1.5778, Validation Accuracy:0.2915\n",
    "Epoch #240: Loss:1.4171, Accuracy:0.3820, Validation Loss:1.5805, Validation Accuracy:0.3009\n",
    "Epoch #241: Loss:1.4175, Accuracy:0.3854, Validation Loss:1.5866, Validation Accuracy:0.3017\n",
    "Epoch #242: Loss:1.4145, Accuracy:0.3782, Validation Loss:1.5915, Validation Accuracy:0.2927\n",
    "Epoch #243: Loss:1.4212, Accuracy:0.3804, Validation Loss:1.5811, Validation Accuracy:0.2939\n",
    "Epoch #244: Loss:1.4137, Accuracy:0.3851, Validation Loss:1.5880, Validation Accuracy:0.2980\n",
    "Epoch #245: Loss:1.4083, Accuracy:0.3886, Validation Loss:1.5807, Validation Accuracy:0.2947\n",
    "Epoch #246: Loss:1.4065, Accuracy:0.3880, Validation Loss:1.5893, Validation Accuracy:0.2952\n",
    "Epoch #247: Loss:1.4059, Accuracy:0.3915, Validation Loss:1.5899, Validation Accuracy:0.2964\n",
    "Epoch #248: Loss:1.4080, Accuracy:0.3901, Validation Loss:1.5999, Validation Accuracy:0.2931\n",
    "Epoch #249: Loss:1.4168, Accuracy:0.3760, Validation Loss:1.5903, Validation Accuracy:0.2853\n",
    "Epoch #250: Loss:1.4221, Accuracy:0.3786, Validation Loss:1.5848, Validation Accuracy:0.2947\n",
    "Epoch #251: Loss:1.3990, Accuracy:0.3957, Validation Loss:1.5857, Validation Accuracy:0.2915\n",
    "Epoch #252: Loss:1.4012, Accuracy:0.3913, Validation Loss:1.5822, Validation Accuracy:0.2968\n",
    "Epoch #253: Loss:1.4029, Accuracy:0.3909, Validation Loss:1.5856, Validation Accuracy:0.2956\n",
    "Epoch #254: Loss:1.4001, Accuracy:0.3932, Validation Loss:1.5966, Validation Accuracy:0.2869\n",
    "Epoch #255: Loss:1.4045, Accuracy:0.3912, Validation Loss:1.5848, Validation Accuracy:0.2964\n",
    "Epoch #256: Loss:1.4068, Accuracy:0.3872, Validation Loss:1.5854, Validation Accuracy:0.2972\n",
    "Epoch #257: Loss:1.3945, Accuracy:0.3945, Validation Loss:1.5856, Validation Accuracy:0.2956\n",
    "Epoch #258: Loss:1.4021, Accuracy:0.3948, Validation Loss:1.5906, Validation Accuracy:0.3013\n",
    "Epoch #259: Loss:1.4020, Accuracy:0.3921, Validation Loss:1.5855, Validation Accuracy:0.2984\n",
    "Epoch #260: Loss:1.3971, Accuracy:0.3916, Validation Loss:1.5880, Validation Accuracy:0.2915\n",
    "Epoch #261: Loss:1.3965, Accuracy:0.3902, Validation Loss:1.5880, Validation Accuracy:0.2968\n",
    "Epoch #262: Loss:1.3915, Accuracy:0.4001, Validation Loss:1.5911, Validation Accuracy:0.3013\n",
    "Epoch #263: Loss:1.3923, Accuracy:0.3983, Validation Loss:1.5940, Validation Accuracy:0.2939\n",
    "Epoch #264: Loss:1.4213, Accuracy:0.3787, Validation Loss:1.6123, Validation Accuracy:0.2882\n",
    "Epoch #265: Loss:1.4000, Accuracy:0.3940, Validation Loss:1.5900, Validation Accuracy:0.2956\n",
    "Epoch #266: Loss:1.4036, Accuracy:0.3850, Validation Loss:1.5891, Validation Accuracy:0.3058\n",
    "Epoch #267: Loss:1.3923, Accuracy:0.3953, Validation Loss:1.6002, Validation Accuracy:0.2828\n",
    "Epoch #268: Loss:1.3909, Accuracy:0.3967, Validation Loss:1.5916, Validation Accuracy:0.2956\n",
    "Epoch #269: Loss:1.3858, Accuracy:0.4008, Validation Loss:1.5964, Validation Accuracy:0.3034\n",
    "Epoch #270: Loss:1.4047, Accuracy:0.3909, Validation Loss:1.5939, Validation Accuracy:0.2886\n",
    "Epoch #271: Loss:1.3949, Accuracy:0.3963, Validation Loss:1.5978, Validation Accuracy:0.2923\n",
    "Epoch #272: Loss:1.3866, Accuracy:0.4028, Validation Loss:1.6024, Validation Accuracy:0.2915\n",
    "Epoch #273: Loss:1.3866, Accuracy:0.4004, Validation Loss:1.6066, Validation Accuracy:0.2956\n",
    "Epoch #274: Loss:1.3878, Accuracy:0.4015, Validation Loss:1.5943, Validation Accuracy:0.3042\n",
    "Epoch #275: Loss:1.3838, Accuracy:0.4023, Validation Loss:1.6062, Validation Accuracy:0.3050\n",
    "Epoch #276: Loss:1.3969, Accuracy:0.3955, Validation Loss:1.5940, Validation Accuracy:0.2931\n",
    "Epoch #277: Loss:1.3924, Accuracy:0.3962, Validation Loss:1.5931, Validation Accuracy:0.3017\n",
    "Epoch #278: Loss:1.3908, Accuracy:0.4001, Validation Loss:1.6057, Validation Accuracy:0.2915\n",
    "Epoch #279: Loss:1.3944, Accuracy:0.3920, Validation Loss:1.6010, Validation Accuracy:0.2968\n",
    "Epoch #280: Loss:1.3884, Accuracy:0.4016, Validation Loss:1.5930, Validation Accuracy:0.3001\n",
    "Epoch #281: Loss:1.3906, Accuracy:0.4007, Validation Loss:1.5964, Validation Accuracy:0.2911\n",
    "Epoch #282: Loss:1.3899, Accuracy:0.3978, Validation Loss:1.6079, Validation Accuracy:0.2960\n",
    "Epoch #283: Loss:1.4068, Accuracy:0.3870, Validation Loss:1.6016, Validation Accuracy:0.3001\n",
    "Epoch #284: Loss:1.3831, Accuracy:0.4035, Validation Loss:1.6009, Validation Accuracy:0.2898\n",
    "Epoch #285: Loss:1.3778, Accuracy:0.4068, Validation Loss:1.5995, Validation Accuracy:0.2935\n",
    "Epoch #286: Loss:1.3808, Accuracy:0.4047, Validation Loss:1.6041, Validation Accuracy:0.3001\n",
    "Epoch #287: Loss:1.3847, Accuracy:0.4005, Validation Loss:1.6013, Validation Accuracy:0.2997\n",
    "Epoch #288: Loss:1.3798, Accuracy:0.3991, Validation Loss:1.6024, Validation Accuracy:0.2894\n",
    "Epoch #289: Loss:1.3764, Accuracy:0.4073, Validation Loss:1.6119, Validation Accuracy:0.2915\n",
    "Epoch #290: Loss:1.3769, Accuracy:0.4046, Validation Loss:1.6047, Validation Accuracy:0.2968\n",
    "Epoch #291: Loss:1.3799, Accuracy:0.4044, Validation Loss:1.6090, Validation Accuracy:0.2993\n",
    "Epoch #292: Loss:1.3763, Accuracy:0.4056, Validation Loss:1.6112, Validation Accuracy:0.2947\n",
    "Epoch #293: Loss:1.3826, Accuracy:0.4053, Validation Loss:1.6048, Validation Accuracy:0.2968\n",
    "Epoch #294: Loss:1.3850, Accuracy:0.4045, Validation Loss:1.6034, Validation Accuracy:0.2931\n",
    "Epoch #295: Loss:1.3757, Accuracy:0.4067, Validation Loss:1.6057, Validation Accuracy:0.3054\n",
    "Epoch #296: Loss:1.3754, Accuracy:0.4132, Validation Loss:1.6092, Validation Accuracy:0.2923\n",
    "Epoch #297: Loss:1.3679, Accuracy:0.4140, Validation Loss:1.6203, Validation Accuracy:0.2943\n",
    "Epoch #298: Loss:1.3693, Accuracy:0.4131, Validation Loss:1.6061, Validation Accuracy:0.3042\n",
    "Epoch #299: Loss:1.3760, Accuracy:0.4091, Validation Loss:1.6096, Validation Accuracy:0.2993\n",
    "Epoch #300: Loss:1.3655, Accuracy:0.4100, Validation Loss:1.6113, Validation Accuracy:0.3009\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61129379, Accuracy:0.3009\n",
    "Labels: ['03', '04', '01', '02', '05']\n",
    "Confusion Matrix:\n",
    "       03   04   01  02   05\n",
    "t:03  154   59   74  64  108\n",
    "t:04   89  128   81  59   93\n",
    "t:01  113   83  148  59  100\n",
    "t:02   93   86   90  96   92\n",
    "t:05  119   87   81  73  207\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.27      0.34      0.30       459\n",
    "          04       0.29      0.28      0.29       450\n",
    "          01       0.31      0.29      0.30       503\n",
    "          02       0.27      0.21      0.24       457\n",
    "          05       0.34      0.37      0.35       567\n",
    "\n",
    "    accuracy                           0.30      2436\n",
    "   macro avg       0.30      0.30      0.30      2436\n",
    "weighted avg       0.30      0.30      0.30      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 10:59:02 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 21 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6063289779356156, 1.6053097782463863, 1.605229347601704, 1.6045800304569438, 1.6036199117920473, 1.6028904903111199, 1.6020708342491112, 1.600994689515463, 1.6002857665514516, 1.599897705471183, 1.599752669459689, 1.599117695208645, 1.5988060764491265, 1.5999448256343847, 1.5991516144600604, 1.5994567319090143, 1.5986983116428644, 1.5981419597353255, 1.5984478331551764, 1.598239663003505, 1.5987824031284876, 1.5983136810105423, 1.598024084845983, 1.5982360142988132, 1.5985167226180654, 1.598090209201443, 1.5977818335609875, 1.597618335965036, 1.5967075212052695, 1.5972752792298892, 1.5978818221632483, 1.5980856980400524, 1.5968081716246205, 1.596490701822616, 1.5954395712694316, 1.5968476604358317, 1.5940321537074198, 1.5951832070922225, 1.5943930542527749, 1.5927093723920374, 1.5932258171792493, 1.5996687885967187, 1.5926752039560153, 1.5945105826717683, 1.5950207837500987, 1.590662883616042, 1.5917363990899573, 1.5924432036911913, 1.5926444747765076, 1.5914925058878506, 1.590368111536812, 1.5920203908519401, 1.5953601602654544, 1.590154518634815, 1.5883485368515666, 1.5896880135356108, 1.5886130035412918, 1.5906061613305253, 1.5932581655693367, 1.5951008503072954, 1.590484642825886, 1.586357930219428, 1.5885747982363396, 1.5878542103790885, 1.5887993374481577, 1.5870030607495988, 1.587514390303388, 1.5916691658336346, 1.5859425843055612, 1.5875823908838733, 1.5862824846371053, 1.5858082841769816, 1.5892103278186718, 1.5859095970006607, 1.5960115131681971, 1.5885507210917857, 1.5870028790973483, 1.5877123632650265, 1.5855531248161554, 1.5899933318199195, 1.5851409656465152, 1.583687073099985, 1.5863607560081043, 1.5864588030061895, 1.5935523572617956, 1.5857513806307062, 1.5872811437240375, 1.5893784604832064, 1.5822002911215345, 1.5821654062552992, 1.5833552795873682, 1.583190156908458, 1.5825287015567273, 1.5815877910513791, 1.5831665376137043, 1.5829275139838408, 1.5787286627273058, 1.5798963181099477, 1.5797717044701913, 1.5873658563115913, 1.5819292685081219, 1.5812357944025, 1.5783609506140397, 1.579335211141552, 1.5808828378154336, 1.5824178631474035, 1.5821460244690844, 1.578766141619001, 1.5776927357628232, 1.5796919689194127, 1.579288831876808, 1.580714713176483, 1.5790617283929158, 1.5758327580437872, 1.583619822422272, 1.5793818771741268, 1.575249448002657, 1.5808066975306996, 1.578552329090037, 1.5763879810843757, 1.5705985124475264, 1.5711516171253372, 1.572970405979501, 1.5743409866965659, 1.5686011893800131, 1.5740200083439768, 1.5711359147759298, 1.5725629251383013, 1.5705234002205735, 1.5742086867002039, 1.5765879175737378, 1.570289867264884, 1.5677209358497206, 1.5697954223661, 1.5679928338390656, 1.568528530632921, 1.5678113789002492, 1.570602766007234, 1.567122235478243, 1.5680594641978323, 1.5640615802288838, 1.5654920778055301, 1.567765133721488, 1.5705717565195119, 1.5679410153813353, 1.5665241335021647, 1.5680758800412633, 1.5624347118712802, 1.5681260109730737, 1.5752310731336596, 1.5712492319163431, 1.57769048879495, 1.567498393246693, 1.561691276937087, 1.5646776912247606, 1.5677891828743695, 1.5620909465357589, 1.5717190969949482, 1.5860640979165515, 1.5639974113755626, 1.5659055247878402, 1.5660148915790377, 1.5647829365847734, 1.5606261481988215, 1.5622553077628851, 1.5646119098162221, 1.5653791682082052, 1.562887591485711, 1.566550740076012, 1.5682383288303618, 1.5737489640027627, 1.5650716015858015, 1.5615439503063708, 1.5623213044919795, 1.5737837341618655, 1.5656528020727223, 1.5639795184331182, 1.5693894650157059, 1.569087179423553, 1.571387471236619, 1.582441625532454, 1.5644220405415752, 1.5694334084177253, 1.5691617821237724, 1.5712256155577786, 1.5641744401067348, 1.5662363751964226, 1.569906092629644, 1.572982094362256, 1.572220996487121, 1.5760449997114234, 1.5711374580370774, 1.5744021761006322, 1.567954645368266, 1.5652884006108752, 1.569384715044244, 1.5685968966711135, 1.5688904306571472, 1.5734734826878765, 1.5686597227071502, 1.5702815051932248, 1.5699913689655622, 1.5685053453069602, 1.568425925102923, 1.5731827778182006, 1.5729501940346704, 1.575049637573693, 1.5689422880683235, 1.5795219447616677, 1.5697285083714376, 1.5761816029869669, 1.5734406827118597, 1.5731670979795784, 1.5704432915779953, 1.5733565460089196, 1.5796390724886815, 1.5772951571224945, 1.5724333646066475, 1.5732358549224528, 1.5697304063242645, 1.5732577929551574, 1.5750937086020784, 1.5740921618707466, 1.5804437349974032, 1.5794135191170453, 1.5682871983752071, 1.573721482444475, 1.5781475681389494, 1.5764483587299465, 1.5814290958868067, 1.576665602294095, 1.5866787453198863, 1.5806157242488392, 1.5796948761384084, 1.5808337725246286, 1.5793987543907855, 1.5804222442442168, 1.5819833141633834, 1.5778483894266715, 1.5804985969133174, 1.5866041205003736, 1.5915001754103035, 1.5811299882303123, 1.5880340733160134, 1.5806661464506377, 1.5893195967368892, 1.5898884476112027, 1.5998794652754058, 1.5903342171451338, 1.5848161876690994, 1.5857330543067067, 1.5822493066928658, 1.5856464261491898, 1.5965609992861944, 1.5847803555881643, 1.5854058569092273, 1.5855718445895342, 1.5905593778503744, 1.5854740763336959, 1.588031184301392, 1.5880251909515932, 1.5911044757354436, 1.5939945190019404, 1.6123323894682384, 1.5899671639127684, 1.5891135745056353, 1.6001541389424616, 1.5916241440671222, 1.5963627434716436, 1.5939049757955892, 1.5977517098237337, 1.6023510528119718, 1.6066050445309217, 1.5943147004727267, 1.606172336341908, 1.5940182625953787, 1.5930534751935936, 1.6056850815837216, 1.6009656474703835, 1.5930407597317875, 1.5964311917231393, 1.6078511999158436, 1.6016089366183102, 1.6009325119857913, 1.5995166272365402, 1.6040559688029423, 1.601297236819964, 1.6024124076213744, 1.6118629622733456, 1.604693792136432, 1.6090464225935035, 1.6111693004473482, 1.6048127698585122, 1.6034336516814083, 1.6056802719097418, 1.6092335788291467, 1.6203337759024208, 1.6060947076048953, 1.609636103186897, 1.6112938146481568], 'val_acc': [0.23275862064071867, 0.23275862064071867, 0.23275862064071867, 0.23275862064071867, 0.23275862064071867, 0.23275862064071867, 0.2335796387275843, 0.24137930804481256, 0.2430213465674953, 0.24220032610720993, 0.2430213466164318, 0.2430213466164318, 0.24220032615614642, 0.24343185564763048, 0.24384236228094117, 0.24343185564763048, 0.24712643702629164, 0.24671592559720495, 0.24671592794615646, 0.24178981707601124, 0.2483579616730632, 0.24220032610720993, 0.24794745499081605, 0.24794745259292802, 0.24425287366109136, 0.243431853200806, 0.24302134651855883, 0.243431853200806, 0.25041050682905663, 0.24425287366109136, 0.25246305443187456, 0.2413793103448276, 0.25369457912758264, 0.25205254305172436, 0.25328407014532045, 0.25328407004744746, 0.24548439830786292, 0.24876847280853095, 0.23850574712643677, 0.26067323476222937, 0.2619047618558254, 0.2508210159091918, 0.25574712408783007, 0.2639573047607403, 0.25369458137866113, 0.26724137935928133, 0.2623152709848971, 0.2631362867472794, 0.25903119861296636, 0.25533661723430523, 0.25862068965517243, 0.24671592755466454, 0.2512315267265724, 0.27175697875140337, 0.27052545175568027, 0.2651888341543514, 0.26190475960474685, 0.25985221657749075, 0.25821018040375954, 0.27175697848225266, 0.26683087037701914, 0.27175697880033983, 0.2610837437200233, 0.2754515599343186, 0.26929392211738673, 0.27339901502300756, 0.2725779968627372, 0.2705254517067438, 0.27380952150950877, 0.2725779969116737, 0.28160919545123536, 0.2725779966669913, 0.2655993430876771, 0.2746305418229847, 0.26559934328342305, 0.272167485384714, 0.26724137906566237, 0.28366173840508674, 0.26929392441740174, 0.2729885057226582, 0.28078817743777446, 0.28366174053382404, 0.28530377668308704, 0.27463053962084266, 0.2775041049435025, 0.28160919540229884, 0.2762725780456524, 0.2684729062081949, 0.28160919545123536, 0.27627257782543824, 0.2857142857632222, 0.280788177388838, 0.29022988275745626, 0.27545155988538206, 0.2754515599343186, 0.28776682876601006, 0.28325122927601504, 0.27873563213497154, 0.2910509031688051, 0.2816091953288941, 0.29022988270851974, 0.2766830869300416, 0.2910509032666781, 0.2848932677497613, 0.2783251208037578, 0.2902298852042807, 0.28407224946714976, 0.27668308678323217, 0.28160919530442585, 0.2840722473384124, 0.2783251231282411, 0.2783251232016459, 0.2853037766341505, 0.27586206876977126, 0.27011494247979917, 0.27996715925303584, 0.27914613906190117, 0.28284072044056235, 0.272577996642523, 0.2754515598609138, 0.2906403941865429, 0.2828407225448314, 0.2803776683087028, 0.2828407224958949, 0.29310344842267155, 0.28817733765039927, 0.2935139575517432, 0.28981937397094, 0.29269293704252136, 0.2889983558106696, 0.2943349730694431, 0.2898193760752091, 0.29064039423547944, 0.2836617383561502, 0.28940886723975634, 0.2881773376993358, 0.2963875205009833, 0.28612479259227885, 0.2844827564674841, 0.28530377448094496, 0.2902298828063927, 0.29474548224745123, 0.2943349755652041, 0.30049261113105735, 0.2992610815906368, 0.29679802745238115, 0.29146141004679826, 0.29474548469427575, 0.2926929371403943, 0.28201970232922846, 0.28776683106602513, 0.28037766850444873, 0.29474548229638775, 0.29515599137652293, 0.2972085388325314, 0.29187191893118747, 0.30172413582676544, 0.2910509033156146, 0.2861247950391033, 0.29433497331412556, 0.28940886714188335, 0.2894088648908048, 0.28735631978374787, 0.2902298828553292, 0.28776683116389806, 0.29269293953828235, 0.2902298853021537, 0.28858784917735897, 0.28776683116389806, 0.28858784917735897, 0.28407224973630046, 0.3000820997019707, 0.29802955694386524, 0.2939244665829419, 0.2857142861547141, 0.29228243050708363, 0.293103448471608, 0.2963875208190705, 0.2926929371403943, 0.29228242801132265, 0.29105090346242407, 0.2922824303602742, 0.28571428346320715, 0.28981937612414554, 0.28448275871856266, 0.29392446428292685, 0.2984400636261124, 0.2922824306538931, 0.2865353016234775, 0.28940886484186834, 0.28243021140936364, 0.2869458131015007, 0.28407224978523693, 0.2914614099489253, 0.2963875183233095, 0.29105090091772656, 0.2939244665340054, 0.2894088672886928, 0.2803776684065758, 0.289819373873067, 0.29064039408866993, 0.29146140999786174, 0.3000821019530492, 0.29187191893118747, 0.28735632181461224, 0.29064039173971845, 0.29146141014467125, 0.29885057255943814, 0.28612479254334233, 0.29844006343036644, 0.28981937622201853, 0.28940886474399535, 0.29392446418505386, 0.2967980275013177, 0.29351395510491873, 0.2959770093410473, 0.29228243040921065, 0.2992610840374613, 0.3025451562381143, 0.29638752067226104, 0.29105090101559955, 0.29885057485945316, 0.29761904546584206, 0.2906403918865279, 0.29310344842267155, 0.29638752067226104, 0.29310344842267155, 0.29351395769855265, 0.2988505727062476, 0.2877668287170736, 0.2906403917886549, 0.2902298829042657, 0.2943349754673311, 0.3000820997019707, 0.2906403940152652, 0.2992610840863978, 0.29597701168999885, 0.29844006597506395, 0.2914614098510523, 0.30090311771543155, 0.30172413822465344, 0.2926929371403943, 0.29392446418505386, 0.2980295546438502, 0.29474548459640276, 0.29515599127864994, 0.29638751837224603, 0.2931034486184175, 0.2853037744320085, 0.29474548224745123, 0.2914614123468133, 0.2967980297034597, 0.2955665025119907, 0.28694581295469124, 0.296387520770134, 0.2972085363367704, 0.2955665027566731, 0.3013136268445032, 0.2984400660240004, 0.29146141249362273, 0.2967980298502692, 0.3013136268934397, 0.2939244641361174, 0.2881773398770096, 0.2955665027566731, 0.30582922618768876, 0.2828407225937679, 0.29556650026091214, 0.30336617439838465, 0.2885878491284225, 0.29228243045814717, 0.2914614122489403, 0.2955665004077216, 0.3041871901118305, 0.3050082082231644, 0.29310344837373503, 0.301724137979971, 0.2914614123957498, 0.2967980297523962, 0.3000820996040977, 0.2910509033156146, 0.29597701168999885, 0.30008210224666815, 0.289819376368828, 0.2935139551538552, 0.3000820996040977, 0.29967159071970845, 0.2894088649886778, 0.2914614098999888, 0.2967980273055717, 0.2992610816395733, 0.29474548474321227, 0.2967980297523962, 0.29310344852054454, 0.3054187170096806, 0.29228243045814717, 0.2943349755162676, 0.304187190062894, 0.2992610839885248, 0.30090311761755856], 'loss': [1.6108562714265358, 1.6056145157412582, 1.6053915356708504, 1.6052080659650925, 1.6039483248085946, 1.6034781386964865, 1.6030567060995395, 1.601865462209165, 1.6008629236377974, 1.6001649457081633, 1.6009378540442465, 1.599591825042662, 1.5988654334442327, 1.5996994475319646, 1.599362165237599, 1.599155230835478, 1.5985945468076201, 1.5979146604420469, 1.5976953324840788, 1.5979649219180034, 1.5974822375808653, 1.596899900152453, 1.5970237284959954, 1.596337166312294, 1.5962842377793862, 1.595692622783982, 1.5956863023417196, 1.59565522984068, 1.5945596665572337, 1.5943459855457596, 1.5951261489053525, 1.5991417260630174, 1.5953940487005873, 1.5939177170426448, 1.5924096294986638, 1.592146154742466, 1.5920138079038146, 1.5910532146998255, 1.5892581876543268, 1.5888723626029075, 1.5865839333015301, 1.5873744525459024, 1.5896690237938256, 1.5869827941702621, 1.5850542772966734, 1.5865268719514538, 1.5847715812052545, 1.581783216347195, 1.582893757310981, 1.5819791365453104, 1.5793558510421972, 1.579715481626914, 1.5784627293169622, 1.5872722477393963, 1.578574531376974, 1.5776690377347034, 1.5770190471496426, 1.5775320425660213, 1.5751164783442535, 1.5768840623342526, 1.579408246878481, 1.5734022454314653, 1.5726499416989712, 1.5713022470963809, 1.5701406620366372, 1.569534580565576, 1.5675616325538995, 1.5695157685074228, 1.568246866937046, 1.565487572544654, 1.5674095546929987, 1.5646619364466265, 1.5631285207227514, 1.5670434834286417, 1.5680892682418197, 1.5683233327689357, 1.563437930710262, 1.5616341964420106, 1.5595870500227753, 1.564796102512054, 1.5619241884846462, 1.5582696591559377, 1.5568152163797335, 1.5565069296521572, 1.5586181605865823, 1.5599980261781132, 1.5606725984530283, 1.5548635266889537, 1.5544264407128523, 1.5524471538022804, 1.5514736945134657, 1.5484624584597484, 1.5493060611846265, 1.5478009223448423, 1.5502174418075374, 1.544980135884373, 1.5464673726220886, 1.5545125209330533, 1.5532047803886617, 1.5452345536230037, 1.5414303277797032, 1.5417676173686001, 1.5427729585577086, 1.5422892702188824, 1.5408427981135782, 1.5447763670151728, 1.5361601660628583, 1.534850858809767, 1.5338201635916864, 1.5308917518513894, 1.5305009090435333, 1.5358424191112636, 1.52908671435879, 1.5259360208158865, 1.5244304162031326, 1.5301361362547357, 1.5267584676860049, 1.5373817163815977, 1.5291220188630434, 1.5275067982977175, 1.5179460348779417, 1.5150015256732885, 1.520485340985919, 1.5177268770930703, 1.5177958799828248, 1.514910226142382, 1.511750903854135, 1.5124827921023358, 1.511647423190013, 1.507385705628679, 1.5132589712280022, 1.5219337244053397, 1.5119126604323025, 1.5072413913033582, 1.5107561892797325, 1.503166933079275, 1.5000367750622163, 1.504299711153003, 1.5010006389578754, 1.494670843588498, 1.4989375288726368, 1.494249631785759, 1.492506772487805, 1.49953424622146, 1.49167315191312, 1.4904485199240933, 1.4925286948558487, 1.4904013232284012, 1.490876552701241, 1.4956113411415775, 1.4956934746775539, 1.5002852195097436, 1.4910818133266066, 1.4915230463662432, 1.4982356353951676, 1.4855349171088217, 1.4863027677888498, 1.4801305292568168, 1.4836492967801418, 1.502770297042643, 1.4811182544951076, 1.47791790692958, 1.4761106663416055, 1.4829595347939086, 1.4832458242988196, 1.4758310932887897, 1.4710715021196088, 1.4720018794159624, 1.474359968996146, 1.4715754205930893, 1.4795202874060285, 1.47839098529894, 1.4768078791776966, 1.4716822555667322, 1.482200795620129, 1.4748361799506435, 1.4731513334740358, 1.4649414160413174, 1.4630253869650056, 1.4691545689620031, 1.468111943660086, 1.4777608951259198, 1.4580101332870108, 1.4601697148238855, 1.4657814386688952, 1.4651143555278896, 1.4554108335741736, 1.4517671003478754, 1.4510960421767813, 1.4538721556536227, 1.4506074317916462, 1.4598391761035645, 1.450195934591352, 1.4522397546552779, 1.4456463097057304, 1.4477810583565025, 1.4452910926552525, 1.4491437301498664, 1.4525298762859995, 1.4605899652661238, 1.4525800812660545, 1.4440310879164897, 1.4404431721513031, 1.4581853127822249, 1.4534558688345875, 1.446068032076716, 1.4389015311822755, 1.4391867995506928, 1.4424082952358395, 1.4439138201472697, 1.4417056555131134, 1.4464459104459633, 1.451404948352054, 1.4348721399933895, 1.4337871845253194, 1.4413821433848668, 1.4473883981332643, 1.4387702131663016, 1.434871716910564, 1.4391967969263848, 1.432873930333821, 1.4315952817517383, 1.4269547346436267, 1.4333489905637393, 1.4648835642871425, 1.4370919582535353, 1.4286533787020423, 1.4463203273514702, 1.439346033249058, 1.4226318075426796, 1.4274867804388247, 1.4268524250700243, 1.4248457084201445, 1.4199959294262363, 1.4203047781754323, 1.4363416234570607, 1.4179477718576514, 1.420392683839896, 1.4154606052982244, 1.4171094237901347, 1.4175071457328248, 1.4145054050049988, 1.4211870869082348, 1.4136765861413316, 1.408336564109066, 1.4064523606819295, 1.405894748337215, 1.40797158501721, 1.416833182967419, 1.4221062533419724, 1.399024307752292, 1.401234315014473, 1.4028520476401953, 1.4001267785164364, 1.4044650434223778, 1.4067511617770185, 1.3945279810218105, 1.4021153259571084, 1.4020255307642098, 1.3971259796154327, 1.396538986170806, 1.3914942211194206, 1.3922640845026568, 1.4212932479454996, 1.4000336027732865, 1.4035700260980908, 1.3922780505440808, 1.3908815067162015, 1.3858036089726786, 1.4046508616735314, 1.394913239694474, 1.386593434844908, 1.3866024760984543, 1.387848662399903, 1.3838071570504127, 1.396882660481964, 1.3924172187487938, 1.3907703985178985, 1.3943780645942296, 1.3883796615032689, 1.3906327863003929, 1.3899349338464912, 1.4068217073377887, 1.3831128666288308, 1.37780050790775, 1.3808477966202848, 1.3847325434674962, 1.379755944637792, 1.3763885026594942, 1.3769059099455878, 1.3799298378476372, 1.3762796812723306, 1.382566027719627, 1.3849656674406612, 1.3756842868284034, 1.3753863480056825, 1.367912639142062, 1.3693436441969822, 1.3759968614676161, 1.3655258131223047], 'acc': [0.20934291581414807, 0.23295687885622224, 0.23295687885010266, 0.2354209445707607, 0.23295687886234182, 0.23295687885316246, 0.23788501026694045, 0.24055441478439424, 0.24291581109441526, 0.24353182752763955, 0.2403490759875985, 0.2446611909650924, 0.24476386037266964, 0.244763860381849, 0.2429158110913555, 0.2444558521560575, 0.24486652977412732, 0.24363449691991787, 0.2451745379999188, 0.2406570841889117, 0.24394250513347024, 0.2418891170553603, 0.24445585215911728, 0.24137577002665345, 0.24414784394250513, 0.2445585215728141, 0.2420944558399169, 0.24281314167153908, 0.24425051334702258, 0.2432238193140872, 0.2429158110913555, 0.23480492813753642, 0.24404517455022684, 0.24373716633667447, 0.24363449691991787, 0.24178644763860369, 0.24897330595788525, 0.24373716632443532, 0.24958932238804976, 0.25297741273100616, 0.2542094455882754, 0.2512320328664486, 0.2528747433295485, 0.25492813142295734, 0.26067761805757605, 0.2606776180698152, 0.25400410678841984, 0.2652977412853398, 0.26365503080082137, 0.2657084188911704, 0.2673511293634497, 0.26519507187470276, 0.26591375770020537, 0.25687885009043027, 0.271457905556387, 0.27022587270217757, 0.2744353182812736, 0.26334702259338855, 0.26858316219318085, 0.2700205338931426, 0.2726899384105964, 0.2795687885071463, 0.27761806981519505, 0.2742299794661191, 0.27731006160164273, 0.2802874743326489, 0.2809034907842319, 0.27802874743938444, 0.2809034907658732, 0.28685831622176594, 0.27915811088601666, 0.2843942505378253, 0.2850102669526909, 0.28008213552361394, 0.2773100615894036, 0.28193018481716725, 0.2809034907658732, 0.28347022587268994, 0.2887063655030801, 0.28675564681724847, 0.2883983572925875, 0.2857289527781935, 0.28778234085018384, 0.29055441478439425, 0.2888090349075975, 0.28418891169207294, 0.2834702258757497, 0.295277207379958, 0.2916837782463254, 0.2972279260657896, 0.2919917864476386, 0.3048254620000812, 0.297125256649033, 0.2987679671580297, 0.29537987679671457, 0.30215605750710567, 0.2989733059425863, 0.29127310060377726, 0.2925051334702259, 0.3046201232032854, 0.3085215605871878, 0.30759753594041117, 0.30041067761806983, 0.3018480492935533, 0.30821355234915715, 0.29209445585215604, 0.3056468172606991, 0.3110882956940046, 0.30780287473108736, 0.30985626283367557, 0.3118069815195072, 0.3062628336878038, 0.3082135523613963, 0.31396303900213457, 0.31457905544147846, 0.31026694043950626, 0.3118069814950289, 0.3059548254620123, 0.311498973318194, 0.3078028747433265, 0.31919917865088343, 0.32125256674123254, 0.31375770020533883, 0.3231006160225467, 0.32546201229585026, 0.32720739219712525, 0.3304928131294446, 0.32607802874743325, 0.3267967145912946, 0.33501026696493, 0.32618069813971157, 0.32022587269605796, 0.3224845995893224, 0.32700205341256866, 0.3218685831683372, 0.3327515400288286, 0.3293634497042309, 0.3298767967268182, 0.332854209448645, 0.33603696099786545, 0.33398357288303804, 0.3381930184835526, 0.33860369611080177, 0.32895277206168283, 0.3399383983450504, 0.3419917864476386, 0.3383983572895277, 0.3440451745502268, 0.3383983572895277, 0.33511293634802897, 0.34045174537987677, 0.3365503080204527, 0.3378850102791796, 0.33726899382759656, 0.3304928131294446, 0.3431211498973306, 0.3455852155812712, 0.3385010266940452, 0.3441478439547443, 0.32638603694874646, 0.3446611909681522, 0.34599589323605845, 0.34455852156669453, 0.3392197125287271, 0.3425051334579867, 0.35112936346193113, 0.351334702270966, 0.35379876796714577, 0.34938398356065614, 0.3575975359465307, 0.34609856262833677, 0.34229979467343014, 0.34650924024640656, 0.3511293634374528, 0.3426078028992216, 0.34445585214381835, 0.35112936346193113, 0.3564681724968387, 0.35790554413560477, 0.3526694045174538, 0.35903490758529677, 0.3468172484599589, 0.36078028744985435, 0.35636550308008214, 0.3549281314045986, 0.35954825460788403, 0.36006160163047135, 0.363757700217578, 0.3648870636305525, 0.36611909650924024, 0.3638603696220954, 0.36016427103498877, 0.3652977412731006, 0.36262833674340766, 0.36365503077634304, 0.3666324435318275, 0.37073921971252566, 0.36755646818472376, 0.3584188911581921, 0.3610882957001242, 0.36457905544147845, 0.36868583160993745, 0.3715605749486653, 0.35821355234915714, 0.36303901434923835, 0.3626283367311685, 0.3680698151828327, 0.3700205338809035, 0.3651950718685832, 0.36365503080082134, 0.37320328542094455, 0.36806981519507187, 0.36016427103498877, 0.3699178644886252, 0.3748459958932238, 0.36652977413342963, 0.3674537987434889, 0.3721765913635309, 0.37381930186028844, 0.36991786448250563, 0.37638603697322476, 0.3737166324435318, 0.3804928131539229, 0.3718685831744568, 0.35328542095679766, 0.37114989733671505, 0.3795687885010267, 0.36057494864081946, 0.3683778233963851, 0.37956878851326586, 0.3783367556590564, 0.3763860369609856, 0.38039014373716634, 0.3870636550308008, 0.3801848049281314, 0.3708418891292823, 0.38459958931014276, 0.3821355236262022, 0.38326488705141587, 0.38203285419720645, 0.3854209445462824, 0.3782340862422998, 0.3803901437494055, 0.3851129363449692, 0.3886036960985626, 0.3879876796714579, 0.3914784394372905, 0.39014373714184614, 0.37597535935515497, 0.37864476387260876, 0.3956878850102669, 0.39127310059153814, 0.3908624229979466, 0.39322381927736977, 0.3911704312359773, 0.3871663244353183, 0.39445585216829665, 0.39476386036960986, 0.39209445586439523, 0.39158110885404707, 0.39024640655860277, 0.40010266940451744, 0.3982546201232033, 0.3787474332648871, 0.39404517452574855, 0.3850102669526909, 0.3952772074044363, 0.39671457906768065, 0.4008213552606179, 0.3908624230224249, 0.3963039014251325, 0.40277207390973213, 0.4004106776180698, 0.40154004107388136, 0.4022587268871448, 0.395482546201232, 0.39620123203285423, 0.40010266940451744, 0.3919917864231603, 0.4016427104845184, 0.40071868583774173, 0.39784394250819327, 0.38696098563240294, 0.40349075975359344, 0.40677618069815197, 0.40472279260780286, 0.40051334703482644, 0.39907597536546247, 0.4072895277085001, 0.4046201231910463, 0.4044147843942505, 0.4056468172362208, 0.40533880904714675, 0.4045174537865288, 0.40667351128139534, 0.41324435318275154, 0.41396303898989545, 0.41314168377823407, 0.4091375770020534, 0.409958932238193]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
