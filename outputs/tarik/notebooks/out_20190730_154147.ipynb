{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf33.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 15:41:47 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': 'Front', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001C495624E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001C48EE66EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0825, Accuracy:0.3955, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0746, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0743, Accuracy:0.3926, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3967, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #7: Loss:1.0739, Accuracy:0.3984, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #8: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #9: Loss:1.0737, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #10: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #11: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #12: Loss:1.0736, Accuracy:0.3926, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #13: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0736, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #16: Loss:1.0737, Accuracy:0.3955, Validation Loss:1.0744, Validation Accuracy:0.4039\n",
    "Epoch #17: Loss:1.0736, Accuracy:0.3959, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #18: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #19: Loss:1.0738, Accuracy:0.3979, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #20: Loss:1.0737, Accuracy:0.3975, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #21: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #22: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.4056\n",
    "Epoch #23: Loss:1.0735, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.4056\n",
    "Epoch #24: Loss:1.0733, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #25: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #27: Loss:1.0736, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.4056\n",
    "Epoch #28: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0744, Validation Accuracy:0.4072\n",
    "Epoch #29: Loss:1.0735, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.4089\n",
    "Epoch #30: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #31: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.4007\n",
    "Epoch #32: Loss:1.0733, Accuracy:0.3971, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #33: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #35: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0744, Validation Accuracy:0.4105\n",
    "Epoch #36: Loss:1.0732, Accuracy:0.3988, Validation Loss:1.0743, Validation Accuracy:0.4072\n",
    "Epoch #37: Loss:1.0731, Accuracy:0.3984, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #38: Loss:1.0732, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #39: Loss:1.0731, Accuracy:0.3971, Validation Loss:1.0737, Validation Accuracy:0.4039\n",
    "Epoch #40: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #41: Loss:1.0731, Accuracy:0.4004, Validation Loss:1.0737, Validation Accuracy:0.4007\n",
    "Epoch #42: Loss:1.0733, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.4056\n",
    "Epoch #43: Loss:1.0729, Accuracy:0.4053, Validation Loss:1.0736, Validation Accuracy:0.4023\n",
    "Epoch #44: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #45: Loss:1.0733, Accuracy:0.3992, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #46: Loss:1.0732, Accuracy:0.3975, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0734, Accuracy:0.4016, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0740, Accuracy:0.3893, Validation Loss:1.0726, Validation Accuracy:0.3974\n",
    "Epoch #49: Loss:1.0735, Accuracy:0.4025, Validation Loss:1.0724, Validation Accuracy:0.3990\n",
    "Epoch #50: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0719, Validation Accuracy:0.4007\n",
    "Epoch #51: Loss:1.0731, Accuracy:0.3967, Validation Loss:1.0716, Validation Accuracy:0.4023\n",
    "Epoch #52: Loss:1.0729, Accuracy:0.3963, Validation Loss:1.0717, Validation Accuracy:0.4023\n",
    "Epoch #53: Loss:1.0730, Accuracy:0.3967, Validation Loss:1.0718, Validation Accuracy:0.4138\n",
    "Epoch #54: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0717, Validation Accuracy:0.3990\n",
    "Epoch #55: Loss:1.0728, Accuracy:0.3938, Validation Loss:1.0718, Validation Accuracy:0.4007\n",
    "Epoch #56: Loss:1.0729, Accuracy:0.3947, Validation Loss:1.0720, Validation Accuracy:0.4007\n",
    "Epoch #57: Loss:1.0730, Accuracy:0.3959, Validation Loss:1.0719, Validation Accuracy:0.4007\n",
    "Epoch #58: Loss:1.0729, Accuracy:0.3979, Validation Loss:1.0718, Validation Accuracy:0.4023\n",
    "Epoch #59: Loss:1.0730, Accuracy:0.4025, Validation Loss:1.0717, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0731, Accuracy:0.3967, Validation Loss:1.0722, Validation Accuracy:0.3974\n",
    "Epoch #61: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0715, Validation Accuracy:0.3957\n",
    "Epoch #62: Loss:1.0729, Accuracy:0.4025, Validation Loss:1.0718, Validation Accuracy:0.4039\n",
    "Epoch #63: Loss:1.0730, Accuracy:0.4037, Validation Loss:1.0721, Validation Accuracy:0.3908\n",
    "Epoch #64: Loss:1.0730, Accuracy:0.3996, Validation Loss:1.0720, Validation Accuracy:0.3990\n",
    "Epoch #65: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0721, Validation Accuracy:0.4023\n",
    "Epoch #66: Loss:1.0729, Accuracy:0.3979, Validation Loss:1.0725, Validation Accuracy:0.4039\n",
    "Epoch #67: Loss:1.0730, Accuracy:0.4012, Validation Loss:1.0724, Validation Accuracy:0.4039\n",
    "Epoch #68: Loss:1.0727, Accuracy:0.4012, Validation Loss:1.0719, Validation Accuracy:0.3974\n",
    "Epoch #69: Loss:1.0728, Accuracy:0.3984, Validation Loss:1.0719, Validation Accuracy:0.4023\n",
    "Epoch #70: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0721, Validation Accuracy:0.4039\n",
    "Epoch #71: Loss:1.0731, Accuracy:0.3959, Validation Loss:1.0720, Validation Accuracy:0.4007\n",
    "Epoch #72: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0721, Validation Accuracy:0.3941\n",
    "Epoch #73: Loss:1.0725, Accuracy:0.4041, Validation Loss:1.0723, Validation Accuracy:0.4056\n",
    "Epoch #74: Loss:1.0725, Accuracy:0.4123, Validation Loss:1.0715, Validation Accuracy:0.3957\n",
    "Epoch #75: Loss:1.0727, Accuracy:0.3967, Validation Loss:1.0722, Validation Accuracy:0.3957\n",
    "Epoch #76: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0717, Validation Accuracy:0.4269\n",
    "Epoch #77: Loss:1.0730, Accuracy:0.4045, Validation Loss:1.0721, Validation Accuracy:0.4171\n",
    "Epoch #78: Loss:1.0729, Accuracy:0.4041, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0735, Accuracy:0.3951, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #80: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0732, Validation Accuracy:0.3859\n",
    "Epoch #81: Loss:1.0735, Accuracy:0.3914, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #82: Loss:1.0740, Accuracy:0.3930, Validation Loss:1.0751, Validation Accuracy:0.3810\n",
    "Epoch #83: Loss:1.0735, Accuracy:0.3910, Validation Loss:1.0736, Validation Accuracy:0.3892\n",
    "Epoch #84: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #85: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #86: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #87: Loss:1.0732, Accuracy:0.3955, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #88: Loss:1.0732, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #89: Loss:1.0731, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #90: Loss:1.0731, Accuracy:0.3984, Validation Loss:1.0731, Validation Accuracy:0.4007\n",
    "Epoch #91: Loss:1.0731, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #92: Loss:1.0733, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.4023\n",
    "Epoch #93: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #94: Loss:1.0730, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.4072\n",
    "Epoch #95: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #96: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #97: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #98: Loss:1.0733, Accuracy:0.3996, Validation Loss:1.0736, Validation Accuracy:0.4039\n",
    "Epoch #99: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #100: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #101: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0736, Validation Accuracy:0.4039\n",
    "Epoch #102: Loss:1.0730, Accuracy:0.3979, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #103: Loss:1.0729, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.4056\n",
    "Epoch #104: Loss:1.0728, Accuracy:0.4004, Validation Loss:1.0729, Validation Accuracy:0.4056\n",
    "Epoch #105: Loss:1.0730, Accuracy:0.3918, Validation Loss:1.0729, Validation Accuracy:0.3924\n",
    "Epoch #106: Loss:1.0729, Accuracy:0.3959, Validation Loss:1.0722, Validation Accuracy:0.4007\n",
    "Epoch #107: Loss:1.0726, Accuracy:0.3996, Validation Loss:1.0723, Validation Accuracy:0.4023\n",
    "Epoch #108: Loss:1.0727, Accuracy:0.3984, Validation Loss:1.0721, Validation Accuracy:0.4056\n",
    "Epoch #109: Loss:1.0728, Accuracy:0.4012, Validation Loss:1.0719, Validation Accuracy:0.4023\n",
    "Epoch #110: Loss:1.0727, Accuracy:0.3988, Validation Loss:1.0718, Validation Accuracy:0.3908\n",
    "Epoch #111: Loss:1.0730, Accuracy:0.3988, Validation Loss:1.0718, Validation Accuracy:0.4039\n",
    "Epoch #112: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0721, Validation Accuracy:0.3957\n",
    "Epoch #113: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #114: Loss:1.0729, Accuracy:0.3984, Validation Loss:1.0725, Validation Accuracy:0.4089\n",
    "Epoch #115: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0729, Validation Accuracy:0.3957\n",
    "Epoch #116: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0726, Validation Accuracy:0.4007\n",
    "Epoch #117: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0728, Validation Accuracy:0.3957\n",
    "Epoch #118: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0720, Validation Accuracy:0.4039\n",
    "Epoch #119: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #120: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #121: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #122: Loss:1.0736, Accuracy:0.3848, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #123: Loss:1.0736, Accuracy:0.3947, Validation Loss:1.0733, Validation Accuracy:0.3990\n",
    "Epoch #124: Loss:1.0731, Accuracy:0.3984, Validation Loss:1.0727, Validation Accuracy:0.4105\n",
    "Epoch #125: Loss:1.0730, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #126: Loss:1.0729, Accuracy:0.4033, Validation Loss:1.0726, Validation Accuracy:0.4023\n",
    "Epoch #127: Loss:1.0726, Accuracy:0.3979, Validation Loss:1.0728, Validation Accuracy:0.4023\n",
    "Epoch #128: Loss:1.0729, Accuracy:0.4000, Validation Loss:1.0728, Validation Accuracy:0.4023\n",
    "Epoch #129: Loss:1.0728, Accuracy:0.4066, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #130: Loss:1.0726, Accuracy:0.4016, Validation Loss:1.0727, Validation Accuracy:0.3924\n",
    "Epoch #131: Loss:1.0730, Accuracy:0.4029, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #132: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #133: Loss:1.0728, Accuracy:0.4025, Validation Loss:1.0728, Validation Accuracy:0.3924\n",
    "Epoch #134: Loss:1.0728, Accuracy:0.4025, Validation Loss:1.0723, Validation Accuracy:0.4056\n",
    "Epoch #135: Loss:1.0726, Accuracy:0.4016, Validation Loss:1.0722, Validation Accuracy:0.4007\n",
    "Epoch #136: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0719, Validation Accuracy:0.4023\n",
    "Epoch #137: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0719, Validation Accuracy:0.3957\n",
    "Epoch #138: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0722, Validation Accuracy:0.4056\n",
    "Epoch #139: Loss:1.0727, Accuracy:0.3992, Validation Loss:1.0718, Validation Accuracy:0.3957\n",
    "Epoch #140: Loss:1.0726, Accuracy:0.3979, Validation Loss:1.0717, Validation Accuracy:0.4039\n",
    "Epoch #141: Loss:1.0728, Accuracy:0.3988, Validation Loss:1.0721, Validation Accuracy:0.3957\n",
    "Epoch #142: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #143: Loss:1.0730, Accuracy:0.3971, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #144: Loss:1.0727, Accuracy:0.3988, Validation Loss:1.0722, Validation Accuracy:0.4056\n",
    "Epoch #145: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0722, Validation Accuracy:0.3990\n",
    "Epoch #146: Loss:1.0726, Accuracy:0.3951, Validation Loss:1.0722, Validation Accuracy:0.3974\n",
    "Epoch #147: Loss:1.0725, Accuracy:0.3955, Validation Loss:1.0720, Validation Accuracy:0.4056\n",
    "Epoch #148: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0725, Validation Accuracy:0.4023\n",
    "Epoch #149: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0725, Validation Accuracy:0.4039\n",
    "Epoch #150: Loss:1.0726, Accuracy:0.4000, Validation Loss:1.0722, Validation Accuracy:0.3941\n",
    "Epoch #151: Loss:1.0726, Accuracy:0.4033, Validation Loss:1.0710, Validation Accuracy:0.3924\n",
    "Epoch #152: Loss:1.0730, Accuracy:0.4049, Validation Loss:1.0708, Validation Accuracy:0.3990\n",
    "Epoch #153: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0707, Validation Accuracy:0.3957\n",
    "Epoch #154: Loss:1.0722, Accuracy:0.4021, Validation Loss:1.0707, Validation Accuracy:0.3941\n",
    "Epoch #155: Loss:1.0722, Accuracy:0.4033, Validation Loss:1.0708, Validation Accuracy:0.3941\n",
    "Epoch #156: Loss:1.0723, Accuracy:0.3988, Validation Loss:1.0703, Validation Accuracy:0.3941\n",
    "Epoch #157: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0709, Validation Accuracy:0.3974\n",
    "Epoch #158: Loss:1.0724, Accuracy:0.4016, Validation Loss:1.0712, Validation Accuracy:0.3957\n",
    "Epoch #159: Loss:1.0725, Accuracy:0.4016, Validation Loss:1.0709, Validation Accuracy:0.4007\n",
    "Epoch #160: Loss:1.0724, Accuracy:0.3992, Validation Loss:1.0707, Validation Accuracy:0.3974\n",
    "Epoch #161: Loss:1.0725, Accuracy:0.3971, Validation Loss:1.0702, Validation Accuracy:0.3974\n",
    "Epoch #162: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0703, Validation Accuracy:0.3974\n",
    "Epoch #163: Loss:1.0726, Accuracy:0.4012, Validation Loss:1.0701, Validation Accuracy:0.3957\n",
    "Epoch #164: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0701, Validation Accuracy:0.3957\n",
    "Epoch #165: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0700, Validation Accuracy:0.3974\n",
    "Epoch #166: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0699, Validation Accuracy:0.3957\n",
    "Epoch #167: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0701, Validation Accuracy:0.3974\n",
    "Epoch #168: Loss:1.0722, Accuracy:0.4029, Validation Loss:1.0704, Validation Accuracy:0.4105\n",
    "Epoch #169: Loss:1.0724, Accuracy:0.4008, Validation Loss:1.0703, Validation Accuracy:0.4007\n",
    "Epoch #170: Loss:1.0726, Accuracy:0.3996, Validation Loss:1.0702, Validation Accuracy:0.4023\n",
    "Epoch #171: Loss:1.0725, Accuracy:0.3984, Validation Loss:1.0702, Validation Accuracy:0.3957\n",
    "Epoch #172: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0705, Validation Accuracy:0.4007\n",
    "Epoch #173: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0701, Validation Accuracy:0.3974\n",
    "Epoch #174: Loss:1.0727, Accuracy:0.3979, Validation Loss:1.0704, Validation Accuracy:0.3990\n",
    "Epoch #175: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0708, Validation Accuracy:0.4056\n",
    "Epoch #176: Loss:1.0721, Accuracy:0.4016, Validation Loss:1.0706, Validation Accuracy:0.4007\n",
    "Epoch #177: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0704, Validation Accuracy:0.4007\n",
    "Epoch #178: Loss:1.0721, Accuracy:0.4016, Validation Loss:1.0702, Validation Accuracy:0.3941\n",
    "Epoch #179: Loss:1.0719, Accuracy:0.3992, Validation Loss:1.0701, Validation Accuracy:0.4023\n",
    "Epoch #180: Loss:1.0720, Accuracy:0.4033, Validation Loss:1.0703, Validation Accuracy:0.4122\n",
    "Epoch #181: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0698, Validation Accuracy:0.3974\n",
    "Epoch #182: Loss:1.0719, Accuracy:0.4037, Validation Loss:1.0699, Validation Accuracy:0.3974\n",
    "Epoch #183: Loss:1.0721, Accuracy:0.4037, Validation Loss:1.0699, Validation Accuracy:0.4007\n",
    "Epoch #184: Loss:1.0718, Accuracy:0.3988, Validation Loss:1.0701, Validation Accuracy:0.3974\n",
    "Epoch #185: Loss:1.0721, Accuracy:0.3926, Validation Loss:1.0707, Validation Accuracy:0.4056\n",
    "Epoch #186: Loss:1.0720, Accuracy:0.4029, Validation Loss:1.0705, Validation Accuracy:0.4105\n",
    "Epoch #187: Loss:1.0724, Accuracy:0.3951, Validation Loss:1.0704, Validation Accuracy:0.4007\n",
    "Epoch #188: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0705, Validation Accuracy:0.3924\n",
    "Epoch #189: Loss:1.0720, Accuracy:0.4078, Validation Loss:1.0708, Validation Accuracy:0.4089\n",
    "Epoch #190: Loss:1.0720, Accuracy:0.4057, Validation Loss:1.0706, Validation Accuracy:0.3908\n",
    "Epoch #191: Loss:1.0716, Accuracy:0.4025, Validation Loss:1.0704, Validation Accuracy:0.3957\n",
    "Epoch #192: Loss:1.0720, Accuracy:0.4029, Validation Loss:1.0701, Validation Accuracy:0.3957\n",
    "Epoch #193: Loss:1.0720, Accuracy:0.4053, Validation Loss:1.0701, Validation Accuracy:0.4138\n",
    "Epoch #194: Loss:1.0723, Accuracy:0.4037, Validation Loss:1.0699, Validation Accuracy:0.4023\n",
    "Epoch #195: Loss:1.0720, Accuracy:0.4025, Validation Loss:1.0695, Validation Accuracy:0.4007\n",
    "Epoch #196: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0695, Validation Accuracy:0.4007\n",
    "Epoch #197: Loss:1.0719, Accuracy:0.4025, Validation Loss:1.0695, Validation Accuracy:0.3974\n",
    "Epoch #198: Loss:1.0720, Accuracy:0.4115, Validation Loss:1.0694, Validation Accuracy:0.4122\n",
    "Epoch #199: Loss:1.0721, Accuracy:0.3975, Validation Loss:1.0694, Validation Accuracy:0.4286\n",
    "Epoch #200: Loss:1.0719, Accuracy:0.4000, Validation Loss:1.0692, Validation Accuracy:0.3974\n",
    "Epoch #201: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0692, Validation Accuracy:0.3974\n",
    "Epoch #202: Loss:1.0721, Accuracy:0.4025, Validation Loss:1.0696, Validation Accuracy:0.3974\n",
    "Epoch #203: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0701, Validation Accuracy:0.4236\n",
    "Epoch #204: Loss:1.0717, Accuracy:0.4103, Validation Loss:1.0698, Validation Accuracy:0.4187\n",
    "Epoch #205: Loss:1.0718, Accuracy:0.4111, Validation Loss:1.0699, Validation Accuracy:0.3974\n",
    "Epoch #206: Loss:1.0717, Accuracy:0.4033, Validation Loss:1.0698, Validation Accuracy:0.3974\n",
    "Epoch #207: Loss:1.0718, Accuracy:0.4016, Validation Loss:1.0698, Validation Accuracy:0.3974\n",
    "Epoch #208: Loss:1.0720, Accuracy:0.4008, Validation Loss:1.0701, Validation Accuracy:0.4236\n",
    "Epoch #209: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0697, Validation Accuracy:0.3941\n",
    "Epoch #210: Loss:1.0717, Accuracy:0.4021, Validation Loss:1.0697, Validation Accuracy:0.3974\n",
    "Epoch #211: Loss:1.0718, Accuracy:0.4025, Validation Loss:1.0695, Validation Accuracy:0.3974\n",
    "Epoch #212: Loss:1.0716, Accuracy:0.4008, Validation Loss:1.0696, Validation Accuracy:0.4236\n",
    "Epoch #213: Loss:1.0716, Accuracy:0.4078, Validation Loss:1.0694, Validation Accuracy:0.4187\n",
    "Epoch #214: Loss:1.0720, Accuracy:0.4049, Validation Loss:1.0694, Validation Accuracy:0.4187\n",
    "Epoch #215: Loss:1.0721, Accuracy:0.3992, Validation Loss:1.0694, Validation Accuracy:0.3974\n",
    "Epoch #216: Loss:1.0717, Accuracy:0.4004, Validation Loss:1.0694, Validation Accuracy:0.3974\n",
    "Epoch #217: Loss:1.0717, Accuracy:0.4078, Validation Loss:1.0698, Validation Accuracy:0.4187\n",
    "Epoch #218: Loss:1.0715, Accuracy:0.4012, Validation Loss:1.0695, Validation Accuracy:0.3974\n",
    "Epoch #219: Loss:1.0714, Accuracy:0.4057, Validation Loss:1.0696, Validation Accuracy:0.4154\n",
    "Epoch #220: Loss:1.0714, Accuracy:0.4041, Validation Loss:1.0694, Validation Accuracy:0.3974\n",
    "Epoch #221: Loss:1.0718, Accuracy:0.4062, Validation Loss:1.0695, Validation Accuracy:0.4154\n",
    "Epoch #222: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0695, Validation Accuracy:0.3924\n",
    "Epoch #223: Loss:1.0716, Accuracy:0.4021, Validation Loss:1.0692, Validation Accuracy:0.3974\n",
    "Epoch #224: Loss:1.0715, Accuracy:0.4037, Validation Loss:1.0696, Validation Accuracy:0.4204\n",
    "Epoch #225: Loss:1.0716, Accuracy:0.4025, Validation Loss:1.0696, Validation Accuracy:0.4236\n",
    "Epoch #226: Loss:1.0715, Accuracy:0.4016, Validation Loss:1.0694, Validation Accuracy:0.3924\n",
    "Epoch #227: Loss:1.0713, Accuracy:0.4037, Validation Loss:1.0695, Validation Accuracy:0.3974\n",
    "Epoch #228: Loss:1.0714, Accuracy:0.4045, Validation Loss:1.0696, Validation Accuracy:0.4154\n",
    "Epoch #229: Loss:1.0712, Accuracy:0.4103, Validation Loss:1.0694, Validation Accuracy:0.4154\n",
    "Epoch #230: Loss:1.0714, Accuracy:0.4111, Validation Loss:1.0692, Validation Accuracy:0.4154\n",
    "Epoch #231: Loss:1.0714, Accuracy:0.4103, Validation Loss:1.0690, Validation Accuracy:0.4154\n",
    "Epoch #232: Loss:1.0712, Accuracy:0.4086, Validation Loss:1.0690, Validation Accuracy:0.4154\n",
    "Epoch #233: Loss:1.0710, Accuracy:0.4103, Validation Loss:1.0689, Validation Accuracy:0.4154\n",
    "Epoch #234: Loss:1.0715, Accuracy:0.4103, Validation Loss:1.0690, Validation Accuracy:0.4154\n",
    "Epoch #235: Loss:1.0712, Accuracy:0.4103, Validation Loss:1.0684, Validation Accuracy:0.4154\n",
    "Epoch #236: Loss:1.0712, Accuracy:0.4107, Validation Loss:1.0686, Validation Accuracy:0.4154\n",
    "Epoch #237: Loss:1.0709, Accuracy:0.4103, Validation Loss:1.0690, Validation Accuracy:0.4138\n",
    "Epoch #238: Loss:1.0709, Accuracy:0.4099, Validation Loss:1.0687, Validation Accuracy:0.4138\n",
    "Epoch #239: Loss:1.0712, Accuracy:0.3996, Validation Loss:1.0683, Validation Accuracy:0.4089\n",
    "Epoch #240: Loss:1.0712, Accuracy:0.4082, Validation Loss:1.0693, Validation Accuracy:0.4253\n",
    "Epoch #241: Loss:1.0714, Accuracy:0.4070, Validation Loss:1.0679, Validation Accuracy:0.4138\n",
    "Epoch #242: Loss:1.0709, Accuracy:0.4107, Validation Loss:1.0681, Validation Accuracy:0.4154\n",
    "Epoch #243: Loss:1.0713, Accuracy:0.4078, Validation Loss:1.0684, Validation Accuracy:0.4236\n",
    "Epoch #244: Loss:1.0712, Accuracy:0.4070, Validation Loss:1.0683, Validation Accuracy:0.3924\n",
    "Epoch #245: Loss:1.0709, Accuracy:0.4148, Validation Loss:1.0681, Validation Accuracy:0.4154\n",
    "Epoch #246: Loss:1.0705, Accuracy:0.4103, Validation Loss:1.0676, Validation Accuracy:0.4154\n",
    "Epoch #247: Loss:1.0704, Accuracy:0.4103, Validation Loss:1.0678, Validation Accuracy:0.4154\n",
    "Epoch #248: Loss:1.0708, Accuracy:0.4103, Validation Loss:1.0678, Validation Accuracy:0.4187\n",
    "Epoch #249: Loss:1.0709, Accuracy:0.4099, Validation Loss:1.0680, Validation Accuracy:0.4138\n",
    "Epoch #250: Loss:1.0705, Accuracy:0.4103, Validation Loss:1.0689, Validation Accuracy:0.4154\n",
    "Epoch #251: Loss:1.0704, Accuracy:0.4103, Validation Loss:1.0680, Validation Accuracy:0.4154\n",
    "Epoch #252: Loss:1.0704, Accuracy:0.4123, Validation Loss:1.0680, Validation Accuracy:0.4138\n",
    "Epoch #253: Loss:1.0706, Accuracy:0.4111, Validation Loss:1.0676, Validation Accuracy:0.3892\n",
    "Epoch #254: Loss:1.0703, Accuracy:0.4078, Validation Loss:1.0674, Validation Accuracy:0.4154\n",
    "Epoch #255: Loss:1.0704, Accuracy:0.4090, Validation Loss:1.0678, Validation Accuracy:0.4154\n",
    "Epoch #256: Loss:1.0701, Accuracy:0.4103, Validation Loss:1.0674, Validation Accuracy:0.4154\n",
    "Epoch #257: Loss:1.0703, Accuracy:0.4099, Validation Loss:1.0676, Validation Accuracy:0.4138\n",
    "Epoch #258: Loss:1.0704, Accuracy:0.4082, Validation Loss:1.0676, Validation Accuracy:0.4154\n",
    "Epoch #259: Loss:1.0705, Accuracy:0.4111, Validation Loss:1.0678, Validation Accuracy:0.4154\n",
    "Epoch #260: Loss:1.0703, Accuracy:0.4103, Validation Loss:1.0679, Validation Accuracy:0.4204\n",
    "Epoch #261: Loss:1.0709, Accuracy:0.4103, Validation Loss:1.0674, Validation Accuracy:0.4171\n",
    "Epoch #262: Loss:1.0702, Accuracy:0.4057, Validation Loss:1.0675, Validation Accuracy:0.3924\n",
    "Epoch #263: Loss:1.0707, Accuracy:0.4004, Validation Loss:1.0676, Validation Accuracy:0.4138\n",
    "Epoch #264: Loss:1.0705, Accuracy:0.4094, Validation Loss:1.0670, Validation Accuracy:0.4154\n",
    "Epoch #265: Loss:1.0705, Accuracy:0.4107, Validation Loss:1.0677, Validation Accuracy:0.4138\n",
    "Epoch #266: Loss:1.0701, Accuracy:0.4103, Validation Loss:1.0676, Validation Accuracy:0.4138\n",
    "Epoch #267: Loss:1.0703, Accuracy:0.4107, Validation Loss:1.0682, Validation Accuracy:0.4187\n",
    "Epoch #268: Loss:1.0701, Accuracy:0.4107, Validation Loss:1.0671, Validation Accuracy:0.4138\n",
    "Epoch #269: Loss:1.0706, Accuracy:0.4127, Validation Loss:1.0670, Validation Accuracy:0.4138\n",
    "Epoch #270: Loss:1.0706, Accuracy:0.4086, Validation Loss:1.0674, Validation Accuracy:0.4154\n",
    "Epoch #271: Loss:1.0706, Accuracy:0.4094, Validation Loss:1.0685, Validation Accuracy:0.4171\n",
    "Epoch #272: Loss:1.0704, Accuracy:0.4107, Validation Loss:1.0671, Validation Accuracy:0.4154\n",
    "Epoch #273: Loss:1.0699, Accuracy:0.4103, Validation Loss:1.0672, Validation Accuracy:0.4154\n",
    "Epoch #274: Loss:1.0700, Accuracy:0.4111, Validation Loss:1.0670, Validation Accuracy:0.4187\n",
    "Epoch #275: Loss:1.0701, Accuracy:0.4082, Validation Loss:1.0669, Validation Accuracy:0.4171\n",
    "Epoch #276: Loss:1.0700, Accuracy:0.4074, Validation Loss:1.0669, Validation Accuracy:0.4154\n",
    "Epoch #277: Loss:1.0704, Accuracy:0.4078, Validation Loss:1.0672, Validation Accuracy:0.4154\n",
    "Epoch #278: Loss:1.0706, Accuracy:0.3992, Validation Loss:1.0675, Validation Accuracy:0.3990\n",
    "Epoch #279: Loss:1.0701, Accuracy:0.4057, Validation Loss:1.0668, Validation Accuracy:0.4154\n",
    "Epoch #280: Loss:1.0709, Accuracy:0.4074, Validation Loss:1.0676, Validation Accuracy:0.4253\n",
    "Epoch #281: Loss:1.0705, Accuracy:0.4057, Validation Loss:1.0668, Validation Accuracy:0.3924\n",
    "Epoch #282: Loss:1.0699, Accuracy:0.4021, Validation Loss:1.0670, Validation Accuracy:0.4122\n",
    "Epoch #283: Loss:1.0705, Accuracy:0.4062, Validation Loss:1.0680, Validation Accuracy:0.4204\n",
    "Epoch #284: Loss:1.0699, Accuracy:0.4103, Validation Loss:1.0667, Validation Accuracy:0.4138\n",
    "Epoch #285: Loss:1.0696, Accuracy:0.4094, Validation Loss:1.0667, Validation Accuracy:0.3908\n",
    "Epoch #286: Loss:1.0698, Accuracy:0.4045, Validation Loss:1.0665, Validation Accuracy:0.4138\n",
    "Epoch #287: Loss:1.0700, Accuracy:0.4082, Validation Loss:1.0669, Validation Accuracy:0.4204\n",
    "Epoch #288: Loss:1.0699, Accuracy:0.4103, Validation Loss:1.0670, Validation Accuracy:0.4187\n",
    "Epoch #289: Loss:1.0695, Accuracy:0.4111, Validation Loss:1.0665, Validation Accuracy:0.4122\n",
    "Epoch #290: Loss:1.0699, Accuracy:0.4099, Validation Loss:1.0664, Validation Accuracy:0.4089\n",
    "Epoch #291: Loss:1.0695, Accuracy:0.4107, Validation Loss:1.0668, Validation Accuracy:0.4204\n",
    "Epoch #292: Loss:1.0696, Accuracy:0.4123, Validation Loss:1.0665, Validation Accuracy:0.4187\n",
    "Epoch #293: Loss:1.0696, Accuracy:0.4103, Validation Loss:1.0665, Validation Accuracy:0.4204\n",
    "Epoch #294: Loss:1.0696, Accuracy:0.4086, Validation Loss:1.0663, Validation Accuracy:0.4204\n",
    "Epoch #295: Loss:1.0696, Accuracy:0.4107, Validation Loss:1.0664, Validation Accuracy:0.4138\n",
    "Epoch #296: Loss:1.0695, Accuracy:0.4103, Validation Loss:1.0664, Validation Accuracy:0.4105\n",
    "Epoch #297: Loss:1.0693, Accuracy:0.4094, Validation Loss:1.0661, Validation Accuracy:0.4187\n",
    "Epoch #298: Loss:1.0694, Accuracy:0.4103, Validation Loss:1.0663, Validation Accuracy:0.4187\n",
    "Epoch #299: Loss:1.0705, Accuracy:0.4057, Validation Loss:1.0658, Validation Accuracy:0.4187\n",
    "Epoch #300: Loss:1.0693, Accuracy:0.4086, Validation Loss:1.0670, Validation Accuracy:0.4138\n",
    "\n",
    "Test:\n",
    "Test Loss:1.06704080, Accuracy:0.4138\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02  03\n",
    "t:01  178  62   0\n",
    "t:02  153  74   0\n",
    "t:03  111  31   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.40      0.74      0.52       240\n",
    "          02       0.44      0.33      0.38       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.41       609\n",
    "   macro avg       0.28      0.36      0.30       609\n",
    "weighted avg       0.32      0.41      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 15:57:26 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 38 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0747684391065575, 1.0744525391871511, 1.0753242805086334, 1.0750057096356045, 1.0745291075682992, 1.0745737883453494, 1.0742530799264391, 1.0743017936575001, 1.074098806271608, 1.074401324214215, 1.0745045921485412, 1.0744390462223923, 1.0744398752065323, 1.0742609117222928, 1.0741999560389026, 1.0743588782687885, 1.0745217600479502, 1.0744935383741883, 1.0742914267557204, 1.0742808325928812, 1.0742731470192595, 1.074393542137835, 1.074164375883018, 1.0742246151361952, 1.0742141785488535, 1.0743455724371673, 1.0744584119574385, 1.07436842773544, 1.0743940530347902, 1.0742835471978525, 1.074745911487022, 1.0744926489045468, 1.0748299606915177, 1.0744298306983484, 1.074402119138558, 1.0742708084422772, 1.0740337295485247, 1.0737779702263317, 1.0736902881725667, 1.0734744573070107, 1.0737156102614254, 1.073498952369189, 1.0736339213617134, 1.0734718900987472, 1.07309222730314, 1.0727631566168248, 1.0726184140285249, 1.0725659316004987, 1.072423610781214, 1.0719409198400813, 1.071635487044386, 1.0716507616888713, 1.071841767268815, 1.071666951054227, 1.0718382154583734, 1.072028408105346, 1.0718969944466903, 1.0718479066450999, 1.0717349210983427, 1.07216617900554, 1.0715479582597078, 1.0718072789838944, 1.072125484399216, 1.072035794774887, 1.0721152572600516, 1.0725140007845875, 1.0724432241348993, 1.0718812167350882, 1.071931301470852, 1.0721252598786002, 1.0719842736552698, 1.072095567565442, 1.0723229323701906, 1.0715245376471032, 1.0721512132481792, 1.071706132348535, 1.0720730646098973, 1.0732707822655614, 1.0731796333551016, 1.0732217610175974, 1.0735370184987636, 1.0751178376193118, 1.0735914321564297, 1.0735071547121446, 1.07377299495127, 1.0735111195465614, 1.0737721992439433, 1.0734632177697419, 1.0733464455173911, 1.0731490304317381, 1.073230167915081, 1.0731675744252447, 1.073000601947014, 1.0732250049196441, 1.0733171353003466, 1.0731702868770105, 1.0733880147166637, 1.073646849207886, 1.073776174844388, 1.0739195928197776, 1.0736403506377648, 1.0735398125765947, 1.0729985045290542, 1.0728691826117254, 1.0728855046928418, 1.0722226241147772, 1.0722555994791743, 1.072064650274067, 1.071856302189318, 1.0717992960721596, 1.0718074066102603, 1.0720514732432875, 1.072752614326665, 1.0725028828055596, 1.072853357333855, 1.0725553047480842, 1.072838135932271, 1.0719585058528607, 1.074240046964686, 1.073607598815254, 1.0735824748213068, 1.0738155407271361, 1.0732885117601292, 1.0726952443177673, 1.0732812096528428, 1.0726100245524315, 1.0728107154467228, 1.072770897586553, 1.072617047330233, 1.0726991288963406, 1.0725204938738218, 1.0727041080863213, 1.072776013798706, 1.0722516483469746, 1.0721729717818387, 1.0718929517053815, 1.071895298895186, 1.0722408543275104, 1.0718264141301999, 1.0717417606579258, 1.0721256192681825, 1.0722696458177614, 1.0723596648825409, 1.0722063625191625, 1.0721989061640598, 1.0722404203587172, 1.0720136120597326, 1.072537235830022, 1.07253527562998, 1.0722483564871677, 1.071009084900416, 1.0707965128135994, 1.0707027896480217, 1.070698579935409, 1.0708065563430536, 1.0703198729673238, 1.0708888465743542, 1.0712260041135089, 1.0709266915109945, 1.0707294696266036, 1.0701508512246394, 1.0702760857705804, 1.0701088053839547, 1.0701459303669545, 1.0699653122421164, 1.0699487746446983, 1.0700595073512036, 1.0703542107236013, 1.070303426783269, 1.0701600747742677, 1.070195247778556, 1.0705276796187477, 1.0701304795511055, 1.0703805790746153, 1.0708087404764737, 1.0706335777915366, 1.0703707577168255, 1.0701761416026525, 1.0701133521711101, 1.0702524087307683, 1.0698356142967989, 1.0698519853144053, 1.0699311132697245, 1.0701442146536164, 1.070683201154073, 1.0705279289991005, 1.0703837413505968, 1.0705267022591702, 1.0707724025879783, 1.070617344774832, 1.0704023239060576, 1.070088310977704, 1.070076585207471, 1.069910467160355, 1.0694643163132942, 1.0694824652914539, 1.0695489292661544, 1.0694436465186634, 1.069354025796912, 1.0691519225955206, 1.0692146110221474, 1.0696406280270154, 1.0700709700388666, 1.0697792617754005, 1.0699131312628685, 1.0698391271537944, 1.0698214755661186, 1.0701405910044077, 1.0697303153023932, 1.0696969290672265, 1.0694879704508289, 1.0696143577447275, 1.0693917869542815, 1.0693843024117606, 1.069355162493701, 1.0694021562050129, 1.0698167271606245, 1.069520120550259, 1.0695624697971813, 1.0693915037098776, 1.0694546540969698, 1.0694975815774577, 1.0692325057263052, 1.069599074878912, 1.0695510503693755, 1.069359819485832, 1.069529951108109, 1.0695913190324906, 1.0693915197610464, 1.0691944051454416, 1.0689719729431353, 1.0690412527234683, 1.0688793058270107, 1.0690075746310757, 1.0683583684742743, 1.0685602829765608, 1.0689576660666755, 1.068667223105094, 1.0682995769582162, 1.0692849014388712, 1.0679345510667573, 1.0680861246018183, 1.068426461642599, 1.0682887369384515, 1.0680936369402656, 1.0676223973335304, 1.0677579629597405, 1.0677745326594963, 1.0680490045124673, 1.068905088701859, 1.0680320043673461, 1.0679861630124998, 1.0676137471238185, 1.0674190123875935, 1.067816557751109, 1.0673982278858303, 1.0675832730012966, 1.0675782286279112, 1.0678224395257108, 1.0679386750426394, 1.0674360088135417, 1.067490703758152, 1.067554571945679, 1.0669511695604998, 1.0676950498167517, 1.0676351893320069, 1.0681898497987068, 1.0670627003232833, 1.067008108145302, 1.067385279682078, 1.0685185145079013, 1.0670912373437866, 1.0672485667887972, 1.067006562926695, 1.0669091918394091, 1.0668894512508498, 1.0671707111822168, 1.0674967898915357, 1.0668311731764444, 1.0675522703646831, 1.0668460545672964, 1.0670495643991555, 1.0680495803970813, 1.0667188114720612, 1.0666989900404205, 1.066493186065912, 1.066920757489447, 1.067021200222335, 1.0664618326525384, 1.0664265392644847, 1.0668211535279974, 1.0665065633448083, 1.066505014602774, 1.06631594025247, 1.0664454177878369, 1.0664487135625629, 1.0661014559233717, 1.0662517367521138, 1.0657598007292974, 1.067040717269008], 'val_acc': [0.3940886684337077, 0.3940886687273267, 0.3957307048521214, 0.3940886687273267, 0.3940886687273267, 0.39244663250465894, 0.39737274097691616, 0.3973727406832972, 0.3891625600593235, 0.39573070446062947, 0.3973727406832972, 0.39573070475424843, 0.3940886687273267, 0.3940886687273267, 0.39573070455850246, 0.40394088508460324, 0.399014776710219, 0.3973727404875513, 0.3973727406832972, 0.399014776710219, 0.40229884905768143, 0.4055829214051439, 0.4055829214051439, 0.3973727406832972, 0.3940886683358347, 0.3973727406832972, 0.4055829214051439, 0.40722495752993865, 0.4088669936547334, 0.4022988492534274, 0.4006568130307597, 0.4022988492534274, 0.3940886684337077, 0.39901477680809194, 0.4105090297795282, 0.40722495752993865, 0.40229884905768143, 0.4022988491555544, 0.4039408851824762, 0.4022988491555544, 0.4006568129328867, 0.4055829214051439, 0.40229884905768143, 0.40229884905768143, 0.39901477680809194, 0.3940886684337077, 0.3940886687273267, 0.3973727407811702, 0.3990147771017109, 0.40065681322650565, 0.4022988492534274, 0.4022988491555544, 0.4137931020291177, 0.3990147770038379, 0.4006568130307597, 0.40065681312863266, 0.4006568130307597, 0.40229884905768143, 0.3940886686294537, 0.3973727406832972, 0.39573070446062947, 0.40394088528034916, 0.39080459608624524, 0.39901477680809194, 0.4022988489598085, 0.40394088508460324, 0.4039408851824762, 0.3973727406832972, 0.4022988492534274, 0.4039408851824762, 0.40065681312863266, 0.3940886683358347, 0.40558292130727097, 0.39573070446062947, 0.39573070455850246, 0.4269293912232216, 0.4170771740829612, 0.3908045961841182, 0.4039408851824762, 0.385878487711861, 0.40065681273714077, 0.3809523791417308, 0.3891625600593235, 0.39901477680809194, 0.39573070455850246, 0.40229884905768143, 0.3973727406832972, 0.4022988491555544, 0.3973727406832972, 0.4006568129328867, 0.4006568129328867, 0.40229884905768143, 0.40394088508460324, 0.4072249574320657, 0.39244663221104, 0.4006568129328867, 0.40229884905768143, 0.4039408851824762, 0.39244663221104, 0.3940886683358347, 0.4039408851824762, 0.4006568129328867, 0.4055829215030169, 0.40558292130727097, 0.39244663221104, 0.40065681283501375, 0.4022988492534274, 0.4055829214051439, 0.4022988489598085, 0.39080459608624524, 0.40394088528034916, 0.39573070446062947, 0.39080459608624524, 0.4088669933611145, 0.39573070465637544, 0.40065681273714077, 0.39573070446062947, 0.40394088508460324, 0.3940886687273267, 0.40065681322650565, 0.3908045963798642, 0.385878487907607, 0.3990147770038379, 0.4105090297795282, 0.4006568130307597, 0.4022988489598085, 0.4022988489598085, 0.4022988489598085, 0.39080459608624524, 0.39244663221104, 0.39080459608624524, 0.3940886683358347, 0.39244663221104, 0.405582921111525, 0.40065681273714077, 0.4022988488619355, 0.39573070446062947, 0.405582921111525, 0.39573070436275654, 0.40394088498673025, 0.39573070446062947, 0.39244663221104, 0.3940886682379618, 0.405582921111525, 0.399014776710219, 0.3973727405854242, 0.405582921111525, 0.4022988489598085, 0.40394088498673025, 0.3940886683358347, 0.39244663221104, 0.39901477680809194, 0.39573070446062947, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3973727405854242, 0.39573070446062947, 0.40065681273714077, 0.3973727405854242, 0.3973727405854242, 0.3973727405854242, 0.39573070446062947, 0.39573070446062947, 0.3973727405854242, 0.39573070446062947, 0.3973727405854242, 0.4105090294859092, 0.40065681283501375, 0.4022988488619355, 0.39573070446062947, 0.40065681283501375, 0.3973727405854242, 0.399014776710219, 0.405582921111525, 0.40065681283501375, 0.40065681283501375, 0.3940886683358347, 0.4022988489598085, 0.41215106561070397, 0.3973727405854242, 0.3973727405854242, 0.4006568129328867, 0.3973727405854242, 0.405582921209398, 0.4105090295837822, 0.4006568129328867, 0.39244663221104, 0.40886699345898747, 0.3908045961841182, 0.39573070446062947, 0.39573070446062947, 0.4137931019312447, 0.4022988489598085, 0.40065681283501375, 0.40065681283501375, 0.3973727405854242, 0.41215106580644995, 0.4285714269565244, 0.3973727405854242, 0.3973727405854242, 0.3973727405854242, 0.42364531858214016, 0.41871921010988294, 0.3973727405854242, 0.3973727405854242, 0.3973727405854242, 0.42364531858214016, 0.3940886683358347, 0.3973727405854242, 0.3973727405854242, 0.42364531858214016, 0.41871921010988294, 0.41871921010988294, 0.3973727406832972, 0.3973727405854242, 0.41871921010988294, 0.3973727405854242, 0.41543513786029346, 0.3973727405854242, 0.41543513786029346, 0.39244663240678596, 0.3973727405854242, 0.4203612463325507, 0.42364531858214016, 0.39244663240678596, 0.3973727405854242, 0.41543513786029346, 0.41543513805603943, 0.41543513786029346, 0.41543513786029346, 0.41543513786029346, 0.41543513786029346, 0.41543513786029346, 0.41543513786029346, 0.41543513786029346, 0.4137931017354987, 0.4137931019312447, 0.4088669936547334, 0.4252873548048079, 0.4137931020291177, 0.41543513786029346, 0.42364531858214016, 0.39244663240678596, 0.41543513786029346, 0.41543513786029346, 0.41543513786029346, 0.41871921010988294, 0.4137931017354987, 0.41543513786029346, 0.41543513786029346, 0.4137931017354987, 0.38916256025506946, 0.41543513786029346, 0.41543513786029346, 0.41543513786029346, 0.4137931017354987, 0.41543513786029346, 0.41543513786029346, 0.4203612463325507, 0.4170771739850882, 0.39244663240678596, 0.4137931020291177, 0.41543513786029346, 0.4137931017354987, 0.4137931017354987, 0.41871921010988294, 0.4137931017354987, 0.4137931020291177, 0.41543513786029346, 0.4170771739850882, 0.41543513786029346, 0.4154351381539124, 0.4187192104035019, 0.4170771740829612, 0.41543513786029346, 0.41543513786029346, 0.3990147771017109, 0.41543513786029346, 0.4252873548048079, 0.39244663250465894, 0.41215106590432293, 0.4203612463325507, 0.4137931017354987, 0.3908045962819912, 0.4137931020291177, 0.4203612463325507, 0.41871921010988294, 0.41215106570857696, 0.40886699355686046, 0.4203612463325507, 0.41871921010988294, 0.4203612463325507, 0.4203612462346777, 0.4137931018333717, 0.4105090296816552, 0.41871921010988294, 0.41871921010988294, 0.41871921020775593, 0.4137931021269906], 'loss': [1.0824685293056637, 1.0745894743431765, 1.0742932135564345, 1.0746086563173016, 1.0744934750288664, 1.073849175793924, 1.0738802111858705, 1.07385267232233, 1.0736923566344339, 1.0737038703424975, 1.0736392577326028, 1.0735530078778277, 1.073599839944859, 1.0736394123375048, 1.0735990527229387, 1.0736503038563032, 1.073646683320862, 1.073794937378572, 1.0738004916502466, 1.0737165412863667, 1.0735952086027643, 1.0736954105952927, 1.0735096430142068, 1.0733428467470518, 1.0735486937744172, 1.0733563425115002, 1.073614959011822, 1.0735184418347337, 1.0735046082698345, 1.0733167140635622, 1.0734523372728477, 1.073271992612913, 1.0733924010940645, 1.073438429881415, 1.073413253222158, 1.0732272742465292, 1.0731170037443878, 1.0732012011921626, 1.0731054875884947, 1.0731950191012154, 1.0731435938292706, 1.0732961215032935, 1.072938261727288, 1.0730039080065623, 1.073308003022196, 1.0732272305282968, 1.0733537488894296, 1.0739807800590624, 1.073498770637434, 1.073156495358665, 1.073054420727724, 1.0728729095302323, 1.0730100089764447, 1.0730537862013987, 1.0728263977861503, 1.0728553032238626, 1.0729764359198066, 1.0728664103964272, 1.0730477463782935, 1.0730837626623666, 1.072741163927427, 1.0728648951900568, 1.0729922128164304, 1.072963535026848, 1.0730188331564838, 1.0729049614077966, 1.0729676013609712, 1.0726941558125083, 1.0728139740730458, 1.073037308980797, 1.0730817610723038, 1.0727549041321145, 1.0725028276933046, 1.0725399746297566, 1.0727307007787654, 1.0728439899930229, 1.0730126853351476, 1.0728880380947732, 1.0734964662998363, 1.0732118420042787, 1.0734969854354859, 1.074022491609781, 1.0734921523922523, 1.073188634525824, 1.0730280946657154, 1.0728237060550792, 1.0731567147331316, 1.0732267528099202, 1.0730653130298278, 1.073096523833226, 1.0730686307198214, 1.0733059328439545, 1.073012390910233, 1.0730493371246776, 1.0733958863624558, 1.0734094970769705, 1.0733917392499637, 1.0732645303072137, 1.0731189475167213, 1.0731674359319636, 1.0731001075043571, 1.0730261602930464, 1.072920692432098, 1.0728309121709585, 1.0729885222730695, 1.0728500292776058, 1.0726308896556283, 1.072676660294895, 1.072760896369417, 1.0727079636262427, 1.0729744665921346, 1.0728522171474826, 1.0728225325656868, 1.0728873444778473, 1.0727718560847415, 1.0727274132460294, 1.072991783760901, 1.0727572891991242, 1.0729345343196173, 1.0739733461971401, 1.0732736025502794, 1.0735789280897294, 1.0736469713814205, 1.073097499244267, 1.073014815142512, 1.0729355486021883, 1.0725893680564678, 1.0728715032033118, 1.072785419654063, 1.0725663842117028, 1.073045106394335, 1.0726093440574787, 1.0727552669004248, 1.0727865472221767, 1.072627923131234, 1.0728360002780108, 1.0728054407440906, 1.0730861195793386, 1.072731633891315, 1.0726014526472933, 1.0727726220595029, 1.0728614487442392, 1.0729759847848568, 1.0726727423481872, 1.0727057620485216, 1.0725875263586182, 1.0725394212244963, 1.0729348095045932, 1.072786234389585, 1.0725634581254493, 1.072599993300389, 1.073038753298029, 1.0727604504238653, 1.0722246671848963, 1.0722335991183836, 1.0722829640523608, 1.0723231041945471, 1.0724441730511016, 1.0724769198674196, 1.0724443524280367, 1.0724856741864088, 1.0721722632218191, 1.0726304919323149, 1.072527360524485, 1.072361244802847, 1.072359279146919, 1.0725390342226753, 1.0721761460176973, 1.072390069070538, 1.0726109347059496, 1.072493689065107, 1.0720902219690092, 1.0719590189031016, 1.07267779530441, 1.0721680457587115, 1.0721269029366651, 1.0719227573465273, 1.072116708168014, 1.0719233358665168, 1.0720321343909545, 1.071855764320499, 1.0718865634969128, 1.072111304535758, 1.0718352169471599, 1.0721356404145885, 1.07204222179781, 1.0724451464549227, 1.071899501205225, 1.0719951840641562, 1.0719976136327034, 1.0716347243996371, 1.0720217669524206, 1.072010960569127, 1.0723069777968484, 1.0719766732358835, 1.071868361046182, 1.071889892349008, 1.0720200439742948, 1.072060041359073, 1.071851883629754, 1.07182162997659, 1.0721235209177162, 1.0717220030281334, 1.071679556736956, 1.0717542144552148, 1.071693272952916, 1.0717563320234327, 1.0719818509824466, 1.0716626823805195, 1.0717205584661182, 1.0718216277735435, 1.0716499808387834, 1.0716155070788562, 1.0719935075947882, 1.0721111284389144, 1.0716792780271056, 1.0717056046276368, 1.0714940721249433, 1.0714348537476401, 1.0714380234418708, 1.0718380377767511, 1.0715051671073177, 1.0716110724443282, 1.0714814207147523, 1.0715685384719034, 1.0714888793487078, 1.071348723395894, 1.0714095562145696, 1.07123822269988, 1.0713851441592896, 1.0713887437902683, 1.071158812863626, 1.0710384000253383, 1.071456777976034, 1.0712153399504676, 1.071210318130634, 1.0708542528583285, 1.0709216919278217, 1.0712177077847584, 1.0711972252788975, 1.0714051993720586, 1.0708856007401704, 1.071313072034221, 1.0711550483468621, 1.0708565339905036, 1.0704675852150887, 1.0703891787930435, 1.0707970480164952, 1.0708926675256027, 1.070511541768021, 1.0704064224779728, 1.070409159493887, 1.0705875260139637, 1.0702550257500683, 1.0703895884128076, 1.070117534406376, 1.0703150250828486, 1.0704310419623122, 1.0705299741188847, 1.0703083844645067, 1.0708735592311414, 1.0701717775215602, 1.0706844812545933, 1.0704592392429924, 1.0705057055553617, 1.0701473249791829, 1.0702877909251063, 1.0701233772771315, 1.070559597358077, 1.0705643434544119, 1.0705674700178895, 1.0704166124488783, 1.069904033261403, 1.0699788620339772, 1.0700937857128512, 1.0699684291404865, 1.0703524478896687, 1.0705755183339363, 1.0701062011522924, 1.070855817804591, 1.0704745543810865, 1.069927048732123, 1.0705466864779745, 1.069926756265472, 1.0696297978963205, 1.0697692281656441, 1.07002075555633, 1.0698867035107935, 1.0695162741800108, 1.0698984204866069, 1.0695137575666518, 1.0695947190819335, 1.0696240113256403, 1.0696088457499198, 1.0696475370219112, 1.0694930316486397, 1.0693094185000818, 1.0693606858380766, 1.0704635669073774, 1.0692588866858512], 'acc': [0.39548254637258007, 0.3950718705169474, 0.3926078012469368, 0.39671458020592126, 0.39425051351837065, 0.39425051351837065, 0.39835728911158974, 0.3975359324679482, 0.3975359348545819, 0.3979466122768253, 0.40041067873183217, 0.3926078044168758, 0.3979466138434361, 0.3950718677753785, 0.3991786441151856, 0.3954825475475382, 0.3958932247739553, 0.39630390121706704, 0.3979466116893463, 0.39753593207629556, 0.39876796551798405, 0.3979466108693234, 0.39917864708929823, 0.3975359358337136, 0.3975359342671028, 0.39753593309214474, 0.3946611909406141, 0.39630390278367783, 0.3979466118484552, 0.39835728989489516, 0.39630389980956515, 0.3971252588031228, 0.3991786465018192, 0.40041067540278424, 0.39835729048237417, 0.39876796653383323, 0.39835728989489516, 0.39876796712131224, 0.39712525801981746, 0.40123203420051556, 0.40041067579443695, 0.4004106771652214, 0.4053388079945801, 0.40082135497911753, 0.3991786447393821, 0.39753593309214474, 0.4016427094319518, 0.3893223803390957, 0.4024640678380304, 0.4008213530575715, 0.396714580597574, 0.3963039025878515, 0.39671457860259307, 0.39425051551335155, 0.39383983805439066, 0.3946611913322668, 0.39589322536143434, 0.3979466098901917, 0.4024640654513968, 0.3967145778560051, 0.3975359322721219, 0.4024640676422041, 0.40369610030058717, 0.3995893201666446, 0.399178644151903, 0.3979466116893463, 0.4012320306389239, 0.4012320310305766, 0.3983572902865478, 0.39835729106985324, 0.39589322379482356, 0.40082135579914036, 0.404106777918657, 0.41232032730594065, 0.39671458118505304, 0.39999999935132524, 0.4045174543617687, 0.40410677733117795, 0.3950718707127738, 0.3979466118851726, 0.391375768196901, 0.3930184785100714, 0.3909650937487702, 0.3971252578239911, 0.39917864591434016, 0.39917864630599287, 0.3954825447692519, 0.40041067736104774, 0.3963039000053915, 0.3983572918531586, 0.39712525469076954, 0.3971252550824222, 0.3991786437235329, 0.39712525606155397, 0.39589322536143434, 0.40205338904500254, 0.39917864532686115, 0.3995893219657992, 0.3987679672804212, 0.39835728989489516, 0.3987679688837494, 0.39794661403926246, 0.4004106785727233, 0.40041067778941786, 0.39178644780995175, 0.3958932227789744, 0.39958932349569254, 0.3983572916573323, 0.40123203361303655, 0.39876796947122845, 0.39876796610546306, 0.39917864708929823, 0.4000000020928941, 0.39835729106985324, 0.3991786429402275, 0.3975359330554273, 0.4008213539999858, 0.40164271123110634, 0.4045174545575951, 0.3946611917239195, 0.4008213569740985, 0.38480492810693856, 0.3946611932905303, 0.398357288879046, 0.3963039031753305, 0.40328542150755925, 0.3979466122768253, 0.4000000007588271, 0.40657084202374766, 0.4016427124060645, 0.4028747426778139, 0.4008213569740985, 0.4024640656839406, 0.40246406705472504, 0.40164271064362733, 0.40000000212961156, 0.4016427100194308, 0.3987679692754021, 0.3991786445435557, 0.3979466109060409, 0.3987679653221577, 0.40082135716992484, 0.3971252574323384, 0.3987679684920967, 0.39753593207629556, 0.3950718701252947, 0.39548254793919085, 0.4008213565824458, 0.401642710447801, 0.40000000212961156, 0.4032854211159066, 0.4049281306090541, 0.39917864336859765, 0.40205338607088986, 0.40328542150755925, 0.3987679651630488, 0.400821355015835, 0.40164270962777815, 0.4016427126018908, 0.39917864391935926, 0.3971252554373575, 0.4024640668588987, 0.4012320334172102, 0.40205338669508633, 0.39835728809574056, 0.40205338963248155, 0.40082135537077024, 0.4028747428736403, 0.40082135341250674, 0.3995893247073681, 0.3983572906414831, 0.40041067540278424, 0.40205338865334983, 0.39794661149351995, 0.40041067540278424, 0.4016427092728429, 0.40164271064362733, 0.401642710447801, 0.39917864552268745, 0.40328541951257835, 0.40041067778941786, 0.4036960977181272, 0.4036960995172818, 0.39876796829627037, 0.3926078010511105, 0.4028747456519266, 0.3950718669553557, 0.40082135497911753, 0.4078028744863044, 0.4057494860043026, 0.40246406505974414, 0.4028747426778139, 0.4053388074071011, 0.4036960987339764, 0.4024640678380304, 0.4008213554074877, 0.4024640678380304, 0.41149897383223816, 0.39753593465875553, 0.40000000173795885, 0.4004106783768969, 0.40246406408061236, 0.40492813374227565, 0.4102669404272671, 0.4110882976216702, 0.40328542307417004, 0.40164270923612544, 0.4008213532166804, 0.40451745612420587, 0.40205338963248155, 0.4024640668588987, 0.4008213551749439, 0.4078028727238673, 0.4049281310007068, 0.39917864708929823, 0.4004106791602023, 0.40780287491467454, 0.401232034983821, 0.4057494889784153, 0.4041067745528916, 0.40616016600900606, 0.4020533888491762, 0.4020533862667162, 0.40369610088806623, 0.40246406349313335, 0.4016427094319518, 0.4036960983423237, 0.40451745295426683, 0.41026694199387786, 0.41108829703419114, 0.4102669423855306, 0.4086242307015758, 0.4102669417980515, 0.4102669417980515, 0.41026694254463947, 0.4106776162461823, 0.4102669409780287, 0.40985626457163443, 0.399589324119889, 0.40821355034193707, 0.40698151948270855, 0.4106776182044458, 0.40780287389882536, 0.4069815183077505, 0.4147843944218614, 0.4102669396439617, 0.41026693960724425, 0.41026694140639886, 0.4098562641799817, 0.41026694062309343, 0.4102669382364598, 0.41232032636352633, 0.4110882980133229, 0.40780287530632725, 0.4090349087112983, 0.4102669423855306, 0.4098562629683062, 0.4082135507335898, 0.41108829644671213, 0.41026694140639886, 0.41026694160222515, 0.40574948561264995, 0.40041067974768135, 0.409445584566931, 0.4106776168336614, 0.4102669402314407, 0.4106776158545296, 0.4106776158545296, 0.41273100414070507, 0.40862422815583327, 0.40944558714939094, 0.41067762000360036, 0.41026694003561437, 0.4110882944517312, 0.4082135525327444, 0.40739219588910286, 0.40780287311552, 0.3991786447393821, 0.4057494889416979, 0.4073921953016238, 0.4057494864326728, 0.4020533894366552, 0.4061601658131797, 0.4102669394114179, 0.40944558535023634, 0.40451745295426683, 0.4082135531202234, 0.41026694058637597, 0.41108829406007846, 0.40985626061839, 0.4106776174211404, 0.41232032946003044, 0.41026694101474614, 0.40862423030992306, 0.4106776174211404, 0.41026693862811253, 0.40944558358779926, 0.41026694199387786, 0.4057494881951099, 0.4086242299182704]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
