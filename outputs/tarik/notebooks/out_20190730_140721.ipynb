{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf27.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 14:07:21 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '2', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000021C47CF4E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000021C444A6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0895, Accuracy:0.3943, Validation Loss:1.0835, Validation Accuracy:0.3957\n",
    "Epoch #2: Loss:1.0812, Accuracy:0.3967, Validation Loss:1.0772, Validation Accuracy:0.4122\n",
    "Epoch #3: Loss:1.0757, Accuracy:0.3782, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #4: Loss:1.0744, Accuracy:0.3676, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3885, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0739, Accuracy:0.3918, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0744, Validation Accuracy:0.3974\n",
    "Epoch #15: Loss:1.0738, Accuracy:0.3930, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0738, Accuracy:0.3930, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0737, Accuracy:0.3910, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #18: Loss:1.0738, Accuracy:0.3971, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #19: Loss:1.0738, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #20: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #21: Loss:1.0742, Accuracy:0.3918, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #27: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0740, Accuracy:0.3918, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #31: Loss:1.0737, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #32: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #35: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #36: Loss:1.0738, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #37: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #38: Loss:1.0737, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #39: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #40: Loss:1.0738, Accuracy:0.3996, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #41: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #42: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #43: Loss:1.0743, Accuracy:0.3951, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #44: Loss:1.0737, Accuracy:0.4008, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #45: Loss:1.0738, Accuracy:0.3988, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0740, Accuracy:0.4016, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0737, Accuracy:0.3955, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #48: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #49: Loss:1.0736, Accuracy:0.4025, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #50: Loss:1.0736, Accuracy:0.4025, Validation Loss:1.0750, Validation Accuracy:0.3859\n",
    "Epoch #51: Loss:1.0736, Accuracy:0.4016, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #52: Loss:1.0735, Accuracy:0.4012, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #53: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #54: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #55: Loss:1.0739, Accuracy:0.3901, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #56: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #57: Loss:1.0735, Accuracy:0.4012, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #58: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #59: Loss:1.0736, Accuracy:0.4012, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #60: Loss:1.0736, Accuracy:0.4012, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #61: Loss:1.0735, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #62: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #63: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #64: Loss:1.0737, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #65: Loss:1.0734, Accuracy:0.4025, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #66: Loss:1.0735, Accuracy:0.4012, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #67: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #68: Loss:1.0733, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #69: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #70: Loss:1.0733, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #71: Loss:1.0734, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #72: Loss:1.0733, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #73: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #74: Loss:1.0734, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #75: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #76: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #77: Loss:1.0732, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #78: Loss:1.0732, Accuracy:0.4025, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #79: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #80: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #81: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #82: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #83: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #84: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #85: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #86: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #87: Loss:1.0732, Accuracy:0.4004, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #88: Loss:1.0730, Accuracy:0.4000, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #89: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #90: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #91: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #92: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #93: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #94: Loss:1.0731, Accuracy:0.3971, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #95: Loss:1.0730, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #96: Loss:1.0729, Accuracy:0.4012, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #97: Loss:1.0728, Accuracy:0.4004, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #98: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #99: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #100: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #101: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #102: Loss:1.0728, Accuracy:0.3996, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #103: Loss:1.0730, Accuracy:0.4012, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #104: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #105: Loss:1.0727, Accuracy:0.4008, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #106: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #107: Loss:1.0728, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #108: Loss:1.0727, Accuracy:0.3979, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #109: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #110: Loss:1.0726, Accuracy:0.3951, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #111: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #112: Loss:1.0727, Accuracy:0.4004, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #113: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #114: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #115: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #116: Loss:1.0727, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #117: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #118: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #119: Loss:1.0727, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #120: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #121: Loss:1.0727, Accuracy:0.4025, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #122: Loss:1.0727, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #123: Loss:1.0725, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #124: Loss:1.0725, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #125: Loss:1.0725, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #126: Loss:1.0724, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #127: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #128: Loss:1.0724, Accuracy:0.4008, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #129: Loss:1.0724, Accuracy:0.3959, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #130: Loss:1.0724, Accuracy:0.4012, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #131: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #132: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #133: Loss:1.0727, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #134: Loss:1.0722, Accuracy:0.4012, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #135: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #136: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #137: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #138: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #139: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #140: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #141: Loss:1.0724, Accuracy:0.4004, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #142: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #143: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #144: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #145: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #146: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #147: Loss:1.0721, Accuracy:0.3996, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #148: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #149: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #150: Loss:1.0722, Accuracy:0.3967, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #151: Loss:1.0725, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #152: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #153: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #154: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #155: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #156: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #157: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #158: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #159: Loss:1.0725, Accuracy:0.3996, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #160: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #161: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #162: Loss:1.0723, Accuracy:0.4033, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #163: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #164: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #165: Loss:1.0722, Accuracy:0.4000, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #166: Loss:1.0722, Accuracy:0.3967, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #167: Loss:1.0720, Accuracy:0.3996, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #168: Loss:1.0717, Accuracy:0.4004, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #169: Loss:1.0720, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #170: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #171: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #172: Loss:1.0725, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #173: Loss:1.0718, Accuracy:0.3992, Validation Loss:1.0737, Validation Accuracy:0.3842\n",
    "Epoch #174: Loss:1.0724, Accuracy:0.4000, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #175: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #176: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #177: Loss:1.0716, Accuracy:0.4004, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #178: Loss:1.0717, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #179: Loss:1.0716, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #180: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #181: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0729, Validation Accuracy:0.3842\n",
    "Epoch #182: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #183: Loss:1.0717, Accuracy:0.4021, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #184: Loss:1.0717, Accuracy:0.4012, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #185: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0729, Validation Accuracy:0.3842\n",
    "Epoch #186: Loss:1.0716, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #187: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #188: Loss:1.0716, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #189: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #190: Loss:1.0712, Accuracy:0.4012, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #191: Loss:1.0714, Accuracy:0.4025, Validation Loss:1.0735, Validation Accuracy:0.3859\n",
    "Epoch #192: Loss:1.0712, Accuracy:0.4012, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #193: Loss:1.0712, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #194: Loss:1.0715, Accuracy:0.4004, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #195: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #196: Loss:1.0710, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #197: Loss:1.0711, Accuracy:0.4025, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #198: Loss:1.0714, Accuracy:0.4021, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #199: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #200: Loss:1.0711, Accuracy:0.4016, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #201: Loss:1.0715, Accuracy:0.4008, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #202: Loss:1.0717, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #203: Loss:1.0714, Accuracy:0.3996, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #204: Loss:1.0714, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #205: Loss:1.0707, Accuracy:0.4004, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #206: Loss:1.0711, Accuracy:0.4012, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #207: Loss:1.0708, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #208: Loss:1.0711, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #209: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #210: Loss:1.0715, Accuracy:0.3992, Validation Loss:1.0737, Validation Accuracy:0.3842\n",
    "Epoch #211: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #212: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #213: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #214: Loss:1.0712, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #215: Loss:1.0706, Accuracy:0.4033, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #216: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #217: Loss:1.0705, Accuracy:0.4012, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #218: Loss:1.0706, Accuracy:0.4016, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #219: Loss:1.0704, Accuracy:0.4025, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #220: Loss:1.0707, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #221: Loss:1.0702, Accuracy:0.4029, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #222: Loss:1.0708, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #223: Loss:1.0722, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #224: Loss:1.0714, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3777\n",
    "Epoch #225: Loss:1.0706, Accuracy:0.4066, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #226: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #227: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3859\n",
    "Epoch #228: Loss:1.0704, Accuracy:0.4029, Validation Loss:1.0737, Validation Accuracy:0.3859\n",
    "Epoch #229: Loss:1.0704, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #230: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3859\n",
    "Epoch #231: Loss:1.0704, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #232: Loss:1.0717, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #233: Loss:1.0709, Accuracy:0.3975, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #234: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #235: Loss:1.0706, Accuracy:0.4012, Validation Loss:1.0735, Validation Accuracy:0.3859\n",
    "Epoch #236: Loss:1.0700, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #237: Loss:1.0703, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #238: Loss:1.0700, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #239: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0737, Validation Accuracy:0.3859\n",
    "Epoch #240: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0737, Validation Accuracy:0.3859\n",
    "Epoch #241: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #242: Loss:1.0701, Accuracy:0.4041, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #243: Loss:1.0702, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #244: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #245: Loss:1.0699, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #246: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #247: Loss:1.0699, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #248: Loss:1.0704, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #249: Loss:1.0700, Accuracy:0.4041, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #250: Loss:1.0700, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3859\n",
    "Epoch #251: Loss:1.0703, Accuracy:0.4053, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #252: Loss:1.0708, Accuracy:0.4021, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #253: Loss:1.0699, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #254: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #255: Loss:1.0697, Accuracy:0.4041, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #256: Loss:1.0696, Accuracy:0.4033, Validation Loss:1.0740, Validation Accuracy:0.3793\n",
    "Epoch #257: Loss:1.0697, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #258: Loss:1.0696, Accuracy:0.4033, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #259: Loss:1.0696, Accuracy:0.4041, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #260: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #261: Loss:1.0697, Accuracy:0.4041, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #262: Loss:1.0697, Accuracy:0.4041, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #263: Loss:1.0703, Accuracy:0.4033, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #264: Loss:1.0705, Accuracy:0.3988, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #265: Loss:1.0694, Accuracy:0.4037, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #266: Loss:1.0702, Accuracy:0.4021, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #267: Loss:1.0692, Accuracy:0.4053, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #268: Loss:1.0702, Accuracy:0.3906, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #269: Loss:1.0701, Accuracy:0.4057, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #270: Loss:1.0704, Accuracy:0.4057, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #271: Loss:1.0695, Accuracy:0.4041, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #272: Loss:1.0698, Accuracy:0.4033, Validation Loss:1.0745, Validation Accuracy:0.3793\n",
    "Epoch #273: Loss:1.0705, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #274: Loss:1.0694, Accuracy:0.4041, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #275: Loss:1.0696, Accuracy:0.4041, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #276: Loss:1.0699, Accuracy:0.4041, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #277: Loss:1.0695, Accuracy:0.4012, Validation Loss:1.0746, Validation Accuracy:0.3793\n",
    "Epoch #278: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #279: Loss:1.0695, Accuracy:0.4062, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #280: Loss:1.0695, Accuracy:0.4062, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #281: Loss:1.0703, Accuracy:0.4021, Validation Loss:1.0745, Validation Accuracy:0.3859\n",
    "Epoch #282: Loss:1.0702, Accuracy:0.4049, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #283: Loss:1.0691, Accuracy:0.4000, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #284: Loss:1.0698, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #285: Loss:1.0689, Accuracy:0.4041, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #286: Loss:1.0702, Accuracy:0.4070, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #287: Loss:1.0698, Accuracy:0.4053, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #288: Loss:1.0694, Accuracy:0.4049, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #289: Loss:1.0693, Accuracy:0.4041, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #290: Loss:1.0698, Accuracy:0.4053, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #291: Loss:1.0693, Accuracy:0.4062, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #292: Loss:1.0695, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #293: Loss:1.0692, Accuracy:0.4049, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #294: Loss:1.0695, Accuracy:0.4053, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #295: Loss:1.0702, Accuracy:0.4057, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #296: Loss:1.0686, Accuracy:0.4057, Validation Loss:1.0747, Validation Accuracy:0.3810\n",
    "Epoch #297: Loss:1.0701, Accuracy:0.4053, Validation Loss:1.0747, Validation Accuracy:0.3810\n",
    "Epoch #298: Loss:1.0692, Accuracy:0.4053, Validation Loss:1.0764, Validation Accuracy:0.3892\n",
    "Epoch #299: Loss:1.0697, Accuracy:0.4037, Validation Loss:1.0746, Validation Accuracy:0.3810\n",
    "Epoch #300: Loss:1.0692, Accuracy:0.4057, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07484818, Accuracy:0.3859\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03   01  02\n",
    "t:03   0  127  15\n",
    "t:01   0  204  36\n",
    "t:02   0  196  31\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.39      0.85      0.53       240\n",
    "          02       0.38      0.14      0.20       227\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.26      0.33      0.24       609\n",
    "weighted avg       0.29      0.39      0.28       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 14:22:55 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0834587276079777, 1.0772396063765477, 1.0749075079982113, 1.0750541763352643, 1.0752596121116225, 1.0748214197080515, 1.074580285740995, 1.0744354507606018, 1.0742877406635503, 1.0743129202493502, 1.0743149470030184, 1.074338986760094, 1.0741965518209147, 1.074400640473577, 1.0744097951206275, 1.0743484326771326, 1.074433680238395, 1.0745226283770282, 1.0744297729532903, 1.0744540442778363, 1.0744838328980069, 1.0742870537909772, 1.074387320939739, 1.074378246940024, 1.074376920761146, 1.0745586518975119, 1.0745779919898373, 1.074544793866538, 1.0745764647798586, 1.0744302537053676, 1.074479881961553, 1.074489327682846, 1.0747284601474631, 1.0747177888607156, 1.0745566558759592, 1.0745904813650597, 1.0744828394872605, 1.0745897138451512, 1.074606804424906, 1.0747230691079828, 1.0747001826861027, 1.0752016188475886, 1.0749495646049236, 1.075113384790217, 1.0753037171998048, 1.0751323680376577, 1.0750569508384993, 1.0750336936737712, 1.0751785954035367, 1.0750044130143666, 1.074885041255669, 1.0748782003258641, 1.0748629994776058, 1.075007321212092, 1.0749832131396766, 1.0749481571914723, 1.0748587390667894, 1.0749010591475638, 1.0748500520568371, 1.0747883775942824, 1.0747994791306494, 1.0747951228043129, 1.074700210482029, 1.0747717442770897, 1.074742176262616, 1.0747546509568915, 1.0747461211309448, 1.0746982800353728, 1.0746865472182852, 1.074745974908713, 1.0747919593538557, 1.0746518412638573, 1.0747075593725997, 1.074639862394098, 1.0746038324139975, 1.0745907119538005, 1.0745677797273658, 1.0745322132736983, 1.0745424873918932, 1.074525359228914, 1.074526792676578, 1.0745293900297193, 1.0744222433891986, 1.0743947483244396, 1.0743458597922364, 1.0743704400039071, 1.0743224160816087, 1.074241036460513, 1.0742249212828763, 1.0742325404986177, 1.0741653097869923, 1.0741532379379022, 1.0741918832797723, 1.0740454239994044, 1.0740193197096901, 1.074073587536616, 1.0740395191267793, 1.0739507001804796, 1.0739197366930582, 1.0738844339091986, 1.0738708972930908, 1.073931146529312, 1.0739987072686257, 1.0737820667977795, 1.0739720243538542, 1.073780864526094, 1.0736394349381646, 1.0735378962236477, 1.073557012382595, 1.073549927944816, 1.0738179521216156, 1.073556075738177, 1.0734969551731603, 1.0734707389167573, 1.0734770395877131, 1.0734290824147867, 1.073345144785488, 1.0733748883840877, 1.0734269386050346, 1.0738214287656085, 1.0734469412974341, 1.0734321374219822, 1.0733299413925321, 1.0734930040409607, 1.0733288229001174, 1.0732887184678628, 1.0733136559159102, 1.0732742681096143, 1.0731895377091782, 1.0732764718176304, 1.0732451139020998, 1.0732828595955384, 1.0732495974633103, 1.0732837304693137, 1.073467692913876, 1.073219700008386, 1.0731783578744272, 1.0732560756758516, 1.073175974667366, 1.0731435628555874, 1.0733684252439852, 1.0730919509098447, 1.0730828185778338, 1.0730677929222094, 1.0730932884419884, 1.072991577275281, 1.0730225436988918, 1.073169732915944, 1.0730544786735121, 1.0730732340726554, 1.0735086967988163, 1.0731223344019873, 1.073131634096794, 1.0733052284651006, 1.0731341371003826, 1.0731226107952825, 1.073188832044993, 1.073154076175345, 1.0735773956051404, 1.0730777004082215, 1.0730952427696516, 1.0735124952491673, 1.0731065993630045, 1.0731534697543617, 1.0733737103849013, 1.073133175596228, 1.0731165033255892, 1.0731992476875167, 1.0732154026015834, 1.0730671285604216, 1.073125846475999, 1.0731405280102257, 1.0736890079939894, 1.073097639287438, 1.0732946423278458, 1.0730686072254025, 1.0731658992313204, 1.0730957464240063, 1.073136558086414, 1.0730911771260654, 1.0729394221344997, 1.073031226206687, 1.0737607054326725, 1.0729736117110855, 1.0729413788111144, 1.0730215442200208, 1.0733500715155515, 1.0730217685448908, 1.073145305777614, 1.0733356886896595, 1.073522652898516, 1.0730862384554984, 1.0732511720438114, 1.0732399182170873, 1.0730667165151762, 1.0734686671415181, 1.073269106288653, 1.0730986839836258, 1.0735903146427448, 1.0732783525448126, 1.0732012810965477, 1.074553668596866, 1.0734578053939519, 1.0734467185385317, 1.0734090352880543, 1.073275474100473, 1.0732171729280444, 1.0733328634882209, 1.0746020928196522, 1.0737150692195923, 1.073986038394358, 1.0734205756868636, 1.07355334057988, 1.0737573941940157, 1.0734535712130944, 1.0739855539231074, 1.0735592053245833, 1.0734980317759397, 1.074072491359241, 1.073832815699585, 1.073543248309682, 1.0734139397030784, 1.0746916640176758, 1.074089028760913, 1.0742637700047986, 1.0740012431575356, 1.0739257856347095, 1.0736829063966749, 1.074201032054444, 1.0738784372317185, 1.074149119051415, 1.0743326698422235, 1.0742920124276323, 1.0748874865141995, 1.0735423958360268, 1.0738462070721906, 1.074079594392886, 1.0742454031613855, 1.0737457598371458, 1.0736590647345106, 1.074650612762213, 1.073814020759758, 1.0740846233023407, 1.0743539630877366, 1.0743177784683278, 1.0745725876396317, 1.0739615634940136, 1.0746028685608913, 1.0742369719914027, 1.0739202174451354, 1.0739765960007466, 1.0755747736772685, 1.0743651319607137, 1.0743321150981735, 1.0746106193179177, 1.0740344998088769, 1.0740068596963617, 1.0746080348840097, 1.0742266499154478, 1.0741620666679295, 1.075064685348611, 1.0743525057590653, 1.0744818783745977, 1.075056108347888, 1.0744722930864357, 1.0748623278732174, 1.0749620804058506, 1.074943823571667, 1.0744961302464426, 1.0752131784295018, 1.0743062443333893, 1.074468160889223, 1.0746149631165127, 1.0745800340116904, 1.0748427521027564, 1.0750331356020397, 1.0745931650421694, 1.0747682091050548, 1.0749008765165833, 1.074460997957314, 1.0744750642619894, 1.0755824093356705, 1.074776716028724, 1.074492013708907, 1.0757091759852393, 1.0750558869592075, 1.0749349290709973, 1.0744485285481795, 1.074783669512456, 1.0742528704782621, 1.0753680632032196, 1.07480704412476, 1.0746963319715803, 1.0745642713725274, 1.0756514148759138, 1.074684231152088, 1.074666586611267, 1.0763724190848214, 1.0746071156609822, 1.0748480998823795], 'val_acc': [0.3957307054393593, 0.41215106600219586, 0.3793103437999199, 0.39080459716284804, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3973727417599, 0.3940886695103105, 0.3940886695103105, 0.39080459726072103, 0.3809523803162066, 0.3891625611359263, 0.39080459716284804, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3924466332876428, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3957307055372323, 0.39080459726072103, 0.39408866931456454, 0.39408866941243753, 0.39408866941243753, 0.39737274156415403, 0.397372741662027, 0.3957307055372323, 0.3957307055372323, 0.3957307055372323, 0.38423645266366907, 0.3891625610380533, 0.3825944166367473, 0.3858784886905908, 0.3858784887884638, 0.3940886695103105, 0.3940886696081835, 0.39573070563510526, 0.38423645266366907, 0.3858784887884638, 0.3858784887884638, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.37931034419141185, 0.38423645266366907, 0.37931034419141185, 0.38423645266366907, 0.3809523804140796, 0.3809523804140796, 0.3825944165388743, 0.38423645266366907, 0.3858784887884638, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.3957307055372323, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.3957307055372323, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3957307055372323, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3858784887884638, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3858784887884638, 0.38423645266366907, 0.3858784887884638, 0.38423645266366907, 0.38423645266366907, 0.3858784887884638, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.38423645266366907, 0.3858784887884638, 0.3858784887884638, 0.38423645266366907, 0.38423645266366907, 0.38752052491325856, 0.3776683081644901, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.38423645266366907, 0.3858784887884638, 0.38423645266366907, 0.3908045967713561, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.38752052491325856, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.3858784887884638, 0.38752052491325856, 0.3858784887884638, 0.38752052491325856, 0.38752052491325856, 0.3858784887884638, 0.38752052491325856, 0.38916256054868836, 0.3858784887884638, 0.38752052491325856, 0.38752052491325856, 0.37931034428928484, 0.3858784887884638, 0.38752052491325856, 0.38752052491325856, 0.3858784887884638, 0.38752052491325856, 0.38752052491325856, 0.38752052491325856, 0.38752052491325856, 0.38423645266366907, 0.38423645266366907, 0.38752052491325856, 0.3858784887884638, 0.38423645266366907, 0.38752052491325856, 0.38423645266366907, 0.37931034428928484, 0.38752052491325856, 0.38752052491325856, 0.38752052491325856, 0.38752052491325856, 0.37931034428928484, 0.38752052491325856, 0.39080459726072103, 0.38752052491325856, 0.3858784887884638, 0.3891625610380533, 0.38752052491325856, 0.38752052491325856, 0.3891625610380533, 0.3858784887884638, 0.38752052491325856, 0.38752052491325856, 0.38752052491325856, 0.3858784887884638, 0.3891625610380533, 0.3809523804140796, 0.3858784887884638, 0.3858784887884638, 0.3891625610380533, 0.3809523804140796, 0.3809523804140796, 0.3891625610380533, 0.3809523804140796, 0.3858784887884638], 'loss': [1.0894991534446543, 1.0812437595038444, 1.0756849409373634, 1.0744450858975827, 1.0744173532149142, 1.0748544185802922, 1.0742598276608288, 1.0742495499107627, 1.074216351812625, 1.073998087730251, 1.0739809189978566, 1.0739386886052282, 1.07383458633932, 1.0737683006870185, 1.0738021682175278, 1.0738366611684371, 1.0736733147251043, 1.0737602474753127, 1.0737792595700806, 1.0737282415190272, 1.0741832440883472, 1.0739897562003478, 1.0740855012341446, 1.0739250124357564, 1.0737434389165295, 1.0738698130515567, 1.0739384583134426, 1.0740186195353953, 1.073914222501876, 1.0740135104749238, 1.0737345346435139, 1.0737793000571783, 1.0737545490754459, 1.0739026611590532, 1.0738823661569208, 1.073757057268272, 1.0738095780907226, 1.0737484209836141, 1.073618333932066, 1.0737565363212287, 1.0736726880807896, 1.0737849939040824, 1.0743435227650637, 1.0736608749052827, 1.0738159357889476, 1.0739760828703582, 1.0737332009681686, 1.0735412657872851, 1.0735826340054584, 1.073612201923707, 1.0736104340034345, 1.073520894266007, 1.0734869230454462, 1.0735879200929488, 1.073931165201708, 1.0736915076782572, 1.0734789015086525, 1.073623203790653, 1.0735913500893532, 1.0735693298081352, 1.073479803484813, 1.0734837686256706, 1.073464470771304, 1.0736503786130118, 1.0733728264391544, 1.0734717436150114, 1.0734321899727384, 1.0733281289282766, 1.073253654699306, 1.073312246383338, 1.0734196997765888, 1.0733056601068078, 1.0733901336697338, 1.073370527339912, 1.0731989004773526, 1.0731711744527797, 1.0732429688960865, 1.0731956366885613, 1.0731971995786476, 1.0731651155366055, 1.0731317279764758, 1.0731328799739266, 1.073091248222445, 1.073054748296248, 1.0730978395904605, 1.0730706747062886, 1.0731581634075, 1.0729845743159738, 1.0730295363392919, 1.0731353178650935, 1.073002635624864, 1.0729140420224388, 1.0730790417297176, 1.0730877662341454, 1.0730426654678595, 1.0728509201895775, 1.072843246489335, 1.0729011475917496, 1.0728167595559812, 1.0728550081625121, 1.0728661316376202, 1.0727804196688673, 1.073049263200231, 1.0730042039001746, 1.0726816199398628, 1.0730005838542993, 1.0727704721799376, 1.0727262929234906, 1.0727596506690587, 1.072627815573612, 1.0728151722365582, 1.0726598025836984, 1.072598453517812, 1.0726058019994464, 1.0726211342723462, 1.0726601414122376, 1.0725831620257493, 1.072644790486878, 1.0726776547990051, 1.072990606795591, 1.0726801324918775, 1.0727493245009279, 1.0725289371224154, 1.0725450161790946, 1.0725491516399188, 1.0723904509319173, 1.07250743206032, 1.0724035927891975, 1.072403042027593, 1.0724205997690284, 1.072313799113953, 1.0725862993107194, 1.0726634523951788, 1.0722025401783186, 1.0722991310350705, 1.0724779664123818, 1.0722844002427996, 1.0722013347202748, 1.072212230645166, 1.0722998837915534, 1.0724306266655423, 1.0722889202576154, 1.0722771114882013, 1.072199009771954, 1.0721902954504965, 1.0721233467301794, 1.0721346354337689, 1.072257344189121, 1.0721325568349944, 1.0722389184963532, 1.072451724653616, 1.0720862524220587, 1.0722107533311942, 1.0721409767315373, 1.072076876158587, 1.072021950784405, 1.0720015312856717, 1.0720264442647507, 1.072453674692393, 1.0719450896280747, 1.0721298400871073, 1.0723261374467696, 1.0725343481471161, 1.0720148497783184, 1.0722114887570453, 1.0721732293311086, 1.0719792294551216, 1.0716988461707897, 1.0719815094612952, 1.0718498521272162, 1.0718056668001523, 1.0724626121579743, 1.0718318110863532, 1.072447656410186, 1.071888064359003, 1.0719523550303809, 1.0716167414702429, 1.0716639460479453, 1.0715982566868745, 1.0717563595859911, 1.0714291090348418, 1.0717870107176857, 1.071664652393584, 1.0716891296590378, 1.0713772432025699, 1.0715811139504279, 1.071368788302067, 1.0715705211647237, 1.0713225019051553, 1.0711909859576998, 1.0713887952925978, 1.0712413995417727, 1.071227081453531, 1.0714566474088163, 1.0712508395467206, 1.070995906882707, 1.0710893439561189, 1.0714075427280558, 1.0708351892122743, 1.0711357235663725, 1.0715221590574762, 1.0716893299404355, 1.0713552118571632, 1.0714233176174595, 1.070689015270993, 1.0710931710394012, 1.0707583284475966, 1.0710574846248118, 1.0715388311252947, 1.0715024039975427, 1.071883794169651, 1.0708701469080648, 1.070931756129255, 1.071183633118929, 1.0706456662203496, 1.0715694565058245, 1.0704679379472988, 1.0706442703701387, 1.0704200633497454, 1.0707412976748645, 1.0701562795306134, 1.0707541007036057, 1.0721907645035573, 1.071404046297563, 1.0706084716491386, 1.0704999828240709, 1.0704940931508184, 1.0703845199863034, 1.070439754031767, 1.0700626209776014, 1.070394042091448, 1.0717246818346655, 1.0708741729019604, 1.0712582283196264, 1.070578231703819, 1.069976770951273, 1.0703054307667381, 1.0700360890531442, 1.0701081091863174, 1.0701161325834616, 1.070105040832222, 1.0701199021427539, 1.0701787507999114, 1.0701194769058384, 1.0699378228530256, 1.0705313243415566, 1.0699354782731136, 1.0704393945925046, 1.070048305581972, 1.070011754447185, 1.0703418912828826, 1.0707658315830897, 1.0699145120761722, 1.0704545537059558, 1.0696902618271125, 1.0695676691478282, 1.069716407582011, 1.0696486086816024, 1.0696364815964592, 1.070016850436248, 1.069746896324706, 1.0696567115352873, 1.0703377970435046, 1.0705354306732116, 1.0693817167066695, 1.070178565303403, 1.069236143658538, 1.0701887289356646, 1.0700595850327665, 1.070434049952936, 1.0694884453955618, 1.0697549090493141, 1.0705388808397298, 1.0693551599612225, 1.069577329359505, 1.0698767167097245, 1.0694910891491773, 1.0699747727881712, 1.0694914638139383, 1.0695106395216203, 1.0702596231652481, 1.0701982943673887, 1.0691033320750054, 1.0698361674862966, 1.0689093262752714, 1.0702141431322822, 1.0697949836875869, 1.0694230590757647, 1.0693389367273947, 1.0697817377486023, 1.0693374339070407, 1.0694729341373796, 1.0691640940045428, 1.0694848796425414, 1.0701793961456425, 1.068631067217253, 1.0701101652650618, 1.0691857255215027, 1.0697371642937161, 1.0692298528839674], 'acc': [0.39425051547663414, 0.39671457781928765, 0.37823408445538437, 0.3675564669730482, 0.38850102467458597, 0.3942505148891551, 0.394250513714197, 0.3942505113275634, 0.39425051367747954, 0.39425051113173704, 0.3946611907447878, 0.39178644542331814, 0.39342915710727294, 0.3934291586738837, 0.39301848125164024, 0.3930184824265983, 0.39096509041972227, 0.39712525723651204, 0.4024640680705742, 0.396714580597574, 0.39178644937656254, 0.3942505156724605, 0.39425051547663414, 0.3942505152808078, 0.39425051391002336, 0.39425051250252147, 0.3938398362919535, 0.394250513714197, 0.3942505141058497, 0.39178644921745365, 0.3991786461101665, 0.3946611893740033, 0.3954825485266699, 0.39383983550864815, 0.3946611893372859, 0.39589322379482356, 0.39507186636787667, 0.3934291586738837, 0.4008213545874649, 0.39958932153742904, 0.3983572893074161, 0.3946611928988776, 0.3950718703578385, 0.40082135560331406, 0.398767968100444, 0.4016427092728429, 0.3954825477433645, 0.3987679677087913, 0.40246406447226507, 0.40246406548811425, 0.4016427114269327, 0.401232034983821, 0.40041067935602864, 0.40041067974768135, 0.3901437349877563, 0.39630390278367783, 0.40123203478799463, 0.40082135521166135, 0.40123203204642577, 0.40123203420051556, 0.4020533894366552, 0.39876796751296495, 0.39876796829627037, 0.4008213532166804, 0.402464063884786, 0.401232034983821, 0.4004106785727233, 0.4004106781443531, 0.4008213547832912, 0.40123203478799463, 0.402464066467246, 0.40123203517964734, 0.40123203122640294, 0.40082135619079307, 0.40082135341250674, 0.40082135716992484, 0.39958932431571537, 0.40246406564722315, 0.40082135419581216, 0.40082135419581216, 0.40082135341250674, 0.40123203517964734, 0.40123203083475023, 0.4008213554074877, 0.40082135619079307, 0.40082135423252957, 0.400410679551855, 0.40000000212961156, 0.40082135716992484, 0.40082135482000864, 0.40082135579914036, 0.40082135736575114, 0.40164271201441176, 0.3971252548865959, 0.39999999778471446, 0.401232032401361, 0.4004106785727233, 0.4008213536083331, 0.40082135482000864, 0.4008213575615775, 0.4008213554074877, 0.39958932036247097, 0.4012320310305766, 0.40164270845282, 0.40082135736575114, 0.4004106755986106, 0.3963039010212407, 0.3979466122401079, 0.39671458138087934, 0.395071869146163, 0.40082135579914036, 0.4004106799435077, 0.4008213575615775, 0.4008213547832912, 0.4008213565824458, 0.40082135560331406, 0.4008213554074877, 0.40082135736575114, 0.40082135576242295, 0.3983572900907215, 0.40246406548811425, 0.39507186992946836, 0.39671458138087934, 0.4004106779852442, 0.40041067638191596, 0.4004106767735687, 0.40082135716992484, 0.4008213554074877, 0.3958932216040163, 0.40123203165477306, 0.40082135619079307, 0.4008213543916385, 0.4012320334172102, 0.4012320314222293, 0.40041067876854963, 0.4008213575615775, 0.40082135419581216, 0.4004106799435077, 0.4004106799435077, 0.40041067540278424, 0.40041067974768135, 0.40041067935602864, 0.40041067696939503, 0.4004106759902633, 0.40041067579443695, 0.40041067974768135, 0.39958932333658365, 0.40082135677827213, 0.4004106779852442, 0.39671457922678954, 0.4004106770061125, 0.40041067579443695, 0.40041067739776515, 0.4004106765777423, 0.40041067974768135, 0.40041067540278424, 0.40041067876854963, 0.40041067818107057, 0.3995893201666446, 0.40041067720193885, 0.3946611927397687, 0.40328541896181674, 0.40082135619079307, 0.40041067579443695, 0.39999999778471446, 0.39671457687687334, 0.39958932294493094, 0.4004106785727233, 0.40123203083475023, 0.40041067681028614, 0.40041067618608966, 0.40041067618608966, 0.399178644151903, 0.39999999880056364, 0.40041067896437593, 0.40041067540278424, 0.40041067876854963, 0.40041067720193885, 0.4004106791602023, 0.40041067739776515, 0.40041067778941786, 0.40041067739776515, 0.4020533876742181, 0.40123203122640294, 0.40041067775270045, 0.40041067834017946, 0.4004106759902633, 0.4004106759902633, 0.4004106760269807, 0.4012320345921683, 0.4024640660755933, 0.401232032401361, 0.40041067818107057, 0.40041067638191596, 0.4004106755986106, 0.4004106779852442, 0.40246406505974414, 0.4020533884575235, 0.4004106791602023, 0.4016427122102381, 0.40082135560331406, 0.40082135736575114, 0.39958932353241, 0.4008213565824458, 0.4004106783768969, 0.40123203083475023, 0.40123203204642577, 0.4004106759902633, 0.40205338708673904, 0.3991786466976456, 0.40205339002413426, 0.40123203517964734, 0.40082135677827213, 0.4028747426778139, 0.40328542229086467, 0.4012320318505994, 0.40123203083475023, 0.40164271103528, 0.4024640672505514, 0.40287474251870503, 0.4028747452602739, 0.4008213575615775, 0.39630389980956515, 0.4028747416986822, 0.4065708408487896, 0.4028747452602739, 0.4028747446727948, 0.4028747440853158, 0.40287474150285585, 0.4028747444769685, 0.40205338806587076, 0.40246406368895965, 0.3975359328596009, 0.4012320343963419, 0.4012320310305766, 0.4028747446727948, 0.40287474209033486, 0.4028747436936631, 0.40287474111120314, 0.40287474248198757, 0.40287474545610025, 0.404106778506136, 0.4028747446727948, 0.40287474428114217, 0.40287474111120314, 0.40287474428114217, 0.4028747444769685, 0.4020533898283079, 0.40410677533619704, 0.40287474251870503, 0.40533880721127474, 0.40205338607088986, 0.40082135677827213, 0.4028747427145314, 0.404106778506136, 0.4032854201000574, 0.4028747448686212, 0.40328542189921196, 0.40410677494454433, 0.4020533858750635, 0.4041067755320234, 0.4041067739654126, 0.40328542229086467, 0.3987679684920967, 0.4036961006922399, 0.4020533878700444, 0.40533881116451914, 0.3905544133891315, 0.40574948878258893, 0.4057494844376918, 0.4041067741612389, 0.40328542287834374, 0.40205338904500254, 0.40410677827359226, 0.4041067755320234, 0.40410677435706527, 0.4012320338088629, 0.4020533858750635, 0.40616016518898324, 0.40616016561735335, 0.402053387833327, 0.40492812939737854, 0.3999999991554989, 0.40451745534090044, 0.40410677533619704, 0.40698152046184033, 0.4053388079945801, 0.4049281329222528, 0.4041067739654126, 0.40533881116451914, 0.4061601634265461, 0.40205338607088986, 0.4049281298257487, 0.40533880803129757, 0.4057494881951099, 0.40574948721597814, 0.4053388076029274, 0.40533881077286643, 0.4036961004964135, 0.40574948721597814]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
