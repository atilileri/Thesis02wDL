{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf29.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 14:38:42 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '3', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '01', '03', '02', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000020A9165BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000020A8EDC7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6089, Accuracy:0.2045, Validation Loss:1.6064, Validation Accuracy:0.2233\n",
    "Epoch #2: Loss:1.6057, Accuracy:0.2333, Validation Loss:1.6055, Validation Accuracy:0.2315\n",
    "Epoch #3: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6003, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6028, Accuracy:0.2333, Validation Loss:1.6001, Validation Accuracy:0.2397\n",
    "Epoch #16: Loss:1.6024, Accuracy:0.2361, Validation Loss:1.5998, Validation Accuracy:0.2381\n",
    "Epoch #17: Loss:1.6022, Accuracy:0.2361, Validation Loss:1.5997, Validation Accuracy:0.2447\n",
    "Epoch #18: Loss:1.6019, Accuracy:0.2382, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #19: Loss:1.6018, Accuracy:0.2386, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #20: Loss:1.6019, Accuracy:0.2390, Validation Loss:1.5990, Validation Accuracy:0.2479\n",
    "Epoch #21: Loss:1.6015, Accuracy:0.2419, Validation Loss:1.5991, Validation Accuracy:0.2496\n",
    "Epoch #22: Loss:1.6014, Accuracy:0.2444, Validation Loss:1.5989, Validation Accuracy:0.2447\n",
    "Epoch #23: Loss:1.6018, Accuracy:0.2398, Validation Loss:1.5986, Validation Accuracy:0.2447\n",
    "Epoch #24: Loss:1.6012, Accuracy:0.2427, Validation Loss:1.5984, Validation Accuracy:0.2414\n",
    "Epoch #25: Loss:1.6009, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2348\n",
    "Epoch #26: Loss:1.6010, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2430\n",
    "Epoch #27: Loss:1.6008, Accuracy:0.2431, Validation Loss:1.5988, Validation Accuracy:0.2479\n",
    "Epoch #28: Loss:1.6008, Accuracy:0.2402, Validation Loss:1.5989, Validation Accuracy:0.2479\n",
    "Epoch #29: Loss:1.6004, Accuracy:0.2419, Validation Loss:1.5987, Validation Accuracy:0.2414\n",
    "Epoch #30: Loss:1.6004, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2430\n",
    "Epoch #31: Loss:1.6001, Accuracy:0.2407, Validation Loss:1.5985, Validation Accuracy:0.2430\n",
    "Epoch #32: Loss:1.6003, Accuracy:0.2415, Validation Loss:1.5993, Validation Accuracy:0.2447\n",
    "Epoch #33: Loss:1.6003, Accuracy:0.2439, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #34: Loss:1.6005, Accuracy:0.2427, Validation Loss:1.5986, Validation Accuracy:0.2479\n",
    "Epoch #35: Loss:1.6000, Accuracy:0.2427, Validation Loss:1.5989, Validation Accuracy:0.2479\n",
    "Epoch #36: Loss:1.6004, Accuracy:0.2415, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #37: Loss:1.5999, Accuracy:0.2427, Validation Loss:1.5995, Validation Accuracy:0.2463\n",
    "Epoch #38: Loss:1.5996, Accuracy:0.2427, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #39: Loss:1.5994, Accuracy:0.2435, Validation Loss:1.5997, Validation Accuracy:0.2496\n",
    "Epoch #40: Loss:1.5992, Accuracy:0.2452, Validation Loss:1.5990, Validation Accuracy:0.2479\n",
    "Epoch #41: Loss:1.5992, Accuracy:0.2448, Validation Loss:1.5987, Validation Accuracy:0.2496\n",
    "Epoch #42: Loss:1.5988, Accuracy:0.2472, Validation Loss:1.5982, Validation Accuracy:0.2496\n",
    "Epoch #43: Loss:1.5991, Accuracy:0.2448, Validation Loss:1.5991, Validation Accuracy:0.2463\n",
    "Epoch #44: Loss:1.5989, Accuracy:0.2452, Validation Loss:1.5991, Validation Accuracy:0.2496\n",
    "Epoch #45: Loss:1.5993, Accuracy:0.2489, Validation Loss:1.5991, Validation Accuracy:0.2397\n",
    "Epoch #46: Loss:1.5982, Accuracy:0.2480, Validation Loss:1.5988, Validation Accuracy:0.2496\n",
    "Epoch #47: Loss:1.5985, Accuracy:0.2472, Validation Loss:1.5993, Validation Accuracy:0.2496\n",
    "Epoch #48: Loss:1.5985, Accuracy:0.2464, Validation Loss:1.5991, Validation Accuracy:0.2545\n",
    "Epoch #49: Loss:1.5984, Accuracy:0.2480, Validation Loss:1.5986, Validation Accuracy:0.2496\n",
    "Epoch #50: Loss:1.5980, Accuracy:0.2468, Validation Loss:1.5990, Validation Accuracy:0.2512\n",
    "Epoch #51: Loss:1.5982, Accuracy:0.2456, Validation Loss:1.5984, Validation Accuracy:0.2463\n",
    "Epoch #52: Loss:1.5981, Accuracy:0.2464, Validation Loss:1.5981, Validation Accuracy:0.2545\n",
    "Epoch #53: Loss:1.5975, Accuracy:0.2476, Validation Loss:1.5983, Validation Accuracy:0.2545\n",
    "Epoch #54: Loss:1.5979, Accuracy:0.2472, Validation Loss:1.5984, Validation Accuracy:0.2545\n",
    "Epoch #55: Loss:1.5973, Accuracy:0.2452, Validation Loss:1.5980, Validation Accuracy:0.2447\n",
    "Epoch #56: Loss:1.5970, Accuracy:0.2489, Validation Loss:1.5981, Validation Accuracy:0.2545\n",
    "Epoch #57: Loss:1.5973, Accuracy:0.2476, Validation Loss:1.5983, Validation Accuracy:0.2545\n",
    "Epoch #58: Loss:1.5976, Accuracy:0.2411, Validation Loss:1.5990, Validation Accuracy:0.2233\n",
    "Epoch #59: Loss:1.5982, Accuracy:0.2337, Validation Loss:1.5982, Validation Accuracy:0.2381\n",
    "Epoch #60: Loss:1.5976, Accuracy:0.2423, Validation Loss:1.5988, Validation Accuracy:0.2529\n",
    "Epoch #61: Loss:1.5976, Accuracy:0.2472, Validation Loss:1.5983, Validation Accuracy:0.2430\n",
    "Epoch #62: Loss:1.5979, Accuracy:0.2485, Validation Loss:1.5984, Validation Accuracy:0.2414\n",
    "Epoch #63: Loss:1.5979, Accuracy:0.2439, Validation Loss:1.5988, Validation Accuracy:0.2365\n",
    "Epoch #64: Loss:1.5977, Accuracy:0.2464, Validation Loss:1.5994, Validation Accuracy:0.2545\n",
    "Epoch #65: Loss:1.5972, Accuracy:0.2468, Validation Loss:1.5997, Validation Accuracy:0.2545\n",
    "Epoch #66: Loss:1.5969, Accuracy:0.2444, Validation Loss:1.5999, Validation Accuracy:0.2365\n",
    "Epoch #67: Loss:1.5974, Accuracy:0.2427, Validation Loss:1.6001, Validation Accuracy:0.2414\n",
    "Epoch #68: Loss:1.5976, Accuracy:0.2456, Validation Loss:1.6006, Validation Accuracy:0.2496\n",
    "Epoch #69: Loss:1.5968, Accuracy:0.2456, Validation Loss:1.6003, Validation Accuracy:0.2299\n",
    "Epoch #70: Loss:1.5970, Accuracy:0.2489, Validation Loss:1.6001, Validation Accuracy:0.2430\n",
    "Epoch #71: Loss:1.5965, Accuracy:0.2415, Validation Loss:1.6005, Validation Accuracy:0.2447\n",
    "Epoch #72: Loss:1.5970, Accuracy:0.2456, Validation Loss:1.6008, Validation Accuracy:0.2512\n",
    "Epoch #73: Loss:1.5965, Accuracy:0.2423, Validation Loss:1.6001, Validation Accuracy:0.2397\n",
    "Epoch #74: Loss:1.5970, Accuracy:0.2439, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #75: Loss:1.5966, Accuracy:0.2407, Validation Loss:1.6008, Validation Accuracy:0.2447\n",
    "Epoch #76: Loss:1.5967, Accuracy:0.2460, Validation Loss:1.6001, Validation Accuracy:0.2562\n",
    "Epoch #77: Loss:1.5960, Accuracy:0.2460, Validation Loss:1.6006, Validation Accuracy:0.2266\n",
    "Epoch #78: Loss:1.5968, Accuracy:0.2476, Validation Loss:1.6003, Validation Accuracy:0.2397\n",
    "Epoch #79: Loss:1.5964, Accuracy:0.2419, Validation Loss:1.6010, Validation Accuracy:0.2479\n",
    "Epoch #80: Loss:1.5971, Accuracy:0.2415, Validation Loss:1.5997, Validation Accuracy:0.2430\n",
    "Epoch #81: Loss:1.5967, Accuracy:0.2427, Validation Loss:1.5997, Validation Accuracy:0.2430\n",
    "Epoch #82: Loss:1.5968, Accuracy:0.2411, Validation Loss:1.5997, Validation Accuracy:0.2414\n",
    "Epoch #83: Loss:1.5971, Accuracy:0.2456, Validation Loss:1.6006, Validation Accuracy:0.2479\n",
    "Epoch #84: Loss:1.5971, Accuracy:0.2448, Validation Loss:1.6005, Validation Accuracy:0.2397\n",
    "Epoch #85: Loss:1.5984, Accuracy:0.2435, Validation Loss:1.6009, Validation Accuracy:0.2463\n",
    "Epoch #86: Loss:1.5979, Accuracy:0.2419, Validation Loss:1.6007, Validation Accuracy:0.2463\n",
    "Epoch #87: Loss:1.5967, Accuracy:0.2431, Validation Loss:1.6004, Validation Accuracy:0.2479\n",
    "Epoch #88: Loss:1.5970, Accuracy:0.2411, Validation Loss:1.5998, Validation Accuracy:0.2463\n",
    "Epoch #89: Loss:1.5971, Accuracy:0.2398, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #90: Loss:1.5971, Accuracy:0.2452, Validation Loss:1.6001, Validation Accuracy:0.2414\n",
    "Epoch #91: Loss:1.5971, Accuracy:0.2378, Validation Loss:1.5987, Validation Accuracy:0.2315\n",
    "Epoch #92: Loss:1.5966, Accuracy:0.2427, Validation Loss:1.5990, Validation Accuracy:0.2479\n",
    "Epoch #93: Loss:1.5966, Accuracy:0.2407, Validation Loss:1.5979, Validation Accuracy:0.2463\n",
    "Epoch #94: Loss:1.5976, Accuracy:0.2353, Validation Loss:1.5982, Validation Accuracy:0.2315\n",
    "Epoch #95: Loss:1.5964, Accuracy:0.2394, Validation Loss:1.5986, Validation Accuracy:0.2365\n",
    "Epoch #96: Loss:1.5967, Accuracy:0.2374, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #97: Loss:1.5989, Accuracy:0.2407, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #98: Loss:1.5992, Accuracy:0.2361, Validation Loss:1.6016, Validation Accuracy:0.2414\n",
    "Epoch #99: Loss:1.5988, Accuracy:0.2407, Validation Loss:1.6001, Validation Accuracy:0.2414\n",
    "Epoch #100: Loss:1.5979, Accuracy:0.2419, Validation Loss:1.5997, Validation Accuracy:0.2365\n",
    "Epoch #101: Loss:1.5976, Accuracy:0.2493, Validation Loss:1.6003, Validation Accuracy:0.2512\n",
    "Epoch #102: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.6009, Validation Accuracy:0.2430\n",
    "Epoch #103: Loss:1.5971, Accuracy:0.2476, Validation Loss:1.6023, Validation Accuracy:0.2397\n",
    "Epoch #104: Loss:1.5970, Accuracy:0.2489, Validation Loss:1.6013, Validation Accuracy:0.2479\n",
    "Epoch #105: Loss:1.5967, Accuracy:0.2480, Validation Loss:1.6016, Validation Accuracy:0.2512\n",
    "Epoch #106: Loss:1.5978, Accuracy:0.2431, Validation Loss:1.6019, Validation Accuracy:0.2479\n",
    "Epoch #107: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.6014, Validation Accuracy:0.2479\n",
    "Epoch #108: Loss:1.5981, Accuracy:0.2435, Validation Loss:1.6063, Validation Accuracy:0.2233\n",
    "Epoch #109: Loss:1.6002, Accuracy:0.2386, Validation Loss:1.6040, Validation Accuracy:0.2414\n",
    "Epoch #110: Loss:1.5991, Accuracy:0.2505, Validation Loss:1.6034, Validation Accuracy:0.2512\n",
    "Epoch #111: Loss:1.5978, Accuracy:0.2464, Validation Loss:1.6029, Validation Accuracy:0.2479\n",
    "Epoch #112: Loss:1.5981, Accuracy:0.2456, Validation Loss:1.6036, Validation Accuracy:0.2463\n",
    "Epoch #113: Loss:1.5974, Accuracy:0.2431, Validation Loss:1.6024, Validation Accuracy:0.2414\n",
    "Epoch #114: Loss:1.5971, Accuracy:0.2452, Validation Loss:1.5998, Validation Accuracy:0.2447\n",
    "Epoch #115: Loss:1.5980, Accuracy:0.2456, Validation Loss:1.6012, Validation Accuracy:0.2479\n",
    "Epoch #116: Loss:1.5980, Accuracy:0.2464, Validation Loss:1.6001, Validation Accuracy:0.2315\n",
    "Epoch #117: Loss:1.5980, Accuracy:0.2374, Validation Loss:1.6009, Validation Accuracy:0.2315\n",
    "Epoch #118: Loss:1.5986, Accuracy:0.2423, Validation Loss:1.6013, Validation Accuracy:0.2447\n",
    "Epoch #119: Loss:1.5984, Accuracy:0.2439, Validation Loss:1.6004, Validation Accuracy:0.2512\n",
    "Epoch #120: Loss:1.5975, Accuracy:0.2435, Validation Loss:1.6001, Validation Accuracy:0.2496\n",
    "Epoch #121: Loss:1.5974, Accuracy:0.2411, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #122: Loss:1.5970, Accuracy:0.2460, Validation Loss:1.6009, Validation Accuracy:0.2447\n",
    "Epoch #123: Loss:1.5969, Accuracy:0.2468, Validation Loss:1.6003, Validation Accuracy:0.2447\n",
    "Epoch #124: Loss:1.5970, Accuracy:0.2390, Validation Loss:1.5995, Validation Accuracy:0.2365\n",
    "Epoch #125: Loss:1.5973, Accuracy:0.2357, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #126: Loss:1.5967, Accuracy:0.2468, Validation Loss:1.5997, Validation Accuracy:0.2496\n",
    "Epoch #127: Loss:1.5963, Accuracy:0.2407, Validation Loss:1.5998, Validation Accuracy:0.2414\n",
    "Epoch #128: Loss:1.5967, Accuracy:0.2398, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #129: Loss:1.5957, Accuracy:0.2419, Validation Loss:1.5997, Validation Accuracy:0.2332\n",
    "Epoch #130: Loss:1.5961, Accuracy:0.2423, Validation Loss:1.6000, Validation Accuracy:0.2447\n",
    "Epoch #131: Loss:1.5959, Accuracy:0.2468, Validation Loss:1.5998, Validation Accuracy:0.2447\n",
    "Epoch #132: Loss:1.5960, Accuracy:0.2472, Validation Loss:1.5999, Validation Accuracy:0.2463\n",
    "Epoch #133: Loss:1.5958, Accuracy:0.2460, Validation Loss:1.5997, Validation Accuracy:0.2365\n",
    "Epoch #134: Loss:1.5957, Accuracy:0.2472, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #135: Loss:1.5962, Accuracy:0.2456, Validation Loss:1.5995, Validation Accuracy:0.2381\n",
    "Epoch #136: Loss:1.5958, Accuracy:0.2431, Validation Loss:1.6002, Validation Accuracy:0.2381\n",
    "Epoch #137: Loss:1.5965, Accuracy:0.2468, Validation Loss:1.6000, Validation Accuracy:0.2282\n",
    "Epoch #138: Loss:1.5950, Accuracy:0.2415, Validation Loss:1.5997, Validation Accuracy:0.2381\n",
    "Epoch #139: Loss:1.5959, Accuracy:0.2468, Validation Loss:1.5992, Validation Accuracy:0.2365\n",
    "Epoch #140: Loss:1.5952, Accuracy:0.2431, Validation Loss:1.5998, Validation Accuracy:0.2365\n",
    "Epoch #141: Loss:1.5964, Accuracy:0.2419, Validation Loss:1.5988, Validation Accuracy:0.2348\n",
    "Epoch #142: Loss:1.5952, Accuracy:0.2460, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #143: Loss:1.5954, Accuracy:0.2431, Validation Loss:1.5998, Validation Accuracy:0.2348\n",
    "Epoch #144: Loss:1.5954, Accuracy:0.2411, Validation Loss:1.5997, Validation Accuracy:0.2381\n",
    "Epoch #145: Loss:1.5954, Accuracy:0.2386, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #146: Loss:1.5957, Accuracy:0.2456, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #147: Loss:1.5942, Accuracy:0.2464, Validation Loss:1.6011, Validation Accuracy:0.2348\n",
    "Epoch #148: Loss:1.5959, Accuracy:0.2419, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #149: Loss:1.5947, Accuracy:0.2505, Validation Loss:1.6022, Validation Accuracy:0.2365\n",
    "Epoch #150: Loss:1.5949, Accuracy:0.2423, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #151: Loss:1.5951, Accuracy:0.2464, Validation Loss:1.6009, Validation Accuracy:0.2315\n",
    "Epoch #152: Loss:1.5954, Accuracy:0.2480, Validation Loss:1.6013, Validation Accuracy:0.2365\n",
    "Epoch #153: Loss:1.5953, Accuracy:0.2444, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #154: Loss:1.5948, Accuracy:0.2427, Validation Loss:1.6012, Validation Accuracy:0.2348\n",
    "Epoch #155: Loss:1.5943, Accuracy:0.2427, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #156: Loss:1.5944, Accuracy:0.2439, Validation Loss:1.6019, Validation Accuracy:0.2348\n",
    "Epoch #157: Loss:1.5946, Accuracy:0.2464, Validation Loss:1.6018, Validation Accuracy:0.2365\n",
    "Epoch #158: Loss:1.5952, Accuracy:0.2444, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #159: Loss:1.5942, Accuracy:0.2472, Validation Loss:1.6018, Validation Accuracy:0.2299\n",
    "Epoch #160: Loss:1.5939, Accuracy:0.2411, Validation Loss:1.6025, Validation Accuracy:0.2365\n",
    "Epoch #161: Loss:1.5941, Accuracy:0.2448, Validation Loss:1.6022, Validation Accuracy:0.2381\n",
    "Epoch #162: Loss:1.5938, Accuracy:0.2472, Validation Loss:1.6023, Validation Accuracy:0.2299\n",
    "Epoch #163: Loss:1.5938, Accuracy:0.2493, Validation Loss:1.6028, Validation Accuracy:0.2200\n",
    "Epoch #164: Loss:1.5946, Accuracy:0.2456, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #165: Loss:1.5957, Accuracy:0.2493, Validation Loss:1.6029, Validation Accuracy:0.2496\n",
    "Epoch #166: Loss:1.5938, Accuracy:0.2497, Validation Loss:1.6043, Validation Accuracy:0.2348\n",
    "Epoch #167: Loss:1.5943, Accuracy:0.2423, Validation Loss:1.6023, Validation Accuracy:0.2348\n",
    "Epoch #168: Loss:1.5946, Accuracy:0.2460, Validation Loss:1.6021, Validation Accuracy:0.2299\n",
    "Epoch #169: Loss:1.5937, Accuracy:0.2378, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #170: Loss:1.5937, Accuracy:0.2439, Validation Loss:1.6029, Validation Accuracy:0.2266\n",
    "Epoch #171: Loss:1.5941, Accuracy:0.2472, Validation Loss:1.6027, Validation Accuracy:0.2266\n",
    "Epoch #172: Loss:1.5931, Accuracy:0.2452, Validation Loss:1.6031, Validation Accuracy:0.2250\n",
    "Epoch #173: Loss:1.5934, Accuracy:0.2378, Validation Loss:1.6035, Validation Accuracy:0.2348\n",
    "Epoch #174: Loss:1.5936, Accuracy:0.2493, Validation Loss:1.6029, Validation Accuracy:0.2266\n",
    "Epoch #175: Loss:1.5939, Accuracy:0.2439, Validation Loss:1.6039, Validation Accuracy:0.2348\n",
    "Epoch #176: Loss:1.5933, Accuracy:0.2431, Validation Loss:1.6036, Validation Accuracy:0.2299\n",
    "Epoch #177: Loss:1.5952, Accuracy:0.2390, Validation Loss:1.6039, Validation Accuracy:0.2512\n",
    "Epoch #178: Loss:1.5927, Accuracy:0.2522, Validation Loss:1.6033, Validation Accuracy:0.2381\n",
    "Epoch #179: Loss:1.5935, Accuracy:0.2522, Validation Loss:1.6038, Validation Accuracy:0.2217\n",
    "Epoch #180: Loss:1.5936, Accuracy:0.2415, Validation Loss:1.6047, Validation Accuracy:0.2266\n",
    "Epoch #181: Loss:1.5930, Accuracy:0.2489, Validation Loss:1.6041, Validation Accuracy:0.2233\n",
    "Epoch #182: Loss:1.5929, Accuracy:0.2464, Validation Loss:1.6042, Validation Accuracy:0.2463\n",
    "Epoch #183: Loss:1.5925, Accuracy:0.2505, Validation Loss:1.6039, Validation Accuracy:0.2496\n",
    "Epoch #184: Loss:1.5923, Accuracy:0.2485, Validation Loss:1.6047, Validation Accuracy:0.2381\n",
    "Epoch #185: Loss:1.5926, Accuracy:0.2456, Validation Loss:1.6048, Validation Accuracy:0.2282\n",
    "Epoch #186: Loss:1.5922, Accuracy:0.2386, Validation Loss:1.6056, Validation Accuracy:0.2217\n",
    "Epoch #187: Loss:1.5924, Accuracy:0.2394, Validation Loss:1.6053, Validation Accuracy:0.2233\n",
    "Epoch #188: Loss:1.5921, Accuracy:0.2480, Validation Loss:1.6047, Validation Accuracy:0.2233\n",
    "Epoch #189: Loss:1.5919, Accuracy:0.2476, Validation Loss:1.6050, Validation Accuracy:0.2266\n",
    "Epoch #190: Loss:1.5930, Accuracy:0.2415, Validation Loss:1.6055, Validation Accuracy:0.2397\n",
    "Epoch #191: Loss:1.5923, Accuracy:0.2468, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #192: Loss:1.5921, Accuracy:0.2513, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #193: Loss:1.5921, Accuracy:0.2435, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #194: Loss:1.5919, Accuracy:0.2468, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #195: Loss:1.5918, Accuracy:0.2472, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #196: Loss:1.5934, Accuracy:0.2493, Validation Loss:1.6060, Validation Accuracy:0.2282\n",
    "Epoch #197: Loss:1.5919, Accuracy:0.2509, Validation Loss:1.6053, Validation Accuracy:0.2266\n",
    "Epoch #198: Loss:1.5917, Accuracy:0.2452, Validation Loss:1.6069, Validation Accuracy:0.2397\n",
    "Epoch #199: Loss:1.5928, Accuracy:0.2435, Validation Loss:1.6065, Validation Accuracy:0.2233\n",
    "Epoch #200: Loss:1.5925, Accuracy:0.2472, Validation Loss:1.6059, Validation Accuracy:0.2365\n",
    "Epoch #201: Loss:1.5933, Accuracy:0.2489, Validation Loss:1.6066, Validation Accuracy:0.2463\n",
    "Epoch #202: Loss:1.5914, Accuracy:0.2472, Validation Loss:1.6056, Validation Accuracy:0.2315\n",
    "Epoch #203: Loss:1.5922, Accuracy:0.2468, Validation Loss:1.6060, Validation Accuracy:0.2430\n",
    "Epoch #204: Loss:1.5920, Accuracy:0.2390, Validation Loss:1.6058, Validation Accuracy:0.2365\n",
    "Epoch #205: Loss:1.5914, Accuracy:0.2464, Validation Loss:1.6056, Validation Accuracy:0.2381\n",
    "Epoch #206: Loss:1.5917, Accuracy:0.2559, Validation Loss:1.6054, Validation Accuracy:0.2529\n",
    "Epoch #207: Loss:1.5923, Accuracy:0.2546, Validation Loss:1.6075, Validation Accuracy:0.2529\n",
    "Epoch #208: Loss:1.5908, Accuracy:0.2505, Validation Loss:1.6067, Validation Accuracy:0.2266\n",
    "Epoch #209: Loss:1.5918, Accuracy:0.2464, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #210: Loss:1.5908, Accuracy:0.2419, Validation Loss:1.6070, Validation Accuracy:0.2430\n",
    "Epoch #211: Loss:1.5928, Accuracy:0.2423, Validation Loss:1.6064, Validation Accuracy:0.2250\n",
    "Epoch #212: Loss:1.5918, Accuracy:0.2534, Validation Loss:1.6070, Validation Accuracy:0.2463\n",
    "Epoch #213: Loss:1.5912, Accuracy:0.2460, Validation Loss:1.6058, Validation Accuracy:0.2381\n",
    "Epoch #214: Loss:1.5908, Accuracy:0.2522, Validation Loss:1.6066, Validation Accuracy:0.2397\n",
    "Epoch #215: Loss:1.5907, Accuracy:0.2542, Validation Loss:1.6072, Validation Accuracy:0.2512\n",
    "Epoch #216: Loss:1.5908, Accuracy:0.2517, Validation Loss:1.6065, Validation Accuracy:0.2348\n",
    "Epoch #217: Loss:1.5905, Accuracy:0.2517, Validation Loss:1.6068, Validation Accuracy:0.2381\n",
    "Epoch #218: Loss:1.5906, Accuracy:0.2517, Validation Loss:1.6071, Validation Accuracy:0.2430\n",
    "Epoch #219: Loss:1.5901, Accuracy:0.2505, Validation Loss:1.6076, Validation Accuracy:0.2447\n",
    "Epoch #220: Loss:1.5905, Accuracy:0.2509, Validation Loss:1.6076, Validation Accuracy:0.2545\n",
    "Epoch #221: Loss:1.5909, Accuracy:0.2559, Validation Loss:1.6067, Validation Accuracy:0.2381\n",
    "Epoch #222: Loss:1.5910, Accuracy:0.2485, Validation Loss:1.6068, Validation Accuracy:0.2266\n",
    "Epoch #223: Loss:1.5899, Accuracy:0.2480, Validation Loss:1.6084, Validation Accuracy:0.2447\n",
    "Epoch #224: Loss:1.5914, Accuracy:0.2452, Validation Loss:1.6066, Validation Accuracy:0.2348\n",
    "Epoch #225: Loss:1.5906, Accuracy:0.2534, Validation Loss:1.6084, Validation Accuracy:0.2447\n",
    "Epoch #226: Loss:1.5907, Accuracy:0.2587, Validation Loss:1.6083, Validation Accuracy:0.2167\n",
    "Epoch #227: Loss:1.5906, Accuracy:0.2439, Validation Loss:1.6077, Validation Accuracy:0.2299\n",
    "Epoch #228: Loss:1.5918, Accuracy:0.2444, Validation Loss:1.6085, Validation Accuracy:0.2266\n",
    "Epoch #229: Loss:1.5908, Accuracy:0.2517, Validation Loss:1.6069, Validation Accuracy:0.2217\n",
    "Epoch #230: Loss:1.5908, Accuracy:0.2567, Validation Loss:1.6071, Validation Accuracy:0.2463\n",
    "Epoch #231: Loss:1.5906, Accuracy:0.2517, Validation Loss:1.6069, Validation Accuracy:0.2562\n",
    "Epoch #232: Loss:1.5902, Accuracy:0.2563, Validation Loss:1.6071, Validation Accuracy:0.2545\n",
    "Epoch #233: Loss:1.5907, Accuracy:0.2554, Validation Loss:1.6068, Validation Accuracy:0.2299\n",
    "Epoch #234: Loss:1.5896, Accuracy:0.2587, Validation Loss:1.6088, Validation Accuracy:0.2430\n",
    "Epoch #235: Loss:1.5897, Accuracy:0.2522, Validation Loss:1.6071, Validation Accuracy:0.2447\n",
    "Epoch #236: Loss:1.5895, Accuracy:0.2505, Validation Loss:1.6079, Validation Accuracy:0.2414\n",
    "Epoch #237: Loss:1.5894, Accuracy:0.2526, Validation Loss:1.6078, Validation Accuracy:0.2397\n",
    "Epoch #238: Loss:1.5890, Accuracy:0.2534, Validation Loss:1.6081, Validation Accuracy:0.2414\n",
    "Epoch #239: Loss:1.5893, Accuracy:0.2513, Validation Loss:1.6080, Validation Accuracy:0.2545\n",
    "Epoch #240: Loss:1.5892, Accuracy:0.2583, Validation Loss:1.6076, Validation Accuracy:0.2479\n",
    "Epoch #241: Loss:1.5893, Accuracy:0.2546, Validation Loss:1.6078, Validation Accuracy:0.2233\n",
    "Epoch #242: Loss:1.5896, Accuracy:0.2480, Validation Loss:1.6070, Validation Accuracy:0.2282\n",
    "Epoch #243: Loss:1.5893, Accuracy:0.2563, Validation Loss:1.6098, Validation Accuracy:0.2447\n",
    "Epoch #244: Loss:1.5898, Accuracy:0.2480, Validation Loss:1.6083, Validation Accuracy:0.2315\n",
    "Epoch #245: Loss:1.5892, Accuracy:0.2534, Validation Loss:1.6090, Validation Accuracy:0.2545\n",
    "Epoch #246: Loss:1.5891, Accuracy:0.2571, Validation Loss:1.6070, Validation Accuracy:0.2529\n",
    "Epoch #247: Loss:1.5895, Accuracy:0.2563, Validation Loss:1.6080, Validation Accuracy:0.2414\n",
    "Epoch #248: Loss:1.5887, Accuracy:0.2517, Validation Loss:1.6076, Validation Accuracy:0.2266\n",
    "Epoch #249: Loss:1.5893, Accuracy:0.2567, Validation Loss:1.6093, Validation Accuracy:0.2447\n",
    "Epoch #250: Loss:1.5895, Accuracy:0.2559, Validation Loss:1.6075, Validation Accuracy:0.2315\n",
    "Epoch #251: Loss:1.5886, Accuracy:0.2546, Validation Loss:1.6092, Validation Accuracy:0.2447\n",
    "Epoch #252: Loss:1.5891, Accuracy:0.2513, Validation Loss:1.6076, Validation Accuracy:0.2447\n",
    "Epoch #253: Loss:1.5884, Accuracy:0.2567, Validation Loss:1.6083, Validation Accuracy:0.2430\n",
    "Epoch #254: Loss:1.5881, Accuracy:0.2505, Validation Loss:1.6096, Validation Accuracy:0.2430\n",
    "Epoch #255: Loss:1.5883, Accuracy:0.2583, Validation Loss:1.6087, Validation Accuracy:0.2562\n",
    "Epoch #256: Loss:1.5881, Accuracy:0.2501, Validation Loss:1.6082, Validation Accuracy:0.2381\n",
    "Epoch #257: Loss:1.5882, Accuracy:0.2542, Validation Loss:1.6088, Validation Accuracy:0.2463\n",
    "Epoch #258: Loss:1.5881, Accuracy:0.2571, Validation Loss:1.6088, Validation Accuracy:0.2463\n",
    "Epoch #259: Loss:1.5891, Accuracy:0.2517, Validation Loss:1.6074, Validation Accuracy:0.2365\n",
    "Epoch #260: Loss:1.5881, Accuracy:0.2485, Validation Loss:1.6093, Validation Accuracy:0.2233\n",
    "Epoch #261: Loss:1.5880, Accuracy:0.2608, Validation Loss:1.6104, Validation Accuracy:0.2447\n",
    "Epoch #262: Loss:1.5890, Accuracy:0.2550, Validation Loss:1.6078, Validation Accuracy:0.2529\n",
    "Epoch #263: Loss:1.5885, Accuracy:0.2595, Validation Loss:1.6099, Validation Accuracy:0.2447\n",
    "Epoch #264: Loss:1.5877, Accuracy:0.2579, Validation Loss:1.6093, Validation Accuracy:0.2266\n",
    "Epoch #265: Loss:1.5881, Accuracy:0.2567, Validation Loss:1.6089, Validation Accuracy:0.2282\n",
    "Epoch #266: Loss:1.5879, Accuracy:0.2587, Validation Loss:1.6086, Validation Accuracy:0.2414\n",
    "Epoch #267: Loss:1.5876, Accuracy:0.2624, Validation Loss:1.6081, Validation Accuracy:0.2430\n",
    "Epoch #268: Loss:1.5878, Accuracy:0.2616, Validation Loss:1.6089, Validation Accuracy:0.2463\n",
    "Epoch #269: Loss:1.5883, Accuracy:0.2542, Validation Loss:1.6090, Validation Accuracy:0.2315\n",
    "Epoch #270: Loss:1.5885, Accuracy:0.2554, Validation Loss:1.6101, Validation Accuracy:0.2430\n",
    "Epoch #271: Loss:1.5885, Accuracy:0.2567, Validation Loss:1.6068, Validation Accuracy:0.2397\n",
    "Epoch #272: Loss:1.5884, Accuracy:0.2493, Validation Loss:1.6115, Validation Accuracy:0.2414\n",
    "Epoch #273: Loss:1.5880, Accuracy:0.2575, Validation Loss:1.6080, Validation Accuracy:0.2233\n",
    "Epoch #274: Loss:1.5869, Accuracy:0.2624, Validation Loss:1.6095, Validation Accuracy:0.2430\n",
    "Epoch #275: Loss:1.5877, Accuracy:0.2632, Validation Loss:1.6091, Validation Accuracy:0.2479\n",
    "Epoch #276: Loss:1.5869, Accuracy:0.2620, Validation Loss:1.6090, Validation Accuracy:0.2463\n",
    "Epoch #277: Loss:1.5872, Accuracy:0.2554, Validation Loss:1.6079, Validation Accuracy:0.2200\n",
    "Epoch #278: Loss:1.5875, Accuracy:0.2579, Validation Loss:1.6095, Validation Accuracy:0.2463\n",
    "Epoch #279: Loss:1.5871, Accuracy:0.2526, Validation Loss:1.6082, Validation Accuracy:0.2529\n",
    "Epoch #280: Loss:1.5872, Accuracy:0.2575, Validation Loss:1.6084, Validation Accuracy:0.2496\n",
    "Epoch #281: Loss:1.5867, Accuracy:0.2645, Validation Loss:1.6090, Validation Accuracy:0.2299\n",
    "Epoch #282: Loss:1.5888, Accuracy:0.2550, Validation Loss:1.6107, Validation Accuracy:0.2282\n",
    "Epoch #283: Loss:1.5869, Accuracy:0.2595, Validation Loss:1.6084, Validation Accuracy:0.2085\n",
    "Epoch #284: Loss:1.5875, Accuracy:0.2534, Validation Loss:1.6090, Validation Accuracy:0.2414\n",
    "Epoch #285: Loss:1.5864, Accuracy:0.2612, Validation Loss:1.6088, Validation Accuracy:0.2414\n",
    "Epoch #286: Loss:1.5870, Accuracy:0.2517, Validation Loss:1.6082, Validation Accuracy:0.2184\n",
    "Epoch #287: Loss:1.5877, Accuracy:0.2595, Validation Loss:1.6096, Validation Accuracy:0.2430\n",
    "Epoch #288: Loss:1.5865, Accuracy:0.2612, Validation Loss:1.6078, Validation Accuracy:0.2496\n",
    "Epoch #289: Loss:1.5862, Accuracy:0.2600, Validation Loss:1.6092, Validation Accuracy:0.2430\n",
    "Epoch #290: Loss:1.5861, Accuracy:0.2628, Validation Loss:1.6087, Validation Accuracy:0.2496\n",
    "Epoch #291: Loss:1.5858, Accuracy:0.2632, Validation Loss:1.6096, Validation Accuracy:0.2414\n",
    "Epoch #292: Loss:1.5864, Accuracy:0.2563, Validation Loss:1.6102, Validation Accuracy:0.2430\n",
    "Epoch #293: Loss:1.5877, Accuracy:0.2571, Validation Loss:1.6083, Validation Accuracy:0.2512\n",
    "Epoch #294: Loss:1.5859, Accuracy:0.2604, Validation Loss:1.6081, Validation Accuracy:0.2496\n",
    "Epoch #295: Loss:1.5857, Accuracy:0.2645, Validation Loss:1.6086, Validation Accuracy:0.2414\n",
    "Epoch #296: Loss:1.5858, Accuracy:0.2628, Validation Loss:1.6083, Validation Accuracy:0.2430\n",
    "Epoch #297: Loss:1.5861, Accuracy:0.2600, Validation Loss:1.6087, Validation Accuracy:0.2250\n",
    "Epoch #298: Loss:1.5860, Accuracy:0.2600, Validation Loss:1.6109, Validation Accuracy:0.2512\n",
    "Epoch #299: Loss:1.5864, Accuracy:0.2649, Validation Loss:1.6074, Validation Accuracy:0.2512\n",
    "Epoch #300: Loss:1.5866, Accuracy:0.2669, Validation Loss:1.6108, Validation Accuracy:0.2430\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61078537, Accuracy:0.2430\n",
    "Labels: ['04', '01', '03', '02', '05']\n",
    "Confusion Matrix:\n",
    "      04  01  03  02  05\n",
    "t:04  19  31   1   5  56\n",
    "t:01  15  43   4   1  63\n",
    "t:03  12  47   4   1  51\n",
    "t:02  18  38   3   1  54\n",
    "t:05  15  43   1   2  81\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.24      0.17      0.20       112\n",
    "          01       0.21      0.34      0.26       126\n",
    "          03       0.31      0.03      0.06       115\n",
    "          02       0.10      0.01      0.02       114\n",
    "          05       0.27      0.57      0.36       142\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.23      0.22      0.18       609\n",
    "weighted avg       0.23      0.24      0.19       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 14:54:17 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6063952019257695, 1.605531993170677, 1.6046934153254593, 1.6042449715102247, 1.6035935704539759, 1.6034458932422457, 1.6030564644849554, 1.6025730719903029, 1.6022699940185046, 1.6017447208927573, 1.6018775384414372, 1.6013721779649481, 1.6009339507185962, 1.6002697936811274, 1.6000541331145564, 1.5998444940852024, 1.599740899450869, 1.5993879035188647, 1.5993712960401387, 1.5990019962313924, 1.599117564450344, 1.5988725136066306, 1.598580304429253, 1.5983642263365496, 1.5983117601554382, 1.5982575880482865, 1.5988051217960801, 1.5989161571258395, 1.598738992742717, 1.5982555201879667, 1.598451476770473, 1.5992837030507856, 1.5998206410697724, 1.59860918380944, 1.5989027444169244, 1.5993859129781989, 1.599489001021988, 1.5996020398116464, 1.5996942788313566, 1.5990319743336519, 1.5987269430129203, 1.598151289379264, 1.5991167276363654, 1.5991111702128193, 1.5990824309867395, 1.598829516832073, 1.599267899304971, 1.599051633491892, 1.5986192903690932, 1.5989966435581202, 1.5984299098721082, 1.5981372653557162, 1.598316080660264, 1.5983595033780302, 1.5979800936819493, 1.598065324213313, 1.5982825562284497, 1.598961280680251, 1.5982366395114092, 1.5988371241073107, 1.5983475726617773, 1.5984398846947305, 1.5988275018231621, 1.599378413950477, 1.599691648201402, 1.5998794960075216, 1.6001146890846967, 1.6006406743342458, 1.6003261669515976, 1.6001220093963573, 1.6005078610919772, 1.6008435039488944, 1.600097061965266, 1.6002907584649197, 1.6007807667815235, 1.6000609211929522, 1.6006306172983205, 1.6002610659560155, 1.6009680758947613, 1.5996795728288848, 1.5997218406454878, 1.599690580994429, 1.6005589813238685, 1.6005403281041162, 1.6008772241266687, 1.6007127881245855, 1.6003729709850743, 1.5998392351742448, 1.6008239183911352, 1.600140504844866, 1.5986748258468553, 1.598992309938315, 1.597927992371307, 1.5981675092809893, 1.5986064770343074, 1.599405942683541, 1.6052300624659497, 1.6015742482810185, 1.600112112676373, 1.5997020890951548, 1.6003068188336877, 1.600948335501948, 1.6022690436718694, 1.6012664788658004, 1.6016323321754318, 1.6019433701566874, 1.6014223431523014, 1.6063348189950577, 1.6039525519059405, 1.60341988447656, 1.6029161528022027, 1.6036287415008044, 1.6024416994382986, 1.5998129167384507, 1.6012435873545254, 1.6000854853534543, 1.6009026102244561, 1.6013438916950196, 1.6003893576623576, 1.600095607964276, 1.60042988373141, 1.600935142224254, 1.6002803024987282, 1.5994934736214248, 1.5998188355090388, 1.5997086424741447, 1.5997978642656299, 1.6003657784955254, 1.5997388775908497, 1.6000259760369613, 1.5998349475547402, 1.5998612156838228, 1.5997403658473826, 1.5996081421919448, 1.599535887073022, 1.6001872321459265, 1.6000212800913844, 1.599748383797644, 1.599201465475148, 1.5997720441990493, 1.5988344288811895, 1.6006564637906053, 1.599838639715035, 1.5996882298897053, 1.6003627274032493, 1.6009774296154529, 1.6010663943924928, 1.6003410526488606, 1.6021817200289572, 1.60076846786712, 1.600875472200328, 1.6012531886938561, 1.600828780525032, 1.6011788946850154, 1.60135455652215, 1.6018916530953644, 1.6018479988101277, 1.6017213049780559, 1.601841888012753, 1.6024930134586903, 1.60223180965837, 1.602309803070106, 1.6028078491073132, 1.6025904738061338, 1.6028741970046596, 1.6043373878757745, 1.6022741473562807, 1.6021234899123118, 1.6034701402942926, 1.602922416868664, 1.6027482239092121, 1.6031471625924698, 1.6035313894008767, 1.6029193358272558, 1.6039208647457055, 1.6036276390595585, 1.6039219464378796, 1.6033298438797248, 1.6038414756652757, 1.6047176843010538, 1.6040516179574926, 1.6041625319247568, 1.6038550324432173, 1.6047218235451208, 1.604760428759069, 1.6055732092442379, 1.605308214236167, 1.604652210996656, 1.6049617538702703, 1.605458935689065, 1.6052554029549284, 1.605514948199731, 1.6063253998952154, 1.6054925615172864, 1.6064585662631958, 1.6059814260902467, 1.6052744116493438, 1.6069060278252036, 1.6064628200186493, 1.6059261828612028, 1.6066136373870674, 1.605617076501079, 1.606024585333951, 1.605838247903658, 1.6056082681286314, 1.6053530101118416, 1.6075353775118373, 1.6066523896062315, 1.60598032854265, 1.6070215052180299, 1.6063662880942935, 1.6070167442847942, 1.6058250432726981, 1.6066424046048193, 1.6072371391631504, 1.6065233681589512, 1.60679694525714, 1.6070981988765922, 1.6075978059878295, 1.6075516406734198, 1.6067020362625373, 1.6068004706418768, 1.6083878799416553, 1.606598670455231, 1.6083610789920701, 1.608295178569988, 1.6077294290946622, 1.6084909630917954, 1.606901563838589, 1.607076156902783, 1.6068979560448031, 1.6070784862797052, 1.606766904711919, 1.6088280713029683, 1.6070996618818962, 1.6078659000459368, 1.6078055960008468, 1.6080618603476162, 1.6080366151869199, 1.6075578201776264, 1.6077810968280035, 1.607012497771941, 1.6098224459023311, 1.6082825509981178, 1.6090271306546842, 1.6070184310277302, 1.6080381349585522, 1.6075657234207554, 1.609254110035638, 1.6074823072586937, 1.6092057674389166, 1.6076114988092132, 1.6083335408632, 1.6096481298186704, 1.6086951124257054, 1.6082429862374743, 1.6087879335939004, 1.6088123117957405, 1.6074433244507889, 1.6093435743563673, 1.610406507216455, 1.607809914743959, 1.6099046243626887, 1.6092771000071309, 1.6089259801044058, 1.6086085725496164, 1.6080755148027919, 1.6089486897676841, 1.6089821636970407, 1.6100595721666058, 1.6067851867973315, 1.6114523299221921, 1.6080375682739985, 1.6094938657553912, 1.6091067520855682, 1.609027775441876, 1.6079460122119422, 1.6094886228956025, 1.6082054155409238, 1.6084044425945563, 1.6089797986943537, 1.6106985597970647, 1.6084428843606282, 1.6090418370486481, 1.6087850355749647, 1.608216151423838, 1.6096132189182226, 1.6078266888024968, 1.6092457417001083, 1.6087061720724372, 1.6096091865514495, 1.6102058991227048, 1.6082915696017261, 1.6080902558437904, 1.608552381127143, 1.6083163118910515, 1.6086789330433937, 1.6108933659805649, 1.6073969913820916, 1.6107852439379262], 'val_acc': [0.22331691284974417, 0.23152709318009895, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.23973727380407267, 0.23809523767927793, 0.2446633821784569, 0.24302134605366216, 0.24302134605366216, 0.24794745433017343, 0.24958949055284116, 0.2446633821784569, 0.2446633821784569, 0.2413793099288674, 0.23481116542968844, 0.24302134605366216, 0.24794745442804642, 0.24794745442804642, 0.2413793099288674, 0.24302134605366216, 0.24302134605366216, 0.24466338208058394, 0.24794745452591938, 0.24794745433017343, 0.24794745452591938, 0.24302134605366216, 0.24630541820537868, 0.24794745433017343, 0.24958949065071412, 0.24794745442804642, 0.24958949055284116, 0.24958949055284116, 0.24630541830325164, 0.24958949055284116, 0.23973727360832672, 0.24958949055284116, 0.24958949065071412, 0.25451559892722536, 0.24958949055284116, 0.2512315266776359, 0.24630541820537868, 0.25451559892722536, 0.25451559892722536, 0.25451559892722536, 0.2446633821784569, 0.25451559892722536, 0.25451559892722536, 0.2233169125561252, 0.23809523758140494, 0.2528735628024306, 0.2430213459557892, 0.24137930983099445, 0.23645320165235617, 0.25451559892722536, 0.25451559892722536, 0.23645320155448318, 0.2413793099288674, 0.24958949055284116, 0.22988505685955823, 0.2430213459557892, 0.24466338208058394, 0.2512315265797629, 0.23973727380407267, 0.23809523767927793, 0.24466338198271095, 0.2561576350520201, 0.22660098460996875, 0.23973727370619968, 0.24794745433017343, 0.24302134605366216, 0.24302134605366216, 0.24137930983099445, 0.24794745442804642, 0.23973727370619968, 0.24630541830325164, 0.24630541820537868, 0.24794745433017343, 0.24630541830325164, 0.2348111651360695, 0.24137930983099445, 0.23152709288648002, 0.24794745442804642, 0.24630541830325164, 0.23152709288648002, 0.23645320126086425, 0.24794745433017343, 0.23481116552756143, 0.24137930983099445, 0.24137930983099445, 0.2364532014566102, 0.25123152687338185, 0.24302134615153514, 0.23973727370619968, 0.24794745442804642, 0.25123152677550886, 0.24794745462379236, 0.24794745462379236, 0.22331691245825225, 0.2413793099288674, 0.25123152677550886, 0.24794745452591938, 0.24630541840112463, 0.24137930983099445, 0.24466338208058394, 0.24794745462379236, 0.23152709308222597, 0.23152709298435298, 0.24466338237420288, 0.25123152687338185, 0.24958949065071412, 0.24302134615153514, 0.2446633822763299, 0.2446633822763299, 0.2364532014566102, 0.24794745442804642, 0.24958949065071412, 0.2413793099288674, 0.2331691292070207, 0.2331691292070207, 0.2446633821784569, 0.2446633821784569, 0.24630541830325164, 0.2364532014566102, 0.24794745442804642, 0.23809523767927793, 0.23809523758140494, 0.22824302034327157, 0.23809523758140494, 0.23645320135873724, 0.2364532014566102, 0.2348111652339425, 0.23973727370619968, 0.23481116474457758, 0.23809523748353198, 0.24302134605366216, 0.2331691292070207, 0.2348111658334145, 0.2430213458579162, 0.2364532014566102, 0.23973727360832672, 0.2315270924949881, 0.2364532014566102, 0.2364532014566102, 0.2348111652339425, 0.23645320135873724, 0.2348111652339425, 0.2364532014566102, 0.2364532014566102, 0.22988505637019335, 0.2364532014566102, 0.23809523748353198, 0.22988505637019335, 0.22003284020866276, 0.2446633821784569, 0.24958948996560326, 0.23481116533181545, 0.2348111651360695, 0.2298850564680663, 0.2331691292070207, 0.22660098412060387, 0.22660098412060387, 0.224958948485174, 0.23481116533181545, 0.22660098412060387, 0.2348111652339425, 0.22988505637019335, 0.2512315265797629, 0.23809523699416707, 0.22167487623558452, 0.22660098412060387, 0.22331691187101435, 0.24630541820537868, 0.24958949045496817, 0.23809523748353198, 0.2282430202453986, 0.22167487623558452, 0.22331691187101435, 0.22331691187101435, 0.22660098412060387, 0.23973727360832672, 0.23645320096724531, 0.23645320096724531, 0.2331691292070207, 0.2282430202453986, 0.23316912901127476, 0.2282430202453986, 0.22660098412060387, 0.23973727360832672, 0.22331691187101435, 0.23645320096724531, 0.2463054181075057, 0.23152709259286106, 0.2430213458579162, 0.23645320086937233, 0.23809523699416707, 0.25287356221519275, 0.2528735627045577, 0.22660098412060387, 0.23481116484245057, 0.2430213458579162, 0.22495894809368208, 0.2463054181075057, 0.23809523709204006, 0.2397372732168348, 0.2512315265797629, 0.23481116484245057, 0.23809523718991302, 0.24302134556429728, 0.24466338159121903, 0.2545155983399875, 0.23809523709204006, 0.22660098412060387, 0.24466338198271095, 0.23481116484245057, 0.24466338208058394, 0.2167487683627993, 0.2298850564680663, 0.22660098412060387, 0.2216748757462196, 0.24630541771601377, 0.25615763446478224, 0.2545155984378605, 0.2298850564680663, 0.2430213459557892, 0.24466338159121903, 0.24137930943950253, 0.2397372733147078, 0.24137930943950253, 0.2545155984378605, 0.2479474539386815, 0.22331691196888734, 0.22824302034327157, 0.2446633821784569, 0.23152709259286106, 0.25451559853573347, 0.2528735624109387, 0.24137930943950253, 0.2266009843163498, 0.24466338159121903, 0.23152709269073404, 0.2446633821784569, 0.244663381786965, 0.24302134566217024, 0.2430213454664243, 0.2561576345626552, 0.238095237287786, 0.24630541781388676, 0.24630541781388676, 0.2364532014566102, 0.22331691196888734, 0.24466338168909202, 0.2528735624109387, 0.24466338168909202, 0.2266009844142228, 0.22824302063689053, 0.2413793095373755, 0.24302134576004322, 0.24630541791175975, 0.23152709269073404, 0.2430213454664243, 0.23973727351045374, 0.24137930924375656, 0.2233169121646333, 0.24302134576004322, 0.24794745413442745, 0.24630541791175975, 0.22003284001291679, 0.24630541791175975, 0.2528735625088117, 0.2495894902592222, 0.2298850566638123, 0.22824302053901754, 0.20853858674786166, 0.24137930963524848, 0.24137930963524848, 0.21839080379024906, 0.24302134576004322, 0.2495894902592222, 0.24302134576004322, 0.2495894902592222, 0.24137930963524848, 0.24302134576004322, 0.25123152638401697, 0.2495894902592222, 0.24137930963524848, 0.24302134576004322, 0.22495894828942806, 0.25123152638401697, 0.25123152638401697, 0.24302134576004322], 'loss': [1.6089440160708262, 1.6056880761465742, 1.6058919688759399, 1.6057124150117565, 1.6051603643801178, 1.605001273634987, 1.604647990565525, 1.604405979943716, 1.6040036587254958, 1.6038255442339293, 1.6034502249723588, 1.603029851257434, 1.6032520306918165, 1.6028880250037818, 1.6027700668487705, 1.6024049140589438, 1.6021750479018664, 1.6018828928593003, 1.6017873170684251, 1.601868847899858, 1.601463359582106, 1.6013929599609218, 1.6018126044185255, 1.6011989691419033, 1.6008904576546357, 1.6010452716012755, 1.6008031098015254, 1.6007821719994046, 1.6004475541672911, 1.6003722978078854, 1.600126685156225, 1.6003202743843596, 1.6003079139721221, 1.6004618361745282, 1.5999968747093938, 1.6004297725963397, 1.5999135184826547, 1.599599966082485, 1.5993895029874798, 1.5992435649679917, 1.5992362951595924, 1.5987963585393385, 1.5990561653211621, 1.5989315009949387, 1.5993133188028354, 1.5982316436219264, 1.5984683995374174, 1.5984536949369208, 1.5984227596611946, 1.5980225356942084, 1.59824626607327, 1.5980776013290123, 1.597465051517839, 1.5978719613390537, 1.5973429006227968, 1.5969969449346806, 1.597304358668396, 1.597646642122915, 1.5982458659021272, 1.5976021311855904, 1.5976072639900067, 1.597858027755847, 1.5978660978086185, 1.5976924433600486, 1.59715546450331, 1.596887211535256, 1.5973731127607749, 1.5975726031669601, 1.5968076172305818, 1.5969595042097495, 1.59645699793798, 1.5969922115181017, 1.5964554681915033, 1.5970496871877744, 1.596592581933039, 1.5967447279904656, 1.596038634233651, 1.5968224153381598, 1.5963963683380973, 1.5971055721110632, 1.5967208961686559, 1.5968316329333327, 1.5971015679028489, 1.5971342374656725, 1.5983699515125345, 1.5978879643171964, 1.5966904111466613, 1.5970031532174018, 1.5971133774555684, 1.5971369941621345, 1.597127979394102, 1.5966102214809317, 1.596610237783475, 1.597553757328762, 1.596438275127685, 1.596742708237509, 1.5988574989032942, 1.5991607737002675, 1.5988095067120185, 1.597923226963568, 1.5976123187086666, 1.5975095067915241, 1.5971413455704644, 1.5970484635178803, 1.596726703986495, 1.5977925655043834, 1.59752185574792, 1.5981403530011187, 1.6002359867585514, 1.5991225894716485, 1.597802649729061, 1.5980558913346434, 1.5973650451068762, 1.5970935047039996, 1.597964784790603, 1.597964955404309, 1.597965115002783, 1.5986141678733747, 1.598351929271001, 1.5974875914242723, 1.5974149332398997, 1.5969652789329356, 1.5968520930170769, 1.597009901833975, 1.5972826496531587, 1.5967365446521515, 1.5962888911519453, 1.596721547193351, 1.5957372678133985, 1.5960900439863577, 1.5959109162403082, 1.5959532344120972, 1.595782614880274, 1.5956789444114639, 1.5961570316271616, 1.5958115253115581, 1.5964849710464477, 1.5949582548846455, 1.5959432642073113, 1.5951550793109244, 1.5964025927764924, 1.5952118395290336, 1.5953965226238023, 1.595355219273107, 1.595418498354526, 1.5956601214849484, 1.594248535892557, 1.5959106287182723, 1.5947337633285679, 1.5948985199663919, 1.595145394523041, 1.5954024521966734, 1.5952738548940701, 1.5948164497312824, 1.5942961750578832, 1.5944371549011012, 1.59456994832174, 1.5951705289327633, 1.5942218794225422, 1.5939450661504537, 1.594053454712431, 1.5937832085258907, 1.5937756333263013, 1.5945537821712925, 1.5956942776145386, 1.5937626531236715, 1.594259290186042, 1.5945887571977149, 1.5936591024026734, 1.5936510404277386, 1.5940672914595084, 1.5930637576496822, 1.5934307595298032, 1.5935789788772927, 1.593869691462977, 1.593317222203562, 1.5951956618248315, 1.5926643545377916, 1.5935063544240087, 1.5936310097911763, 1.5929881939408226, 1.5928908490547165, 1.5924939498764288, 1.5922684091806902, 1.5926016042854263, 1.5922254962353246, 1.592376601916319, 1.5921059427320101, 1.5919497578051056, 1.5929930806894321, 1.592320139158433, 1.5921497932939315, 1.5921412304441542, 1.5919292983088404, 1.591830941294253, 1.5933903884593956, 1.5919113282060722, 1.5916948545640008, 1.5927603332413784, 1.5925099641145868, 1.5933221085605191, 1.591417196692872, 1.5922024916329667, 1.592043825194576, 1.5913682014300838, 1.5916655609495096, 1.5923305566305987, 1.5907807049565246, 1.5918035151777326, 1.5908004173752708, 1.5928170065615457, 1.5917643975428242, 1.5912394553484124, 1.5908114308938843, 1.5906574625743732, 1.5907697507243381, 1.5904699725047273, 1.590629810816943, 1.5900899803369197, 1.5905056865797884, 1.5908798357789276, 1.5910351764494879, 1.5898668858549678, 1.5914013051399216, 1.590642474662107, 1.5906850124041891, 1.590578209154415, 1.591825176411341, 1.5907626174069038, 1.5908097605440896, 1.590610120330748, 1.5902101853055386, 1.5907394448344958, 1.5895528309154314, 1.5897381236665793, 1.589495919568338, 1.5894066473297026, 1.5890399741930639, 1.5893026117426658, 1.5892393085256493, 1.5893182904813323, 1.5895976225208697, 1.5892634997867217, 1.5898031234251646, 1.5892278365775545, 1.5891400039563188, 1.5894941397516145, 1.5887271794939923, 1.589295506477356, 1.5894681954041154, 1.5886309974247426, 1.5890857407199774, 1.588375556836138, 1.5881203837463254, 1.5882940577775302, 1.5881371758067389, 1.5882360741832662, 1.588051824158467, 1.589139575096616, 1.5881297698990275, 1.5879915644745561, 1.5890249776644383, 1.588483226715417, 1.5877388692244856, 1.5880653886090068, 1.5878824271215795, 1.5875879610833201, 1.587764068844382, 1.588339205835879, 1.5884560348561656, 1.5885394726445787, 1.5883529895629727, 1.5879924913696195, 1.5868902773827742, 1.5876969672326435, 1.5868964876727156, 1.58717211012478, 1.5874895448312623, 1.5871238442661826, 1.5871760041806733, 1.5867265472666683, 1.5888247953548078, 1.5869089958848894, 1.5875461660126642, 1.586439487527773, 1.5870068323440865, 1.5877424092752979, 1.586525724702792, 1.586225263342965, 1.5861494006562282, 1.5857931396065306, 1.586387664483558, 1.5876735780273374, 1.5859461822059364, 1.5856882986346799, 1.5858226672335083, 1.5860687729759138, 1.5859583561914903, 1.5863549869897673, 1.5865698050669332], 'acc': [0.2045174534927893, 0.23326488632930623, 0.2328542083195837, 0.2328542094945418, 0.23285420890706277, 0.2328542094945418, 0.23285421010037957, 0.2328542083195837, 0.23285420929871545, 0.23285421049203225, 0.23285420969036816, 0.23285421008202084, 0.23285420912124782, 0.2328542106694999, 0.233264887895917, 0.236139630830753, 0.2361396323973638, 0.2381930193127548, 0.23860369516838748, 0.23901437378394774, 0.2418891173062628, 0.24435318397545472, 0.23983572902008737, 0.24271047389482817, 0.24188911787538314, 0.24188911670042504, 0.2431211487713291, 0.24024640603231942, 0.24188911732462154, 0.24188911789374185, 0.24065708325873655, 0.241478440648966, 0.2439425051824268, 0.2427104715632707, 0.24271047271987006, 0.2414784414322714, 0.24271047136744434, 0.24271047134908563, 0.24353182517772337, 0.24517453803663625, 0.2447638610244042, 0.2472279255395063, 0.24476386122023056, 0.245174539015768, 0.2488706374009287, 0.24804928040235194, 0.24722792413200442, 0.24640657090920443, 0.24804928214643035, 0.24681724931057963, 0.2455852166338378, 0.24640657188833617, 0.24763860492001324, 0.24722792614534406, 0.24517453842828896, 0.24887063601178555, 0.24763860337176116, 0.24106776068098001, 0.23367556555070426, 0.24229979510180025, 0.2472279255395063, 0.24845995958703254, 0.24394250576990587, 0.24640657108667205, 0.24681724813562153, 0.2443531838163458, 0.24271047175909705, 0.2455852160647175, 0.2455852178087959, 0.24887063640343823, 0.24147844025731333, 0.24558521665219654, 0.24229979472850627, 0.24394250598409092, 0.24065708365038924, 0.24599589309530825, 0.2459958928994819, 0.2476386031575761, 0.2418891180895682, 0.24147843929654028, 0.2427104736990018, 0.24106776167847047, 0.2455852156730648, 0.24476386081021914, 0.24353182697687795, 0.24188911787538314, 0.2431211503562986, 0.24106776187429682, 0.23983572821842328, 0.2451745376633423, 0.2377823401280742, 0.24271047213239103, 0.24065708482534734, 0.2353182740280026, 0.2394250523811493, 0.23737166409497387, 0.24065708462952098, 0.2361396310449381, 0.24065708443369463, 0.24188911509709682, 0.24928131382568172, 0.24681724987969997, 0.24763860413670785, 0.24887063836170173, 0.24804928292973574, 0.24312115092541892, 0.246817249506406, 0.24353182817019475, 0.23860369634334555, 0.2505133464840648, 0.2464065708908457, 0.24558521624218513, 0.24312115131707163, 0.24517453842828896, 0.24558521467557434, 0.24640657030336666, 0.23737166505574692, 0.24229979572599675, 0.24394250420329508, 0.24353182660358397, 0.24106776244341716, 0.24599589231200286, 0.24681724833144789, 0.23901437298228365, 0.23572895283938922, 0.24681724811726283, 0.24065708308126893, 0.2398357284142496, 0.24188911709207775, 0.24229979431849485, 0.24681724772561012, 0.24722792612698533, 0.24599589268529684, 0.247227925931159, 0.24558521522633595, 0.2431211499646459, 0.24681724772561012, 0.24147844007984567, 0.2468172475297838, 0.2431211499462872, 0.2418891168962514, 0.2459958920978178, 0.24312115014211352, 0.2410677622475908, 0.23860369595169287, 0.24558521586889115, 0.24640657149668346, 0.24188911728790408, 0.2505133471082613, 0.24229979570763802, 0.24640657108667205, 0.24804928038399324, 0.24435318060968936, 0.24271046994158374, 0.242710471171618, 0.24394250420329508, 0.24640657265328283, 0.24435318377962836, 0.24722792653699674, 0.24106776087680637, 0.24476385963526104, 0.24722792788942247, 0.24928131482317217, 0.24558521585053242, 0.24928131521482488, 0.24969199263706834, 0.24229979568927928, 0.24599589483938666, 0.23778234190887004, 0.2439425058066233, 0.2472279276935961, 0.24517453686167817, 0.2377823417130437, 0.2492813157839452, 0.24394250500495918, 0.24312115033793988, 0.23901437339229506, 0.25215605814966086, 0.25215605814966086, 0.24147843908235522, 0.2488706369909173, 0.24640657226163015, 0.2505133468940762, 0.24845995976450017, 0.2455852164563702, 0.238603696380063, 0.23942505236279057, 0.24804928136312496, 0.24763860335340246, 0.2414784400982044, 0.24681724833144789, 0.2513347019160308, 0.2435318283660211, 0.24681724811726283, 0.2472279251478536, 0.24928131521482488, 0.25092402527709273, 0.24517453686167817, 0.2435318263893989, 0.24722792573533264, 0.2488706354243065, 0.2472279271244758, 0.24681725027135265, 0.23901437552802618, 0.24640657030336666, 0.25585215612481016, 0.25462012366225345, 0.25051334787320795, 0.2464065708908457, 0.24188911671878377, 0.24229979431849485, 0.2533880921604697, 0.24599589346860223, 0.2521560579354758, 0.25420944778826204, 0.25174537812659875, 0.2517453814923641, 0.25174538051323236, 0.25051334628823846, 0.25092402527709273, 0.25585215571479875, 0.2484599585895421, 0.24804928097147227, 0.2451745384466477, 0.25338808982891226, 0.2587269004120719, 0.24394250400746872, 0.24435318377962836, 0.251745380317406, 0.2566735133008546, 0.2517453779307724, 0.25626283374288, 0.2554414784883816, 0.25872690021624556, 0.2521560577580082, 0.2505133480690343, 0.25256673557190434, 0.25338809000637985, 0.2513347019160308, 0.25831622197397924, 0.2546201220772839, 0.24804927936814405, 0.2562628327453895, 0.24804928118565733, 0.253388091181338, 0.25708418954813994, 0.2562628329412159, 0.25174537914244793, 0.25667351212589645, 0.25585215767306224, 0.2546201236438947, 0.25133470211185716, 0.256673510755112, 0.25051334628823846, 0.2583162212273913, 0.2501026696493004, 0.2542094448508668, 0.25708418798152916, 0.2517453811007114, 0.24845995996032652, 0.2607802888940737, 0.2550308008703118, 0.2595482542774271, 0.25790554278929867, 0.2566735103267419, 0.25872689982459285, 0.2624229956089349, 0.2616016431510816, 0.25420944403084395, 0.2554414788800343, 0.2566735109509384, 0.24928131402150805, 0.25749486697038343, 0.26242299975800565, 0.26324435518997163, 0.26201232135663044, 0.2554414790758607, 0.2579055457634113, 0.2525667349660666, 0.2574948665787307, 0.264476386869223, 0.2550308010661382, 0.25954825408160076, 0.2533880896330859, 0.2611909641622273, 0.251745380904885, 0.2595482568231696, 0.2611909637338572, 0.25995893404958675, 0.26283367522198564, 0.26324435207510877, 0.25626283254956317, 0.25708418657402726, 0.26036961010104576, 0.26447638369928395, 0.26283367483033293, 0.2599589308796477, 0.2599589318954969, 0.2648870646831191, 0.26694044940770284]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
