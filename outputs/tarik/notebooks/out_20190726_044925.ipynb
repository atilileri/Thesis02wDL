{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf22.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 04:49:25 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '3', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eg', 'ek', 'ce', 'mb', 'eo', 'sg', 'by', 'aa', 'yd', 'ib', 'ds', 'eb', 'sk', 'ck', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000018A0219D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000018A56966EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7052, Accuracy:0.0891, Validation Loss:2.6992, Validation Accuracy:0.0887\n",
    "Epoch #2: Loss:2.6959, Accuracy:0.0891, Validation Loss:2.6907, Validation Accuracy:0.0887\n",
    "Epoch #3: Loss:2.6877, Accuracy:0.0891, Validation Loss:2.6834, Validation Accuracy:0.0821\n",
    "Epoch #4: Loss:2.6816, Accuracy:0.0813, Validation Loss:2.6774, Validation Accuracy:0.0821\n",
    "Epoch #5: Loss:2.6761, Accuracy:0.0813, Validation Loss:2.6725, Validation Accuracy:0.0821\n",
    "Epoch #6: Loss:2.6712, Accuracy:0.0813, Validation Loss:2.6689, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6680, Accuracy:0.1023, Validation Loss:2.6662, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6659, Accuracy:0.1023, Validation Loss:2.6642, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6641, Accuracy:0.1023, Validation Loss:2.6630, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6630, Accuracy:0.1023, Validation Loss:2.6622, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6622, Accuracy:0.1023, Validation Loss:2.6616, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6618, Accuracy:0.1023, Validation Loss:2.6613, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6613, Accuracy:0.1023, Validation Loss:2.6611, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6612, Accuracy:0.1023, Validation Loss:2.6609, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6611, Accuracy:0.1023, Validation Loss:2.6607, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6610, Accuracy:0.1023, Validation Loss:2.6607, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6609, Accuracy:0.1023, Validation Loss:2.6605, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6607, Accuracy:0.1023, Validation Loss:2.6604, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6605, Accuracy:0.1023, Validation Loss:2.6603, Validation Accuracy:0.1018\n",
    "Epoch #20: Loss:2.6604, Accuracy:0.1023, Validation Loss:2.6601, Validation Accuracy:0.1018\n",
    "Epoch #21: Loss:2.6603, Accuracy:0.1023, Validation Loss:2.6600, Validation Accuracy:0.1018\n",
    "Epoch #22: Loss:2.6602, Accuracy:0.1023, Validation Loss:2.6599, Validation Accuracy:0.1018\n",
    "Epoch #23: Loss:2.6601, Accuracy:0.1023, Validation Loss:2.6597, Validation Accuracy:0.1018\n",
    "Epoch #24: Loss:2.6598, Accuracy:0.1023, Validation Loss:2.6596, Validation Accuracy:0.1018\n",
    "Epoch #25: Loss:2.6598, Accuracy:0.1023, Validation Loss:2.6594, Validation Accuracy:0.1018\n",
    "Epoch #26: Loss:2.6595, Accuracy:0.1023, Validation Loss:2.6591, Validation Accuracy:0.1018\n",
    "Epoch #27: Loss:2.6594, Accuracy:0.1023, Validation Loss:2.6589, Validation Accuracy:0.1018\n",
    "Epoch #28: Loss:2.6589, Accuracy:0.1023, Validation Loss:2.6585, Validation Accuracy:0.1018\n",
    "Epoch #29: Loss:2.6587, Accuracy:0.1023, Validation Loss:2.6581, Validation Accuracy:0.1018\n",
    "Epoch #30: Loss:2.6582, Accuracy:0.1023, Validation Loss:2.6576, Validation Accuracy:0.1018\n",
    "Epoch #31: Loss:2.6577, Accuracy:0.1023, Validation Loss:2.6569, Validation Accuracy:0.1018\n",
    "Epoch #32: Loss:2.6569, Accuracy:0.1023, Validation Loss:2.6561, Validation Accuracy:0.1018\n",
    "Epoch #33: Loss:2.6562, Accuracy:0.1023, Validation Loss:2.6551, Validation Accuracy:0.1018\n",
    "Epoch #34: Loss:2.6551, Accuracy:0.1023, Validation Loss:2.6537, Validation Accuracy:0.1018\n",
    "Epoch #35: Loss:2.6538, Accuracy:0.1023, Validation Loss:2.6521, Validation Accuracy:0.1018\n",
    "Epoch #36: Loss:2.6518, Accuracy:0.1023, Validation Loss:2.6498, Validation Accuracy:0.1018\n",
    "Epoch #37: Loss:2.6492, Accuracy:0.1023, Validation Loss:2.6464, Validation Accuracy:0.1018\n",
    "Epoch #38: Loss:2.6454, Accuracy:0.1023, Validation Loss:2.6411, Validation Accuracy:0.1018\n",
    "Epoch #39: Loss:2.6395, Accuracy:0.1035, Validation Loss:2.6330, Validation Accuracy:0.1018\n",
    "Epoch #40: Loss:2.6302, Accuracy:0.1068, Validation Loss:2.6207, Validation Accuracy:0.1117\n",
    "Epoch #41: Loss:2.6178, Accuracy:0.1187, Validation Loss:2.6030, Validation Accuracy:0.1297\n",
    "Epoch #42: Loss:2.6007, Accuracy:0.1429, Validation Loss:2.5863, Validation Accuracy:0.1297\n",
    "Epoch #43: Loss:2.5861, Accuracy:0.1499, Validation Loss:2.5674, Validation Accuracy:0.1494\n",
    "Epoch #44: Loss:2.5703, Accuracy:0.1639, Validation Loss:2.5536, Validation Accuracy:0.1511\n",
    "Epoch #45: Loss:2.5588, Accuracy:0.1618, Validation Loss:2.5426, Validation Accuracy:0.1494\n",
    "Epoch #46: Loss:2.5491, Accuracy:0.1626, Validation Loss:2.5425, Validation Accuracy:0.1461\n",
    "Epoch #47: Loss:2.5378, Accuracy:0.1639, Validation Loss:2.5231, Validation Accuracy:0.1593\n",
    "Epoch #48: Loss:2.5187, Accuracy:0.1651, Validation Loss:2.5094, Validation Accuracy:0.1527\n",
    "Epoch #49: Loss:2.5026, Accuracy:0.1733, Validation Loss:2.4980, Validation Accuracy:0.1461\n",
    "Epoch #50: Loss:2.4857, Accuracy:0.1754, Validation Loss:2.4794, Validation Accuracy:0.1658\n",
    "Epoch #51: Loss:2.4679, Accuracy:0.1770, Validation Loss:2.4616, Validation Accuracy:0.1642\n",
    "Epoch #52: Loss:2.4427, Accuracy:0.1799, Validation Loss:2.4412, Validation Accuracy:0.1757\n",
    "Epoch #53: Loss:2.4153, Accuracy:0.1840, Validation Loss:2.4128, Validation Accuracy:0.1773\n",
    "Epoch #54: Loss:2.3870, Accuracy:0.2000, Validation Loss:2.3832, Validation Accuracy:0.1790\n",
    "Epoch #55: Loss:2.3585, Accuracy:0.2004, Validation Loss:2.3651, Validation Accuracy:0.1708\n",
    "Epoch #56: Loss:2.3375, Accuracy:0.2103, Validation Loss:2.3510, Validation Accuracy:0.1773\n",
    "Epoch #57: Loss:2.3158, Accuracy:0.2160, Validation Loss:2.3432, Validation Accuracy:0.1954\n",
    "Epoch #58: Loss:2.2989, Accuracy:0.2259, Validation Loss:2.3176, Validation Accuracy:0.2085\n",
    "Epoch #59: Loss:2.2851, Accuracy:0.2267, Validation Loss:2.3087, Validation Accuracy:0.2085\n",
    "Epoch #60: Loss:2.2708, Accuracy:0.2390, Validation Loss:2.2902, Validation Accuracy:0.2135\n",
    "Epoch #61: Loss:2.2522, Accuracy:0.2493, Validation Loss:2.2573, Validation Accuracy:0.2102\n",
    "Epoch #62: Loss:2.2301, Accuracy:0.2620, Validation Loss:2.2429, Validation Accuracy:0.2299\n",
    "Epoch #63: Loss:2.2174, Accuracy:0.2632, Validation Loss:2.2259, Validation Accuracy:0.2365\n",
    "Epoch #64: Loss:2.2058, Accuracy:0.2719, Validation Loss:2.2115, Validation Accuracy:0.2496\n",
    "Epoch #65: Loss:2.1867, Accuracy:0.2772, Validation Loss:2.2039, Validation Accuracy:0.2430\n",
    "Epoch #66: Loss:2.1750, Accuracy:0.2793, Validation Loss:2.2025, Validation Accuracy:0.2414\n",
    "Epoch #67: Loss:2.1751, Accuracy:0.2858, Validation Loss:2.1936, Validation Accuracy:0.2529\n",
    "Epoch #68: Loss:2.1526, Accuracy:0.2908, Validation Loss:2.1702, Validation Accuracy:0.2578\n",
    "Epoch #69: Loss:2.1478, Accuracy:0.2821, Validation Loss:2.1475, Validation Accuracy:0.2644\n",
    "Epoch #70: Loss:2.1434, Accuracy:0.2862, Validation Loss:2.1420, Validation Accuracy:0.2808\n",
    "Epoch #71: Loss:2.1334, Accuracy:0.2858, Validation Loss:2.1211, Validation Accuracy:0.2759\n",
    "Epoch #72: Loss:2.1192, Accuracy:0.2932, Validation Loss:2.1222, Validation Accuracy:0.2808\n",
    "Epoch #73: Loss:2.1150, Accuracy:0.2965, Validation Loss:2.1293, Validation Accuracy:0.2808\n",
    "Epoch #74: Loss:2.1104, Accuracy:0.3006, Validation Loss:2.1123, Validation Accuracy:0.2841\n",
    "Epoch #75: Loss:2.0914, Accuracy:0.3023, Validation Loss:2.0997, Validation Accuracy:0.2890\n",
    "Epoch #76: Loss:2.0837, Accuracy:0.3055, Validation Loss:2.0860, Validation Accuracy:0.2841\n",
    "Epoch #77: Loss:2.0796, Accuracy:0.3018, Validation Loss:2.0774, Validation Accuracy:0.2906\n",
    "Epoch #78: Loss:2.0735, Accuracy:0.3064, Validation Loss:2.0692, Validation Accuracy:0.2972\n",
    "Epoch #79: Loss:2.0632, Accuracy:0.3125, Validation Loss:2.0715, Validation Accuracy:0.2923\n",
    "Epoch #80: Loss:2.0597, Accuracy:0.3129, Validation Loss:2.0686, Validation Accuracy:0.2906\n",
    "Epoch #81: Loss:2.0598, Accuracy:0.3121, Validation Loss:2.0585, Validation Accuracy:0.3005\n",
    "Epoch #82: Loss:2.0558, Accuracy:0.3125, Validation Loss:2.0534, Validation Accuracy:0.3103\n",
    "Epoch #83: Loss:2.0546, Accuracy:0.3154, Validation Loss:2.0476, Validation Accuracy:0.3103\n",
    "Epoch #84: Loss:2.0434, Accuracy:0.3138, Validation Loss:2.0496, Validation Accuracy:0.2956\n",
    "Epoch #85: Loss:2.0434, Accuracy:0.3187, Validation Loss:2.0634, Validation Accuracy:0.2956\n",
    "Epoch #86: Loss:2.0391, Accuracy:0.3105, Validation Loss:2.0539, Validation Accuracy:0.2857\n",
    "Epoch #87: Loss:2.0343, Accuracy:0.3195, Validation Loss:2.0545, Validation Accuracy:0.2939\n",
    "Epoch #88: Loss:2.0446, Accuracy:0.3138, Validation Loss:2.0386, Validation Accuracy:0.3218\n",
    "Epoch #89: Loss:2.0465, Accuracy:0.3035, Validation Loss:2.0499, Validation Accuracy:0.3054\n",
    "Epoch #90: Loss:2.0300, Accuracy:0.3179, Validation Loss:2.0779, Validation Accuracy:0.2874\n",
    "Epoch #91: Loss:2.0359, Accuracy:0.3109, Validation Loss:2.0360, Validation Accuracy:0.2874\n",
    "Epoch #92: Loss:2.0356, Accuracy:0.3175, Validation Loss:2.0537, Validation Accuracy:0.3136\n",
    "Epoch #93: Loss:2.0389, Accuracy:0.3121, Validation Loss:2.0375, Validation Accuracy:0.2857\n",
    "Epoch #94: Loss:2.0296, Accuracy:0.3138, Validation Loss:2.0552, Validation Accuracy:0.2857\n",
    "Epoch #95: Loss:2.0258, Accuracy:0.3121, Validation Loss:2.0255, Validation Accuracy:0.3153\n",
    "Epoch #96: Loss:2.0260, Accuracy:0.3138, Validation Loss:2.0262, Validation Accuracy:0.2989\n",
    "Epoch #97: Loss:2.0145, Accuracy:0.3179, Validation Loss:2.0249, Validation Accuracy:0.3038\n",
    "Epoch #98: Loss:2.0115, Accuracy:0.3179, Validation Loss:2.0173, Validation Accuracy:0.2989\n",
    "Epoch #99: Loss:2.0075, Accuracy:0.3228, Validation Loss:2.0165, Validation Accuracy:0.3038\n",
    "Epoch #100: Loss:2.0051, Accuracy:0.3211, Validation Loss:2.0163, Validation Accuracy:0.2972\n",
    "Epoch #101: Loss:2.0025, Accuracy:0.3203, Validation Loss:2.0160, Validation Accuracy:0.3087\n",
    "Epoch #102: Loss:2.0035, Accuracy:0.3175, Validation Loss:2.0203, Validation Accuracy:0.3136\n",
    "Epoch #103: Loss:2.0036, Accuracy:0.3220, Validation Loss:2.0288, Validation Accuracy:0.3005\n",
    "Epoch #104: Loss:2.0026, Accuracy:0.3170, Validation Loss:2.0127, Validation Accuracy:0.3251\n",
    "Epoch #105: Loss:2.0014, Accuracy:0.3175, Validation Loss:2.0301, Validation Accuracy:0.3071\n",
    "Epoch #106: Loss:1.9980, Accuracy:0.3211, Validation Loss:2.0212, Validation Accuracy:0.3120\n",
    "Epoch #107: Loss:2.0048, Accuracy:0.3117, Validation Loss:2.0200, Validation Accuracy:0.3005\n",
    "Epoch #108: Loss:1.9972, Accuracy:0.3195, Validation Loss:2.0087, Validation Accuracy:0.3235\n",
    "Epoch #109: Loss:1.9928, Accuracy:0.3302, Validation Loss:2.0087, Validation Accuracy:0.3087\n",
    "Epoch #110: Loss:1.9951, Accuracy:0.3232, Validation Loss:2.0064, Validation Accuracy:0.3005\n",
    "Epoch #111: Loss:1.9933, Accuracy:0.3261, Validation Loss:1.9998, Validation Accuracy:0.3153\n",
    "Epoch #112: Loss:1.9911, Accuracy:0.3166, Validation Loss:2.0018, Validation Accuracy:0.3038\n",
    "Epoch #113: Loss:1.9871, Accuracy:0.3240, Validation Loss:2.0009, Validation Accuracy:0.3186\n",
    "Epoch #114: Loss:1.9917, Accuracy:0.3203, Validation Loss:1.9997, Validation Accuracy:0.3038\n",
    "Epoch #115: Loss:1.9862, Accuracy:0.3207, Validation Loss:1.9977, Validation Accuracy:0.3169\n",
    "Epoch #116: Loss:1.9838, Accuracy:0.3265, Validation Loss:1.9974, Validation Accuracy:0.3038\n",
    "Epoch #117: Loss:1.9817, Accuracy:0.3261, Validation Loss:2.0065, Validation Accuracy:0.3054\n",
    "Epoch #118: Loss:1.9833, Accuracy:0.3244, Validation Loss:1.9963, Validation Accuracy:0.3054\n",
    "Epoch #119: Loss:1.9848, Accuracy:0.3257, Validation Loss:1.9934, Validation Accuracy:0.3202\n",
    "Epoch #120: Loss:1.9774, Accuracy:0.3240, Validation Loss:2.0004, Validation Accuracy:0.3005\n",
    "Epoch #121: Loss:1.9769, Accuracy:0.3261, Validation Loss:1.9915, Validation Accuracy:0.3153\n",
    "Epoch #122: Loss:1.9762, Accuracy:0.3253, Validation Loss:1.9913, Validation Accuracy:0.3103\n",
    "Epoch #123: Loss:1.9755, Accuracy:0.3228, Validation Loss:1.9919, Validation Accuracy:0.3054\n",
    "Epoch #124: Loss:1.9740, Accuracy:0.3273, Validation Loss:2.0283, Validation Accuracy:0.2972\n",
    "Epoch #125: Loss:1.9879, Accuracy:0.3224, Validation Loss:2.0035, Validation Accuracy:0.3087\n",
    "Epoch #126: Loss:1.9823, Accuracy:0.3191, Validation Loss:1.9898, Validation Accuracy:0.3038\n",
    "Epoch #127: Loss:1.9747, Accuracy:0.3207, Validation Loss:1.9875, Validation Accuracy:0.3169\n",
    "Epoch #128: Loss:1.9739, Accuracy:0.3187, Validation Loss:1.9889, Validation Accuracy:0.3071\n",
    "Epoch #129: Loss:1.9700, Accuracy:0.3240, Validation Loss:1.9889, Validation Accuracy:0.3169\n",
    "Epoch #130: Loss:1.9669, Accuracy:0.3244, Validation Loss:1.9870, Validation Accuracy:0.3054\n",
    "Epoch #131: Loss:1.9680, Accuracy:0.3273, Validation Loss:1.9870, Validation Accuracy:0.3153\n",
    "Epoch #132: Loss:1.9662, Accuracy:0.3281, Validation Loss:2.0057, Validation Accuracy:0.3071\n",
    "Epoch #133: Loss:1.9709, Accuracy:0.3207, Validation Loss:1.9878, Validation Accuracy:0.3251\n",
    "Epoch #134: Loss:1.9646, Accuracy:0.3281, Validation Loss:1.9884, Validation Accuracy:0.3054\n",
    "Epoch #135: Loss:1.9685, Accuracy:0.3244, Validation Loss:1.9824, Validation Accuracy:0.3120\n",
    "Epoch #136: Loss:1.9666, Accuracy:0.3281, Validation Loss:1.9840, Validation Accuracy:0.3218\n",
    "Epoch #137: Loss:1.9687, Accuracy:0.3248, Validation Loss:1.9828, Validation Accuracy:0.3021\n",
    "Epoch #138: Loss:1.9622, Accuracy:0.3257, Validation Loss:1.9799, Validation Accuracy:0.3169\n",
    "Epoch #139: Loss:1.9601, Accuracy:0.3253, Validation Loss:1.9896, Validation Accuracy:0.3021\n",
    "Epoch #140: Loss:1.9642, Accuracy:0.3248, Validation Loss:1.9787, Validation Accuracy:0.3136\n",
    "Epoch #141: Loss:1.9696, Accuracy:0.3191, Validation Loss:1.9804, Validation Accuracy:0.3169\n",
    "Epoch #142: Loss:1.9588, Accuracy:0.3265, Validation Loss:1.9779, Validation Accuracy:0.3153\n",
    "Epoch #143: Loss:1.9589, Accuracy:0.3257, Validation Loss:1.9850, Validation Accuracy:0.3186\n",
    "Epoch #144: Loss:1.9604, Accuracy:0.3265, Validation Loss:2.0095, Validation Accuracy:0.2989\n",
    "Epoch #145: Loss:1.9676, Accuracy:0.3244, Validation Loss:1.9798, Validation Accuracy:0.3136\n",
    "Epoch #146: Loss:1.9594, Accuracy:0.3257, Validation Loss:1.9782, Validation Accuracy:0.3202\n",
    "Epoch #147: Loss:1.9541, Accuracy:0.3257, Validation Loss:1.9768, Validation Accuracy:0.3120\n",
    "Epoch #148: Loss:1.9544, Accuracy:0.3232, Validation Loss:1.9936, Validation Accuracy:0.3136\n",
    "Epoch #149: Loss:1.9586, Accuracy:0.3224, Validation Loss:1.9815, Validation Accuracy:0.3087\n",
    "Epoch #150: Loss:1.9540, Accuracy:0.3290, Validation Loss:1.9739, Validation Accuracy:0.3235\n",
    "Epoch #151: Loss:1.9511, Accuracy:0.3281, Validation Loss:1.9752, Validation Accuracy:0.3038\n",
    "Epoch #152: Loss:1.9499, Accuracy:0.3290, Validation Loss:1.9794, Validation Accuracy:0.3268\n",
    "Epoch #153: Loss:1.9597, Accuracy:0.3232, Validation Loss:1.9745, Validation Accuracy:0.3071\n",
    "Epoch #154: Loss:1.9554, Accuracy:0.3322, Validation Loss:1.9702, Validation Accuracy:0.3153\n",
    "Epoch #155: Loss:1.9492, Accuracy:0.3281, Validation Loss:1.9691, Validation Accuracy:0.3202\n",
    "Epoch #156: Loss:1.9460, Accuracy:0.3277, Validation Loss:1.9725, Validation Accuracy:0.3103\n",
    "Epoch #157: Loss:1.9468, Accuracy:0.3265, Validation Loss:1.9664, Validation Accuracy:0.3235\n",
    "Epoch #158: Loss:1.9455, Accuracy:0.3285, Validation Loss:1.9698, Validation Accuracy:0.3202\n",
    "Epoch #159: Loss:1.9571, Accuracy:0.3306, Validation Loss:1.9854, Validation Accuracy:0.3235\n",
    "Epoch #160: Loss:1.9810, Accuracy:0.3170, Validation Loss:1.9667, Validation Accuracy:0.3153\n",
    "Epoch #161: Loss:1.9563, Accuracy:0.3310, Validation Loss:2.0075, Validation Accuracy:0.3136\n",
    "Epoch #162: Loss:1.9634, Accuracy:0.3224, Validation Loss:1.9756, Validation Accuracy:0.3120\n",
    "Epoch #163: Loss:1.9565, Accuracy:0.3244, Validation Loss:1.9910, Validation Accuracy:0.3087\n",
    "Epoch #164: Loss:1.9685, Accuracy:0.3175, Validation Loss:1.9641, Validation Accuracy:0.3169\n",
    "Epoch #165: Loss:1.9572, Accuracy:0.3220, Validation Loss:1.9863, Validation Accuracy:0.2972\n",
    "Epoch #166: Loss:1.9480, Accuracy:0.3244, Validation Loss:1.9663, Validation Accuracy:0.3103\n",
    "Epoch #167: Loss:1.9415, Accuracy:0.3322, Validation Loss:1.9639, Validation Accuracy:0.3218\n",
    "Epoch #168: Loss:1.9386, Accuracy:0.3343, Validation Loss:1.9629, Validation Accuracy:0.3186\n",
    "Epoch #169: Loss:1.9385, Accuracy:0.3310, Validation Loss:1.9630, Validation Accuracy:0.3186\n",
    "Epoch #170: Loss:1.9374, Accuracy:0.3326, Validation Loss:1.9709, Validation Accuracy:0.3120\n",
    "Epoch #171: Loss:1.9406, Accuracy:0.3240, Validation Loss:1.9613, Validation Accuracy:0.3120\n",
    "Epoch #172: Loss:1.9390, Accuracy:0.3359, Validation Loss:1.9634, Validation Accuracy:0.3268\n",
    "Epoch #173: Loss:1.9412, Accuracy:0.3302, Validation Loss:1.9653, Validation Accuracy:0.3087\n",
    "Epoch #174: Loss:1.9421, Accuracy:0.3294, Validation Loss:1.9735, Validation Accuracy:0.3268\n",
    "Epoch #175: Loss:1.9372, Accuracy:0.3302, Validation Loss:1.9740, Validation Accuracy:0.2906\n",
    "Epoch #176: Loss:1.9390, Accuracy:0.3281, Validation Loss:1.9702, Validation Accuracy:0.3300\n",
    "Epoch #177: Loss:1.9390, Accuracy:0.3244, Validation Loss:1.9646, Validation Accuracy:0.3087\n",
    "Epoch #178: Loss:1.9337, Accuracy:0.3322, Validation Loss:1.9585, Validation Accuracy:0.3218\n",
    "Epoch #179: Loss:1.9349, Accuracy:0.3310, Validation Loss:1.9576, Validation Accuracy:0.3235\n",
    "Epoch #180: Loss:1.9318, Accuracy:0.3310, Validation Loss:1.9600, Validation Accuracy:0.3186\n",
    "Epoch #181: Loss:1.9313, Accuracy:0.3322, Validation Loss:1.9552, Validation Accuracy:0.3202\n",
    "Epoch #182: Loss:1.9326, Accuracy:0.3290, Validation Loss:1.9589, Validation Accuracy:0.3235\n",
    "Epoch #183: Loss:1.9406, Accuracy:0.3269, Validation Loss:1.9568, Validation Accuracy:0.3186\n",
    "Epoch #184: Loss:1.9449, Accuracy:0.3281, Validation Loss:1.9594, Validation Accuracy:0.3218\n",
    "Epoch #185: Loss:1.9349, Accuracy:0.3306, Validation Loss:1.9986, Validation Accuracy:0.3054\n",
    "Epoch #186: Loss:1.9480, Accuracy:0.3261, Validation Loss:1.9570, Validation Accuracy:0.3169\n",
    "Epoch #187: Loss:1.9331, Accuracy:0.3339, Validation Loss:1.9558, Validation Accuracy:0.3153\n",
    "Epoch #188: Loss:1.9278, Accuracy:0.3343, Validation Loss:1.9607, Validation Accuracy:0.3169\n",
    "Epoch #189: Loss:1.9274, Accuracy:0.3322, Validation Loss:1.9545, Validation Accuracy:0.3268\n",
    "Epoch #190: Loss:1.9260, Accuracy:0.3294, Validation Loss:1.9552, Validation Accuracy:0.3218\n",
    "Epoch #191: Loss:1.9268, Accuracy:0.3335, Validation Loss:1.9542, Validation Accuracy:0.3153\n",
    "Epoch #192: Loss:1.9261, Accuracy:0.3347, Validation Loss:1.9530, Validation Accuracy:0.3169\n",
    "Epoch #193: Loss:1.9273, Accuracy:0.3273, Validation Loss:1.9542, Validation Accuracy:0.3153\n",
    "Epoch #194: Loss:1.9252, Accuracy:0.3310, Validation Loss:1.9525, Validation Accuracy:0.3186\n",
    "Epoch #195: Loss:1.9251, Accuracy:0.3310, Validation Loss:1.9534, Validation Accuracy:0.3153\n",
    "Epoch #196: Loss:1.9281, Accuracy:0.3322, Validation Loss:1.9590, Validation Accuracy:0.3268\n",
    "Epoch #197: Loss:1.9336, Accuracy:0.3248, Validation Loss:1.9640, Validation Accuracy:0.3251\n",
    "Epoch #198: Loss:1.9366, Accuracy:0.3326, Validation Loss:1.9564, Validation Accuracy:0.3136\n",
    "Epoch #199: Loss:1.9284, Accuracy:0.3335, Validation Loss:1.9554, Validation Accuracy:0.3136\n",
    "Epoch #200: Loss:1.9251, Accuracy:0.3298, Validation Loss:1.9564, Validation Accuracy:0.3103\n",
    "Epoch #201: Loss:1.9268, Accuracy:0.3281, Validation Loss:1.9529, Validation Accuracy:0.3136\n",
    "Epoch #202: Loss:1.9270, Accuracy:0.3343, Validation Loss:1.9673, Validation Accuracy:0.3251\n",
    "Epoch #203: Loss:1.9329, Accuracy:0.3326, Validation Loss:1.9497, Validation Accuracy:0.3186\n",
    "Epoch #204: Loss:1.9328, Accuracy:0.3290, Validation Loss:2.0058, Validation Accuracy:0.3103\n",
    "Epoch #205: Loss:1.9625, Accuracy:0.3240, Validation Loss:1.9870, Validation Accuracy:0.3021\n",
    "Epoch #206: Loss:1.9543, Accuracy:0.3322, Validation Loss:1.9616, Validation Accuracy:0.3153\n",
    "Epoch #207: Loss:1.9298, Accuracy:0.3273, Validation Loss:1.9540, Validation Accuracy:0.3218\n",
    "Epoch #208: Loss:1.9233, Accuracy:0.3318, Validation Loss:1.9600, Validation Accuracy:0.3087\n",
    "Epoch #209: Loss:1.9213, Accuracy:0.3310, Validation Loss:1.9485, Validation Accuracy:0.3218\n",
    "Epoch #210: Loss:1.9169, Accuracy:0.3363, Validation Loss:1.9493, Validation Accuracy:0.3218\n",
    "Epoch #211: Loss:1.9158, Accuracy:0.3343, Validation Loss:1.9476, Validation Accuracy:0.3169\n",
    "Epoch #212: Loss:1.9167, Accuracy:0.3290, Validation Loss:1.9494, Validation Accuracy:0.3186\n",
    "Epoch #213: Loss:1.9148, Accuracy:0.3363, Validation Loss:1.9504, Validation Accuracy:0.3202\n",
    "Epoch #214: Loss:1.9181, Accuracy:0.3331, Validation Loss:1.9480, Validation Accuracy:0.3186\n",
    "Epoch #215: Loss:1.9150, Accuracy:0.3310, Validation Loss:1.9470, Validation Accuracy:0.3169\n",
    "Epoch #216: Loss:1.9176, Accuracy:0.3368, Validation Loss:1.9525, Validation Accuracy:0.3169\n",
    "Epoch #217: Loss:1.9226, Accuracy:0.3318, Validation Loss:1.9582, Validation Accuracy:0.3136\n",
    "Epoch #218: Loss:1.9156, Accuracy:0.3347, Validation Loss:1.9503, Validation Accuracy:0.3169\n",
    "Epoch #219: Loss:1.9167, Accuracy:0.3322, Validation Loss:1.9490, Validation Accuracy:0.3153\n",
    "Epoch #220: Loss:1.9148, Accuracy:0.3335, Validation Loss:1.9533, Validation Accuracy:0.3087\n",
    "Epoch #221: Loss:1.9163, Accuracy:0.3314, Validation Loss:1.9486, Validation Accuracy:0.3268\n",
    "Epoch #222: Loss:1.9129, Accuracy:0.3322, Validation Loss:1.9480, Validation Accuracy:0.3186\n",
    "Epoch #223: Loss:1.9106, Accuracy:0.3359, Validation Loss:1.9490, Validation Accuracy:0.3186\n",
    "Epoch #224: Loss:1.9131, Accuracy:0.3359, Validation Loss:1.9449, Validation Accuracy:0.3186\n",
    "Epoch #225: Loss:1.9097, Accuracy:0.3339, Validation Loss:1.9478, Validation Accuracy:0.3186\n",
    "Epoch #226: Loss:1.9145, Accuracy:0.3339, Validation Loss:1.9532, Validation Accuracy:0.3136\n",
    "Epoch #227: Loss:1.9188, Accuracy:0.3314, Validation Loss:1.9516, Validation Accuracy:0.3235\n",
    "Epoch #228: Loss:1.9136, Accuracy:0.3335, Validation Loss:1.9534, Validation Accuracy:0.3153\n",
    "Epoch #229: Loss:1.9090, Accuracy:0.3380, Validation Loss:1.9484, Validation Accuracy:0.3317\n",
    "Epoch #230: Loss:1.9103, Accuracy:0.3322, Validation Loss:1.9546, Validation Accuracy:0.3136\n",
    "Epoch #231: Loss:1.9056, Accuracy:0.3363, Validation Loss:1.9454, Validation Accuracy:0.3202\n",
    "Epoch #232: Loss:1.9087, Accuracy:0.3335, Validation Loss:1.9438, Validation Accuracy:0.3169\n",
    "Epoch #233: Loss:1.9081, Accuracy:0.3359, Validation Loss:1.9473, Validation Accuracy:0.3153\n",
    "Epoch #234: Loss:1.9071, Accuracy:0.3347, Validation Loss:1.9454, Validation Accuracy:0.3169\n",
    "Epoch #235: Loss:1.9054, Accuracy:0.3368, Validation Loss:1.9452, Validation Accuracy:0.3186\n",
    "Epoch #236: Loss:1.9050, Accuracy:0.3355, Validation Loss:1.9427, Validation Accuracy:0.3169\n",
    "Epoch #237: Loss:1.9058, Accuracy:0.3318, Validation Loss:1.9448, Validation Accuracy:0.3169\n",
    "Epoch #238: Loss:1.9068, Accuracy:0.3339, Validation Loss:1.9530, Validation Accuracy:0.3202\n",
    "Epoch #239: Loss:1.9053, Accuracy:0.3384, Validation Loss:1.9497, Validation Accuracy:0.3284\n",
    "Epoch #240: Loss:1.9053, Accuracy:0.3359, Validation Loss:1.9479, Validation Accuracy:0.3136\n",
    "Epoch #241: Loss:1.9032, Accuracy:0.3396, Validation Loss:1.9453, Validation Accuracy:0.3317\n",
    "Epoch #242: Loss:1.9064, Accuracy:0.3437, Validation Loss:1.9476, Validation Accuracy:0.3169\n",
    "Epoch #243: Loss:1.9043, Accuracy:0.3363, Validation Loss:1.9414, Validation Accuracy:0.3202\n",
    "Epoch #244: Loss:1.9031, Accuracy:0.3363, Validation Loss:1.9425, Validation Accuracy:0.3218\n",
    "Epoch #245: Loss:1.9022, Accuracy:0.3380, Validation Loss:1.9501, Validation Accuracy:0.3153\n",
    "Epoch #246: Loss:1.9055, Accuracy:0.3376, Validation Loss:1.9496, Validation Accuracy:0.3186\n",
    "Epoch #247: Loss:1.9122, Accuracy:0.3351, Validation Loss:1.9445, Validation Accuracy:0.3186\n",
    "Epoch #248: Loss:1.9059, Accuracy:0.3351, Validation Loss:1.9420, Validation Accuracy:0.3153\n",
    "Epoch #249: Loss:1.9013, Accuracy:0.3351, Validation Loss:1.9378, Validation Accuracy:0.3186\n",
    "Epoch #250: Loss:1.9022, Accuracy:0.3351, Validation Loss:1.9587, Validation Accuracy:0.3120\n",
    "Epoch #251: Loss:1.9082, Accuracy:0.3355, Validation Loss:1.9486, Validation Accuracy:0.3235\n",
    "Epoch #252: Loss:1.9031, Accuracy:0.3368, Validation Loss:1.9412, Validation Accuracy:0.3136\n",
    "Epoch #253: Loss:1.9015, Accuracy:0.3335, Validation Loss:1.9393, Validation Accuracy:0.3202\n",
    "Epoch #254: Loss:1.9032, Accuracy:0.3351, Validation Loss:1.9355, Validation Accuracy:0.3268\n",
    "Epoch #255: Loss:1.8955, Accuracy:0.3392, Validation Loss:1.9391, Validation Accuracy:0.3153\n",
    "Epoch #256: Loss:1.8979, Accuracy:0.3380, Validation Loss:1.9441, Validation Accuracy:0.3153\n",
    "Epoch #257: Loss:1.9000, Accuracy:0.3425, Validation Loss:1.9353, Validation Accuracy:0.3202\n",
    "Epoch #258: Loss:1.8995, Accuracy:0.3409, Validation Loss:1.9386, Validation Accuracy:0.3218\n",
    "Epoch #259: Loss:1.8973, Accuracy:0.3326, Validation Loss:1.9396, Validation Accuracy:0.3202\n",
    "Epoch #260: Loss:1.9007, Accuracy:0.3335, Validation Loss:1.9344, Validation Accuracy:0.3136\n",
    "Epoch #261: Loss:1.9004, Accuracy:0.3380, Validation Loss:1.9371, Validation Accuracy:0.3153\n",
    "Epoch #262: Loss:1.8976, Accuracy:0.3433, Validation Loss:1.9410, Validation Accuracy:0.3120\n",
    "Epoch #263: Loss:1.8998, Accuracy:0.3400, Validation Loss:1.9495, Validation Accuracy:0.3383\n",
    "Epoch #264: Loss:1.8996, Accuracy:0.3368, Validation Loss:1.9494, Validation Accuracy:0.3153\n",
    "Epoch #265: Loss:1.8986, Accuracy:0.3396, Validation Loss:1.9450, Validation Accuracy:0.3300\n",
    "Epoch #266: Loss:1.9032, Accuracy:0.3376, Validation Loss:1.9343, Validation Accuracy:0.3153\n",
    "Epoch #267: Loss:1.8968, Accuracy:0.3376, Validation Loss:1.9543, Validation Accuracy:0.3317\n",
    "Epoch #268: Loss:1.9096, Accuracy:0.3355, Validation Loss:1.9421, Validation Accuracy:0.3284\n",
    "Epoch #269: Loss:1.9052, Accuracy:0.3355, Validation Loss:1.9474, Validation Accuracy:0.3186\n",
    "Epoch #270: Loss:1.9085, Accuracy:0.3376, Validation Loss:1.9579, Validation Accuracy:0.3202\n",
    "Epoch #271: Loss:1.9025, Accuracy:0.3372, Validation Loss:1.9574, Validation Accuracy:0.3186\n",
    "Epoch #272: Loss:1.9144, Accuracy:0.3335, Validation Loss:1.9435, Validation Accuracy:0.3235\n",
    "Epoch #273: Loss:1.8990, Accuracy:0.3331, Validation Loss:1.9362, Validation Accuracy:0.3317\n",
    "Epoch #274: Loss:1.8943, Accuracy:0.3331, Validation Loss:1.9408, Validation Accuracy:0.3218\n",
    "Epoch #275: Loss:1.8985, Accuracy:0.3359, Validation Loss:1.9489, Validation Accuracy:0.3136\n",
    "Epoch #276: Loss:1.9020, Accuracy:0.3400, Validation Loss:1.9539, Validation Accuracy:0.3153\n",
    "Epoch #277: Loss:1.9011, Accuracy:0.3355, Validation Loss:1.9335, Validation Accuracy:0.3235\n",
    "Epoch #278: Loss:1.8891, Accuracy:0.3425, Validation Loss:1.9302, Validation Accuracy:0.3251\n",
    "Epoch #279: Loss:1.8959, Accuracy:0.3405, Validation Loss:1.9520, Validation Accuracy:0.3169\n",
    "Epoch #280: Loss:1.8981, Accuracy:0.3409, Validation Loss:1.9390, Validation Accuracy:0.3218\n",
    "Epoch #281: Loss:1.8951, Accuracy:0.3355, Validation Loss:1.9351, Validation Accuracy:0.3202\n",
    "Epoch #282: Loss:1.8932, Accuracy:0.3388, Validation Loss:1.9423, Validation Accuracy:0.3153\n",
    "Epoch #283: Loss:1.8948, Accuracy:0.3359, Validation Loss:1.9365, Validation Accuracy:0.3333\n",
    "Epoch #284: Loss:1.8908, Accuracy:0.3483, Validation Loss:1.9394, Validation Accuracy:0.3136\n",
    "Epoch #285: Loss:1.8918, Accuracy:0.3384, Validation Loss:1.9455, Validation Accuracy:0.3333\n",
    "Epoch #286: Loss:1.8911, Accuracy:0.3454, Validation Loss:1.9363, Validation Accuracy:0.3202\n",
    "Epoch #287: Loss:1.8904, Accuracy:0.3392, Validation Loss:1.9298, Validation Accuracy:0.3317\n",
    "Epoch #288: Loss:1.8868, Accuracy:0.3454, Validation Loss:1.9314, Validation Accuracy:0.3268\n",
    "Epoch #289: Loss:1.8866, Accuracy:0.3433, Validation Loss:1.9363, Validation Accuracy:0.3300\n",
    "Epoch #290: Loss:1.8854, Accuracy:0.3417, Validation Loss:1.9332, Validation Accuracy:0.3383\n",
    "Epoch #291: Loss:1.8891, Accuracy:0.3376, Validation Loss:1.9291, Validation Accuracy:0.3284\n",
    "Epoch #292: Loss:1.8852, Accuracy:0.3450, Validation Loss:1.9294, Validation Accuracy:0.3218\n",
    "Epoch #293: Loss:1.8822, Accuracy:0.3450, Validation Loss:1.9309, Validation Accuracy:0.3169\n",
    "Epoch #294: Loss:1.8849, Accuracy:0.3429, Validation Loss:1.9377, Validation Accuracy:0.3383\n",
    "Epoch #295: Loss:1.8857, Accuracy:0.3380, Validation Loss:1.9348, Validation Accuracy:0.3268\n",
    "Epoch #296: Loss:1.8866, Accuracy:0.3400, Validation Loss:1.9346, Validation Accuracy:0.3251\n",
    "Epoch #297: Loss:1.8833, Accuracy:0.3433, Validation Loss:1.9371, Validation Accuracy:0.3169\n",
    "Epoch #298: Loss:1.8885, Accuracy:0.3405, Validation Loss:1.9266, Validation Accuracy:0.3366\n",
    "Epoch #299: Loss:1.8858, Accuracy:0.3384, Validation Loss:1.9379, Validation Accuracy:0.3465\n",
    "Epoch #300: Loss:1.8905, Accuracy:0.3335, Validation Loss:1.9320, Validation Accuracy:0.3300\n",
    "\n",
    "Test:\n",
    "Test Loss:1.93203211, Accuracy:0.3300\n",
    "Labels: ['eg', 'ek', 'ce', 'mb', 'eo', 'sg', 'by', 'aa', 'yd', 'ib', 'ds', 'eb', 'sk', 'ck', 'my']\n",
    "Confusion Matrix:\n",
    "      eg  ek  ce  mb  eo  sg  by  aa  yd  ib  ds  eb  sk  ck  my\n",
    "t:eg  16   0   0   2   2   0   3  16   2   1   3   5   0   0   0\n",
    "t:ek   0  15   0   6   0   0   0   1   4   0   7  15   0   0   0\n",
    "t:ce   3   2   0   4   2   1   6   2   1   4   2   0   0   0   0\n",
    "t:mb   0   6   0  17   1   2   4   1   9   7   4   0   1   0   0\n",
    "t:eo   4   0   0   0  26   0   2   0   0   2   0   0   0   0   0\n",
    "t:sg   0   2   0  14   0   3   0   0  22   7   2   1   0   0   0\n",
    "t:by   5   0   0   6   4   1  17   2   0   5   0   0   0   0   0\n",
    "t:aa   4   2   0   5   0   0   3  11   5   0   3   1   0   0   0\n",
    "t:yd   0   3   0   7   0   2   1   1  42   5   0   1   0   0   0\n",
    "t:ib   2   3   0   5   0   2   2   2  21  16   0   1   0   0   0\n",
    "t:ds   4   4   0   3   0   0   0   3   2   0   6   9   0   0   0\n",
    "t:eb   0   5   0   0   0   0   0   0   6   0   8  31   0   0   0\n",
    "t:sk   3   3   0   0   0   0   0   2   0   0   6  19   0   0   0\n",
    "t:ck   2   9   0   5   0   0   0   2   2   0   0   2   0   1   0\n",
    "t:my   0   4   0   2   0   0   0   2   3   1   2   6   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eg       0.37      0.32      0.34        50\n",
    "          ek       0.26      0.31      0.28        48\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          mb       0.22      0.33      0.27        52\n",
    "          eo       0.74      0.76      0.75        34\n",
    "          sg       0.27      0.06      0.10        51\n",
    "          by       0.45      0.42      0.44        40\n",
    "          aa       0.24      0.32      0.28        34\n",
    "          yd       0.35      0.68      0.46        62\n",
    "          ib       0.33      0.30      0.31        54\n",
    "          ds       0.14      0.19      0.16        31\n",
    "          eb       0.34      0.62      0.44        50\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ck       1.00      0.04      0.08        23\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.33       609\n",
    "   macro avg       0.32      0.29      0.26       609\n",
    "weighted avg       0.32      0.33      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 05:05:17 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 51 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.699210520056864, 2.690670908769755, 2.6834247300190293, 2.6773664086127322, 2.672511494609914, 2.668897942760699, 2.666177623573391, 2.6642370251403458, 2.6630255573097315, 2.662202317139198, 2.6616422581946715, 2.6612812905084517, 2.661053405410942, 2.660861827274066, 2.6607379306517602, 2.660659610735763, 2.660526860132202, 2.6603883737805245, 2.66026384920518, 2.6601414833162806, 2.6600162250850783, 2.6599014474840588, 2.659729515977681, 2.6595620928922505, 2.659370843217095, 2.6591223026144095, 2.6588701465838454, 2.6584857528041343, 2.658084695562353, 2.6575538806727366, 2.6569060194864256, 2.6561071473389424, 2.6550781996966584, 2.653743155484129, 2.652104480317465, 2.6497730547179925, 2.646371290210041, 2.6410949989688417, 2.6330023226871084, 2.62065579738523, 2.602985284990082, 2.5863439672686197, 2.5673533789630008, 2.5536401385352727, 2.5425970158944966, 2.542496894967967, 2.5230592224985506, 2.5094219475544146, 2.498008775006374, 2.4794471831548783, 2.4615576737032736, 2.441195470358938, 2.4127862355587713, 2.3832301283117587, 2.3650783631210452, 2.351034086130327, 2.343226793755843, 2.3175606058148914, 2.3087323404885276, 2.2902368152474337, 2.25726608141694, 2.2429279247528227, 2.2259316236906255, 2.2115124599099745, 2.2038595312334635, 2.202467533559439, 2.1936138294796246, 2.1702029431003265, 2.1474994963221556, 2.141966189069701, 2.1211300671394238, 2.1221901129423495, 2.129338639123099, 2.1123004922725883, 2.0997310874888853, 2.0859707169149115, 2.0773755537073795, 2.0692472786738954, 2.0715394521190222, 2.068637316254364, 2.058453379789205, 2.0534270702324475, 2.0475896215203946, 2.0495730307693356, 2.063409573926127, 2.0538697552015432, 2.0544803741530244, 2.0386149233393676, 2.049899686146252, 2.0778968044493977, 2.0359753524924344, 2.053651262386679, 2.03750030473731, 2.055187574552589, 2.0254525030383532, 2.0261741279576997, 2.0248774272467704, 2.0172577781238776, 2.0165080761870335, 2.016303960521429, 2.015985870987715, 2.0202543563247706, 2.0288061582787673, 2.0126693859476172, 2.030088986865014, 2.0212220570136763, 2.020032807523981, 2.008702059684716, 2.0086947227346488, 2.006359049839339, 1.9998456402169464, 2.0017542929093435, 2.0009333754603693, 1.9996719043243107, 1.9977047372921346, 1.997422329114967, 2.006513468346181, 1.9962815349716663, 1.9934239528449298, 2.00035699521771, 1.9915085038528066, 1.9912750673998754, 1.9919333309179847, 2.028333145213636, 2.003515166797857, 1.9898115441521205, 1.9874509651281171, 1.9888548353818445, 1.9889159891601462, 1.9870032442027126, 1.98698087631188, 2.005680540708094, 1.9878220540549367, 1.9884462986869373, 1.9823796594475682, 1.9840219453442076, 1.9828316147495764, 1.9799098158117585, 1.9895747815838392, 1.9786686403998013, 1.980384330444148, 1.9778917252723807, 1.9849764004912478, 2.009465369097705, 1.9797753954951596, 1.9782178206201062, 1.9767747473442692, 1.993634113145775, 1.9815090610867454, 1.9739477358428128, 1.9751738732671502, 1.9794193355516456, 1.974510201092424, 1.9702359864668697, 1.9690778594103158, 1.9724554739562161, 1.966386762158624, 1.9697838976661168, 1.9854036256401801, 1.9666693151878019, 2.007476493838581, 1.9756302872706322, 1.990954500505294, 1.9641315805892443, 1.9863034332131322, 1.966266456691698, 1.963856714699656, 1.9629477096112882, 1.9630282557460865, 1.9709033711594706, 1.961317650986031, 1.9633726856391418, 1.9652913973249238, 1.973450560287889, 1.9739971603274542, 1.9702000682577123, 1.9646462916544898, 1.9585052427204175, 1.9576441734686665, 1.9599666505415843, 1.9552288803169489, 1.9588999303886652, 1.9567578602306948, 1.9594455724475028, 1.9986070932817381, 1.9570109554503743, 1.9558479552981498, 1.9606808294803637, 1.9545033030909271, 1.955229435060999, 1.95418937609505, 1.952992524615258, 1.954158980466658, 1.9525372146189899, 1.9534148733408385, 1.9589999818253792, 1.9640443800705407, 1.9564052166414183, 1.9553651398625866, 1.9563515430992264, 1.9529449244829626, 1.96732366496119, 1.9496652684579734, 2.005813741918855, 1.987014499203912, 1.9616016869866006, 1.9540364110019603, 1.9599695473860441, 1.9484623517896154, 1.9492879826055567, 1.947617734790044, 1.9494372788321208, 1.950436941312843, 1.9479736502730396, 1.946997836501336, 1.9525229187043038, 1.9581673947852625, 1.9503021986026483, 1.9490249186313797, 1.9532719177174058, 1.9486374445932448, 1.9480359213692802, 1.9490116570383458, 1.944892460489508, 1.947848887279116, 1.9531770672508453, 1.9515953075709602, 1.953421701742902, 1.9483609311099124, 1.954614101176583, 1.945399811310917, 1.9438482769604386, 1.9473379629194638, 1.9454305453840735, 1.9451658518247807, 1.9426508233660744, 1.9447710792027866, 1.9529983701768572, 1.9497395716668742, 1.9479366636824333, 1.9453089701131059, 1.947622381603385, 1.941379594098171, 1.942515790951859, 1.9500943453636859, 1.9495685472472744, 1.9444826175818106, 1.942024139347922, 1.9378303810097706, 1.9586861705153642, 1.9485995129411444, 1.9412246893583651, 1.9393349425937547, 1.9354597093240773, 1.93910846471395, 1.9440749392329375, 1.9353066132769405, 1.9386266877107041, 1.9396114967922466, 1.9344339678048696, 1.937070569185592, 1.9409864714188725, 1.9495170562725348, 1.9494012825203255, 1.9450494602983222, 1.9342592500504994, 1.9542787057425588, 1.9420805989423604, 1.9474371043332104, 1.9579474236969094, 1.9574151234869495, 1.9435057739906123, 1.93619095711481, 1.9407775331600545, 1.948865591403103, 1.9539352256089009, 1.9335202188131648, 1.9301908339185667, 1.951989608445191, 1.9390320360954172, 1.9350551646722753, 1.942308696032745, 1.9365137388749272, 1.93942917411159, 1.9455229603794018, 1.9363285489074507, 1.9298234257987763, 1.9313641669127741, 1.9363419195309843, 1.9332391186105011, 1.92905217477645, 1.9294085545688624, 1.9308941742078032, 1.937677151463889, 1.934805188860212, 1.9345970236021897, 1.9370789204912233, 1.9266059572865026, 1.9378626260459912, 1.9320321734902894], 'val_acc': [0.0886699504330632, 0.0886699504330632, 0.08210180603787426, 0.08210180603787426, 0.08210180603787426, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.10180623873407618, 0.11165845558071763, 0.12972085314920578, 0.1297208530513328, 0.14942528654886975, 0.15106732257579153, 0.14942528645099679, 0.14614121429928026, 0.15927750359125717, 0.15270935889633222, 0.14614121439715325, 0.16584564838405508, 0.16420361216138737, 0.17569786523069655, 0.1773399012576183, 0.17898193738241305, 0.17077175666056635, 0.1773399013554913, 0.19540229882610646, 0.20853858554891766, 0.20853858554891766, 0.21346469619884867, 0.2101806238513862, 0.2298850554648683, 0.23645319996404726, 0.24958948896240524, 0.24302134446322624, 0.2413793082405585, 0.2528735613098677, 0.25779966978212493, 0.2643678143791769, 0.2807881756271243, 0.2758620673506131, 0.2807881757249973, 0.28078817533350536, 0.2840722479745868, 0.28899835625109804, 0.2840722479745868, 0.29064039266951175, 0.2972085371686907, 0.29228242850068753, 0.29064039257163876, 0.3004926094182802, 0.31034482636279465, 0.31034482616704867, 0.2955665005545311, 0.29556650084815, 0.28571428370788965, 0.29392446442973635, 0.32183907923635785, 0.30541871779266444, 0.2873563199305573, 0.2873563198326844, 0.31362889851451115, 0.28571428370788965, 0.2857142838057626, 0.3152709345414329, 0.29885057290199357, 0.3037766813742508, 0.2988505729998665, 0.3037766813742508, 0.29720853677719883, 0.308702789846508, 0.31362889822089224, 0.3004926090267883, 0.32512315138807435, 0.3070607536238403, 0.3119868620960975, 0.30049260892891533, 0.3234811149696607, 0.308702789748635, 0.30049260912466125, 0.31527093444355997, 0.3037766814721237, 0.31855500669314946, 0.3037766813742508, 0.3169129705683547, 0.3037766813742508, 0.30541871749904553, 0.30541871759691847, 0.3201970428179442, 0.3004926090267883, 0.31527093444355997, 0.31034482597130275, 0.30541871740117255, 0.29720853658145285, 0.308702789846508, 0.3037766812763778, 0.3169129704704817, 0.3070607536238403, 0.31691297037260874, 0.30541871740117255, 0.315270934345687, 0.3070607535259673, 0.32512315099658246, 0.30541871740117255, 0.3119868620960975, 0.32183907894273894, 0.3021346449558371, 0.31691297037260874, 0.30213464515158306, 0.31362889802514626, 0.3169129705683547, 0.315270934247814, 0.3185550064974035, 0.2988505727062476, 0.31362889812301925, 0.3201970426221982, 0.31198686180247853, 0.31362889812301925, 0.30870278965076203, 0.3234811149696607, 0.3037766812763778, 0.3267651872192502, 0.3070607536238403, 0.315270934247814, 0.3201970426221982, 0.3103448257755568, 0.3234811148717877, 0.3201970426221982, 0.3234811152632796, 0.315270934247814, 0.3136288979272733, 0.3119868619003515, 0.3087027902379999, 0.3169129704704817, 0.29720853667932584, 0.3103448257755568, 0.32183907874699297, 0.3185550063016575, 0.3185550063995305, 0.31198686180247853, 0.3119868619003515, 0.32676518741499616, 0.30870278965076203, 0.3267651871213772, 0.29064039218014687, 0.33004925956671266, 0.308702789748635, 0.32183907884486596, 0.3234811148717877, 0.3185550063995305, 0.3201970426221982, 0.3234811150675337, 0.3185550062037845, 0.32183907884486596, 0.3054187171075536, 0.31691297027473575, 0.315270934149941, 0.31691297017686276, 0.3267651873171232, 0.321839078551247, 0.315270934149941, 0.31691297037260874, 0.315270934149941, 0.3185550064974035, 0.31527093444355997, 0.3267651873171232, 0.32512315138807435, 0.3136288979272733, 0.31362889822089224, 0.3103448256776838, 0.31362889802514626, 0.3251231512902014, 0.3185550064974035, 0.3103448256776838, 0.30213464505371007, 0.31527093444355997, 0.32183907884486596, 0.308702789748635, 0.32183907884486596, 0.32183907884486596, 0.31691297037260874, 0.31855500659527647, 0.3201970429158172, 0.3185550064974035, 0.31691297037260874, 0.3169129704704817, 0.31362889812301925, 0.3169129704704817, 0.3152709345414329, 0.30870278955288905, 0.32676518751286915, 0.31855500659527647, 0.31855500669314946, 0.31855500669314946, 0.3185550064974035, 0.31362889802514626, 0.3234811151654067, 0.315270934149941, 0.3316912957893804, 0.31362889812301925, 0.3201970427200712, 0.31691297037260874, 0.315270934149941, 0.3169129704704817, 0.31855500659527647, 0.3169129704704817, 0.3169129705683547, 0.32019704252432524, 0.3284072235397909, 0.3136288979272733, 0.3316912958872534, 0.31691297037260874, 0.3201970426221982, 0.32183907904061193, 0.315270934247814, 0.3185550063995305, 0.31855500659527647, 0.315270934247814, 0.31855500669314946, 0.31198686170460554, 0.3234811152632796, 0.31362889822089224, 0.3201970426221982, 0.32676518741499616, 0.315270934345687, 0.315270934149941, 0.3201970428179442, 0.32183907904061193, 0.3201970426221982, 0.31362889802514626, 0.315270934247814, 0.31198686160673256, 0.33825944048430534, 0.31527093395419503, 0.33004925986033157, 0.315270934247814, 0.3316912958872534, 0.3284072235397909, 0.31855500659527647, 0.3201970426221982, 0.31855500659527647, 0.3234811149696607, 0.3316912957893804, 0.32183907874699297, 0.31362889802514626, 0.315270934247814, 0.3234811147739147, 0.32512315109445544, 0.31691297037260874, 0.32183907864912, 0.3201970427200712, 0.31527093395419503, 0.3333333320120481, 0.3136288978294003, 0.3333333321099211, 0.32019704232857926, 0.3316912957893804, 0.3267651873171232, 0.3300492594688397, 0.33825944038643235, 0.32840722305042597, 0.32183907884486596, 0.31691297017686276, 0.33825944048430534, 0.3267651870235042, 0.32512315138807435, 0.31691297017686276, 0.33661740396801865, 0.34646962101040607, 0.3300492594688397], 'loss': [2.705154291756099, 2.695885208451038, 2.687746080872459, 2.681563387465428, 2.676120234514898, 2.671174257932502, 2.6679581641171746, 2.665943127833842, 2.6641312315233923, 2.662989285447514, 2.662207794581106, 2.661807044869331, 2.6613342521127, 2.661155133374663, 2.661075552973659, 2.6610283446262994, 2.6609088328829538, 2.6607008374936774, 2.660527338776011, 2.660436826075372, 2.6603253299940293, 2.6602352998095125, 2.660121651694515, 2.6598141025958366, 2.6598358451952926, 2.659521440509898, 2.659383268571732, 2.6589404787615827, 2.6586830370235246, 2.6581601734278872, 2.657693831289084, 2.656946871363896, 2.656150174875279, 2.6551088198009705, 2.653783359370927, 2.651752005661293, 2.6491724124924114, 2.6453869134248893, 2.639484062527729, 2.6301588095678685, 2.6178288687915527, 2.60072397167433, 2.5861453074938954, 2.570286162237367, 2.558771700927609, 2.54906615701789, 2.537757016207403, 2.518665088226663, 2.5025925521733092, 2.485666647094476, 2.4678776637239865, 2.4426516812929626, 2.415327861715391, 2.3869784535323815, 2.358512282714217, 2.3374806383552005, 2.3157613464449467, 2.298853429386993, 2.2851196836397145, 2.270754481145244, 2.2522025440263063, 2.2300787736747787, 2.2174105685349605, 2.2057922135143553, 2.1866946768711726, 2.1749501552425126, 2.175140616046819, 2.152553041465963, 2.1477900994631787, 2.143386357129232, 2.1334125932971553, 2.1192084429934774, 2.1149520164152924, 2.110399845444446, 2.0913619480093892, 2.083677490242208, 2.0795583157079176, 2.073538730570423, 2.063187112357827, 2.059731069972138, 2.0597610514756344, 2.055807180669029, 2.054552574666863, 2.043386343274518, 2.0434491268173627, 2.0390820610939353, 2.0342871040289405, 2.0445986622412837, 2.046511562942724, 2.029985846141525, 2.035927227439332, 2.0355928412208324, 2.0388662096411294, 2.02962975188692, 2.025788163894011, 2.0260485151709964, 2.014507854156181, 2.0115076925720277, 2.0075473763369924, 2.00510841385295, 2.0024999068258236, 2.003451746445172, 2.0035643099759395, 2.0025676338090053, 2.0013797033494014, 1.9979794916431028, 2.004817913494071, 1.9971776412987365, 1.992813265250204, 1.9951188743481645, 1.99329394138814, 1.9911179914611565, 1.987111774362333, 1.99173624995308, 1.9862269986091943, 1.9838270860041436, 1.9816743385130864, 1.983281321594113, 1.9847974035529385, 1.977447017507142, 1.9769099277637332, 1.976247955837289, 1.975514895715263, 1.9739500526040485, 1.98789675828123, 1.9822806561996804, 1.9747315719142342, 1.9738906858882865, 1.9699638587982993, 1.9669273854281133, 1.9680044706352438, 1.9661837386399568, 1.9709299668149536, 1.9645613427524449, 1.9684516832324268, 1.966568617458461, 1.9686620428821635, 1.96215151012311, 1.9600707250454097, 1.9642341817428934, 1.9695658868832755, 1.9587961286000402, 1.9589397369224189, 1.9603998627261214, 1.9676187496655286, 1.9594248090191788, 1.9540790084451132, 1.954381619341809, 1.9585863067873694, 1.9539693283104553, 1.9510507228682907, 1.9499005903698337, 1.9596928790364667, 1.955379416076065, 1.9492152210133766, 1.9459529737672276, 1.946772981473308, 1.9455477420309486, 1.9570732597452904, 1.980974659342051, 1.9562508349056362, 1.9633508238704298, 1.9564731574890795, 1.9684865514355763, 1.9572493136051499, 1.948001642442582, 1.9415298669979557, 1.9385977019029965, 1.9385335959448218, 1.9373748251544867, 1.9406075796797047, 1.938972037920472, 1.9411874669288463, 1.9420757581565904, 1.9371647553767022, 1.9390251266393328, 1.9390153976436513, 1.9337424180346103, 1.9348508933241608, 1.9317662866698153, 1.9312610716300824, 1.9325730978341074, 1.9406159764687383, 1.9449468437406316, 1.9349070256740406, 1.9480092447151638, 1.9330945481020323, 1.9278080973047496, 1.9274295945921474, 1.9260478283100795, 1.926757181400636, 1.9260834862319351, 1.927318576180225, 1.9252022851419155, 1.9251378137717747, 1.9280985216830056, 1.9336164685490196, 1.9366310395744057, 1.9284454183656823, 1.9251414521763701, 1.9268333024312827, 1.927010401954886, 1.932905391892858, 1.9327741182805087, 1.9624925595778948, 1.9543083298622461, 1.9297758091156978, 1.9232885819440995, 1.9213386112170052, 1.9169130712563986, 1.9158299074035896, 1.916744158498071, 1.9147876503041636, 1.918139978160114, 1.914986275549542, 1.9175828941059307, 1.9225824286071183, 1.9155903614522005, 1.9166795875012752, 1.914772329780845, 1.9163308315942909, 1.912902937777478, 1.9105951507967844, 1.9130976376837039, 1.9097010799501957, 1.9144794160090923, 1.918779405822989, 1.9136008070724455, 1.9090167356957155, 1.9102504462921643, 1.905637721555189, 1.9087036212611737, 1.9081045157121193, 1.9071271241323169, 1.905356435413478, 1.904995561233536, 1.9057796729418777, 1.906847546379669, 1.9052648113003992, 1.9052917339963344, 1.9031733474202714, 1.9064395230898377, 1.9042885328465173, 1.903106781099856, 1.9021534274980518, 1.9055077247306307, 1.9121678993687248, 1.9058595584892883, 1.9013139641015682, 1.9022289814645506, 1.9081611650435586, 1.903135995306763, 1.9015209709594383, 1.9032082687902745, 1.8955320789094334, 1.897910583729127, 1.8999986512460258, 1.8995441742746246, 1.8972807641391636, 1.9007125234701796, 1.900366954098492, 1.8976381992657327, 1.8997923791775713, 1.8996075487234754, 1.8986424563601767, 1.9032475141529186, 1.8968200811860008, 1.9096010223795992, 1.9052050343773939, 1.908467722624479, 1.902464198478683, 1.9144489107680271, 1.8989570566271365, 1.8942756077102567, 1.898470669360621, 1.9019802747076298, 1.9011257497192164, 1.8891216466069467, 1.8958735730368987, 1.8981188904333408, 1.895070868400088, 1.8931942991652284, 1.894844350432958, 1.8907741089376335, 1.8918078325122778, 1.8911154153655443, 1.8904044904748027, 1.886825932565411, 1.8865531235015367, 1.885432374844561, 1.8890692993355973, 1.8851748428305561, 1.8822482494358164, 1.8849478711337768, 1.8857052569027064, 1.8866430570457506, 1.8833318540447792, 1.8885068450375504, 1.8857616977769982, 1.8905014752362543], 'acc': [0.08911704306607374, 0.08911704345772643, 0.08911704345772643, 0.08131416814527961, 0.08131416794027391, 0.08131416831356789, 0.10225872743178686, 0.10225872664848147, 0.10225872643429641, 0.10225872663930212, 0.10225872663930212, 0.10225872744096623, 0.10225872643429641, 0.10225872663012275, 0.10225872723596052, 0.10225872683512846, 0.10225872704931353, 0.10225872683512846, 0.10225872645265513, 0.10225872645265513, 0.10225872683512846, 0.10225872682594911, 0.10225872663012275, 0.10225872703095482, 0.10225872682594911, 0.10225872723596052, 0.10225872685348718, 0.10225872644347576, 0.10225872644347576, 0.10225872683512846, 0.10225872683512846, 0.10225872664848147, 0.10225872743178686, 0.10225872644347576, 0.10225872742260751, 0.10225872722678116, 0.10225872743178686, 0.10225872643429641, 0.10349075949351157, 0.10677618043807008, 0.11868583135291537, 0.14291581080067575, 0.14989733048533022, 0.163860369713889, 0.16180698123188725, 0.1626283366454945, 0.16386036951806265, 0.1650924035472302, 0.17330595395036302, 0.1753593433931378, 0.17700205448961356, 0.17987679623113276, 0.1839835727851249, 0.2000000000673153, 0.20041067690207973, 0.2102669413207248, 0.21601642597872128, 0.2258726893998759, 0.22669404426272155, 0.23901437317810997, 0.24928131480481344, 0.2620123221399358, 0.2632443520200326, 0.27186858458195867, 0.27720739358014884, 0.2792607799447782, 0.2858316208180461, 0.29075975282236294, 0.28213552581700946, 0.28624230180188126, 0.28583162140552515, 0.2932238177474764, 0.2965092410419511, 0.3006160154602121, 0.302258725736665, 0.305544148052008, 0.3018480487060743, 0.30636550328814766, 0.31252566814667393, 0.312936345373091, 0.3121149889619933, 0.31252566556421396, 0.31540040873159375, 0.3137577021758414, 0.3186858318302421, 0.31047227888136675, 0.3195071854630535, 0.31375770198001507, 0.3034907599616589, 0.3178644767899288, 0.3108829570869156, 0.31745380015099073, 0.3121149913486269, 0.3137577004134043, 0.31211499111608315, 0.31375769845514084, 0.31786447463583895, 0.31786447600662343, 0.32279260660343834, 0.32114989554368006, 0.32032854151921597, 0.3174537968219428, 0.3219712527748006, 0.3170431217496155, 0.31745379658939904, 0.3211498977344873, 0.3117043125188816, 0.3195071849122919, 0.3301848033370423, 0.3232032873914472, 0.3260780283680197, 0.31663244217328224, 0.3240246424317605, 0.3203285432816531, 0.32073921952893847, 0.3264887055944368, 0.32607802934715147, 0.3244353176999141, 0.3256673489507953, 0.3240246390659951, 0.3260780277805407, 0.32525667509014355, 0.3227926070318085, 0.32731006278883995, 0.3223819307845231, 0.3190965100357909, 0.320739222074681, 0.31868583085111035, 0.3240246400818443, 0.32443531946235127, 0.3273100629846663, 0.3281314152834107, 0.3207392183539804, 0.32813141724167416, 0.3244353184832195, 0.3281314146592142, 0.32484599747207377, 0.3256673527082134, 0.3252566719202046, 0.32484599469378744, 0.31909650745333096, 0.3264887085685495, 0.32566735133742897, 0.3264887075527003, 0.3244353164882386, 0.3256673532956924, 0.3256673499666445, 0.32320328484570465, 0.32238193098034945, 0.3289527714986821, 0.32813141880828495, 0.3289527740444246, 0.3232032866081418, 0.3322381940098514, 0.3281314184166323, 0.32772073864447265, 0.32648870735687396, 0.32854209505557036, 0.33059548252172294, 0.3170431233162263, 0.3310061601765102, 0.32238193254696024, 0.32443532063730934, 0.31745379662611645, 0.32197125257897424, 0.32443531672078235, 0.33223819381402503, 0.3342915791260878, 0.33100615857318194, 0.3326488718237475, 0.32402464164845507, 0.33593429374743783, 0.33018480298210706, 0.3293634483334465, 0.33018480337375977, 0.32813141626254244, 0.3244353180915668, 0.33223819162321777, 0.33100616135146826, 0.33100616037233654, 0.33223819224741424, 0.3289527700911802, 0.3268993837999857, 0.3281314184166323, 0.3305954825584404, 0.3260780271930616, 0.3338809013121916, 0.3342915803010459, 0.3322381953806358, 0.32936345009588364, 0.33347022705988716, 0.3347022571358103, 0.3273100600105536, 0.3310061611556419, 0.3310061615472946, 0.33223819083991235, 0.324845995905463, 0.3326488708446158, 0.3334702272557135, 0.32977412849725884, 0.3281314184166323, 0.33429158327515857, 0.33264887064878945, 0.3289527722819875, 0.3240246394943652, 0.33223819518480946, 0.32731006079385905, 0.33182751521682347, 0.33100616037233654, 0.33634497058220225, 0.33429158010521953, 0.3289527718903348, 0.3363449699580057, 0.33305954666353105, 0.33100616135146826, 0.3367556452628768, 0.33182751756673967, 0.3347022581516595, 0.33223819083991235, 0.33347022807573634, 0.3314168385778854, 0.33223819358148127, 0.3359342929641324, 0.33593428999001973, 0.33388090448213065, 0.33388090405376053, 0.3314168375987537, 0.3334702251016237, 0.3379876796469796, 0.3322381953806358, 0.3363449681955686, 0.3334702258849291, 0.33593429120169527, 0.33470225693998396, 0.33675564820027204, 0.3355236133877991, 0.3318275174076307, 0.3338809044454132, 0.3383983556984386, 0.3359342895983671, 0.33963038874847445, 0.3437371684907643, 0.3363449674122632, 0.33634497077802855, 0.3379876798795234, 0.3375770028489326, 0.3351129357697293, 0.33511293733634007, 0.33511293596555564, 0.3351129375321664, 0.3355236145627572, 0.33675564820027204, 0.33347022529745, 0.33511293455805374, 0.33921971312538557, 0.33798767749288977, 0.3425051336415739, 0.3408624210152048, 0.33264887221540024, 0.33347022803901893, 0.3379876816419605, 0.3433264875069291, 0.34004106816569885, 0.33675564522615936, 0.33963039090256425, 0.33757700049901646, 0.33757700206562724, 0.3355236149544099, 0.3355236121761236, 0.33757700402389074, 0.3371663252308628, 0.33347022428160084, 0.3330595472510101, 0.33305954944181737, 0.33593429315995876, 0.3400410665623706, 0.3355236141711045, 0.34250513485324946, 0.34045174578376863, 0.34086242379349113, 0.33552361198029723, 0.33880903491983666, 0.3359342905774988, 0.3482546205270951, 0.3383983562859177, 0.3453798767722363, 0.3392197105429256, 0.345379875829822, 0.34332648911025737, 0.3416837804004152, 0.33757700065812535, 0.3449692015408001, 0.3449691975875557, 0.34291581047633835, 0.3379876792920444, 0.34004106816569885, 0.3433264904810418, 0.3404517473503794, 0.3383983594558567, 0.33347022467325355]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
