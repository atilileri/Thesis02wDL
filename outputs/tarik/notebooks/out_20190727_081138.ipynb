{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf33.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 08:11:39 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'All', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023501678550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002354B276EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1077, Accuracy:0.2329, Validation Loss:1.0972, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0927, Accuracy:0.3943, Validation Loss:1.0856, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0818, Accuracy:0.3943, Validation Loss:1.0781, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0762, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0735, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.4138\n",
    "Epoch #10: Loss:1.0730, Accuracy:0.4140, Validation Loss:1.0728, Validation Accuracy:0.4204\n",
    "Epoch #11: Loss:1.0725, Accuracy:0.4131, Validation Loss:1.0721, Validation Accuracy:0.4204\n",
    "Epoch #12: Loss:1.0719, Accuracy:0.4168, Validation Loss:1.0713, Validation Accuracy:0.4319\n",
    "Epoch #13: Loss:1.0709, Accuracy:0.4189, Validation Loss:1.0702, Validation Accuracy:0.4220\n",
    "Epoch #14: Loss:1.0697, Accuracy:0.4259, Validation Loss:1.0684, Validation Accuracy:0.4236\n",
    "Epoch #15: Loss:1.0678, Accuracy:0.4366, Validation Loss:1.0661, Validation Accuracy:0.4335\n",
    "Epoch #16: Loss:1.0649, Accuracy:0.4366, Validation Loss:1.0630, Validation Accuracy:0.4384\n",
    "Epoch #17: Loss:1.0615, Accuracy:0.4386, Validation Loss:1.0576, Validation Accuracy:0.4401\n",
    "Epoch #18: Loss:1.0559, Accuracy:0.4476, Validation Loss:1.0494, Validation Accuracy:0.4762\n",
    "Epoch #19: Loss:1.0466, Accuracy:0.4517, Validation Loss:1.0365, Validation Accuracy:0.4696\n",
    "Epoch #20: Loss:1.0332, Accuracy:0.4669, Validation Loss:1.0146, Validation Accuracy:0.4992\n",
    "Epoch #21: Loss:1.0110, Accuracy:0.4871, Validation Loss:0.9816, Validation Accuracy:0.5304\n",
    "Epoch #22: Loss:0.9783, Accuracy:0.5068, Validation Loss:0.9391, Validation Accuracy:0.5369\n",
    "Epoch #23: Loss:0.9342, Accuracy:0.5326, Validation Loss:0.8813, Validation Accuracy:0.5599\n",
    "Epoch #24: Loss:0.8831, Accuracy:0.5540, Validation Loss:0.8326, Validation Accuracy:0.5681\n",
    "Epoch #25: Loss:0.8383, Accuracy:0.5565, Validation Loss:0.7947, Validation Accuracy:0.5862\n",
    "Epoch #26: Loss:0.7982, Accuracy:0.5795, Validation Loss:0.7623, Validation Accuracy:0.6076\n",
    "Epoch #27: Loss:0.7558, Accuracy:0.6193, Validation Loss:0.7087, Validation Accuracy:0.6535\n",
    "Epoch #28: Loss:0.7086, Accuracy:0.6616, Validation Loss:0.6637, Validation Accuracy:0.6929\n",
    "Epoch #29: Loss:0.6670, Accuracy:0.6903, Validation Loss:0.6074, Validation Accuracy:0.7225\n",
    "Epoch #30: Loss:0.6172, Accuracy:0.7248, Validation Loss:0.5559, Validation Accuracy:0.7586\n",
    "Epoch #31: Loss:0.5750, Accuracy:0.7425, Validation Loss:0.5113, Validation Accuracy:0.7898\n",
    "Epoch #32: Loss:0.5370, Accuracy:0.7659, Validation Loss:0.4728, Validation Accuracy:0.8062\n",
    "Epoch #33: Loss:0.5124, Accuracy:0.7766, Validation Loss:0.4474, Validation Accuracy:0.8095\n",
    "Epoch #34: Loss:0.4801, Accuracy:0.7975, Validation Loss:0.4166, Validation Accuracy:0.8325\n",
    "Epoch #35: Loss:0.4508, Accuracy:0.8103, Validation Loss:0.3951, Validation Accuracy:0.8391\n",
    "Epoch #36: Loss:0.4388, Accuracy:0.8123, Validation Loss:0.3679, Validation Accuracy:0.8539\n",
    "Epoch #37: Loss:0.4159, Accuracy:0.8324, Validation Loss:0.3669, Validation Accuracy:0.8621\n",
    "Epoch #38: Loss:0.3942, Accuracy:0.8423, Validation Loss:0.3398, Validation Accuracy:0.8670\n",
    "Epoch #39: Loss:0.3741, Accuracy:0.8493, Validation Loss:0.3269, Validation Accuracy:0.8703\n",
    "Epoch #40: Loss:0.3546, Accuracy:0.8587, Validation Loss:0.3143, Validation Accuracy:0.8719\n",
    "Epoch #41: Loss:0.3492, Accuracy:0.8657, Validation Loss:0.3008, Validation Accuracy:0.8785\n",
    "Epoch #42: Loss:0.3407, Accuracy:0.8682, Validation Loss:0.2991, Validation Accuracy:0.8851\n",
    "Epoch #43: Loss:0.3277, Accuracy:0.8731, Validation Loss:0.3057, Validation Accuracy:0.8785\n",
    "Epoch #44: Loss:0.3093, Accuracy:0.8768, Validation Loss:0.2982, Validation Accuracy:0.8818\n",
    "Epoch #45: Loss:0.2928, Accuracy:0.8871, Validation Loss:0.2653, Validation Accuracy:0.8982\n",
    "Epoch #46: Loss:0.2683, Accuracy:0.8998, Validation Loss:0.2489, Validation Accuracy:0.9064\n",
    "Epoch #47: Loss:0.2538, Accuracy:0.9088, Validation Loss:0.2327, Validation Accuracy:0.9048\n",
    "Epoch #48: Loss:0.2623, Accuracy:0.8994, Validation Loss:0.2743, Validation Accuracy:0.8801\n",
    "Epoch #49: Loss:0.2529, Accuracy:0.9068, Validation Loss:0.2482, Validation Accuracy:0.8949\n",
    "Epoch #50: Loss:0.2271, Accuracy:0.9191, Validation Loss:0.2239, Validation Accuracy:0.9179\n",
    "Epoch #51: Loss:0.2144, Accuracy:0.9236, Validation Loss:0.2062, Validation Accuracy:0.9195\n",
    "Epoch #52: Loss:0.2096, Accuracy:0.9240, Validation Loss:0.2417, Validation Accuracy:0.9048\n",
    "Epoch #53: Loss:0.2223, Accuracy:0.9097, Validation Loss:0.1982, Validation Accuracy:0.9245\n",
    "Epoch #54: Loss:0.2184, Accuracy:0.9211, Validation Loss:0.1973, Validation Accuracy:0.9278\n",
    "Epoch #55: Loss:0.1986, Accuracy:0.9224, Validation Loss:0.2129, Validation Accuracy:0.9113\n",
    "Epoch #56: Loss:0.1992, Accuracy:0.9265, Validation Loss:0.1968, Validation Accuracy:0.9228\n",
    "Epoch #57: Loss:0.1858, Accuracy:0.9331, Validation Loss:0.1904, Validation Accuracy:0.9310\n",
    "Epoch #58: Loss:0.1879, Accuracy:0.9273, Validation Loss:0.1890, Validation Accuracy:0.9212\n",
    "Epoch #59: Loss:0.1929, Accuracy:0.9273, Validation Loss:0.2022, Validation Accuracy:0.9261\n",
    "Epoch #60: Loss:0.1982, Accuracy:0.9244, Validation Loss:0.1809, Validation Accuracy:0.9310\n",
    "Epoch #61: Loss:0.1858, Accuracy:0.9306, Validation Loss:0.2013, Validation Accuracy:0.9130\n",
    "Epoch #62: Loss:0.1920, Accuracy:0.9265, Validation Loss:0.2132, Validation Accuracy:0.9113\n",
    "Epoch #63: Loss:0.2112, Accuracy:0.9146, Validation Loss:0.1703, Validation Accuracy:0.9376\n",
    "Epoch #64: Loss:0.2008, Accuracy:0.9203, Validation Loss:0.1766, Validation Accuracy:0.9310\n",
    "Epoch #65: Loss:0.1965, Accuracy:0.9240, Validation Loss:0.1818, Validation Accuracy:0.9310\n",
    "Epoch #66: Loss:0.1831, Accuracy:0.9326, Validation Loss:0.2163, Validation Accuracy:0.9195\n",
    "Epoch #67: Loss:0.2087, Accuracy:0.9199, Validation Loss:0.1935, Validation Accuracy:0.9261\n",
    "Epoch #68: Loss:0.1859, Accuracy:0.9339, Validation Loss:0.1703, Validation Accuracy:0.9392\n",
    "Epoch #69: Loss:0.1711, Accuracy:0.9347, Validation Loss:0.1627, Validation Accuracy:0.9392\n",
    "Epoch #70: Loss:0.1635, Accuracy:0.9417, Validation Loss:0.1663, Validation Accuracy:0.9392\n",
    "Epoch #71: Loss:0.1631, Accuracy:0.9363, Validation Loss:0.1608, Validation Accuracy:0.9392\n",
    "Epoch #72: Loss:0.1601, Accuracy:0.9409, Validation Loss:0.1634, Validation Accuracy:0.9425\n",
    "Epoch #73: Loss:0.1605, Accuracy:0.9425, Validation Loss:0.1648, Validation Accuracy:0.9376\n",
    "Epoch #74: Loss:0.1681, Accuracy:0.9380, Validation Loss:0.1599, Validation Accuracy:0.9425\n",
    "Epoch #75: Loss:0.1591, Accuracy:0.9421, Validation Loss:0.1529, Validation Accuracy:0.9475\n",
    "Epoch #76: Loss:0.1589, Accuracy:0.9425, Validation Loss:0.1543, Validation Accuracy:0.9392\n",
    "Epoch #77: Loss:0.1597, Accuracy:0.9433, Validation Loss:0.1505, Validation Accuracy:0.9475\n",
    "Epoch #78: Loss:0.1561, Accuracy:0.9450, Validation Loss:0.1490, Validation Accuracy:0.9475\n",
    "Epoch #79: Loss:0.1601, Accuracy:0.9421, Validation Loss:0.1542, Validation Accuracy:0.9376\n",
    "Epoch #80: Loss:0.1585, Accuracy:0.9376, Validation Loss:0.1463, Validation Accuracy:0.9491\n",
    "Epoch #81: Loss:0.1609, Accuracy:0.9376, Validation Loss:0.1532, Validation Accuracy:0.9425\n",
    "Epoch #82: Loss:0.1487, Accuracy:0.9441, Validation Loss:0.1477, Validation Accuracy:0.9491\n",
    "Epoch #83: Loss:0.1523, Accuracy:0.9396, Validation Loss:0.1491, Validation Accuracy:0.9425\n",
    "Epoch #84: Loss:0.1655, Accuracy:0.9359, Validation Loss:0.1766, Validation Accuracy:0.9327\n",
    "Epoch #85: Loss:0.1538, Accuracy:0.9400, Validation Loss:0.1728, Validation Accuracy:0.9327\n",
    "Epoch #86: Loss:0.1539, Accuracy:0.9376, Validation Loss:0.1656, Validation Accuracy:0.9343\n",
    "Epoch #87: Loss:0.1538, Accuracy:0.9454, Validation Loss:0.1421, Validation Accuracy:0.9458\n",
    "Epoch #88: Loss:0.1554, Accuracy:0.9396, Validation Loss:0.1452, Validation Accuracy:0.9491\n",
    "Epoch #89: Loss:0.1479, Accuracy:0.9433, Validation Loss:0.1407, Validation Accuracy:0.9491\n",
    "Epoch #90: Loss:0.1514, Accuracy:0.9454, Validation Loss:0.1695, Validation Accuracy:0.9376\n",
    "Epoch #91: Loss:0.1547, Accuracy:0.9380, Validation Loss:0.1580, Validation Accuracy:0.9442\n",
    "Epoch #92: Loss:0.1468, Accuracy:0.9433, Validation Loss:0.1559, Validation Accuracy:0.9475\n",
    "Epoch #93: Loss:0.1531, Accuracy:0.9441, Validation Loss:0.1506, Validation Accuracy:0.9507\n",
    "Epoch #94: Loss:0.1420, Accuracy:0.9478, Validation Loss:0.1436, Validation Accuracy:0.9442\n",
    "Epoch #95: Loss:0.1364, Accuracy:0.9499, Validation Loss:0.1369, Validation Accuracy:0.9557\n",
    "Epoch #96: Loss:0.1364, Accuracy:0.9515, Validation Loss:0.1542, Validation Accuracy:0.9458\n",
    "Epoch #97: Loss:0.1392, Accuracy:0.9478, Validation Loss:0.1362, Validation Accuracy:0.9442\n",
    "Epoch #98: Loss:0.1345, Accuracy:0.9520, Validation Loss:0.1369, Validation Accuracy:0.9458\n",
    "Epoch #99: Loss:0.1326, Accuracy:0.9524, Validation Loss:0.1408, Validation Accuracy:0.9475\n",
    "Epoch #100: Loss:0.1301, Accuracy:0.9552, Validation Loss:0.1404, Validation Accuracy:0.9491\n",
    "Epoch #101: Loss:0.1445, Accuracy:0.9462, Validation Loss:0.1334, Validation Accuracy:0.9540\n",
    "Epoch #102: Loss:0.1482, Accuracy:0.9417, Validation Loss:0.1320, Validation Accuracy:0.9557\n",
    "Epoch #103: Loss:0.1371, Accuracy:0.9495, Validation Loss:0.1548, Validation Accuracy:0.9475\n",
    "Epoch #104: Loss:0.1428, Accuracy:0.9478, Validation Loss:0.1333, Validation Accuracy:0.9524\n",
    "Epoch #105: Loss:0.1313, Accuracy:0.9503, Validation Loss:0.1341, Validation Accuracy:0.9540\n",
    "Epoch #106: Loss:0.1325, Accuracy:0.9487, Validation Loss:0.1444, Validation Accuracy:0.9491\n",
    "Epoch #107: Loss:0.1598, Accuracy:0.9359, Validation Loss:0.1312, Validation Accuracy:0.9524\n",
    "Epoch #108: Loss:0.1678, Accuracy:0.9310, Validation Loss:0.1292, Validation Accuracy:0.9491\n",
    "Epoch #109: Loss:0.1917, Accuracy:0.9253, Validation Loss:0.1272, Validation Accuracy:0.9573\n",
    "Epoch #110: Loss:0.1904, Accuracy:0.9248, Validation Loss:0.1562, Validation Accuracy:0.9475\n",
    "Epoch #111: Loss:0.1841, Accuracy:0.9265, Validation Loss:0.1771, Validation Accuracy:0.9360\n",
    "Epoch #112: Loss:0.1782, Accuracy:0.9281, Validation Loss:0.1679, Validation Accuracy:0.9376\n",
    "Epoch #113: Loss:0.1695, Accuracy:0.9351, Validation Loss:0.1365, Validation Accuracy:0.9507\n",
    "Epoch #114: Loss:0.1662, Accuracy:0.9322, Validation Loss:0.1603, Validation Accuracy:0.9360\n",
    "Epoch #115: Loss:0.1683, Accuracy:0.9331, Validation Loss:0.1996, Validation Accuracy:0.9146\n",
    "Epoch #116: Loss:0.1631, Accuracy:0.9392, Validation Loss:0.1312, Validation Accuracy:0.9475\n",
    "Epoch #117: Loss:0.1396, Accuracy:0.9470, Validation Loss:0.1652, Validation Accuracy:0.9392\n",
    "Epoch #118: Loss:0.1564, Accuracy:0.9368, Validation Loss:0.1469, Validation Accuracy:0.9475\n",
    "Epoch #119: Loss:0.1488, Accuracy:0.9421, Validation Loss:0.1317, Validation Accuracy:0.9524\n",
    "Epoch #120: Loss:0.1313, Accuracy:0.9511, Validation Loss:0.1346, Validation Accuracy:0.9540\n",
    "Epoch #121: Loss:0.1249, Accuracy:0.9556, Validation Loss:0.1302, Validation Accuracy:0.9507\n",
    "Epoch #122: Loss:0.1228, Accuracy:0.9581, Validation Loss:0.1270, Validation Accuracy:0.9540\n",
    "Epoch #123: Loss:0.1213, Accuracy:0.9585, Validation Loss:0.1303, Validation Accuracy:0.9524\n",
    "Epoch #124: Loss:0.1215, Accuracy:0.9565, Validation Loss:0.1516, Validation Accuracy:0.9491\n",
    "Epoch #125: Loss:0.1466, Accuracy:0.9441, Validation Loss:0.1511, Validation Accuracy:0.9491\n",
    "Epoch #126: Loss:0.1352, Accuracy:0.9491, Validation Loss:0.1246, Validation Accuracy:0.9573\n",
    "Epoch #127: Loss:0.1230, Accuracy:0.9569, Validation Loss:0.1291, Validation Accuracy:0.9540\n",
    "Epoch #128: Loss:0.1311, Accuracy:0.9511, Validation Loss:0.1303, Validation Accuracy:0.9524\n",
    "Epoch #129: Loss:0.1275, Accuracy:0.9515, Validation Loss:0.1431, Validation Accuracy:0.9491\n",
    "Epoch #130: Loss:0.1315, Accuracy:0.9478, Validation Loss:0.1387, Validation Accuracy:0.9491\n",
    "Epoch #131: Loss:0.1329, Accuracy:0.9520, Validation Loss:0.1876, Validation Accuracy:0.9294\n",
    "Epoch #132: Loss:0.1471, Accuracy:0.9429, Validation Loss:0.1700, Validation Accuracy:0.9294\n",
    "Epoch #133: Loss:0.1423, Accuracy:0.9454, Validation Loss:0.1407, Validation Accuracy:0.9491\n",
    "Epoch #134: Loss:0.1296, Accuracy:0.9544, Validation Loss:0.1238, Validation Accuracy:0.9540\n",
    "Epoch #135: Loss:0.1228, Accuracy:0.9540, Validation Loss:0.1278, Validation Accuracy:0.9524\n",
    "Epoch #136: Loss:0.1193, Accuracy:0.9581, Validation Loss:0.1400, Validation Accuracy:0.9507\n",
    "Epoch #137: Loss:0.1289, Accuracy:0.9483, Validation Loss:0.1366, Validation Accuracy:0.9507\n",
    "Epoch #138: Loss:0.1375, Accuracy:0.9495, Validation Loss:0.1219, Validation Accuracy:0.9606\n",
    "Epoch #139: Loss:0.1299, Accuracy:0.9503, Validation Loss:0.1207, Validation Accuracy:0.9557\n",
    "Epoch #140: Loss:0.1200, Accuracy:0.9569, Validation Loss:0.1210, Validation Accuracy:0.9573\n",
    "Epoch #141: Loss:0.1195, Accuracy:0.9581, Validation Loss:0.1374, Validation Accuracy:0.9491\n",
    "Epoch #142: Loss:0.1256, Accuracy:0.9561, Validation Loss:0.1218, Validation Accuracy:0.9589\n",
    "Epoch #143: Loss:0.1194, Accuracy:0.9548, Validation Loss:0.1215, Validation Accuracy:0.9589\n",
    "Epoch #144: Loss:0.1138, Accuracy:0.9598, Validation Loss:0.1210, Validation Accuracy:0.9573\n",
    "Epoch #145: Loss:0.1147, Accuracy:0.9606, Validation Loss:0.1223, Validation Accuracy:0.9557\n",
    "Epoch #146: Loss:0.1153, Accuracy:0.9581, Validation Loss:0.1212, Validation Accuracy:0.9573\n",
    "Epoch #147: Loss:0.1166, Accuracy:0.9606, Validation Loss:0.1259, Validation Accuracy:0.9606\n",
    "Epoch #148: Loss:0.1190, Accuracy:0.9573, Validation Loss:0.1313, Validation Accuracy:0.9557\n",
    "Epoch #149: Loss:0.1228, Accuracy:0.9556, Validation Loss:0.1256, Validation Accuracy:0.9606\n",
    "Epoch #150: Loss:0.1204, Accuracy:0.9577, Validation Loss:0.1526, Validation Accuracy:0.9392\n",
    "Epoch #151: Loss:0.1297, Accuracy:0.9503, Validation Loss:0.1621, Validation Accuracy:0.9360\n",
    "Epoch #152: Loss:0.1313, Accuracy:0.9507, Validation Loss:0.1274, Validation Accuracy:0.9557\n",
    "Epoch #153: Loss:0.1179, Accuracy:0.9536, Validation Loss:0.1288, Validation Accuracy:0.9589\n",
    "Epoch #154: Loss:0.1171, Accuracy:0.9581, Validation Loss:0.1407, Validation Accuracy:0.9475\n",
    "Epoch #155: Loss:0.1198, Accuracy:0.9569, Validation Loss:0.1220, Validation Accuracy:0.9606\n",
    "Epoch #156: Loss:0.1123, Accuracy:0.9614, Validation Loss:0.1222, Validation Accuracy:0.9589\n",
    "Epoch #157: Loss:0.1120, Accuracy:0.9634, Validation Loss:0.1211, Validation Accuracy:0.9557\n",
    "Epoch #158: Loss:0.1111, Accuracy:0.9606, Validation Loss:0.1175, Validation Accuracy:0.9557\n",
    "Epoch #159: Loss:0.1124, Accuracy:0.9602, Validation Loss:0.1216, Validation Accuracy:0.9573\n",
    "Epoch #160: Loss:0.1166, Accuracy:0.9565, Validation Loss:0.1211, Validation Accuracy:0.9540\n",
    "Epoch #161: Loss:0.1124, Accuracy:0.9577, Validation Loss:0.1212, Validation Accuracy:0.9540\n",
    "Epoch #162: Loss:0.1102, Accuracy:0.9610, Validation Loss:0.1183, Validation Accuracy:0.9589\n",
    "Epoch #163: Loss:0.1110, Accuracy:0.9585, Validation Loss:0.1230, Validation Accuracy:0.9573\n",
    "Epoch #164: Loss:0.1136, Accuracy:0.9581, Validation Loss:0.1209, Validation Accuracy:0.9589\n",
    "Epoch #165: Loss:0.1172, Accuracy:0.9565, Validation Loss:0.1166, Validation Accuracy:0.9589\n",
    "Epoch #166: Loss:0.1153, Accuracy:0.9581, Validation Loss:0.1309, Validation Accuracy:0.9507\n",
    "Epoch #167: Loss:0.1163, Accuracy:0.9556, Validation Loss:0.1942, Validation Accuracy:0.9228\n",
    "Epoch #168: Loss:0.1507, Accuracy:0.9437, Validation Loss:0.1667, Validation Accuracy:0.9327\n",
    "Epoch #169: Loss:0.1248, Accuracy:0.9520, Validation Loss:0.1301, Validation Accuracy:0.9557\n",
    "Epoch #170: Loss:0.1157, Accuracy:0.9581, Validation Loss:0.1223, Validation Accuracy:0.9622\n",
    "Epoch #171: Loss:0.1131, Accuracy:0.9606, Validation Loss:0.1184, Validation Accuracy:0.9622\n",
    "Epoch #172: Loss:0.1180, Accuracy:0.9569, Validation Loss:0.1202, Validation Accuracy:0.9589\n",
    "Epoch #173: Loss:0.1127, Accuracy:0.9585, Validation Loss:0.1221, Validation Accuracy:0.9573\n",
    "Epoch #174: Loss:0.1161, Accuracy:0.9589, Validation Loss:0.1207, Validation Accuracy:0.9589\n",
    "Epoch #175: Loss:0.1119, Accuracy:0.9585, Validation Loss:0.1171, Validation Accuracy:0.9639\n",
    "Epoch #176: Loss:0.1061, Accuracy:0.9647, Validation Loss:0.1171, Validation Accuracy:0.9606\n",
    "Epoch #177: Loss:0.1087, Accuracy:0.9581, Validation Loss:0.1167, Validation Accuracy:0.9540\n",
    "Epoch #178: Loss:0.1076, Accuracy:0.9606, Validation Loss:0.1177, Validation Accuracy:0.9622\n",
    "Epoch #179: Loss:0.1080, Accuracy:0.9610, Validation Loss:0.1255, Validation Accuracy:0.9573\n",
    "Epoch #180: Loss:0.1168, Accuracy:0.9581, Validation Loss:0.1184, Validation Accuracy:0.9557\n",
    "Epoch #181: Loss:0.1056, Accuracy:0.9626, Validation Loss:0.1202, Validation Accuracy:0.9606\n",
    "Epoch #182: Loss:0.1090, Accuracy:0.9634, Validation Loss:0.1164, Validation Accuracy:0.9622\n",
    "Epoch #183: Loss:0.1064, Accuracy:0.9634, Validation Loss:0.1160, Validation Accuracy:0.9606\n",
    "Epoch #184: Loss:0.1053, Accuracy:0.9618, Validation Loss:0.1172, Validation Accuracy:0.9540\n",
    "Epoch #185: Loss:0.1091, Accuracy:0.9626, Validation Loss:0.1161, Validation Accuracy:0.9540\n",
    "Epoch #186: Loss:0.1100, Accuracy:0.9614, Validation Loss:0.1278, Validation Accuracy:0.9557\n",
    "Epoch #187: Loss:0.1071, Accuracy:0.9618, Validation Loss:0.1528, Validation Accuracy:0.9425\n",
    "Epoch #188: Loss:0.1182, Accuracy:0.9577, Validation Loss:0.1315, Validation Accuracy:0.9507\n",
    "Epoch #189: Loss:0.1236, Accuracy:0.9577, Validation Loss:0.1574, Validation Accuracy:0.9392\n",
    "Epoch #190: Loss:0.1199, Accuracy:0.9581, Validation Loss:0.1314, Validation Accuracy:0.9540\n",
    "Epoch #191: Loss:0.1121, Accuracy:0.9569, Validation Loss:0.1415, Validation Accuracy:0.9507\n",
    "Epoch #192: Loss:0.1075, Accuracy:0.9610, Validation Loss:0.1201, Validation Accuracy:0.9589\n",
    "Epoch #193: Loss:0.1119, Accuracy:0.9602, Validation Loss:0.1276, Validation Accuracy:0.9557\n",
    "Epoch #194: Loss:0.1193, Accuracy:0.9589, Validation Loss:0.1308, Validation Accuracy:0.9524\n",
    "Epoch #195: Loss:0.1142, Accuracy:0.9561, Validation Loss:0.1548, Validation Accuracy:0.9425\n",
    "Epoch #196: Loss:0.1115, Accuracy:0.9610, Validation Loss:0.1194, Validation Accuracy:0.9639\n",
    "Epoch #197: Loss:0.1023, Accuracy:0.9634, Validation Loss:0.1153, Validation Accuracy:0.9639\n",
    "Epoch #198: Loss:0.1026, Accuracy:0.9651, Validation Loss:0.1209, Validation Accuracy:0.9639\n",
    "Epoch #199: Loss:0.1125, Accuracy:0.9569, Validation Loss:0.1180, Validation Accuracy:0.9589\n",
    "Epoch #200: Loss:0.1207, Accuracy:0.9569, Validation Loss:0.1250, Validation Accuracy:0.9573\n",
    "Epoch #201: Loss:0.1222, Accuracy:0.9515, Validation Loss:0.1114, Validation Accuracy:0.9655\n",
    "Epoch #202: Loss:0.1078, Accuracy:0.9626, Validation Loss:0.1164, Validation Accuracy:0.9622\n",
    "Epoch #203: Loss:0.1016, Accuracy:0.9647, Validation Loss:0.1180, Validation Accuracy:0.9655\n",
    "Epoch #204: Loss:0.1085, Accuracy:0.9602, Validation Loss:0.1131, Validation Accuracy:0.9589\n",
    "Epoch #205: Loss:0.1063, Accuracy:0.9585, Validation Loss:0.1146, Validation Accuracy:0.9589\n",
    "Epoch #206: Loss:0.1059, Accuracy:0.9602, Validation Loss:0.1153, Validation Accuracy:0.9573\n",
    "Epoch #207: Loss:0.1039, Accuracy:0.9610, Validation Loss:0.1214, Validation Accuracy:0.9573\n",
    "Epoch #208: Loss:0.1051, Accuracy:0.9622, Validation Loss:0.1134, Validation Accuracy:0.9622\n",
    "Epoch #209: Loss:0.1006, Accuracy:0.9634, Validation Loss:0.1265, Validation Accuracy:0.9524\n",
    "Epoch #210: Loss:0.1179, Accuracy:0.9552, Validation Loss:0.1249, Validation Accuracy:0.9573\n",
    "Epoch #211: Loss:0.1223, Accuracy:0.9528, Validation Loss:0.1644, Validation Accuracy:0.9475\n",
    "Epoch #212: Loss:0.1420, Accuracy:0.9462, Validation Loss:0.1508, Validation Accuracy:0.9458\n",
    "Epoch #213: Loss:0.1239, Accuracy:0.9474, Validation Loss:0.1260, Validation Accuracy:0.9524\n",
    "Epoch #214: Loss:0.1103, Accuracy:0.9602, Validation Loss:0.1183, Validation Accuracy:0.9589\n",
    "Epoch #215: Loss:0.1158, Accuracy:0.9540, Validation Loss:0.1228, Validation Accuracy:0.9557\n",
    "Epoch #216: Loss:0.1144, Accuracy:0.9593, Validation Loss:0.1120, Validation Accuracy:0.9639\n",
    "Epoch #217: Loss:0.0995, Accuracy:0.9655, Validation Loss:0.1108, Validation Accuracy:0.9639\n",
    "Epoch #218: Loss:0.0998, Accuracy:0.9639, Validation Loss:0.1158, Validation Accuracy:0.9589\n",
    "Epoch #219: Loss:0.1024, Accuracy:0.9622, Validation Loss:0.1160, Validation Accuracy:0.9573\n",
    "Epoch #220: Loss:0.1054, Accuracy:0.9622, Validation Loss:0.1181, Validation Accuracy:0.9573\n",
    "Epoch #221: Loss:0.1072, Accuracy:0.9598, Validation Loss:0.1146, Validation Accuracy:0.9589\n",
    "Epoch #222: Loss:0.1021, Accuracy:0.9630, Validation Loss:0.1153, Validation Accuracy:0.9606\n",
    "Epoch #223: Loss:0.0982, Accuracy:0.9651, Validation Loss:0.1440, Validation Accuracy:0.9540\n",
    "Epoch #224: Loss:0.1083, Accuracy:0.9610, Validation Loss:0.1456, Validation Accuracy:0.9524\n",
    "Epoch #225: Loss:0.1064, Accuracy:0.9569, Validation Loss:0.1154, Validation Accuracy:0.9639\n",
    "Epoch #226: Loss:0.1049, Accuracy:0.9610, Validation Loss:0.1171, Validation Accuracy:0.9589\n",
    "Epoch #227: Loss:0.1057, Accuracy:0.9614, Validation Loss:0.1192, Validation Accuracy:0.9589\n",
    "Epoch #228: Loss:0.1029, Accuracy:0.9618, Validation Loss:0.1135, Validation Accuracy:0.9622\n",
    "Epoch #229: Loss:0.0992, Accuracy:0.9639, Validation Loss:0.1131, Validation Accuracy:0.9622\n",
    "Epoch #230: Loss:0.0981, Accuracy:0.9647, Validation Loss:0.1168, Validation Accuracy:0.9589\n",
    "Epoch #231: Loss:0.0990, Accuracy:0.9630, Validation Loss:0.1136, Validation Accuracy:0.9589\n",
    "Epoch #232: Loss:0.1015, Accuracy:0.9626, Validation Loss:0.1142, Validation Accuracy:0.9606\n",
    "Epoch #233: Loss:0.0953, Accuracy:0.9655, Validation Loss:0.1121, Validation Accuracy:0.9672\n",
    "Epoch #234: Loss:0.0973, Accuracy:0.9659, Validation Loss:0.1142, Validation Accuracy:0.9622\n",
    "Epoch #235: Loss:0.0970, Accuracy:0.9676, Validation Loss:0.1117, Validation Accuracy:0.9639\n",
    "Epoch #236: Loss:0.0974, Accuracy:0.9647, Validation Loss:0.1153, Validation Accuracy:0.9606\n",
    "Epoch #237: Loss:0.0958, Accuracy:0.9651, Validation Loss:0.1151, Validation Accuracy:0.9622\n",
    "Epoch #238: Loss:0.0986, Accuracy:0.9639, Validation Loss:0.1155, Validation Accuracy:0.9606\n",
    "Epoch #239: Loss:0.0990, Accuracy:0.9663, Validation Loss:0.1160, Validation Accuracy:0.9557\n",
    "Epoch #240: Loss:0.0970, Accuracy:0.9667, Validation Loss:0.1151, Validation Accuracy:0.9606\n",
    "Epoch #241: Loss:0.0935, Accuracy:0.9663, Validation Loss:0.1149, Validation Accuracy:0.9606\n",
    "Epoch #242: Loss:0.0983, Accuracy:0.9655, Validation Loss:0.1660, Validation Accuracy:0.9376\n",
    "Epoch #243: Loss:0.1094, Accuracy:0.9598, Validation Loss:0.1266, Validation Accuracy:0.9573\n",
    "Epoch #244: Loss:0.0975, Accuracy:0.9643, Validation Loss:0.1142, Validation Accuracy:0.9639\n",
    "Epoch #245: Loss:0.0975, Accuracy:0.9663, Validation Loss:0.1165, Validation Accuracy:0.9540\n",
    "Epoch #246: Loss:0.0942, Accuracy:0.9655, Validation Loss:0.1292, Validation Accuracy:0.9540\n",
    "Epoch #247: Loss:0.1072, Accuracy:0.9598, Validation Loss:0.1197, Validation Accuracy:0.9557\n",
    "Epoch #248: Loss:0.1009, Accuracy:0.9626, Validation Loss:0.1102, Validation Accuracy:0.9639\n",
    "Epoch #249: Loss:0.0949, Accuracy:0.9659, Validation Loss:0.1157, Validation Accuracy:0.9606\n",
    "Epoch #250: Loss:0.0935, Accuracy:0.9671, Validation Loss:0.1149, Validation Accuracy:0.9589\n",
    "Epoch #251: Loss:0.0957, Accuracy:0.9663, Validation Loss:0.1109, Validation Accuracy:0.9639\n",
    "Epoch #252: Loss:0.0963, Accuracy:0.9671, Validation Loss:0.1139, Validation Accuracy:0.9606\n",
    "Epoch #253: Loss:0.1156, Accuracy:0.9556, Validation Loss:0.1204, Validation Accuracy:0.9573\n",
    "Epoch #254: Loss:0.1066, Accuracy:0.9618, Validation Loss:0.1162, Validation Accuracy:0.9606\n",
    "Epoch #255: Loss:0.1208, Accuracy:0.9532, Validation Loss:0.1112, Validation Accuracy:0.9606\n",
    "Epoch #256: Loss:0.1076, Accuracy:0.9577, Validation Loss:0.1107, Validation Accuracy:0.9639\n",
    "Epoch #257: Loss:0.1020, Accuracy:0.9606, Validation Loss:0.1134, Validation Accuracy:0.9639\n",
    "Epoch #258: Loss:0.0971, Accuracy:0.9680, Validation Loss:0.1111, Validation Accuracy:0.9639\n",
    "Epoch #259: Loss:0.0941, Accuracy:0.9680, Validation Loss:0.1094, Validation Accuracy:0.9672\n",
    "Epoch #260: Loss:0.0929, Accuracy:0.9667, Validation Loss:0.1265, Validation Accuracy:0.9557\n",
    "Epoch #261: Loss:0.0958, Accuracy:0.9651, Validation Loss:0.1208, Validation Accuracy:0.9557\n",
    "Epoch #262: Loss:0.1046, Accuracy:0.9626, Validation Loss:0.1223, Validation Accuracy:0.9573\n",
    "Epoch #263: Loss:0.1065, Accuracy:0.9585, Validation Loss:0.1114, Validation Accuracy:0.9622\n",
    "Epoch #264: Loss:0.1028, Accuracy:0.9614, Validation Loss:0.1132, Validation Accuracy:0.9639\n",
    "Epoch #265: Loss:0.1013, Accuracy:0.9639, Validation Loss:0.1128, Validation Accuracy:0.9622\n",
    "Epoch #266: Loss:0.1001, Accuracy:0.9602, Validation Loss:0.1129, Validation Accuracy:0.9606\n",
    "Epoch #267: Loss:0.1002, Accuracy:0.9606, Validation Loss:0.1199, Validation Accuracy:0.9589\n",
    "Epoch #268: Loss:0.1084, Accuracy:0.9589, Validation Loss:0.1222, Validation Accuracy:0.9540\n",
    "Epoch #269: Loss:0.1123, Accuracy:0.9577, Validation Loss:0.1160, Validation Accuracy:0.9622\n",
    "Epoch #270: Loss:0.0949, Accuracy:0.9671, Validation Loss:0.1083, Validation Accuracy:0.9672\n",
    "Epoch #271: Loss:0.0919, Accuracy:0.9696, Validation Loss:0.1098, Validation Accuracy:0.9639\n",
    "Epoch #272: Loss:0.0990, Accuracy:0.9622, Validation Loss:0.1093, Validation Accuracy:0.9622\n",
    "Epoch #273: Loss:0.1087, Accuracy:0.9585, Validation Loss:0.1080, Validation Accuracy:0.9655\n",
    "Epoch #274: Loss:0.1118, Accuracy:0.9565, Validation Loss:0.1167, Validation Accuracy:0.9557\n",
    "Epoch #275: Loss:0.1164, Accuracy:0.9548, Validation Loss:0.1122, Validation Accuracy:0.9622\n",
    "Epoch #276: Loss:0.1046, Accuracy:0.9618, Validation Loss:0.1135, Validation Accuracy:0.9606\n",
    "Epoch #277: Loss:0.1019, Accuracy:0.9606, Validation Loss:0.1063, Validation Accuracy:0.9672\n",
    "Epoch #278: Loss:0.1167, Accuracy:0.9515, Validation Loss:0.1155, Validation Accuracy:0.9573\n",
    "Epoch #279: Loss:0.1078, Accuracy:0.9552, Validation Loss:0.1176, Validation Accuracy:0.9606\n",
    "Epoch #280: Loss:0.1059, Accuracy:0.9593, Validation Loss:0.1050, Validation Accuracy:0.9655\n",
    "Epoch #281: Loss:0.0946, Accuracy:0.9634, Validation Loss:0.1066, Validation Accuracy:0.9639\n",
    "Epoch #282: Loss:0.0899, Accuracy:0.9700, Validation Loss:0.1083, Validation Accuracy:0.9622\n",
    "Epoch #283: Loss:0.0891, Accuracy:0.9708, Validation Loss:0.1105, Validation Accuracy:0.9622\n",
    "Epoch #284: Loss:0.0926, Accuracy:0.9692, Validation Loss:0.1115, Validation Accuracy:0.9639\n",
    "Epoch #285: Loss:0.0972, Accuracy:0.9647, Validation Loss:0.1114, Validation Accuracy:0.9622\n",
    "Epoch #286: Loss:0.0976, Accuracy:0.9610, Validation Loss:0.1085, Validation Accuracy:0.9655\n",
    "Epoch #287: Loss:0.0915, Accuracy:0.9680, Validation Loss:0.1098, Validation Accuracy:0.9622\n",
    "Epoch #288: Loss:0.0910, Accuracy:0.9676, Validation Loss:0.1100, Validation Accuracy:0.9639\n",
    "Epoch #289: Loss:0.1047, Accuracy:0.9610, Validation Loss:0.1091, Validation Accuracy:0.9639\n",
    "Epoch #290: Loss:0.0993, Accuracy:0.9622, Validation Loss:0.1224, Validation Accuracy:0.9573\n",
    "Epoch #291: Loss:0.0928, Accuracy:0.9676, Validation Loss:0.1141, Validation Accuracy:0.9589\n",
    "Epoch #292: Loss:0.0881, Accuracy:0.9696, Validation Loss:0.1076, Validation Accuracy:0.9672\n",
    "Epoch #293: Loss:0.0879, Accuracy:0.9667, Validation Loss:0.1109, Validation Accuracy:0.9655\n",
    "Epoch #294: Loss:0.0965, Accuracy:0.9614, Validation Loss:0.1124, Validation Accuracy:0.9639\n",
    "Epoch #295: Loss:0.0991, Accuracy:0.9626, Validation Loss:0.1137, Validation Accuracy:0.9622\n",
    "Epoch #296: Loss:0.0920, Accuracy:0.9671, Validation Loss:0.1183, Validation Accuracy:0.9573\n",
    "Epoch #297: Loss:0.0956, Accuracy:0.9647, Validation Loss:0.1093, Validation Accuracy:0.9655\n",
    "Epoch #298: Loss:0.0913, Accuracy:0.9655, Validation Loss:0.1105, Validation Accuracy:0.9655\n",
    "Epoch #299: Loss:0.0896, Accuracy:0.9688, Validation Loss:0.1077, Validation Accuracy:0.9655\n",
    "Epoch #300: Loss:0.0868, Accuracy:0.9717, Validation Loss:0.1110, Validation Accuracy:0.9639\n",
    "\n",
    "Test:\n",
    "Test Loss:0.11103872, Accuracy:0.9639\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "       02   01   03\n",
    "t:02  218    9    0\n",
    "t:01   11  228    1\n",
    "t:03    0    1  141\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.95      0.96      0.96       227\n",
    "          01       0.96      0.95      0.95       240\n",
    "          03       0.99      0.99      0.99       142\n",
    "\n",
    "    accuracy                           0.96       609\n",
    "   macro avg       0.97      0.97      0.97       609\n",
    "weighted avg       0.96      0.96      0.96       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 08:54:49 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 43 minutes, 10 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0972107603827916, 1.0855926296785352, 1.0780587977376477, 1.0746648207869631, 1.074290692316879, 1.0746176845725925, 1.0749415515483112, 1.0740350093356104, 1.0733914369432798, 1.0727997014087995, 1.0721439778902653, 1.0712600188889527, 1.0701776049994483, 1.068363783394762, 1.0660844434462549, 1.0629609157690665, 1.0575703079085828, 1.0494431071289263, 1.036504659942414, 1.0146261140435004, 0.9815865040608424, 0.9390599259797772, 0.8813391652796264, 0.8325858541897365, 0.7947121195017998, 0.76232659983126, 0.7087175965504889, 0.6636981400362963, 0.607426287896919, 0.555886925343418, 0.5112630581033641, 0.47275979623614467, 0.4474321770648455, 0.4165886823571179, 0.395123529463566, 0.36791007286809346, 0.36692505644264284, 0.3398088229015739, 0.3269407326854117, 0.31427176905970267, 0.3008443822805909, 0.29907262511245525, 0.30566663230189745, 0.2982084130125093, 0.2652977303233248, 0.24894775976296912, 0.23268046627686725, 0.27427712829144324, 0.24819085430139784, 0.22386511157103164, 0.20617550276281016, 0.2417390815093991, 0.19820480418126962, 0.19729852830541544, 0.21289358998852215, 0.19682324754780736, 0.19035864311877534, 0.18898490724598832, 0.20215599755152497, 0.18085478104296185, 0.20131878163329095, 0.2131652986425876, 0.17026513085772446, 0.17659986809458833, 0.18177035712745584, 0.21627513303349563, 0.19350443993295943, 0.17026944081673676, 0.16274448646896186, 0.16627367078181363, 0.16081872907863265, 0.16336811457459366, 0.1647734646677775, 0.15992620195857019, 0.15294020939832445, 0.15426698936323816, 0.1505325515379851, 0.1489912221584414, 0.1541549334385125, 0.1462581250174292, 0.1531918161167887, 0.1476737831173272, 0.14912085773420256, 0.17662886453085933, 0.172771493422574, 0.16560451172549148, 0.14211725442378195, 0.14523629658230028, 0.14073168905987138, 0.16945679434414568, 0.1579796482238472, 0.1559496259160817, 0.1505638073964659, 0.14362930950566466, 0.1369388004144033, 0.15420235421857223, 0.13623244514802016, 0.13685476596425908, 0.1407936772542634, 0.140447143410227, 0.13339588202670682, 0.13199576684113207, 0.15482596632197182, 0.13333154331482885, 0.13408898543841732, 0.14437396743614686, 0.1312297192113153, 0.1292003191554879, 0.12715958895648055, 0.15623548095938802, 0.17711662463170946, 0.167930206837521, 0.13646345941597604, 0.1602968512889004, 0.19963156482073277, 0.1311510605569348, 0.1652257614241445, 0.14688558218318645, 0.1317416830309506, 0.13462464584799236, 0.13024206295585006, 0.12704866529979142, 0.1303490741401666, 0.15163417525087866, 0.15113557745474704, 0.12460051190676948, 0.12910237426338916, 0.13032207653929642, 0.14311900991818, 0.13872066954967424, 0.18759403687294676, 0.1699572030840249, 0.14067634083758826, 0.1238153353211132, 0.12781419483898895, 0.1400105150413435, 0.13655950618397034, 0.12192825642712597, 0.1207148880745194, 0.12100257392978825, 0.13741079613884485, 0.12178106004087796, 0.12153311820061531, 0.12098800397075847, 0.12225785092962982, 0.1211973454662536, 0.12586454176061063, 0.1312960085316832, 0.12556934537754466, 0.15263621900567084, 0.1621221597558759, 0.1273841334534396, 0.12878297445515693, 0.14065333545110104, 0.12201338035034624, 0.12220381447442842, 0.1211353421749544, 0.11750006005290302, 0.12162453158833515, 0.12108679717691073, 0.12116463100675291, 0.11834021186691591, 0.12295816417887488, 0.12092636271161203, 0.11657259697690973, 0.1308625372025767, 0.19416519152902814, 0.16673966508193555, 0.1300600182344565, 0.1222910091452215, 0.11837635873182262, 0.1202211737975307, 0.12212174590095902, 0.1207297450949993, 0.11714848724296331, 0.11706700394991387, 0.11670179001999215, 0.11767427642846538, 0.12550380392908464, 0.11837671075646318, 0.12019898205359385, 0.11639295812017225, 0.11598984412665438, 0.1171871377231648, 0.11608711022070084, 0.1277838996038061, 0.15278338213957393, 0.13153321732734813, 0.1574208901335649, 0.13140858279171053, 0.14147123941549136, 0.1201338441598983, 0.12761405232700418, 0.13084291154136407, 0.154826061723659, 0.11938616834054831, 0.11526847485153155, 0.12088677125611329, 0.11799852809393152, 0.12498245729601441, 0.11143807535884025, 0.1164043479169335, 0.11799970376863464, 0.11308945137976817, 0.11463935695258268, 0.11529982055740795, 0.12139174997904421, 0.11341112582945863, 0.12651700062802665, 0.12489616750985727, 0.164357726748158, 0.15078754108919104, 0.12603511089957603, 0.11833429361016097, 0.1228028786397724, 0.11198521651364313, 0.11083522979751205, 0.1158447599127179, 0.1160010322561405, 0.11805919999461652, 0.11459976694071039, 0.11532384538885408, 0.14400135252276078, 0.14557704686727038, 0.11539019079043947, 0.11709777142222487, 0.11921067420876476, 0.11351620240751746, 0.11313016389684724, 0.11681077620763888, 0.11361886702832721, 0.11423177850755369, 0.11212562491936832, 0.1141818666888771, 0.11171695196080482, 0.11531328146876568, 0.11508935349817542, 0.11548190778014304, 0.11595332784018493, 0.11513860746361744, 0.1148517195422857, 0.16601419282468474, 0.12659611652050112, 0.11424427194450484, 0.1164727616682037, 0.12916009947779927, 0.11969866873987006, 0.11022512998878467, 0.11565547205250838, 0.11489389501573222, 0.11093800569989998, 0.11394789154306421, 0.12040006713131182, 0.11616011448388029, 0.11124070590646395, 0.1107239774099516, 0.11343086910952488, 0.11114299852761925, 0.10942218634980456, 0.12652734854244835, 0.12079878080458868, 0.1223402329168492, 0.11139430663562173, 0.11317880133592045, 0.11282686319061493, 0.11292419451700252, 0.11992774066960284, 0.12217089993808852, 0.11599462749727057, 0.10825537896312908, 0.10978675321698776, 0.10928890931195226, 0.10802820102236736, 0.11672596254176498, 0.11222865477767092, 0.11353597996759493, 0.1063228077293421, 0.11552916572403242, 0.11755827142687267, 0.10504378194879428, 0.10663269611904383, 0.10827252941667936, 0.11051839218155309, 0.11146231803107144, 0.11139963099913448, 0.10847105044253745, 0.10979701037868882, 0.10999947710479617, 0.10908177923197034, 0.12238064564899075, 0.11407616964506202, 0.10757135941183625, 0.11094316906236075, 0.11239904586121759, 0.11370385087084496, 0.11828916841637717, 0.10931117945214602, 0.11053352850704945, 0.10774579098561322, 0.1110387114908895], 'val_acc': [0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.4137931019312447, 0.42036124652829665, 0.42036124652829665, 0.43185549959760583, 0.4220032826530914, 0.423645319267251, 0.4334975360160195, 0.4384236444882767, 0.44006568071094443, 0.4761904757500478, 0.4696223312508688, 0.4991789816929202, 0.5303776638065457, 0.5369458125631993, 0.559934314542216, 0.5681444951661898, 0.586206892538932, 0.6075533624548826, 0.6535303740470084, 0.6929392415314473, 0.7224958923649906, 0.7586206875019669, 0.789819373873067, 0.8062397355125064, 0.8095238077620959, 0.8325123136070953, 0.8390804583998932, 0.8538587836209189, 0.8620689644406386, 0.8669950727171498, 0.8702791449667393, 0.8719211811894071, 0.878489325786459, 0.8850574700898921, 0.878489325590713, 0.8817733976445566, 0.89819375908825, 0.9064039399079697, 0.904761903783175, 0.8801313617155078, 0.8949096871322795, 0.9178981929772789, 0.9195402291020737, 0.904761903783175, 0.9244663374764579, 0.9277504098239203, 0.9113300483802269, 0.9228243013516632, 0.9310344818777638, 0.9211822649332494, 0.9261083734055067, 0.9310344818777638, 0.9129720845050217, 0.9113300483802269, 0.9376026265726888, 0.9310344818777638, 0.9310344818777638, 0.9195402287105817, 0.9261083734055067, 0.9392446626974835, 0.9392446626974835, 0.9392446625017375, 0.9392446625996106, 0.942528734751327, 0.9376026263769428, 0.942528734947073, 0.9474548432235843, 0.9392446625017375, 0.9474548432235843, 0.9474548432235843, 0.9376026263769428, 0.949096879348379, 0.942528734751327, 0.949096879446252, 0.942528734751327, 0.9326765182961776, 0.9326765182961776, 0.9343185544209723, 0.9458128070009166, 0.949096879446252, 0.949096879348379, 0.9376026262790699, 0.9441707708761218, 0.9474548432235843, 0.9507389154731738, 0.9441707710718678, 0.955665023847558, 0.9458128073924085, 0.9441707708761218, 0.9458128070009166, 0.9474548433214572, 0.949096879544125, 0.9540229877227633, 0.955665023847558, 0.9474548432235843, 0.9523809515979685, 0.9540229877227633, 0.949096879348379, 0.9523809515979685, 0.949096879348379, 0.9573070599723528, 0.9474548432235843, 0.9359605900564022, 0.9376026261811969, 0.9507389154731738, 0.9359605905457671, 0.9146141209234353, 0.9474548433214572, 0.9392446623059917, 0.9474548431257114, 0.9523809516958415, 0.9540229879185093, 0.9507389155710467, 0.9540229878206362, 0.9523809516958415, 0.949096879348379, 0.949096879348379, 0.9573070600702258, 0.9540229878206362, 0.9523809517937145, 0.949096879641998, 0.949096879641998, 0.9293924459487151, 0.9293924459487151, 0.949096879641998, 0.9540229878206362, 0.9523809517937145, 0.9507389157667927, 0.9507389157667927, 0.9605911325155612, 0.955665023945431, 0.9573070600702258, 0.9490968792505061, 0.9589490961950206, 0.9589490963907664, 0.9573070600702258, 0.955665023945431, 0.9573070600702258, 0.9605911326134342, 0.9556650241411769, 0.9605911325155612, 0.9392446626974835, 0.935960590447894, 0.9556650241411769, 0.9589490963907664, 0.9474548435172032, 0.9605911325155612, 0.9589490963907664, 0.955665023945431, 0.955665023945431, 0.9573070599723528, 0.9540229877227633, 0.9540229877227633, 0.9589490963907664, 0.9573070602659717, 0.9589490964886395, 0.9589490962928935, 0.9507389157667927, 0.9228243013516632, 0.9326765181983045, 0.9556650241411769, 0.9622331686403559, 0.9622331686403559, 0.9589490963907664, 0.9573070602659717, 0.9589490963907664, 0.9638752047651507, 0.9605911324176882, 0.9540229877227633, 0.962233168542483, 0.9573070599723528, 0.955665023847558, 0.9605911326134342, 0.9622331686403559, 0.9605911324176882, 0.9540229877227633, 0.9540229877227633, 0.9556650241411769, 0.942528734947073, 0.9507389157667927, 0.9392446626974835, 0.9540229880163822, 0.9507389156689198, 0.9589490964886395, 0.9556650241411769, 0.9523809518915874, 0.942528735044946, 0.9638752048630237, 0.9638752047651507, 0.9638752048630237, 0.9589490963907664, 0.9573070602659717, 0.9655172407920725, 0.962233168738229, 0.9655172409878184, 0.9589490961950206, 0.9589490960971475, 0.9573070600702258, 0.9573070599723528, 0.962233168542483, 0.9523809515979685, 0.9573070599723528, 0.9474548432235843, 0.9458128070009166, 0.9523809515979685, 0.9589490960971475, 0.955665023847558, 0.9638752047651507, 0.9638752046672777, 0.9589490961950206, 0.9573070599723528, 0.9573070600702258, 0.9589490960971475, 0.9605911324176882, 0.9540229880163822, 0.9523809518915874, 0.9638752047651507, 0.9589490961950206, 0.9589490960971475, 0.9622331686403559, 0.962233168542483, 0.9589490963907664, 0.9589490961950206, 0.9605911324176882, 0.9671592770147401, 0.9622331686403559, 0.9638752047651507, 0.9605911325155612, 0.9622331686403559, 0.9605911324176882, 0.9556650241411769, 0.9605911325155612, 0.9605911325155612, 0.9376026266705618, 0.9573070602659717, 0.9638752047651507, 0.9540229880163822, 0.9540229879185093, 0.9556650241411769, 0.9638752047651507, 0.9605911325155612, 0.9589490963907664, 0.9638752047651507, 0.9605911323198153, 0.9573070599723528, 0.9605911322219423, 0.9605911324176882, 0.9638752047651507, 0.9638752045694048, 0.9638752047651507, 0.9671592770147401, 0.9556650241411769, 0.9556650241411769, 0.9573070602659717, 0.962233168542483, 0.9638752047651507, 0.962233168542483, 0.9605911323198153, 0.9589490960971475, 0.9540229877227633, 0.962233168542483, 0.9671592770147401, 0.9638752047651507, 0.9622331686403559, 0.9655172408899454, 0.9556650241411769, 0.962233168542483, 0.9605911325155612, 0.9671592770147401, 0.9573070599723528, 0.9605911322219423, 0.9655172407920725, 0.9638752047651507, 0.9622331686403559, 0.9622331686403559, 0.9638752046672777, 0.962233168542483, 0.9655172408899454, 0.9622331686403559, 0.9638752046672777, 0.9638752047651507, 0.9573070602659717, 0.9589490963907664, 0.9671592770147401, 0.9655172407920725, 0.9638752046672777, 0.962233168542483, 0.9573070599723528, 0.9655172407920725, 0.9655172407920725, 0.9655172408899454, 0.9638752047651507], 'loss': [1.1076668612521288, 1.0926680800361555, 1.0817835079815843, 1.0762365208514173, 1.07434864523964, 1.0743362797848743, 1.0748612438628806, 1.0744439481954555, 1.0735465203467336, 1.0730307020445868, 1.0725144209068658, 1.071853156745801, 1.0709248255900043, 1.0696615346892904, 1.067821960331723, 1.0649294281887078, 1.0614750724064006, 1.0558779510384468, 1.0466242366258123, 1.033164853041177, 1.0110093806558567, 0.9782873021258955, 0.9341672950211981, 0.8830990133099488, 0.8382937565476498, 0.7982419224245593, 0.7557606130410024, 0.7085593471536891, 0.6670433582956051, 0.6172449099944113, 0.5749667334360753, 0.5370465896457617, 0.5123693215284014, 0.4800741524911759, 0.45076798438046745, 0.43883191033800034, 0.415909447611235, 0.3942308053466084, 0.37409969942770455, 0.35458714449185363, 0.34924698824510436, 0.3407257669393042, 0.3276514902374338, 0.30931074892226185, 0.2928449766959008, 0.2682820220625131, 0.25380949898789307, 0.26234908139436397, 0.25293937302223224, 0.22714081720527438, 0.21442224825187386, 0.20964107830421636, 0.2223445719027666, 0.2183784223985868, 0.19860305276984308, 0.19918056603452264, 0.18583163382213952, 0.18793935961791866, 0.1928675898964645, 0.19821049602247118, 0.18578591486878954, 0.19200785164960357, 0.2111962988942066, 0.2007872930236421, 0.1965216064783582, 0.18312839119096555, 0.2086815434682051, 0.18590609630764876, 0.17114708360582895, 0.16345785181258005, 0.16308049514308356, 0.1601084139924764, 0.16050853384104108, 0.16814382704743613, 0.15914386117972387, 0.15889365286920104, 0.15969252002557446, 0.15613754362296275, 0.16014485522584504, 0.15851874271335054, 0.16088751621192485, 0.14872304560957014, 0.15234366909312028, 0.16548829925011316, 0.1537630364145831, 0.15391337192278867, 0.15376759561302725, 0.1553567226662528, 0.147871573731395, 0.15143989671243535, 0.15466962730860073, 0.14684732563380587, 0.15313864946365358, 0.14200221874753063, 0.13643682711973817, 0.13639707540829324, 0.1392190421141638, 0.13449590977212486, 0.13264924151023555, 0.13008283470935156, 0.14448398105050503, 0.14815579894509404, 0.13705981108809398, 0.14278940138936777, 0.1313394285355995, 0.13250030621855655, 0.1597963267099686, 0.16783055036464511, 0.19167874009947022, 0.19042783255939366, 0.1840787885982153, 0.17816301503159426, 0.16950420639965325, 0.16615354968292267, 0.16833073016432032, 0.16306454965221318, 0.13957293943580415, 0.15643086251047358, 0.14879201353820198, 0.13128870230558226, 0.12489144916529528, 0.12278176959168005, 0.12134676339751152, 0.12153289414773977, 0.14661444687500627, 0.13520391610980767, 0.12298642420487238, 0.13109244080661994, 0.12749125598270056, 0.13153541282094725, 0.13292374765726087, 0.14711075620362402, 0.14231869258001845, 0.12959473731887414, 0.12275718144567595, 0.11932939300791684, 0.12885419297083692, 0.13752810306984786, 0.1298744369122282, 0.1199813091718196, 0.11951316109183388, 0.12562363523477402, 0.11944011499382387, 0.11380303818770747, 0.11473650440298311, 0.11534452849589824, 0.11664399535565406, 0.11896287212932379, 0.12278016843590159, 0.12039678993900699, 0.12972703830662205, 0.13133041628271158, 0.11792950683734256, 0.11706478471200324, 0.11981423837448292, 0.1122690909697045, 0.11202568964424564, 0.11107718219686093, 0.11241923478533355, 0.11664158824594113, 0.11242042975136876, 0.11024777135320268, 0.11103054408664821, 0.11363902839485869, 0.11723133322516996, 0.11532677840403217, 0.11632963288613658, 0.15067164032917002, 0.1247901625946562, 0.11565320411012403, 0.11309223389539876, 0.11798633022168824, 0.11268789327915689, 0.1161041937385986, 0.11185095503283722, 0.10607231099625143, 0.1087085945803282, 0.10762711139491452, 0.10803505101181889, 0.11677121822961302, 0.1056277140148856, 0.108993966296101, 0.10643010890031007, 0.10529439223498045, 0.10905497273625289, 0.11003115120854466, 0.10707493450729753, 0.11823651194878428, 0.12364429136015306, 0.11991651162841727, 0.11209445785754026, 0.10754410034883194, 0.11194187440054618, 0.11930311481015149, 0.1142395595077127, 0.1115410090839104, 0.10227646683581801, 0.10262595285992358, 0.11253977815106174, 0.12065331886925981, 0.1222053150315549, 0.10783972467852324, 0.10159474770085278, 0.10854930807616432, 0.10629278042294406, 0.10590082890443978, 0.10386171634375926, 0.10511817195944229, 0.10063466648791115, 0.11792074800822769, 0.1223498510812587, 0.14202241231467444, 0.12389575896260675, 0.11028667794238371, 0.11576804299006961, 0.1143744821849545, 0.09954899608232158, 0.09982009253156748, 0.10240370614695109, 0.10535805075748746, 0.10717177009496845, 0.10213113656034215, 0.09817422470517717, 0.10832723513459767, 0.10641171054979615, 0.10490826096316873, 0.10565030604478515, 0.10289512380988201, 0.09923008965821727, 0.09805277660642071, 0.09903596608729333, 0.10148473863912559, 0.09528815640132775, 0.09726697829163784, 0.09699582529141428, 0.09741676496284453, 0.09578581104777921, 0.09857857909351893, 0.09904979730227645, 0.09696293688346717, 0.09347816804534846, 0.09825041622718991, 0.1093892483434638, 0.09749555356754659, 0.09751609621963461, 0.09418853242906457, 0.10720909636245861, 0.10090930759906769, 0.09494179981077966, 0.09354974325799845, 0.09574994798558449, 0.09626688858995203, 0.1156153449869009, 0.1066164884234356, 0.12080986152806567, 0.10760394531537375, 0.10197340932347691, 0.09710595895377518, 0.09412483319303583, 0.09286224993224995, 0.0958121593452577, 0.10458967335353886, 0.10652325002197367, 0.10277959578641876, 0.10131278418662366, 0.10008525456124018, 0.1001935380501424, 0.10842948111665322, 0.11230831091164074, 0.09488189985681118, 0.09185470690227877, 0.09899826147411883, 0.10869885230823219, 0.11176141288123825, 0.11640776780106939, 0.10460363328456879, 0.10192578515722521, 0.116695298783833, 0.10778868495806042, 0.10589526638358036, 0.09455609537003222, 0.08985737141152916, 0.08906221977555531, 0.09261299333166048, 0.09717382661982973, 0.09755520380314371, 0.09151086491175012, 0.09097908791575343, 0.1047395099115078, 0.09930274461451497, 0.09275926928011054, 0.08811794043933587, 0.0879233973594172, 0.09653860616059763, 0.09908123639024013, 0.09195185074510026, 0.0955880495465267, 0.09126443715127341, 0.08959848497988507, 0.08679813945195514], 'acc': [0.23285421049203225, 0.3942505152808078, 0.3942505152808078, 0.3942505152808078, 0.39425051211086876, 0.3942505115233897, 0.3942505148891551, 0.3942505152808078, 0.39507186699207314, 0.4139630373865672, 0.4131416831295593, 0.4168377812638175, 0.4188911692317751, 0.4258726898772026, 0.4365503101011077, 0.4365503061845807, 0.43860369662484594, 0.4476386046140346, 0.45174538154132066, 0.46694045027668224, 0.4870636539537559, 0.506776183537634, 0.5326488713219425, 0.5540041082448783, 0.556468176694866, 0.5794661187293348, 0.6193018484654123, 0.6616016443015613, 0.6903490795247119, 0.7248459948651355, 0.7425051324421376, 0.7659137580429014, 0.7765913784626328, 0.79753592986101, 0.8102669368044796, 0.8123203300597487, 0.8324435281802497, 0.8422997937554941, 0.8492813130668546, 0.8587268994574184, 0.8657084191604316, 0.8681724854563296, 0.8731006174606464, 0.8767967156316221, 0.8870636556917147, 0.899794662439358, 0.9088295710160258, 0.899383986975378, 0.9067761759983196, 0.9190965132302082, 0.9236139587308347, 0.924024644059567, 0.9096509282104288, 0.9211498999497729, 0.9223819333914614, 0.9264887020573234, 0.9330595506779712, 0.9273100651999518, 0.9273100651999518, 0.9244353216776368, 0.9305954782380216, 0.9264887101596386, 0.9145790578648295, 0.9203285458885914, 0.9240246363489045, 0.9326488738432067, 0.9199178684663479, 0.9338808987909274, 0.9347022542228933, 0.9416837735342539, 0.9363449731891405, 0.9408624275753875, 0.9425051293578726, 0.9379876761465836, 0.9420944596462916, 0.9425051364810559, 0.9433264928921536, 0.944969194870465, 0.9420944517398028, 0.937577006630829, 0.9375770064350026, 0.9441478396343255, 0.9396303945253517, 0.9359342874687555, 0.940041063257801, 0.9375770062391763, 0.945379872488535, 0.9396303927629146, 0.9433264847898385, 0.9453798738593193, 0.9379876757549309, 0.9433264857689703, 0.9441478392426728, 0.9478439383927801, 0.94989732804974, 0.9515400377387139, 0.9478439383927801, 0.9519507141818256, 0.9523613925832008, 0.9552361355180369, 0.9462012366102951, 0.9416837741217329, 0.9494866488650594, 0.9478439395677383, 0.9503080048845044, 0.9486652953913569, 0.9359342959627234, 0.9310061627834485, 0.9252566763262974, 0.9248459987082276, 0.9264887093763332, 0.9281314131170817, 0.9351129395516257, 0.932238196812616, 0.9330595449255722, 0.9392197090007931, 0.9470225829608142, 0.9367556432923742, 0.9420944517398028, 0.951129358945686, 0.9556468141152384, 0.9581108875343197, 0.9585215578333798, 0.9564681689597253, 0.9441478406134572, 0.949075972617774, 0.9568788471652742, 0.9511293587498596, 0.9515400379345402, 0.9478439464950953, 0.9519507232632726, 0.9429158079550741, 0.9453798746426247, 0.954414781261029, 0.9540041036429592, 0.9581108782570464, 0.9482546171858081, 0.9494866482775803, 0.9503080054719835, 0.9568788457944898, 0.9581108800194836, 0.9560574923207873, 0.9548254598582305, 0.9597535906875893, 0.9605749467070342, 0.9581108800194836, 0.9605749457279025, 0.957289524783344, 0.955646813331933, 0.9577002029888928, 0.9503080044928517, 0.9507186832858796, 0.9535934258290629, 0.9581108800194836, 0.9568788473611005, 0.9613963005723895, 0.9634496894460439, 0.9605749459237288, 0.9601642686973118, 0.9564681699388571, 0.9577002022055875, 0.9609856235417987, 0.9585215647607369, 0.9581108788445255, 0.956468168763899, 0.9581108796278309, 0.9556468148985438, 0.9437371629953873, 0.9519507153567837, 0.9581108804111363, 0.9605749451404235, 0.9568788471652742, 0.9585215570500744, 0.9589322348639706, 0.9585215570500744, 0.964681722104427, 0.9581108800194836, 0.9605749453362499, 0.9609856229543197, 0.9581108808027891, 0.9626283328391199, 0.9634496890543912, 0.9634496892502176, 0.9618069779946329, 0.9626283338182516, 0.9613963003765631, 0.9618069783862856, 0.9577002033805455, 0.9577002024014138, 0.9581108808027891, 0.9568788465777951, 0.9609856233459724, 0.9601642686973118, 0.958932236822234, 0.9560574923207873, 0.9609856233459724, 0.9634496884669121, 0.9650923983517123, 0.9568788454028371, 0.9568788454028371, 0.9515400367595821, 0.9626283344057306, 0.9646817228877325, 0.9601642681098327, 0.9585215582250325, 0.9601642688931381, 0.960985623150146, 0.9622176567876608, 0.9634496902293493, 0.9552361368888213, 0.9527720707887497, 0.9462012292912854, 0.9474332625371474, 0.9601642692847908, 0.9540041048179172, 0.9593429115029086, 0.9655030771447403, 0.9638603668682874, 0.9622176563960081, 0.9622176569834872, 0.9597535912750683, 0.9630390112404951, 0.9650923999183232, 0.9609856237376251, 0.9568788469694478, 0.9609856227584933, 0.9613963005723895, 0.9618069783862856, 0.9638603672599401, 0.964681721516948, 0.9630390112404951, 0.9626283342099042, 0.9655030773405666, 0.9659137547628102, 0.9675564664100474, 0.9646817217127743, 0.9650923991350178, 0.9638603678474191, 0.9663244331641854, 0.9667351101947761, 0.96632443238088, 0.9655030765572613, 0.9597535908834156, 0.9642710440947045, 0.9663244325767063, 0.9655030771447403, 0.9597535889251521, 0.9626283344057306, 0.9659137533920257, 0.9671457878128459, 0.96632443238088, 0.9671457880086722, 0.9556468137235857, 0.9618069776029802, 0.953182747036035, 0.9577002018139348, 0.9605749453362499, 0.9679671438322909, 0.9679671440281172, 0.9667351099989497, 0.9650923985475387, 0.9626283336224253, 0.9585215574417271, 0.9613963007682158, 0.9638603664766346, 0.960164268305659, 0.9605749453362499, 0.958932236822234, 0.9577002025972402, 0.9671457878128459, 0.9696098535212648, 0.9622176552210501, 0.9585215572459007, 0.9564681764745614, 0.9548254590749251, 0.9618069783862856, 0.9605749445529445, 0.9515400381303666, 0.9552361353222105, 0.9593429120903877, 0.9634496886627385, 0.9700205315309873, 0.9708418877462587, 0.9691991768823267, 0.9646817226919061, 0.960985623150146, 0.9679671444197699, 0.967556464451784, 0.9609856304691557, 0.9622176571793135, 0.967556465626742, 0.9696098537170912, 0.9667351105864288, 0.9613963017473476, 0.9626283342099042, 0.9671457876170195, 0.9646817223002534, 0.965503077536393, 0.9687884988726042, 0.9716632418074402]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
