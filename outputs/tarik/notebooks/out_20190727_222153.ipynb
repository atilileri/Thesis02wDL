{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf51.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 22:21:53 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '1Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000018F031F9550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000018F24AB6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0910, Accuracy:0.3943, Validation Loss:1.0821, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0788, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0748, Accuracy:0.3914, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0748, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0726, Validation Accuracy:0.3957\n",
    "Epoch #10: Loss:1.0722, Accuracy:0.4078, Validation Loss:1.0718, Validation Accuracy:0.4122\n",
    "Epoch #11: Loss:1.0711, Accuracy:0.4185, Validation Loss:1.0705, Validation Accuracy:0.4269\n",
    "Epoch #12: Loss:1.0696, Accuracy:0.4333, Validation Loss:1.0686, Validation Accuracy:0.4384\n",
    "Epoch #13: Loss:1.0671, Accuracy:0.4468, Validation Loss:1.0654, Validation Accuracy:0.4450\n",
    "Epoch #14: Loss:1.0631, Accuracy:0.4575, Validation Loss:1.0599, Validation Accuracy:0.4516\n",
    "Epoch #15: Loss:1.0560, Accuracy:0.4604, Validation Loss:1.0506, Validation Accuracy:0.4548\n",
    "Epoch #16: Loss:1.0439, Accuracy:0.4637, Validation Loss:1.0347, Validation Accuracy:0.4532\n",
    "Epoch #17: Loss:1.0258, Accuracy:0.4641, Validation Loss:1.0125, Validation Accuracy:0.4581\n",
    "Epoch #18: Loss:1.0066, Accuracy:0.4624, Validation Loss:0.9918, Validation Accuracy:0.4647\n",
    "Epoch #19: Loss:0.9908, Accuracy:0.4678, Validation Loss:0.9780, Validation Accuracy:0.4647\n",
    "Epoch #20: Loss:0.9813, Accuracy:0.4608, Validation Loss:0.9686, Validation Accuracy:0.4614\n",
    "Epoch #21: Loss:0.9694, Accuracy:0.4945, Validation Loss:0.9643, Validation Accuracy:0.4975\n",
    "Epoch #22: Loss:0.9561, Accuracy:0.5117, Validation Loss:0.9589, Validation Accuracy:0.4926\n",
    "Epoch #23: Loss:0.9522, Accuracy:0.5142, Validation Loss:0.9542, Validation Accuracy:0.5041\n",
    "Epoch #24: Loss:0.9520, Accuracy:0.5216, Validation Loss:0.9465, Validation Accuracy:0.5074\n",
    "Epoch #25: Loss:0.9499, Accuracy:0.5166, Validation Loss:0.9442, Validation Accuracy:0.5041\n",
    "Epoch #26: Loss:0.9450, Accuracy:0.5191, Validation Loss:0.9433, Validation Accuracy:0.4992\n",
    "Epoch #27: Loss:0.9396, Accuracy:0.5298, Validation Loss:0.9413, Validation Accuracy:0.5172\n",
    "Epoch #28: Loss:0.9370, Accuracy:0.5277, Validation Loss:0.9400, Validation Accuracy:0.5057\n",
    "Epoch #29: Loss:0.9345, Accuracy:0.5310, Validation Loss:0.9361, Validation Accuracy:0.5205\n",
    "Epoch #30: Loss:0.9324, Accuracy:0.5310, Validation Loss:0.9348, Validation Accuracy:0.5123\n",
    "Epoch #31: Loss:0.9301, Accuracy:0.5290, Validation Loss:0.9332, Validation Accuracy:0.5255\n",
    "Epoch #32: Loss:0.9302, Accuracy:0.5298, Validation Loss:0.9323, Validation Accuracy:0.5156\n",
    "Epoch #33: Loss:0.9266, Accuracy:0.5277, Validation Loss:0.9309, Validation Accuracy:0.5140\n",
    "Epoch #34: Loss:0.9274, Accuracy:0.5326, Validation Loss:0.9292, Validation Accuracy:0.5205\n",
    "Epoch #35: Loss:0.9237, Accuracy:0.5343, Validation Loss:0.9288, Validation Accuracy:0.5238\n",
    "Epoch #36: Loss:0.9243, Accuracy:0.5331, Validation Loss:0.9285, Validation Accuracy:0.5255\n",
    "Epoch #37: Loss:0.9228, Accuracy:0.5335, Validation Loss:0.9264, Validation Accuracy:0.5271\n",
    "Epoch #38: Loss:0.9224, Accuracy:0.5318, Validation Loss:0.9259, Validation Accuracy:0.5304\n",
    "Epoch #39: Loss:0.9211, Accuracy:0.5351, Validation Loss:0.9254, Validation Accuracy:0.5320\n",
    "Epoch #40: Loss:0.9204, Accuracy:0.5351, Validation Loss:0.9256, Validation Accuracy:0.5369\n",
    "Epoch #41: Loss:0.9212, Accuracy:0.5343, Validation Loss:0.9257, Validation Accuracy:0.5320\n",
    "Epoch #42: Loss:0.9225, Accuracy:0.5359, Validation Loss:0.9271, Validation Accuracy:0.5287\n",
    "Epoch #43: Loss:0.9215, Accuracy:0.5322, Validation Loss:0.9232, Validation Accuracy:0.5320\n",
    "Epoch #44: Loss:0.9192, Accuracy:0.5368, Validation Loss:0.9239, Validation Accuracy:0.5337\n",
    "Epoch #45: Loss:0.9202, Accuracy:0.5294, Validation Loss:0.9230, Validation Accuracy:0.5337\n",
    "Epoch #46: Loss:0.9188, Accuracy:0.5421, Validation Loss:0.9237, Validation Accuracy:0.5320\n",
    "Epoch #47: Loss:0.9192, Accuracy:0.5363, Validation Loss:0.9213, Validation Accuracy:0.5337\n",
    "Epoch #48: Loss:0.9170, Accuracy:0.5384, Validation Loss:0.9211, Validation Accuracy:0.5369\n",
    "Epoch #49: Loss:0.9177, Accuracy:0.5433, Validation Loss:0.9213, Validation Accuracy:0.5353\n",
    "Epoch #50: Loss:0.9197, Accuracy:0.5331, Validation Loss:0.9218, Validation Accuracy:0.5435\n",
    "Epoch #51: Loss:0.9168, Accuracy:0.5437, Validation Loss:0.9247, Validation Accuracy:0.5271\n",
    "Epoch #52: Loss:0.9172, Accuracy:0.5392, Validation Loss:0.9198, Validation Accuracy:0.5189\n",
    "Epoch #53: Loss:0.9151, Accuracy:0.5372, Validation Loss:0.9202, Validation Accuracy:0.5402\n",
    "Epoch #54: Loss:0.9150, Accuracy:0.5425, Validation Loss:0.9191, Validation Accuracy:0.5353\n",
    "Epoch #55: Loss:0.9145, Accuracy:0.5405, Validation Loss:0.9187, Validation Accuracy:0.5353\n",
    "Epoch #56: Loss:0.9144, Accuracy:0.5417, Validation Loss:0.9203, Validation Accuracy:0.5353\n",
    "Epoch #57: Loss:0.9138, Accuracy:0.5413, Validation Loss:0.9178, Validation Accuracy:0.5369\n",
    "Epoch #58: Loss:0.9133, Accuracy:0.5417, Validation Loss:0.9225, Validation Accuracy:0.5353\n",
    "Epoch #59: Loss:0.9146, Accuracy:0.5417, Validation Loss:0.9174, Validation Accuracy:0.5353\n",
    "Epoch #60: Loss:0.9125, Accuracy:0.5429, Validation Loss:0.9197, Validation Accuracy:0.5337\n",
    "Epoch #61: Loss:0.9119, Accuracy:0.5433, Validation Loss:0.9161, Validation Accuracy:0.5353\n",
    "Epoch #62: Loss:0.9115, Accuracy:0.5417, Validation Loss:0.9162, Validation Accuracy:0.5353\n",
    "Epoch #63: Loss:0.9118, Accuracy:0.5380, Validation Loss:0.9178, Validation Accuracy:0.5402\n",
    "Epoch #64: Loss:0.9128, Accuracy:0.5405, Validation Loss:0.9204, Validation Accuracy:0.5304\n",
    "Epoch #65: Loss:0.9154, Accuracy:0.5396, Validation Loss:0.9248, Validation Accuracy:0.5337\n",
    "Epoch #66: Loss:0.9117, Accuracy:0.5433, Validation Loss:0.9221, Validation Accuracy:0.5123\n",
    "Epoch #67: Loss:0.9164, Accuracy:0.5405, Validation Loss:0.9228, Validation Accuracy:0.5369\n",
    "Epoch #68: Loss:0.9104, Accuracy:0.5462, Validation Loss:0.9158, Validation Accuracy:0.5369\n",
    "Epoch #69: Loss:0.9090, Accuracy:0.5446, Validation Loss:0.9188, Validation Accuracy:0.5452\n",
    "Epoch #70: Loss:0.9102, Accuracy:0.5429, Validation Loss:0.9136, Validation Accuracy:0.5402\n",
    "Epoch #71: Loss:0.9084, Accuracy:0.5437, Validation Loss:0.9150, Validation Accuracy:0.5501\n",
    "Epoch #72: Loss:0.9083, Accuracy:0.5462, Validation Loss:0.9144, Validation Accuracy:0.5320\n",
    "Epoch #73: Loss:0.9086, Accuracy:0.5495, Validation Loss:0.9222, Validation Accuracy:0.5304\n",
    "Epoch #74: Loss:0.9095, Accuracy:0.5478, Validation Loss:0.9141, Validation Accuracy:0.5419\n",
    "Epoch #75: Loss:0.9090, Accuracy:0.5450, Validation Loss:0.9193, Validation Accuracy:0.5435\n",
    "Epoch #76: Loss:0.9056, Accuracy:0.5470, Validation Loss:0.9163, Validation Accuracy:0.5353\n",
    "Epoch #77: Loss:0.9089, Accuracy:0.5466, Validation Loss:0.9186, Validation Accuracy:0.5337\n",
    "Epoch #78: Loss:0.9076, Accuracy:0.5466, Validation Loss:0.9123, Validation Accuracy:0.5337\n",
    "Epoch #79: Loss:0.9066, Accuracy:0.5466, Validation Loss:0.9127, Validation Accuracy:0.5534\n",
    "Epoch #80: Loss:0.9066, Accuracy:0.5532, Validation Loss:0.9123, Validation Accuracy:0.5534\n",
    "Epoch #81: Loss:0.9073, Accuracy:0.5446, Validation Loss:0.9136, Validation Accuracy:0.5369\n",
    "Epoch #82: Loss:0.9071, Accuracy:0.5458, Validation Loss:0.9115, Validation Accuracy:0.5419\n",
    "Epoch #83: Loss:0.9048, Accuracy:0.5454, Validation Loss:0.9120, Validation Accuracy:0.5468\n",
    "Epoch #84: Loss:0.9050, Accuracy:0.5495, Validation Loss:0.9119, Validation Accuracy:0.5419\n",
    "Epoch #85: Loss:0.9071, Accuracy:0.5441, Validation Loss:0.9170, Validation Accuracy:0.5419\n",
    "Epoch #86: Loss:0.9045, Accuracy:0.5466, Validation Loss:0.9107, Validation Accuracy:0.5386\n",
    "Epoch #87: Loss:0.9037, Accuracy:0.5478, Validation Loss:0.9101, Validation Accuracy:0.5435\n",
    "Epoch #88: Loss:0.9047, Accuracy:0.5499, Validation Loss:0.9125, Validation Accuracy:0.5468\n",
    "Epoch #89: Loss:0.9043, Accuracy:0.5495, Validation Loss:0.9122, Validation Accuracy:0.5320\n",
    "Epoch #90: Loss:0.9046, Accuracy:0.5511, Validation Loss:0.9102, Validation Accuracy:0.5435\n",
    "Epoch #91: Loss:0.9028, Accuracy:0.5495, Validation Loss:0.9087, Validation Accuracy:0.5419\n",
    "Epoch #92: Loss:0.9053, Accuracy:0.5462, Validation Loss:0.9164, Validation Accuracy:0.5255\n",
    "Epoch #93: Loss:0.9061, Accuracy:0.5589, Validation Loss:0.9257, Validation Accuracy:0.5369\n",
    "Epoch #94: Loss:0.9055, Accuracy:0.5499, Validation Loss:0.9142, Validation Accuracy:0.5304\n",
    "Epoch #95: Loss:0.9048, Accuracy:0.5491, Validation Loss:0.9124, Validation Accuracy:0.5419\n",
    "Epoch #96: Loss:0.9045, Accuracy:0.5544, Validation Loss:0.9086, Validation Accuracy:0.5353\n",
    "Epoch #97: Loss:0.9051, Accuracy:0.5491, Validation Loss:0.9079, Validation Accuracy:0.5435\n",
    "Epoch #98: Loss:0.9039, Accuracy:0.5540, Validation Loss:0.9104, Validation Accuracy:0.5435\n",
    "Epoch #99: Loss:0.9026, Accuracy:0.5511, Validation Loss:0.9105, Validation Accuracy:0.5353\n",
    "Epoch #100: Loss:0.9032, Accuracy:0.5552, Validation Loss:0.9110, Validation Accuracy:0.5468\n",
    "Epoch #101: Loss:0.9006, Accuracy:0.5544, Validation Loss:0.9083, Validation Accuracy:0.5468\n",
    "Epoch #102: Loss:0.8999, Accuracy:0.5487, Validation Loss:0.9103, Validation Accuracy:0.5468\n",
    "Epoch #103: Loss:0.9022, Accuracy:0.5499, Validation Loss:0.9079, Validation Accuracy:0.5402\n",
    "Epoch #104: Loss:0.8994, Accuracy:0.5577, Validation Loss:0.9116, Validation Accuracy:0.5484\n",
    "Epoch #105: Loss:0.9013, Accuracy:0.5532, Validation Loss:0.9064, Validation Accuracy:0.5435\n",
    "Epoch #106: Loss:0.8992, Accuracy:0.5524, Validation Loss:0.9064, Validation Accuracy:0.5484\n",
    "Epoch #107: Loss:0.8997, Accuracy:0.5552, Validation Loss:0.9072, Validation Accuracy:0.5468\n",
    "Epoch #108: Loss:0.8988, Accuracy:0.5556, Validation Loss:0.9059, Validation Accuracy:0.5402\n",
    "Epoch #109: Loss:0.8992, Accuracy:0.5561, Validation Loss:0.9160, Validation Accuracy:0.5452\n",
    "Epoch #110: Loss:0.9013, Accuracy:0.5552, Validation Loss:0.9123, Validation Accuracy:0.5386\n",
    "Epoch #111: Loss:0.8999, Accuracy:0.5585, Validation Loss:0.9252, Validation Accuracy:0.5517\n",
    "Epoch #112: Loss:0.9039, Accuracy:0.5569, Validation Loss:0.9118, Validation Accuracy:0.5402\n",
    "Epoch #113: Loss:0.8977, Accuracy:0.5573, Validation Loss:0.9156, Validation Accuracy:0.5517\n",
    "Epoch #114: Loss:0.9058, Accuracy:0.5503, Validation Loss:0.9064, Validation Accuracy:0.5435\n",
    "Epoch #115: Loss:0.9043, Accuracy:0.5528, Validation Loss:0.9098, Validation Accuracy:0.5386\n",
    "Epoch #116: Loss:0.9000, Accuracy:0.5569, Validation Loss:0.9129, Validation Accuracy:0.5534\n",
    "Epoch #117: Loss:0.8987, Accuracy:0.5524, Validation Loss:0.9090, Validation Accuracy:0.5386\n",
    "Epoch #118: Loss:0.8999, Accuracy:0.5524, Validation Loss:0.9070, Validation Accuracy:0.5468\n",
    "Epoch #119: Loss:0.9005, Accuracy:0.5544, Validation Loss:0.9071, Validation Accuracy:0.5534\n",
    "Epoch #120: Loss:0.8979, Accuracy:0.5548, Validation Loss:0.9050, Validation Accuracy:0.5435\n",
    "Epoch #121: Loss:0.8978, Accuracy:0.5565, Validation Loss:0.9067, Validation Accuracy:0.5452\n",
    "Epoch #122: Loss:0.8981, Accuracy:0.5573, Validation Loss:0.9052, Validation Accuracy:0.5468\n",
    "Epoch #123: Loss:0.8968, Accuracy:0.5556, Validation Loss:0.9062, Validation Accuracy:0.5452\n",
    "Epoch #124: Loss:0.8968, Accuracy:0.5585, Validation Loss:0.9047, Validation Accuracy:0.5452\n",
    "Epoch #125: Loss:0.8966, Accuracy:0.5614, Validation Loss:0.9048, Validation Accuracy:0.5419\n",
    "Epoch #126: Loss:0.8974, Accuracy:0.5528, Validation Loss:0.9066, Validation Accuracy:0.5534\n",
    "Epoch #127: Loss:0.8979, Accuracy:0.5581, Validation Loss:0.9074, Validation Accuracy:0.5501\n",
    "Epoch #128: Loss:0.8979, Accuracy:0.5536, Validation Loss:0.9061, Validation Accuracy:0.5517\n",
    "Epoch #129: Loss:0.8978, Accuracy:0.5544, Validation Loss:0.9064, Validation Accuracy:0.5435\n",
    "Epoch #130: Loss:0.8984, Accuracy:0.5556, Validation Loss:0.9089, Validation Accuracy:0.5550\n",
    "Epoch #131: Loss:0.8970, Accuracy:0.5561, Validation Loss:0.9042, Validation Accuracy:0.5517\n",
    "Epoch #132: Loss:0.8970, Accuracy:0.5515, Validation Loss:0.9043, Validation Accuracy:0.5468\n",
    "Epoch #133: Loss:0.8944, Accuracy:0.5552, Validation Loss:0.9082, Validation Accuracy:0.5583\n",
    "Epoch #134: Loss:0.9019, Accuracy:0.5552, Validation Loss:0.9035, Validation Accuracy:0.5501\n",
    "Epoch #135: Loss:0.8956, Accuracy:0.5528, Validation Loss:0.9035, Validation Accuracy:0.5501\n",
    "Epoch #136: Loss:0.8949, Accuracy:0.5565, Validation Loss:0.9088, Validation Accuracy:0.5567\n",
    "Epoch #137: Loss:0.8957, Accuracy:0.5532, Validation Loss:0.9030, Validation Accuracy:0.5517\n",
    "Epoch #138: Loss:0.8957, Accuracy:0.5552, Validation Loss:0.9046, Validation Accuracy:0.5501\n",
    "Epoch #139: Loss:0.8942, Accuracy:0.5618, Validation Loss:0.9178, Validation Accuracy:0.5567\n",
    "Epoch #140: Loss:0.8966, Accuracy:0.5569, Validation Loss:0.9068, Validation Accuracy:0.5501\n",
    "Epoch #141: Loss:0.8968, Accuracy:0.5606, Validation Loss:0.9030, Validation Accuracy:0.5567\n",
    "Epoch #142: Loss:0.8934, Accuracy:0.5565, Validation Loss:0.9045, Validation Accuracy:0.5452\n",
    "Epoch #143: Loss:0.8958, Accuracy:0.5577, Validation Loss:0.9026, Validation Accuracy:0.5484\n",
    "Epoch #144: Loss:0.8948, Accuracy:0.5548, Validation Loss:0.9031, Validation Accuracy:0.5534\n",
    "Epoch #145: Loss:0.8936, Accuracy:0.5606, Validation Loss:0.9034, Validation Accuracy:0.5534\n",
    "Epoch #146: Loss:0.8942, Accuracy:0.5622, Validation Loss:0.9085, Validation Accuracy:0.5501\n",
    "Epoch #147: Loss:0.8957, Accuracy:0.5593, Validation Loss:0.9116, Validation Accuracy:0.5583\n",
    "Epoch #148: Loss:0.8947, Accuracy:0.5589, Validation Loss:0.9025, Validation Accuracy:0.5484\n",
    "Epoch #149: Loss:0.8918, Accuracy:0.5602, Validation Loss:0.9051, Validation Accuracy:0.5484\n",
    "Epoch #150: Loss:0.8920, Accuracy:0.5606, Validation Loss:0.9025, Validation Accuracy:0.5484\n",
    "Epoch #151: Loss:0.8928, Accuracy:0.5639, Validation Loss:0.9024, Validation Accuracy:0.5501\n",
    "Epoch #152: Loss:0.8929, Accuracy:0.5585, Validation Loss:0.9031, Validation Accuracy:0.5583\n",
    "Epoch #153: Loss:0.8904, Accuracy:0.5634, Validation Loss:0.9030, Validation Accuracy:0.5501\n",
    "Epoch #154: Loss:0.8921, Accuracy:0.5585, Validation Loss:0.9028, Validation Accuracy:0.5517\n",
    "Epoch #155: Loss:0.8905, Accuracy:0.5618, Validation Loss:0.9015, Validation Accuracy:0.5517\n",
    "Epoch #156: Loss:0.8895, Accuracy:0.5618, Validation Loss:0.9068, Validation Accuracy:0.5534\n",
    "Epoch #157: Loss:0.8921, Accuracy:0.5573, Validation Loss:0.9018, Validation Accuracy:0.5583\n",
    "Epoch #158: Loss:0.8909, Accuracy:0.5655, Validation Loss:0.9013, Validation Accuracy:0.5567\n",
    "Epoch #159: Loss:0.8892, Accuracy:0.5598, Validation Loss:0.9008, Validation Accuracy:0.5517\n",
    "Epoch #160: Loss:0.8901, Accuracy:0.5614, Validation Loss:0.9040, Validation Accuracy:0.5550\n",
    "Epoch #161: Loss:0.8934, Accuracy:0.5589, Validation Loss:0.9029, Validation Accuracy:0.5567\n",
    "Epoch #162: Loss:0.8943, Accuracy:0.5614, Validation Loss:0.9067, Validation Accuracy:0.5517\n",
    "Epoch #163: Loss:0.8920, Accuracy:0.5585, Validation Loss:0.9004, Validation Accuracy:0.5550\n",
    "Epoch #164: Loss:0.8901, Accuracy:0.5622, Validation Loss:0.9041, Validation Accuracy:0.5616\n",
    "Epoch #165: Loss:0.8956, Accuracy:0.5680, Validation Loss:0.9156, Validation Accuracy:0.5599\n",
    "Epoch #166: Loss:0.8927, Accuracy:0.5606, Validation Loss:0.9003, Validation Accuracy:0.5517\n",
    "Epoch #167: Loss:0.8923, Accuracy:0.5610, Validation Loss:0.9002, Validation Accuracy:0.5583\n",
    "Epoch #168: Loss:0.8937, Accuracy:0.5569, Validation Loss:0.9066, Validation Accuracy:0.5616\n",
    "Epoch #169: Loss:0.8879, Accuracy:0.5630, Validation Loss:0.9044, Validation Accuracy:0.5550\n",
    "Epoch #170: Loss:0.8905, Accuracy:0.5556, Validation Loss:0.9082, Validation Accuracy:0.5649\n",
    "Epoch #171: Loss:0.8918, Accuracy:0.5626, Validation Loss:0.8992, Validation Accuracy:0.5534\n",
    "Epoch #172: Loss:0.8881, Accuracy:0.5626, Validation Loss:0.8991, Validation Accuracy:0.5534\n",
    "Epoch #173: Loss:0.8874, Accuracy:0.5626, Validation Loss:0.8993, Validation Accuracy:0.5534\n",
    "Epoch #174: Loss:0.8862, Accuracy:0.5634, Validation Loss:0.9005, Validation Accuracy:0.5484\n",
    "Epoch #175: Loss:0.8869, Accuracy:0.5614, Validation Loss:0.8991, Validation Accuracy:0.5567\n",
    "Epoch #176: Loss:0.8869, Accuracy:0.5671, Validation Loss:0.8978, Validation Accuracy:0.5534\n",
    "Epoch #177: Loss:0.8885, Accuracy:0.5602, Validation Loss:0.9082, Validation Accuracy:0.5632\n",
    "Epoch #178: Loss:0.8904, Accuracy:0.5556, Validation Loss:0.9047, Validation Accuracy:0.5567\n",
    "Epoch #179: Loss:0.8892, Accuracy:0.5643, Validation Loss:0.9010, Validation Accuracy:0.5649\n",
    "Epoch #180: Loss:0.8887, Accuracy:0.5618, Validation Loss:0.9014, Validation Accuracy:0.5649\n",
    "Epoch #181: Loss:0.8896, Accuracy:0.5671, Validation Loss:0.9009, Validation Accuracy:0.5583\n",
    "Epoch #182: Loss:0.8874, Accuracy:0.5680, Validation Loss:0.9104, Validation Accuracy:0.5599\n",
    "Epoch #183: Loss:0.8896, Accuracy:0.5577, Validation Loss:0.8971, Validation Accuracy:0.5567\n",
    "Epoch #184: Loss:0.8853, Accuracy:0.5704, Validation Loss:0.8976, Validation Accuracy:0.5550\n",
    "Epoch #185: Loss:0.8840, Accuracy:0.5684, Validation Loss:0.8964, Validation Accuracy:0.5599\n",
    "Epoch #186: Loss:0.8843, Accuracy:0.5618, Validation Loss:0.8982, Validation Accuracy:0.5567\n",
    "Epoch #187: Loss:0.8818, Accuracy:0.5696, Validation Loss:0.8964, Validation Accuracy:0.5517\n",
    "Epoch #188: Loss:0.8817, Accuracy:0.5684, Validation Loss:0.8962, Validation Accuracy:0.5517\n",
    "Epoch #189: Loss:0.8815, Accuracy:0.5692, Validation Loss:0.8956, Validation Accuracy:0.5517\n",
    "Epoch #190: Loss:0.8810, Accuracy:0.5708, Validation Loss:0.8972, Validation Accuracy:0.5649\n",
    "Epoch #191: Loss:0.8800, Accuracy:0.5737, Validation Loss:0.8982, Validation Accuracy:0.5567\n",
    "Epoch #192: Loss:0.8821, Accuracy:0.5721, Validation Loss:0.8977, Validation Accuracy:0.5747\n",
    "Epoch #193: Loss:0.8839, Accuracy:0.5721, Validation Loss:0.8977, Validation Accuracy:0.5632\n",
    "Epoch #194: Loss:0.8820, Accuracy:0.5684, Validation Loss:0.8997, Validation Accuracy:0.5714\n",
    "Epoch #195: Loss:0.8794, Accuracy:0.5733, Validation Loss:0.8955, Validation Accuracy:0.5534\n",
    "Epoch #196: Loss:0.8789, Accuracy:0.5758, Validation Loss:0.8963, Validation Accuracy:0.5599\n",
    "Epoch #197: Loss:0.8785, Accuracy:0.5717, Validation Loss:0.8948, Validation Accuracy:0.5616\n",
    "Epoch #198: Loss:0.8781, Accuracy:0.5778, Validation Loss:0.8944, Validation Accuracy:0.5583\n",
    "Epoch #199: Loss:0.8792, Accuracy:0.5741, Validation Loss:0.8984, Validation Accuracy:0.5780\n",
    "Epoch #200: Loss:0.8788, Accuracy:0.5700, Validation Loss:0.8938, Validation Accuracy:0.5583\n",
    "Epoch #201: Loss:0.8792, Accuracy:0.5770, Validation Loss:0.8931, Validation Accuracy:0.5649\n",
    "Epoch #202: Loss:0.8817, Accuracy:0.5762, Validation Loss:0.9009, Validation Accuracy:0.5731\n",
    "Epoch #203: Loss:0.8808, Accuracy:0.5733, Validation Loss:0.8922, Validation Accuracy:0.5665\n",
    "Epoch #204: Loss:0.8796, Accuracy:0.5786, Validation Loss:0.8988, Validation Accuracy:0.5599\n",
    "Epoch #205: Loss:0.8860, Accuracy:0.5680, Validation Loss:0.9074, Validation Accuracy:0.5764\n",
    "Epoch #206: Loss:0.8801, Accuracy:0.5639, Validation Loss:0.8937, Validation Accuracy:0.5599\n",
    "Epoch #207: Loss:0.8847, Accuracy:0.5692, Validation Loss:0.8912, Validation Accuracy:0.5698\n",
    "Epoch #208: Loss:0.8844, Accuracy:0.5659, Validation Loss:0.9042, Validation Accuracy:0.5764\n",
    "Epoch #209: Loss:0.8788, Accuracy:0.5704, Validation Loss:0.9007, Validation Accuracy:0.5649\n",
    "Epoch #210: Loss:0.8869, Accuracy:0.5762, Validation Loss:0.8999, Validation Accuracy:0.5796\n",
    "Epoch #211: Loss:0.8749, Accuracy:0.5749, Validation Loss:0.8930, Validation Accuracy:0.5616\n",
    "Epoch #212: Loss:0.8763, Accuracy:0.5713, Validation Loss:0.8917, Validation Accuracy:0.5649\n",
    "Epoch #213: Loss:0.8748, Accuracy:0.5844, Validation Loss:0.8885, Validation Accuracy:0.5780\n",
    "Epoch #214: Loss:0.8761, Accuracy:0.5786, Validation Loss:0.8884, Validation Accuracy:0.5681\n",
    "Epoch #215: Loss:0.8737, Accuracy:0.5832, Validation Loss:0.8891, Validation Accuracy:0.5747\n",
    "Epoch #216: Loss:0.8721, Accuracy:0.5848, Validation Loss:0.8871, Validation Accuracy:0.5731\n",
    "Epoch #217: Loss:0.8727, Accuracy:0.5823, Validation Loss:0.8879, Validation Accuracy:0.5698\n",
    "Epoch #218: Loss:0.8714, Accuracy:0.5840, Validation Loss:0.8861, Validation Accuracy:0.5780\n",
    "Epoch #219: Loss:0.8708, Accuracy:0.5893, Validation Loss:0.8956, Validation Accuracy:0.5731\n",
    "Epoch #220: Loss:0.8793, Accuracy:0.5745, Validation Loss:0.8991, Validation Accuracy:0.5616\n",
    "Epoch #221: Loss:0.8812, Accuracy:0.5758, Validation Loss:0.8918, Validation Accuracy:0.5632\n",
    "Epoch #222: Loss:0.8770, Accuracy:0.5811, Validation Loss:0.8837, Validation Accuracy:0.5813\n",
    "Epoch #223: Loss:0.8723, Accuracy:0.5856, Validation Loss:0.8830, Validation Accuracy:0.5895\n",
    "Epoch #224: Loss:0.8701, Accuracy:0.5860, Validation Loss:0.8838, Validation Accuracy:0.5780\n",
    "Epoch #225: Loss:0.8724, Accuracy:0.5869, Validation Loss:0.8821, Validation Accuracy:0.5862\n",
    "Epoch #226: Loss:0.8681, Accuracy:0.5947, Validation Loss:0.8819, Validation Accuracy:0.5796\n",
    "Epoch #227: Loss:0.8688, Accuracy:0.5914, Validation Loss:0.8808, Validation Accuracy:0.5846\n",
    "Epoch #228: Loss:0.8692, Accuracy:0.5926, Validation Loss:0.8803, Validation Accuracy:0.5878\n",
    "Epoch #229: Loss:0.8692, Accuracy:0.5881, Validation Loss:0.8791, Validation Accuracy:0.5780\n",
    "Epoch #230: Loss:0.8669, Accuracy:0.5943, Validation Loss:0.8806, Validation Accuracy:0.5813\n",
    "Epoch #231: Loss:0.8674, Accuracy:0.5889, Validation Loss:0.8777, Validation Accuracy:0.5928\n",
    "Epoch #232: Loss:0.8652, Accuracy:0.5996, Validation Loss:0.8777, Validation Accuracy:0.5813\n",
    "Epoch #233: Loss:0.8665, Accuracy:0.5926, Validation Loss:0.8750, Validation Accuracy:0.5944\n",
    "Epoch #234: Loss:0.8662, Accuracy:0.5906, Validation Loss:0.8754, Validation Accuracy:0.5944\n",
    "Epoch #235: Loss:0.8666, Accuracy:0.5955, Validation Loss:0.8786, Validation Accuracy:0.5829\n",
    "Epoch #236: Loss:0.8675, Accuracy:0.5893, Validation Loss:0.8742, Validation Accuracy:0.5878\n",
    "Epoch #237: Loss:0.8640, Accuracy:0.5930, Validation Loss:0.8736, Validation Accuracy:0.5928\n",
    "Epoch #238: Loss:0.8622, Accuracy:0.5951, Validation Loss:0.8721, Validation Accuracy:0.5928\n",
    "Epoch #239: Loss:0.8636, Accuracy:0.5930, Validation Loss:0.8721, Validation Accuracy:0.5961\n",
    "Epoch #240: Loss:0.8634, Accuracy:0.6000, Validation Loss:0.8705, Validation Accuracy:0.5944\n",
    "Epoch #241: Loss:0.8620, Accuracy:0.5922, Validation Loss:0.8715, Validation Accuracy:0.5993\n",
    "Epoch #242: Loss:0.8617, Accuracy:0.5938, Validation Loss:0.8690, Validation Accuracy:0.5977\n",
    "Epoch #243: Loss:0.8629, Accuracy:0.5947, Validation Loss:0.8739, Validation Accuracy:0.5862\n",
    "Epoch #244: Loss:0.8619, Accuracy:0.6004, Validation Loss:0.8694, Validation Accuracy:0.5928\n",
    "Epoch #245: Loss:0.8638, Accuracy:0.5930, Validation Loss:0.8661, Validation Accuracy:0.6010\n",
    "Epoch #246: Loss:0.8592, Accuracy:0.6037, Validation Loss:0.8666, Validation Accuracy:0.5944\n",
    "Epoch #247: Loss:0.8589, Accuracy:0.6000, Validation Loss:0.8654, Validation Accuracy:0.5977\n",
    "Epoch #248: Loss:0.8572, Accuracy:0.6049, Validation Loss:0.8672, Validation Accuracy:0.5944\n",
    "Epoch #249: Loss:0.8565, Accuracy:0.6045, Validation Loss:0.8674, Validation Accuracy:0.5993\n",
    "Epoch #250: Loss:0.8608, Accuracy:0.5910, Validation Loss:0.8726, Validation Accuracy:0.5780\n",
    "Epoch #251: Loss:0.8606, Accuracy:0.6012, Validation Loss:0.8640, Validation Accuracy:0.6010\n",
    "Epoch #252: Loss:0.8568, Accuracy:0.6037, Validation Loss:0.8688, Validation Accuracy:0.5993\n",
    "Epoch #253: Loss:0.8589, Accuracy:0.6025, Validation Loss:0.8660, Validation Accuracy:0.5977\n",
    "Epoch #254: Loss:0.8613, Accuracy:0.6004, Validation Loss:0.8702, Validation Accuracy:0.5796\n",
    "Epoch #255: Loss:0.8611, Accuracy:0.5914, Validation Loss:0.8647, Validation Accuracy:0.5961\n",
    "Epoch #256: Loss:0.8551, Accuracy:0.6123, Validation Loss:0.8629, Validation Accuracy:0.5977\n",
    "Epoch #257: Loss:0.8559, Accuracy:0.5984, Validation Loss:0.8686, Validation Accuracy:0.5813\n",
    "Epoch #258: Loss:0.8591, Accuracy:0.6008, Validation Loss:0.8705, Validation Accuracy:0.5944\n",
    "Epoch #259: Loss:0.8578, Accuracy:0.5979, Validation Loss:0.8621, Validation Accuracy:0.5993\n",
    "Epoch #260: Loss:0.8553, Accuracy:0.5975, Validation Loss:0.8686, Validation Accuracy:0.5829\n",
    "Epoch #261: Loss:0.8548, Accuracy:0.6057, Validation Loss:0.8620, Validation Accuracy:0.5977\n",
    "Epoch #262: Loss:0.8538, Accuracy:0.6062, Validation Loss:0.8618, Validation Accuracy:0.6010\n",
    "Epoch #263: Loss:0.8510, Accuracy:0.6070, Validation Loss:0.8615, Validation Accuracy:0.5993\n",
    "Epoch #264: Loss:0.8507, Accuracy:0.6127, Validation Loss:0.8690, Validation Accuracy:0.6010\n",
    "Epoch #265: Loss:0.8608, Accuracy:0.6033, Validation Loss:0.8691, Validation Accuracy:0.5829\n",
    "Epoch #266: Loss:0.8534, Accuracy:0.6021, Validation Loss:0.8647, Validation Accuracy:0.5928\n",
    "Epoch #267: Loss:0.8534, Accuracy:0.6070, Validation Loss:0.8669, Validation Accuracy:0.5961\n",
    "Epoch #268: Loss:0.8552, Accuracy:0.6008, Validation Loss:0.8705, Validation Accuracy:0.5764\n",
    "Epoch #269: Loss:0.8527, Accuracy:0.6021, Validation Loss:0.8609, Validation Accuracy:0.6010\n",
    "Epoch #270: Loss:0.8539, Accuracy:0.6053, Validation Loss:0.8706, Validation Accuracy:0.5977\n",
    "Epoch #271: Loss:0.8525, Accuracy:0.6025, Validation Loss:0.8662, Validation Accuracy:0.5862\n",
    "Epoch #272: Loss:0.8482, Accuracy:0.6082, Validation Loss:0.8617, Validation Accuracy:0.5977\n",
    "Epoch #273: Loss:0.8489, Accuracy:0.6090, Validation Loss:0.8599, Validation Accuracy:0.5944\n",
    "Epoch #274: Loss:0.8460, Accuracy:0.6074, Validation Loss:0.8615, Validation Accuracy:0.5977\n",
    "Epoch #275: Loss:0.8482, Accuracy:0.6136, Validation Loss:0.8608, Validation Accuracy:0.6010\n",
    "Epoch #276: Loss:0.8468, Accuracy:0.6107, Validation Loss:0.8637, Validation Accuracy:0.5862\n",
    "Epoch #277: Loss:0.8479, Accuracy:0.6103, Validation Loss:0.8616, Validation Accuracy:0.5944\n",
    "Epoch #278: Loss:0.8460, Accuracy:0.6090, Validation Loss:0.8638, Validation Accuracy:0.5862\n",
    "Epoch #279: Loss:0.8469, Accuracy:0.6045, Validation Loss:0.8624, Validation Accuracy:0.5944\n",
    "Epoch #280: Loss:0.8483, Accuracy:0.6115, Validation Loss:0.8710, Validation Accuracy:0.5993\n",
    "Epoch #281: Loss:0.8496, Accuracy:0.6057, Validation Loss:0.8672, Validation Accuracy:0.5862\n",
    "Epoch #282: Loss:0.8488, Accuracy:0.6082, Validation Loss:0.8639, Validation Accuracy:0.5911\n",
    "Epoch #283: Loss:0.8542, Accuracy:0.6078, Validation Loss:0.8640, Validation Accuracy:0.6059\n",
    "Epoch #284: Loss:0.8515, Accuracy:0.6000, Validation Loss:0.8637, Validation Accuracy:0.5829\n",
    "Epoch #285: Loss:0.8521, Accuracy:0.6037, Validation Loss:0.8636, Validation Accuracy:0.5944\n",
    "Epoch #286: Loss:0.8449, Accuracy:0.6099, Validation Loss:0.8593, Validation Accuracy:0.5977\n",
    "Epoch #287: Loss:0.8438, Accuracy:0.6164, Validation Loss:0.8640, Validation Accuracy:0.5993\n",
    "Epoch #288: Loss:0.8501, Accuracy:0.6008, Validation Loss:0.8703, Validation Accuracy:0.5698\n",
    "Epoch #289: Loss:0.8521, Accuracy:0.6029, Validation Loss:0.8615, Validation Accuracy:0.5846\n",
    "Epoch #290: Loss:0.8436, Accuracy:0.6086, Validation Loss:0.8613, Validation Accuracy:0.5944\n",
    "Epoch #291: Loss:0.8420, Accuracy:0.6099, Validation Loss:0.8601, Validation Accuracy:0.6043\n",
    "Epoch #292: Loss:0.8415, Accuracy:0.6111, Validation Loss:0.8604, Validation Accuracy:0.5846\n",
    "Epoch #293: Loss:0.8397, Accuracy:0.6177, Validation Loss:0.8618, Validation Accuracy:0.5846\n",
    "Epoch #294: Loss:0.8412, Accuracy:0.6107, Validation Loss:0.8596, Validation Accuracy:0.6010\n",
    "Epoch #295: Loss:0.8455, Accuracy:0.6045, Validation Loss:0.8646, Validation Accuracy:0.6043\n",
    "Epoch #296: Loss:0.8494, Accuracy:0.6021, Validation Loss:0.8768, Validation Accuracy:0.5550\n",
    "Epoch #297: Loss:0.8454, Accuracy:0.6012, Validation Loss:0.8724, Validation Accuracy:0.5928\n",
    "Epoch #298: Loss:0.8489, Accuracy:0.6074, Validation Loss:0.8604, Validation Accuracy:0.5977\n",
    "Epoch #299: Loss:0.8388, Accuracy:0.6066, Validation Loss:0.8597, Validation Accuracy:0.5928\n",
    "Epoch #300: Loss:0.8384, Accuracy:0.6119, Validation Loss:0.8622, Validation Accuracy:0.5961\n",
    "\n",
    "Test:\n",
    "Test Loss:0.86219436, Accuracy:0.5961\n",
    "Labels: ['03', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03   02   01\n",
    "t:03  73   22   47\n",
    "t:02  12  136   79\n",
    "t:01  38   48  154\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.59      0.51      0.55       142\n",
    "          02       0.66      0.60      0.63       227\n",
    "          01       0.55      0.64      0.59       240\n",
    "\n",
    "    accuracy                           0.60       609\n",
    "   macro avg       0.60      0.58      0.59       609\n",
    "weighted avg       0.60      0.60      0.60       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 23:02:20 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 26 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0820971721498838, 1.0755847539807775, 1.0745048201925844, 1.074818213389229, 1.0747730855284066, 1.074240583504362, 1.0735400921018252, 1.0731391053285897, 1.0726064656951353, 1.0718127489089966, 1.0705088719554332, 1.0685625330763693, 1.065419961079001, 1.0599032188283986, 1.0505863999694047, 1.0346824960364105, 1.012535518026117, 0.9917640018541433, 0.9779876055584361, 0.9685784055681651, 0.9642948989797695, 0.9588696789271726, 0.9541806279144851, 0.9464947509843923, 0.9441508108563416, 0.9432523207515723, 0.9412903799407784, 0.9399671657332058, 0.9361152588244533, 0.9347755043768922, 0.9332401535193908, 0.9323486329495222, 0.930935034904574, 0.9292027056706559, 0.9288307496871071, 0.9284852655063122, 0.9264450968779954, 0.925869057722671, 0.9253906214178489, 0.9256463006799445, 0.9257442780903408, 0.9270873708873743, 0.9232125852887071, 0.9238968480788232, 0.9230124302489808, 0.9236725171407064, 0.9213169273876008, 0.9211460014282189, 0.9213163676520286, 0.921831942250576, 0.924725860895586, 0.9197752673441945, 0.920158663113129, 0.9191082344070836, 0.9186977439717511, 0.9203384853348944, 0.917827513417587, 0.9225036175967437, 0.9174243800941555, 0.919748277499758, 0.9161043465626847, 0.9162046925970682, 0.9178257954531702, 0.9204004832676479, 0.9247860350632315, 0.9220968902796164, 0.9228406927268493, 0.9157592593546963, 0.9188223158197449, 0.9135820268605926, 0.9149769526984304, 0.9143826674749502, 0.9222216288052952, 0.9140688004752098, 0.9193001846570296, 0.9163038414491613, 0.918590905924736, 0.9123192872907141, 0.912671900245748, 0.9122635887565675, 0.9135637973329703, 0.9114548840937747, 0.9119544465749331, 0.9118556494783299, 0.9170397049296274, 0.9106874247685638, 0.9101101898011708, 0.9125200304491766, 0.9121830114981615, 0.9101975061036096, 0.9086528831514818, 0.9163653648937081, 0.9256673682303656, 0.914175213362, 0.9124390837202714, 0.9086011340856943, 0.9078978885375024, 0.9104315476073028, 0.9104807834907118, 0.9109644556084682, 0.9083016267159498, 0.9103451217532353, 0.9078960938406695, 0.9116159470993506, 0.9063884964325941, 0.9064402559707905, 0.9071651548391884, 0.9059405632206959, 0.9159510671994565, 0.9123296595755077, 0.9251939573115707, 0.9117966920675706, 0.9156233016493285, 0.9064089563875558, 0.9097516147178186, 0.912889028711272, 0.9089546643062961, 0.9070334961065909, 0.9070843680347324, 0.9050249732382387, 0.9067043596496331, 0.905209925472247, 0.9061598917729358, 0.9046998667012295, 0.904784493160561, 0.9065587484386363, 0.9074293590335815, 0.9060606934949877, 0.9063874822727761, 0.9088773146051491, 0.9042128867703706, 0.9043397482588569, 0.9082288720533374, 0.9035276419227738, 0.903467897515383, 0.9088064750231349, 0.902994506954168, 0.9045839188329887, 0.9177543835099695, 0.9068073205564214, 0.9030192113666503, 0.9045303264274973, 0.9025596818704714, 0.9030662371802995, 0.90344380167709, 0.9085216958730288, 0.9115917444816364, 0.902529337136029, 0.9050512746441345, 0.9025062231594706, 0.9023613504001072, 0.9030697076582948, 0.9029997630268091, 0.9027903239519529, 0.9014911329609224, 0.9068436935813164, 0.9018414083177038, 0.9013340050363775, 0.9008002036506515, 0.9039771271065147, 0.9029447655568178, 0.9066664926915725, 0.9003913866475298, 0.9040648252114483, 0.9156112644472733, 0.9002846431458134, 0.900192233924991, 0.9065553970720576, 0.9044060510367595, 0.9082184361511068, 0.899152885908368, 0.8991232141485355, 0.8992904539178745, 0.9005096777906558, 0.899139628425999, 0.8977619934160329, 0.9081584876785529, 0.9046650129586018, 0.9009682683913383, 0.9013521038056986, 0.90087070508152, 0.9103940263366073, 0.8970977337098083, 0.8976259609357086, 0.8963919239874152, 0.898164441334986, 0.8963936708243609, 0.8962467842305627, 0.8956239807781915, 0.8971647973522568, 0.8982037472215975, 0.8976796745079492, 0.8976759118791089, 0.8996646632310401, 0.8954742736808576, 0.8962991685898629, 0.8948159052429137, 0.8944364605083058, 0.8984333552750461, 0.8937521531859838, 0.8931307821046739, 0.9009000533711539, 0.8922494714483252, 0.8988039616880745, 0.9074046060761012, 0.8937189501690356, 0.891201923241952, 0.9041930802154228, 0.9006676086651281, 0.8998776233842221, 0.893020476613726, 0.8917018468744062, 0.8885057488294267, 0.888361510972084, 0.8891219634727892, 0.8870666928479237, 0.8879108490615055, 0.8860981085617554, 0.8955564560561344, 0.8991073931770763, 0.8918269386236695, 0.883722581495401, 0.8830237896571606, 0.8837627451408085, 0.8820924512271223, 0.881926529806823, 0.8807698403085981, 0.8802689468527858, 0.8790860120298827, 0.8806440275290917, 0.8777273492077106, 0.8777245359467756, 0.87504925036861, 0.8753684312839226, 0.878587836506723, 0.8741645248261187, 0.8735526316663119, 0.8721129862937238, 0.8720841913779185, 0.8705202063120449, 0.8714663896263135, 0.8689598107377101, 0.873919510293281, 0.8694341107346546, 0.8661088841693546, 0.866586183288023, 0.8653536921455747, 0.8672265023825008, 0.8674196847553911, 0.8725908056072805, 0.8640076855720558, 0.8688004176409178, 0.8659836442403996, 0.8702339668970781, 0.864682640054543, 0.862932879447154, 0.8685742368056073, 0.8705200583280992, 0.862136594865514, 0.8685621339307825, 0.8620379708083392, 0.8617796120776723, 0.8614545667112754, 0.8689790709656839, 0.8690784715470814, 0.8646922761387817, 0.8669331082569555, 0.8705325720736937, 0.8608851608971657, 0.8705746407187827, 0.8661574904358837, 0.8617187501565967, 0.8598623756313167, 0.8615100545248962, 0.8608191221805629, 0.8637416926510816, 0.8616121709836136, 0.8638247653768567, 0.8624471652096716, 0.8709873374068287, 0.8672291264549656, 0.8638510712849096, 0.863994256224734, 0.863706833427567, 0.8636360875099947, 0.8592938045758528, 0.863975626103005, 0.8703420606544257, 0.8614541762959586, 0.8613006996208028, 0.8600523415065947, 0.8603886057590616, 0.8618087622723948, 0.8596071770234257, 0.8646463113661078, 0.8768121989685522, 0.8723971783038235, 0.8603830305226331, 0.8597068344235225, 0.862194401779394], 'val_acc': [0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.39573070465637544, 0.41215106600219586, 0.42692939132109453, 0.4384236442925308, 0.44499178879170975, 0.4515599332908887, 0.4548440056383512, 0.45320196951355646, 0.4581280779858137, 0.46469622258286564, 0.46469622258286564, 0.46141215033327615, 0.49753694537237947, 0.49261083699799524, 0.5041050857119568, 0.5073891579615463, 0.5041050857119568, 0.49917897733757255, 0.5172413748081878, 0.5057471218367515, 0.5205254472535232, 0.5123152664338035, 0.5254515553342884, 0.51559933858552, 0.5139573024607252, 0.5205254470577773, 0.5238095193073667, 0.5254515554321615, 0.5270935914590833, 0.5303776637086728, 0.5320196998334675, 0.5369458083057247, 0.5320196998334675, 0.5287356277796239, 0.5320196997355945, 0.5336617361540081, 0.5336617360561352, 0.5320196998334675, 0.5336617359582623, 0.5369458083057247, 0.535303771985184, 0.5435139528049037, 0.5270935915569562, 0.5188834109329825, 0.5402298805553142, 0.5353037766341505, 0.535303772083057, 0.535303771985184, 0.5369458081099787, 0.535303772083057, 0.5353037766341505, 0.5336617359582623, 0.535303772083057, 0.53530377218093, 0.5402298804574412, 0.5303776681618934, 0.5336617360561352, 0.5123152706912781, 0.5369458083057247, 0.5369458082078518, 0.5451559889296984, 0.5402298850085347, 0.5500820974019557, 0.5320197042866881, 0.5303776637086728, 0.5418719210354566, 0.5435139529027767, 0.5353037764384045, 0.5336617358603892, 0.5336617358603892, 0.5533661696515452, 0.5533661694557992, 0.5369458126610723, 0.541871916582236, 0.5467980248587472, 0.5418719209375835, 0.5418719166801089, 0.5385878487858671, 0.5435139572581242, 0.5467980250544932, 0.5320197040909421, 0.5435139526091577, 0.5418719211333295, 0.5254515596896361, 0.5369458083057247, 0.5303776680640203, 0.541871916484363, 0.5353037763405316, 0.5435139571602513, 0.5435139526091577, 0.5353037763405316, 0.5467980251523662, 0.5467980293119677, 0.5467980249566202, 0.5402298847149158, 0.5484400612771609, 0.5435139572581242, 0.5484400656325085, 0.5467980295077137, 0.5402298849106618, 0.5451559889296984, 0.53858784888374, 0.5517241333310045, 0.5402298848127888, 0.5517241335267504, 0.5435139572581242, 0.5385878487858671, 0.5533661696515452, 0.538587848687994, 0.5467980248587472, 0.5533661694557992, 0.5435139571602513, 0.5451559888318255, 0.5467980249566202, 0.5451559888318255, 0.545155993382919, 0.5418719210354566, 0.5533661694557992, 0.5500821017573032, 0.5517241333310045, 0.5435139571602513, 0.5550082055805939, 0.5517241333310045, 0.5467980248587472, 0.5582922778301834, 0.5500820971083367, 0.5500820971083367, 0.5566502417053887, 0.5517241332331314, 0.5500820971083367, 0.5566502417053887, 0.5500821017573032, 0.5566502417053887, 0.5451559888318255, 0.5484400609835419, 0.5533661694557992, 0.5533661695536721, 0.5500820972062097, 0.5582922777323104, 0.548440061081415, 0.548440061081415, 0.5484400609835419, 0.5500820971083367, 0.5582922778301834, 0.5500820971083367, 0.5517241332331314, 0.5517241332331314, 0.5533661694557992, 0.5582922777323104, 0.5566502417053887, 0.5517241332331314, 0.5550082054827209, 0.5566502417053887, 0.5517241332331314, 0.5550082055805939, 0.5615763501776458, 0.5599343139549782, 0.5517241332331314, 0.5582922778301834, 0.5615763502755189, 0.5550082056784669, 0.5648604224272353, 0.5533661693579262, 0.5533661693579262, 0.5533661693579262, 0.5484400609835419, 0.5566502416075156, 0.5533661693579262, 0.5632183863024406, 0.5566502419011347, 0.5648604224272353, 0.5648604224272353, 0.5582922778301834, 0.5599343140528511, 0.5566502416075156, 0.5550082101316874, 0.5599343139549782, 0.5566502417053887, 0.5517241333310045, 0.551724137784225, 0.551724137784225, 0.5648604223293624, 0.5566502460607363, 0.5747126392738768, 0.5632183862045677, 0.5714285669264143, 0.5533661695536721, 0.5599343140528511, 0.5615763546308664, 0.558292282283404, 0.5779967114255933, 0.558292282283404, 0.564860426782583, 0.573070603246955, 0.5665024583562842, 0.5599343138571051, 0.5763546753007985, 0.5599343185060717, 0.5697865352548402, 0.5763546754965445, 0.5648604224272353, 0.579638747746134, 0.5615763545329935, 0.5648604225251084, 0.5779967113277203, 0.5681444990321725, 0.5747126394696228, 0.573070603246955, 0.5697865311931115, 0.5779967112298474, 0.573070603051209, 0.5615763500797729, 0.5632183864003136, 0.5812807836751829, 0.5894909644949025, 0.5779967116213393, 0.586206892245313, 0.5796387475503881, 0.5845648561205182, 0.5878489281743619, 0.5779967117192123, 0.5812807839688018, 0.5927750367444921, 0.5812807839688018, 0.5944170729671597, 0.5944170729671597, 0.5829228198978506, 0.5878489285658537, 0.5927750367444921, 0.592775037038111, 0.5960591092877005, 0.5944170728692868, 0.5993431815372899, 0.5977011450210032, 0.58620689214744, 0.592775037038111, 0.6009852177599576, 0.5944170729671597, 0.5977011451188763, 0.5944170730650327, 0.599343181145798, 0.5779967116213393, 0.6009852175642116, 0.599343181341544, 0.5977011453146222, 0.579638747844007, 0.5960591088962085, 0.5977011452167492, 0.5812807838709287, 0.5944170728692868, 0.599343181243671, 0.5829228199957235, 0.5977011451188763, 0.6009852174663387, 0.599343181145798, 0.6009852173684658, 0.5829228199957235, 0.5927750365487461, 0.5960591089940815, 0.5763546754965445, 0.6009852172705927, 0.5977011452167492, 0.586206892343186, 0.5977011452167492, 0.5944170726735408, 0.5977011450210032, 0.6009852175642116, 0.586206892441059, 0.5944170726735408, 0.586206892343186, 0.5944170727714138, 0.599343181341544, 0.586206892343186, 0.5911330004239513, 0.60591132574285, 0.5829228197999776, 0.5944170728692868, 0.5977011451188763, 0.599343181145798, 0.5697865308994926, 0.5845648562183913, 0.5944170729671597, 0.6042692896180553, 0.5845648561205182, 0.5845648563162642, 0.6009852174663387, 0.6042692896180553, 0.5550082055805939, 0.592775036842365, 0.5977011451188763, 0.5927750367444921, 0.5960591088962085], 'loss': [1.0910138899295971, 1.078754922692536, 1.0748469895650719, 1.074256467819214, 1.0746830265135245, 1.0748326555659393, 1.0738891911457695, 1.073202274026812, 1.0727627366475256, 1.072204972684261, 1.0711464772723784, 1.0695505753679686, 1.0671356975665083, 1.0631028413772583, 1.0560484008867392, 1.0438988001194822, 1.0257681325231, 1.0065614129973144, 0.9908477842195812, 0.9813073461305434, 0.9694464485992886, 0.956102215166699, 0.9522269807557061, 0.9520253107043507, 0.9498855723982229, 0.9449985500968212, 0.9395524815612261, 0.9369945194931735, 0.9345489472823956, 0.9323692334750839, 0.9301311612618778, 0.9302440709891505, 0.9265903369357209, 0.9274265522584778, 0.9237066270634379, 0.9242581129074097, 0.9228134904798786, 0.9224376378362918, 0.9210810370269008, 0.9203502864073924, 0.921191347624487, 0.9225436026310774, 0.921492507717203, 0.919155425604364, 0.9201549743235233, 0.9188021731817257, 0.9191777100063692, 0.9169757079784385, 0.9177014269378396, 0.9196658315110255, 0.9168487350553948, 0.9172494777663778, 0.915134692143121, 0.9150470338073354, 0.9144684141176682, 0.9143840955757752, 0.9138331001299362, 0.9133002230518897, 0.9145566762350423, 0.9124780456388266, 0.9119182192324613, 0.9115262806782732, 0.9117946091863409, 0.9128491579384774, 0.9154200858893581, 0.9117161184610527, 0.9163823705189527, 0.9104235679217187, 0.9089915082439994, 0.9101621505905715, 0.9083706245530068, 0.9082921357125472, 0.9086083879461034, 0.9095317134866969, 0.9089915428563065, 0.9056120423810438, 0.9089395478276012, 0.9076047917411068, 0.9066034359608832, 0.9066126260669325, 0.9072981221964717, 0.9071192046944855, 0.9048433959116926, 0.9049920270330363, 0.9070913795328238, 0.9044532718844482, 0.9036678646134645, 0.9046837609651397, 0.9043315182721101, 0.9045744055595242, 0.9028425617874036, 0.9053361747054349, 0.9061350902003185, 0.9054786000897997, 0.904827396531859, 0.9044514348374745, 0.9050813374822879, 0.9039324199639307, 0.9026058583778522, 0.903240833047479, 0.9005708575738284, 0.8999118138877273, 0.9021670782345765, 0.8993683196925529, 0.9012600329622351, 0.8992319176084452, 0.8997178256144513, 0.8987906643986947, 0.8991571920608348, 0.9012737256300768, 0.8998899370248313, 0.9038886315523966, 0.8976603514604745, 0.9058353538630679, 0.9042525987850323, 0.899971791141087, 0.8986715286419377, 0.8999358188445074, 0.9005378031877521, 0.8979144636365667, 0.897763871363301, 0.8980909452056494, 0.8968401241841013, 0.8967767444234609, 0.8966447297797311, 0.8973974219582653, 0.8979413013928235, 0.8979377514038243, 0.8978322999188543, 0.8983511772978232, 0.8970144376372899, 0.8970242421729854, 0.8943531668896059, 0.9019311966837309, 0.8956289134720757, 0.8949415268349696, 0.8956593350708117, 0.8957413622486028, 0.8942462792141971, 0.8965515918800229, 0.8967510622384857, 0.8934468892565498, 0.8957863141868638, 0.8947546118583523, 0.8936131620553974, 0.8942426516779639, 0.8957200706372271, 0.8947374551448, 0.8918443710407438, 0.8919908124318603, 0.8928435450951422, 0.8929170171582969, 0.8904396040483666, 0.8920813083159116, 0.8905282085681109, 0.8894988991886194, 0.8920572092645712, 0.8908722118920125, 0.8892237887734995, 0.890126176169276, 0.8934025113342724, 0.8943228242578448, 0.8919612678903819, 0.8900505630142634, 0.8956431382735407, 0.8926724877200822, 0.8923435620948275, 0.8936606803224317, 0.8878845223900719, 0.8904555765020774, 0.8917975484957685, 0.8881462713041834, 0.8874201073783624, 0.8861752481675981, 0.8869155940823487, 0.8868577391459956, 0.8885372474942609, 0.8903791945817779, 0.8891627622336088, 0.8886779188865019, 0.8896430236847739, 0.8873914675790916, 0.889641169037907, 0.8852976126347724, 0.8839602655453849, 0.884298197420226, 0.881806810191035, 0.8816974026711325, 0.8814670282467679, 0.8810374686605387, 0.8800422188437695, 0.8820795141940734, 0.8839417398343096, 0.8820358873882332, 0.8793610805603513, 0.8789025322857333, 0.8784545010854576, 0.878076480668673, 0.8792286439597974, 0.8788425343237374, 0.8792050425276864, 0.8816592353814926, 0.880805023494932, 0.8796365511001258, 0.886019165266221, 0.8801428307253233, 0.884654935160212, 0.8843771277756661, 0.8788224918152028, 0.8868558613916198, 0.8749133322762758, 0.8763294932533828, 0.8748091405911612, 0.876105449723512, 0.8736855845676555, 0.8720597887186055, 0.8727112864077213, 0.871391986454292, 0.8707938919811523, 0.87927955286214, 0.8812192577845752, 0.8770297537594116, 0.8722912212906432, 0.8700583580827811, 0.8724171620374832, 0.8680957460795095, 0.8687545641247008, 0.8691711843625721, 0.8691690576639508, 0.8669149131500745, 0.8673915937206339, 0.8652168640121053, 0.8665381733641733, 0.8661852031762595, 0.8665735107182967, 0.867468550831875, 0.8639683779994565, 0.8622455928604705, 0.8636048832957994, 0.8633839685079743, 0.8620294272777236, 0.8616540201880359, 0.8629316691255667, 0.8619389657122398, 0.8638483972764848, 0.8592114295313246, 0.8589485562067991, 0.8571762549069383, 0.8565144058615275, 0.8607793707867177, 0.8605763499496899, 0.8567785337475535, 0.8588960292402968, 0.8612784488979551, 0.8611353799058182, 0.8551054749155925, 0.8558844382023665, 0.8590763910595152, 0.8577534472428308, 0.8552502536430986, 0.8548072344713388, 0.853806791046072, 0.8510186964236736, 0.8507172985732923, 0.8608229008543418, 0.8534172504834326, 0.8533879899635941, 0.8551981266519127, 0.85266043150939, 0.8539376603259687, 0.8524572736673531, 0.8482390662727904, 0.8489140356835398, 0.8460368855043603, 0.8481980629525391, 0.846755714803261, 0.8478873569128205, 0.8459899434318777, 0.8468558078184265, 0.8482883123156961, 0.8495503316669738, 0.8488316947674605, 0.8541909097156486, 0.851549669384222, 0.8520718262915249, 0.8448958773387776, 0.8438207707121143, 0.8500661882531716, 0.852093610185862, 0.8436398571521595, 0.8419774635616514, 0.8415040347855194, 0.8396666184098324, 0.8412473199303879, 0.845459789833249, 0.8493935708392573, 0.8453886731204556, 0.8489346003140757, 0.8388190629545913, 0.8383738433800684], 'acc': [0.39425051113173704, 0.3942505133225443, 0.39137577038770827, 0.39425051171921605, 0.39425051113173704, 0.3942505146933287, 0.3942505123066951, 0.394250514301676, 0.39507186738372585, 0.40780287311552, 0.4184804907936825, 0.433264887981591, 0.44681724757874036, 0.45749486881849455, 0.46036960999089344, 0.46365503250206275, 0.46406571031595895, 0.4624229992562006, 0.46776180789945554, 0.4607802895672267, 0.4944558528414497, 0.5117043137795136, 0.5141683806628907, 0.5215605726966623, 0.5166324448046988, 0.5190965134505129, 0.5297741228305339, 0.527720738338494, 0.5310061608496632, 0.5310061594788789, 0.528952768573526, 0.529774129757891, 0.5277207373593622, 0.5326488709302898, 0.5342915812067427, 0.533059549331665, 0.5334702255789504, 0.5318275170649346, 0.5351129391844512, 0.5351129370303614, 0.5342915819900481, 0.5359342932456328, 0.5322381950746572, 0.5367556475026407, 0.5293634519439948, 0.5420944575166801, 0.5363449645238246, 0.5383983581707463, 0.5433264846184905, 0.5330595518774076, 0.5437371693597437, 0.5392197120361015, 0.5371663278622794, 0.5425051380721451, 0.5404517450861372, 0.5416837824443528, 0.5412730980947522, 0.5416837798986102, 0.5416837816610474, 0.5429158125569933, 0.5433264907625421, 0.5416837798986102, 0.5379876811401555, 0.5404517470444008, 0.5396303860558621, 0.543326489000105, 0.5404517418794809, 0.5462012277491528, 0.5445585224417935, 0.5429158151027358, 0.5437371624323867, 0.546201235851468, 0.5494866538586313, 0.5478439469112263, 0.5449692026056059, 0.5470225897168232, 0.5466119138611906, 0.5466119120987534, 0.5466119053672227, 0.5531827561419602, 0.5445585170810472, 0.5457905497394303, 0.5453798800278493, 0.5494866496728431, 0.5441478458028555, 0.546611911315448, 0.5478439392005638, 0.5498973280742183, 0.5494866575793319, 0.5511293678557848, 0.5494866498686695, 0.5462012348723363, 0.5589322410325005, 0.5498973334349646, 0.5490759710756415, 0.5544147808938545, 0.5490759785904776, 0.5540041026883057, 0.551129365701695, 0.5552361422740458, 0.5544147808938545, 0.5486652938492245, 0.54989733421827, 0.5577002083741174, 0.5531827530087386, 0.5523613991433834, 0.5552361349550361, 0.5556468212629001, 0.5560574988809699, 0.5552361351508625, 0.5585215645893888, 0.5568788533338042, 0.5572895321268321, 0.5503080114446871, 0.5527720781322377, 0.5568788537254569, 0.5523613985559045, 0.5523614007099943, 0.5544147797188964, 0.5548254575327927, 0.5564681768906924, 0.5572895240245169, 0.555646820087942, 0.5585215651768678, 0.5613963083075302, 0.5527720785238904, 0.5581108852088819, 0.5535934246785832, 0.5544147878212116, 0.5556468185213312, 0.5560574911703074, 0.5515400444947229, 0.5552361373049522, 0.5552361353466888, 0.5527720702257489, 0.5564681747366026, 0.5531827553586549, 0.5552361430573513, 0.5618069770399795, 0.5568788545087623, 0.5605749461440335, 0.5564681759115607, 0.5577002010551076, 0.5548254634810179, 0.5605749513089534, 0.5622176604104483, 0.5593429119190396, 0.5589322406408478, 0.5601642665677002, 0.5605749524839115, 0.5638603726451646, 0.5585215576620317, 0.5634496882955641, 0.5585215612603409, 0.5618069851422947, 0.5618069861214263, 0.5572895267660858, 0.5655030783441767, 0.5597535978352509, 0.5613963000093887, 0.5589322410325005, 0.5613963075242249, 0.5585215582495108, 0.5622176633478435, 0.5679671411152003, 0.5605749453607282, 0.560985629906155, 0.5568788531379778, 0.5630390187798094, 0.5556468216545528, 0.5626283401826079, 0.5626283324719454, 0.5626283403784342, 0.5634496898621749, 0.5613963083075302, 0.5671457943730286, 0.560164268134311, 0.5556468191088103, 0.5642710441191828, 0.5618069833798575, 0.567145795156334, 0.5679671420943321, 0.5577002083741174, 0.5704312097610145, 0.5683778274230644, 0.5618069770399795, 0.5696098517833059, 0.5683778270314117, 0.5691991818758986, 0.5708418937189623, 0.5737166370454511, 0.5720739196458147, 0.5720739248107346, 0.5683778260522799, 0.5733059505417606, 0.5757700164460059, 0.571663243202703, 0.5778234074737502, 0.5741273083236428, 0.5700205384828226, 0.5770020579900096, 0.5761807009914328, 0.5733059550457666, 0.5786447617307581, 0.5679671440525955, 0.5638603653261549, 0.5691991822675513, 0.565913753416504, 0.5704312155134135, 0.5761807005997801, 0.5749486612098662, 0.5712525624514115, 0.5843942470129511, 0.5786447591850156, 0.583162214942047, 0.5848049317542042, 0.5823408668290908, 0.5839835765180647, 0.5893223796047469, 0.5745379901275008, 0.5757700214150995, 0.5811088282224823, 0.5856262794755078, 0.586036957289404, 0.5868583154629388, 0.5946611917728761, 0.5913757667159643, 0.5926077997660001, 0.5880903481213219, 0.5942505108257583, 0.5889117062948568, 0.5995893200564923, 0.5926078032908744, 0.5905544103048666, 0.5954825438757941, 0.589322386532104, 0.5930184805172914, 0.5950718664535507, 0.5930184773840699, 0.5999999980662148, 0.592197123127062, 0.5938398335993412, 0.5946611872688701, 0.6004106760759373, 0.5930184783632017, 0.603696095453885, 0.5999999996328256, 0.6049281288955736, 0.604517454802378, 0.5909650889020681, 0.6012320311162506, 0.6036960970204959, 0.6024640671036816, 0.6004106737260212, 0.5913757659326588, 0.6123203238667404, 0.5983572915104625, 0.6008213529107017, 0.5979466086050813, 0.597535934707712, 0.6057494841317131, 0.6061601637080465, 0.6069815191400124, 0.6127310093178642, 0.6032854184232943, 0.6020533853732585, 0.6069815239132796, 0.6008213534981808, 0.6020533902689172, 0.6053388074927751, 0.6024640651454182, 0.6082135502317848, 0.6090349066428825, 0.6073921951914715, 0.6135523598541714, 0.6106776175068145, 0.6102669377346548, 0.6090349060554034, 0.6045174536274199, 0.6114989719596486, 0.6057494853066713, 0.608213549252653, 0.6078028793452457, 0.599999996499604, 0.6036960985871066, 0.6098562616831957, 0.6164271078804925, 0.6008213560439233, 0.6028747437426197, 0.60862422863316, 0.6098562614873694, 0.6110882929707945, 0.6176591381889593, 0.6106776155485509, 0.6045174518649827, 0.6020533851774321, 0.6012320303329453, 0.6073921969539087, 0.6065708411302899, 0.6119096511443293]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
