{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf49.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 21:00:28 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '1Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['yd', 'eo', 'my', 'by', 'ek', 'eg', 'sk', 'eb', 'ib', 'sg', 'aa', 'mb', 'ce', 'ds', 'ck'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001E40356E278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001E45D0B7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7120, Accuracy:0.0891, Validation Loss:2.7068, Validation Accuracy:0.0887\n",
    "Epoch #2: Loss:2.7045, Accuracy:0.0768, Validation Loss:2.6998, Validation Accuracy:0.0788\n",
    "Epoch #3: Loss:2.6975, Accuracy:0.0784, Validation Loss:2.6940, Validation Accuracy:0.0788\n",
    "Epoch #4: Loss:2.6923, Accuracy:0.0784, Validation Loss:2.6885, Validation Accuracy:0.0788\n",
    "Epoch #5: Loss:2.6867, Accuracy:0.0784, Validation Loss:2.6833, Validation Accuracy:0.0854\n",
    "Epoch #6: Loss:2.6815, Accuracy:0.0850, Validation Loss:2.6785, Validation Accuracy:0.0854\n",
    "Epoch #7: Loss:2.6769, Accuracy:0.0850, Validation Loss:2.6743, Validation Accuracy:0.0854\n",
    "Epoch #8: Loss:2.6732, Accuracy:0.0850, Validation Loss:2.6706, Validation Accuracy:0.0854\n",
    "Epoch #9: Loss:2.6698, Accuracy:0.0850, Validation Loss:2.6675, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6668, Accuracy:0.1023, Validation Loss:2.6654, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6649, Accuracy:0.1023, Validation Loss:2.6638, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6633, Accuracy:0.1023, Validation Loss:2.6626, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6625, Accuracy:0.1023, Validation Loss:2.6616, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6616, Accuracy:0.1023, Validation Loss:2.6607, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6608, Accuracy:0.1023, Validation Loss:2.6598, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6598, Accuracy:0.1023, Validation Loss:2.6587, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6588, Accuracy:0.1023, Validation Loss:2.6573, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6574, Accuracy:0.1023, Validation Loss:2.6551, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6549, Accuracy:0.1023, Validation Loss:2.6511, Validation Accuracy:0.1018\n",
    "Epoch #20: Loss:2.6504, Accuracy:0.1023, Validation Loss:2.6437, Validation Accuracy:0.1018\n",
    "Epoch #21: Loss:2.6419, Accuracy:0.1023, Validation Loss:2.6302, Validation Accuracy:0.1051\n",
    "Epoch #22: Loss:2.6267, Accuracy:0.1125, Validation Loss:2.6067, Validation Accuracy:0.1314\n",
    "Epoch #23: Loss:2.6021, Accuracy:0.1462, Validation Loss:2.5729, Validation Accuracy:0.1642\n",
    "Epoch #24: Loss:2.5694, Accuracy:0.1544, Validation Loss:2.5354, Validation Accuracy:0.1773\n",
    "Epoch #25: Loss:2.5377, Accuracy:0.1659, Validation Loss:2.5080, Validation Accuracy:0.1823\n",
    "Epoch #26: Loss:2.5159, Accuracy:0.1651, Validation Loss:2.4841, Validation Accuracy:0.1741\n",
    "Epoch #27: Loss:2.4954, Accuracy:0.1630, Validation Loss:2.4639, Validation Accuracy:0.1790\n",
    "Epoch #28: Loss:2.4755, Accuracy:0.1688, Validation Loss:2.4447, Validation Accuracy:0.1773\n",
    "Epoch #29: Loss:2.4581, Accuracy:0.1696, Validation Loss:2.4245, Validation Accuracy:0.1790\n",
    "Epoch #30: Loss:2.4393, Accuracy:0.1713, Validation Loss:2.4051, Validation Accuracy:0.1724\n",
    "Epoch #31: Loss:2.4220, Accuracy:0.1725, Validation Loss:2.3861, Validation Accuracy:0.1790\n",
    "Epoch #32: Loss:2.4034, Accuracy:0.1791, Validation Loss:2.3702, Validation Accuracy:0.1970\n",
    "Epoch #33: Loss:2.3908, Accuracy:0.1807, Validation Loss:2.3673, Validation Accuracy:0.1905\n",
    "Epoch #34: Loss:2.3765, Accuracy:0.1803, Validation Loss:2.3418, Validation Accuracy:0.2053\n",
    "Epoch #35: Loss:2.3608, Accuracy:0.1811, Validation Loss:2.3290, Validation Accuracy:0.1938\n",
    "Epoch #36: Loss:2.3443, Accuracy:0.1943, Validation Loss:2.3158, Validation Accuracy:0.2118\n",
    "Epoch #37: Loss:2.3353, Accuracy:0.2041, Validation Loss:2.3081, Validation Accuracy:0.2217\n",
    "Epoch #38: Loss:2.3289, Accuracy:0.2123, Validation Loss:2.3183, Validation Accuracy:0.2069\n",
    "Epoch #39: Loss:2.3209, Accuracy:0.2115, Validation Loss:2.2887, Validation Accuracy:0.2151\n",
    "Epoch #40: Loss:2.3037, Accuracy:0.2156, Validation Loss:2.2813, Validation Accuracy:0.2217\n",
    "Epoch #41: Loss:2.2965, Accuracy:0.2152, Validation Loss:2.2700, Validation Accuracy:0.2151\n",
    "Epoch #42: Loss:2.2879, Accuracy:0.2148, Validation Loss:2.2611, Validation Accuracy:0.2250\n",
    "Epoch #43: Loss:2.2794, Accuracy:0.2189, Validation Loss:2.2542, Validation Accuracy:0.2233\n",
    "Epoch #44: Loss:2.2716, Accuracy:0.2234, Validation Loss:2.2586, Validation Accuracy:0.2217\n",
    "Epoch #45: Loss:2.2690, Accuracy:0.2222, Validation Loss:2.2431, Validation Accuracy:0.2282\n",
    "Epoch #46: Loss:2.2589, Accuracy:0.2246, Validation Loss:2.2373, Validation Accuracy:0.2217\n",
    "Epoch #47: Loss:2.2556, Accuracy:0.2181, Validation Loss:2.2349, Validation Accuracy:0.2233\n",
    "Epoch #48: Loss:2.2472, Accuracy:0.2300, Validation Loss:2.2295, Validation Accuracy:0.2167\n",
    "Epoch #49: Loss:2.2384, Accuracy:0.2292, Validation Loss:2.2260, Validation Accuracy:0.2250\n",
    "Epoch #50: Loss:2.2371, Accuracy:0.2275, Validation Loss:2.2237, Validation Accuracy:0.2381\n",
    "Epoch #51: Loss:2.2316, Accuracy:0.2337, Validation Loss:2.2159, Validation Accuracy:0.2184\n",
    "Epoch #52: Loss:2.2246, Accuracy:0.2320, Validation Loss:2.2152, Validation Accuracy:0.2200\n",
    "Epoch #53: Loss:2.2216, Accuracy:0.2275, Validation Loss:2.2070, Validation Accuracy:0.2282\n",
    "Epoch #54: Loss:2.2134, Accuracy:0.2394, Validation Loss:2.2040, Validation Accuracy:0.2250\n",
    "Epoch #55: Loss:2.2086, Accuracy:0.2353, Validation Loss:2.2030, Validation Accuracy:0.2184\n",
    "Epoch #56: Loss:2.2068, Accuracy:0.2345, Validation Loss:2.1952, Validation Accuracy:0.2250\n",
    "Epoch #57: Loss:2.1999, Accuracy:0.2370, Validation Loss:2.1940, Validation Accuracy:0.2447\n",
    "Epoch #58: Loss:2.1959, Accuracy:0.2419, Validation Loss:2.1883, Validation Accuracy:0.2299\n",
    "Epoch #59: Loss:2.1907, Accuracy:0.2423, Validation Loss:2.1866, Validation Accuracy:0.2282\n",
    "Epoch #60: Loss:2.1884, Accuracy:0.2407, Validation Loss:2.1910, Validation Accuracy:0.2217\n",
    "Epoch #61: Loss:2.1920, Accuracy:0.2415, Validation Loss:2.2201, Validation Accuracy:0.2217\n",
    "Epoch #62: Loss:2.2006, Accuracy:0.2394, Validation Loss:2.1808, Validation Accuracy:0.2250\n",
    "Epoch #63: Loss:2.1893, Accuracy:0.2304, Validation Loss:2.1842, Validation Accuracy:0.2381\n",
    "Epoch #64: Loss:2.1810, Accuracy:0.2329, Validation Loss:2.1810, Validation Accuracy:0.2381\n",
    "Epoch #65: Loss:2.1720, Accuracy:0.2448, Validation Loss:2.1785, Validation Accuracy:0.2266\n",
    "Epoch #66: Loss:2.1711, Accuracy:0.2427, Validation Loss:2.1692, Validation Accuracy:0.2479\n",
    "Epoch #67: Loss:2.1751, Accuracy:0.2423, Validation Loss:2.1740, Validation Accuracy:0.2397\n",
    "Epoch #68: Loss:2.1687, Accuracy:0.2452, Validation Loss:2.1637, Validation Accuracy:0.2348\n",
    "Epoch #69: Loss:2.1612, Accuracy:0.2464, Validation Loss:2.1783, Validation Accuracy:0.2217\n",
    "Epoch #70: Loss:2.1624, Accuracy:0.2398, Validation Loss:2.1639, Validation Accuracy:0.2414\n",
    "Epoch #71: Loss:2.1622, Accuracy:0.2407, Validation Loss:2.1674, Validation Accuracy:0.2365\n",
    "Epoch #72: Loss:2.1581, Accuracy:0.2439, Validation Loss:2.1607, Validation Accuracy:0.2315\n",
    "Epoch #73: Loss:2.1471, Accuracy:0.2489, Validation Loss:2.1513, Validation Accuracy:0.2348\n",
    "Epoch #74: Loss:2.1444, Accuracy:0.2439, Validation Loss:2.1468, Validation Accuracy:0.2414\n",
    "Epoch #75: Loss:2.1407, Accuracy:0.2476, Validation Loss:2.1496, Validation Accuracy:0.2315\n",
    "Epoch #76: Loss:2.1407, Accuracy:0.2431, Validation Loss:2.1447, Validation Accuracy:0.2282\n",
    "Epoch #77: Loss:2.1353, Accuracy:0.2460, Validation Loss:2.1462, Validation Accuracy:0.2447\n",
    "Epoch #78: Loss:2.1309, Accuracy:0.2509, Validation Loss:2.1365, Validation Accuracy:0.2299\n",
    "Epoch #79: Loss:2.1271, Accuracy:0.2517, Validation Loss:2.1491, Validation Accuracy:0.2447\n",
    "Epoch #80: Loss:2.1271, Accuracy:0.2444, Validation Loss:2.1301, Validation Accuracy:0.2430\n",
    "Epoch #81: Loss:2.1210, Accuracy:0.2554, Validation Loss:2.1286, Validation Accuracy:0.2447\n",
    "Epoch #82: Loss:2.1160, Accuracy:0.2559, Validation Loss:2.1347, Validation Accuracy:0.2414\n",
    "Epoch #83: Loss:2.1111, Accuracy:0.2616, Validation Loss:2.1187, Validation Accuracy:0.2463\n",
    "Epoch #84: Loss:2.1025, Accuracy:0.2608, Validation Loss:2.1260, Validation Accuracy:0.2644\n",
    "Epoch #85: Loss:2.1148, Accuracy:0.2575, Validation Loss:2.1191, Validation Accuracy:0.2512\n",
    "Epoch #86: Loss:2.1037, Accuracy:0.2674, Validation Loss:2.1327, Validation Accuracy:0.2397\n",
    "Epoch #87: Loss:2.1114, Accuracy:0.2604, Validation Loss:2.1286, Validation Accuracy:0.2644\n",
    "Epoch #88: Loss:2.1071, Accuracy:0.2604, Validation Loss:2.1164, Validation Accuracy:0.2529\n",
    "Epoch #89: Loss:2.0961, Accuracy:0.2624, Validation Loss:2.1378, Validation Accuracy:0.2430\n",
    "Epoch #90: Loss:2.0944, Accuracy:0.2612, Validation Loss:2.1046, Validation Accuracy:0.2594\n",
    "Epoch #91: Loss:2.0778, Accuracy:0.2665, Validation Loss:2.0924, Validation Accuracy:0.2545\n",
    "Epoch #92: Loss:2.0720, Accuracy:0.2686, Validation Loss:2.0944, Validation Accuracy:0.2644\n",
    "Epoch #93: Loss:2.0682, Accuracy:0.2743, Validation Loss:2.0930, Validation Accuracy:0.2479\n",
    "Epoch #94: Loss:2.0674, Accuracy:0.2780, Validation Loss:2.1019, Validation Accuracy:0.2726\n",
    "Epoch #95: Loss:2.0679, Accuracy:0.2719, Validation Loss:2.0813, Validation Accuracy:0.2611\n",
    "Epoch #96: Loss:2.0553, Accuracy:0.2739, Validation Loss:2.0799, Validation Accuracy:0.2677\n",
    "Epoch #97: Loss:2.0502, Accuracy:0.2715, Validation Loss:2.0750, Validation Accuracy:0.2693\n",
    "Epoch #98: Loss:2.0503, Accuracy:0.2768, Validation Loss:2.0812, Validation Accuracy:0.2611\n",
    "Epoch #99: Loss:2.0429, Accuracy:0.2735, Validation Loss:2.0828, Validation Accuracy:0.2759\n",
    "Epoch #100: Loss:2.0470, Accuracy:0.2789, Validation Loss:2.0768, Validation Accuracy:0.2660\n",
    "Epoch #101: Loss:2.0390, Accuracy:0.2871, Validation Loss:2.0695, Validation Accuracy:0.2594\n",
    "Epoch #102: Loss:2.0385, Accuracy:0.2825, Validation Loss:2.0793, Validation Accuracy:0.2759\n",
    "Epoch #103: Loss:2.0354, Accuracy:0.2825, Validation Loss:2.0822, Validation Accuracy:0.2611\n",
    "Epoch #104: Loss:2.0334, Accuracy:0.2862, Validation Loss:2.0556, Validation Accuracy:0.2709\n",
    "Epoch #105: Loss:2.0237, Accuracy:0.2936, Validation Loss:2.0587, Validation Accuracy:0.2709\n",
    "Epoch #106: Loss:2.0174, Accuracy:0.2957, Validation Loss:2.0552, Validation Accuracy:0.2677\n",
    "Epoch #107: Loss:2.0108, Accuracy:0.2998, Validation Loss:2.0497, Validation Accuracy:0.2824\n",
    "Epoch #108: Loss:2.0028, Accuracy:0.3018, Validation Loss:2.0511, Validation Accuracy:0.2841\n",
    "Epoch #109: Loss:2.0053, Accuracy:0.2945, Validation Loss:2.0483, Validation Accuracy:0.2890\n",
    "Epoch #110: Loss:1.9992, Accuracy:0.3043, Validation Loss:2.0528, Validation Accuracy:0.2759\n",
    "Epoch #111: Loss:1.9957, Accuracy:0.3072, Validation Loss:2.0476, Validation Accuracy:0.2742\n",
    "Epoch #112: Loss:1.9896, Accuracy:0.3121, Validation Loss:2.0322, Validation Accuracy:0.2972\n",
    "Epoch #113: Loss:1.9838, Accuracy:0.3146, Validation Loss:2.0413, Validation Accuracy:0.2742\n",
    "Epoch #114: Loss:1.9806, Accuracy:0.3150, Validation Loss:2.0267, Validation Accuracy:0.2906\n",
    "Epoch #115: Loss:1.9759, Accuracy:0.3146, Validation Loss:2.0742, Validation Accuracy:0.2709\n",
    "Epoch #116: Loss:1.9946, Accuracy:0.3084, Validation Loss:2.0230, Validation Accuracy:0.2906\n",
    "Epoch #117: Loss:1.9748, Accuracy:0.3080, Validation Loss:2.0335, Validation Accuracy:0.2956\n",
    "Epoch #118: Loss:1.9766, Accuracy:0.3335, Validation Loss:2.0017, Validation Accuracy:0.3038\n",
    "Epoch #119: Loss:1.9561, Accuracy:0.3261, Validation Loss:2.0224, Validation Accuracy:0.2841\n",
    "Epoch #120: Loss:1.9469, Accuracy:0.3306, Validation Loss:1.9908, Validation Accuracy:0.3251\n",
    "Epoch #121: Loss:1.9441, Accuracy:0.3216, Validation Loss:1.9913, Validation Accuracy:0.3054\n",
    "Epoch #122: Loss:1.9346, Accuracy:0.3355, Validation Loss:1.9837, Validation Accuracy:0.3103\n",
    "Epoch #123: Loss:1.9335, Accuracy:0.3302, Validation Loss:1.9819, Validation Accuracy:0.3038\n",
    "Epoch #124: Loss:1.9248, Accuracy:0.3326, Validation Loss:1.9968, Validation Accuracy:0.3071\n",
    "Epoch #125: Loss:1.9342, Accuracy:0.3331, Validation Loss:1.9785, Validation Accuracy:0.3186\n",
    "Epoch #126: Loss:1.9254, Accuracy:0.3331, Validation Loss:1.9741, Validation Accuracy:0.3103\n",
    "Epoch #127: Loss:1.9237, Accuracy:0.3318, Validation Loss:1.9839, Validation Accuracy:0.3186\n",
    "Epoch #128: Loss:1.9151, Accuracy:0.3396, Validation Loss:1.9690, Validation Accuracy:0.3186\n",
    "Epoch #129: Loss:1.9054, Accuracy:0.3470, Validation Loss:1.9723, Validation Accuracy:0.3169\n",
    "Epoch #130: Loss:1.9095, Accuracy:0.3376, Validation Loss:1.9720, Validation Accuracy:0.3103\n",
    "Epoch #131: Loss:1.9072, Accuracy:0.3351, Validation Loss:1.9716, Validation Accuracy:0.2989\n",
    "Epoch #132: Loss:1.8967, Accuracy:0.3520, Validation Loss:1.9606, Validation Accuracy:0.3169\n",
    "Epoch #133: Loss:1.8952, Accuracy:0.3483, Validation Loss:1.9494, Validation Accuracy:0.3169\n",
    "Epoch #134: Loss:1.8868, Accuracy:0.3470, Validation Loss:1.9525, Validation Accuracy:0.3202\n",
    "Epoch #135: Loss:1.8826, Accuracy:0.3503, Validation Loss:1.9400, Validation Accuracy:0.3333\n",
    "Epoch #136: Loss:1.8824, Accuracy:0.3520, Validation Loss:1.9510, Validation Accuracy:0.3235\n",
    "Epoch #137: Loss:1.8818, Accuracy:0.3552, Validation Loss:1.9797, Validation Accuracy:0.2956\n",
    "Epoch #138: Loss:1.8899, Accuracy:0.3454, Validation Loss:1.9729, Validation Accuracy:0.3103\n",
    "Epoch #139: Loss:1.8943, Accuracy:0.3491, Validation Loss:1.9302, Validation Accuracy:0.3317\n",
    "Epoch #140: Loss:1.8875, Accuracy:0.3425, Validation Loss:1.9762, Validation Accuracy:0.3071\n",
    "Epoch #141: Loss:1.8834, Accuracy:0.3446, Validation Loss:1.9425, Validation Accuracy:0.3333\n",
    "Epoch #142: Loss:1.8890, Accuracy:0.3470, Validation Loss:1.9330, Validation Accuracy:0.3218\n",
    "Epoch #143: Loss:1.8663, Accuracy:0.3503, Validation Loss:1.9222, Validation Accuracy:0.3251\n",
    "Epoch #144: Loss:1.8571, Accuracy:0.3589, Validation Loss:1.9190, Validation Accuracy:0.3268\n",
    "Epoch #145: Loss:1.8534, Accuracy:0.3593, Validation Loss:1.9174, Validation Accuracy:0.3333\n",
    "Epoch #146: Loss:1.8546, Accuracy:0.3602, Validation Loss:1.9246, Validation Accuracy:0.3333\n",
    "Epoch #147: Loss:1.8541, Accuracy:0.3647, Validation Loss:1.9123, Validation Accuracy:0.3350\n",
    "Epoch #148: Loss:1.8470, Accuracy:0.3577, Validation Loss:1.9205, Validation Accuracy:0.3300\n",
    "Epoch #149: Loss:1.8500, Accuracy:0.3614, Validation Loss:1.9219, Validation Accuracy:0.3448\n",
    "Epoch #150: Loss:1.8499, Accuracy:0.3569, Validation Loss:1.9145, Validation Accuracy:0.3383\n",
    "Epoch #151: Loss:1.8419, Accuracy:0.3610, Validation Loss:1.9306, Validation Accuracy:0.3366\n",
    "Epoch #152: Loss:1.8571, Accuracy:0.3569, Validation Loss:1.9316, Validation Accuracy:0.3300\n",
    "Epoch #153: Loss:1.8401, Accuracy:0.3630, Validation Loss:1.9166, Validation Accuracy:0.3284\n",
    "Epoch #154: Loss:1.8378, Accuracy:0.3626, Validation Loss:1.9165, Validation Accuracy:0.3350\n",
    "Epoch #155: Loss:1.8401, Accuracy:0.3684, Validation Loss:1.9166, Validation Accuracy:0.3366\n",
    "Epoch #156: Loss:1.8424, Accuracy:0.3655, Validation Loss:1.9513, Validation Accuracy:0.3218\n",
    "Epoch #157: Loss:1.8447, Accuracy:0.3622, Validation Loss:1.9067, Validation Accuracy:0.3383\n",
    "Epoch #158: Loss:1.8322, Accuracy:0.3614, Validation Loss:1.9177, Validation Accuracy:0.3399\n",
    "Epoch #159: Loss:1.8298, Accuracy:0.3659, Validation Loss:1.9060, Validation Accuracy:0.3350\n",
    "Epoch #160: Loss:1.8264, Accuracy:0.3643, Validation Loss:1.8975, Validation Accuracy:0.3399\n",
    "Epoch #161: Loss:1.8198, Accuracy:0.3729, Validation Loss:1.9216, Validation Accuracy:0.3415\n",
    "Epoch #162: Loss:1.8310, Accuracy:0.3618, Validation Loss:1.9036, Validation Accuracy:0.3432\n",
    "Epoch #163: Loss:1.8252, Accuracy:0.3676, Validation Loss:1.8914, Validation Accuracy:0.3432\n",
    "Epoch #164: Loss:1.8130, Accuracy:0.3717, Validation Loss:1.9055, Validation Accuracy:0.3333\n",
    "Epoch #165: Loss:1.8127, Accuracy:0.3708, Validation Loss:1.8946, Validation Accuracy:0.3547\n",
    "Epoch #166: Loss:1.8175, Accuracy:0.3741, Validation Loss:1.9006, Validation Accuracy:0.3465\n",
    "Epoch #167: Loss:1.8093, Accuracy:0.3725, Validation Loss:1.9001, Validation Accuracy:0.3530\n",
    "Epoch #168: Loss:1.8135, Accuracy:0.3704, Validation Loss:1.8863, Validation Accuracy:0.3498\n",
    "Epoch #169: Loss:1.8138, Accuracy:0.3717, Validation Loss:1.9203, Validation Accuracy:0.3300\n",
    "Epoch #170: Loss:1.8144, Accuracy:0.3721, Validation Loss:1.9538, Validation Accuracy:0.3268\n",
    "Epoch #171: Loss:1.8349, Accuracy:0.3593, Validation Loss:1.9027, Validation Accuracy:0.3366\n",
    "Epoch #172: Loss:1.8134, Accuracy:0.3717, Validation Loss:1.8891, Validation Accuracy:0.3465\n",
    "Epoch #173: Loss:1.8178, Accuracy:0.3688, Validation Loss:1.8837, Validation Accuracy:0.3481\n",
    "Epoch #174: Loss:1.8053, Accuracy:0.3729, Validation Loss:1.8864, Validation Accuracy:0.3498\n",
    "Epoch #175: Loss:1.8092, Accuracy:0.3663, Validation Loss:1.8819, Validation Accuracy:0.3432\n",
    "Epoch #176: Loss:1.7995, Accuracy:0.3766, Validation Loss:1.8850, Validation Accuracy:0.3465\n",
    "Epoch #177: Loss:1.7979, Accuracy:0.3770, Validation Loss:1.8844, Validation Accuracy:0.3481\n",
    "Epoch #178: Loss:1.7977, Accuracy:0.3778, Validation Loss:1.8820, Validation Accuracy:0.3498\n",
    "Epoch #179: Loss:1.7908, Accuracy:0.3799, Validation Loss:1.8773, Validation Accuracy:0.3481\n",
    "Epoch #180: Loss:1.7897, Accuracy:0.3807, Validation Loss:1.8812, Validation Accuracy:0.3563\n",
    "Epoch #181: Loss:1.7960, Accuracy:0.3708, Validation Loss:1.8854, Validation Accuracy:0.3530\n",
    "Epoch #182: Loss:1.7934, Accuracy:0.3745, Validation Loss:1.8833, Validation Accuracy:0.3432\n",
    "Epoch #183: Loss:1.7867, Accuracy:0.3819, Validation Loss:1.8893, Validation Accuracy:0.3415\n",
    "Epoch #184: Loss:1.7954, Accuracy:0.3770, Validation Loss:1.9317, Validation Accuracy:0.3333\n",
    "Epoch #185: Loss:1.8040, Accuracy:0.3758, Validation Loss:1.8775, Validation Accuracy:0.3465\n",
    "Epoch #186: Loss:1.7881, Accuracy:0.3803, Validation Loss:1.8703, Validation Accuracy:0.3514\n",
    "Epoch #187: Loss:1.7894, Accuracy:0.3774, Validation Loss:1.8845, Validation Accuracy:0.3563\n",
    "Epoch #188: Loss:1.7883, Accuracy:0.3803, Validation Loss:1.8703, Validation Accuracy:0.3563\n",
    "Epoch #189: Loss:1.7801, Accuracy:0.3819, Validation Loss:1.8631, Validation Accuracy:0.3498\n",
    "Epoch #190: Loss:1.7735, Accuracy:0.3828, Validation Loss:1.8674, Validation Accuracy:0.3580\n",
    "Epoch #191: Loss:1.7755, Accuracy:0.3832, Validation Loss:1.8653, Validation Accuracy:0.3596\n",
    "Epoch #192: Loss:1.7794, Accuracy:0.3836, Validation Loss:1.9079, Validation Accuracy:0.3629\n",
    "Epoch #193: Loss:1.8012, Accuracy:0.3745, Validation Loss:1.8696, Validation Accuracy:0.3629\n",
    "Epoch #194: Loss:1.7883, Accuracy:0.3713, Validation Loss:1.8846, Validation Accuracy:0.3645\n",
    "Epoch #195: Loss:1.7901, Accuracy:0.3786, Validation Loss:1.8693, Validation Accuracy:0.3580\n",
    "Epoch #196: Loss:1.7934, Accuracy:0.3791, Validation Loss:1.8670, Validation Accuracy:0.3596\n",
    "Epoch #197: Loss:1.7898, Accuracy:0.3774, Validation Loss:1.9289, Validation Accuracy:0.3383\n",
    "Epoch #198: Loss:1.7890, Accuracy:0.3684, Validation Loss:1.8850, Validation Accuracy:0.3498\n",
    "Epoch #199: Loss:1.7752, Accuracy:0.3823, Validation Loss:1.8628, Validation Accuracy:0.3530\n",
    "Epoch #200: Loss:1.7731, Accuracy:0.3791, Validation Loss:1.8625, Validation Accuracy:0.3596\n",
    "Epoch #201: Loss:1.7619, Accuracy:0.3901, Validation Loss:1.8585, Validation Accuracy:0.3547\n",
    "Epoch #202: Loss:1.7677, Accuracy:0.3762, Validation Loss:1.8579, Validation Accuracy:0.3498\n",
    "Epoch #203: Loss:1.7588, Accuracy:0.3856, Validation Loss:1.8773, Validation Accuracy:0.3580\n",
    "Epoch #204: Loss:1.7641, Accuracy:0.3873, Validation Loss:1.8827, Validation Accuracy:0.3514\n",
    "Epoch #205: Loss:1.7748, Accuracy:0.3864, Validation Loss:1.8567, Validation Accuracy:0.3596\n",
    "Epoch #206: Loss:1.7752, Accuracy:0.3856, Validation Loss:1.8635, Validation Accuracy:0.3678\n",
    "Epoch #207: Loss:1.7641, Accuracy:0.3852, Validation Loss:1.8551, Validation Accuracy:0.3563\n",
    "Epoch #208: Loss:1.7596, Accuracy:0.3881, Validation Loss:1.8693, Validation Accuracy:0.3744\n",
    "Epoch #209: Loss:1.7711, Accuracy:0.3836, Validation Loss:1.8786, Validation Accuracy:0.3645\n",
    "Epoch #210: Loss:1.7739, Accuracy:0.3869, Validation Loss:1.8611, Validation Accuracy:0.3744\n",
    "Epoch #211: Loss:1.7647, Accuracy:0.3848, Validation Loss:1.8703, Validation Accuracy:0.3612\n",
    "Epoch #212: Loss:1.7598, Accuracy:0.3852, Validation Loss:1.8909, Validation Accuracy:0.3481\n",
    "Epoch #213: Loss:1.7674, Accuracy:0.3844, Validation Loss:1.8673, Validation Accuracy:0.3596\n",
    "Epoch #214: Loss:1.7550, Accuracy:0.3877, Validation Loss:1.8547, Validation Accuracy:0.3711\n",
    "Epoch #215: Loss:1.7520, Accuracy:0.3914, Validation Loss:1.8687, Validation Accuracy:0.3711\n",
    "Epoch #216: Loss:1.7603, Accuracy:0.3938, Validation Loss:1.8515, Validation Accuracy:0.3596\n",
    "Epoch #217: Loss:1.7512, Accuracy:0.3930, Validation Loss:1.8741, Validation Accuracy:0.3530\n",
    "Epoch #218: Loss:1.7516, Accuracy:0.3955, Validation Loss:1.8722, Validation Accuracy:0.3629\n",
    "Epoch #219: Loss:1.7474, Accuracy:0.3963, Validation Loss:1.8510, Validation Accuracy:0.3563\n",
    "Epoch #220: Loss:1.7499, Accuracy:0.3922, Validation Loss:1.8708, Validation Accuracy:0.3629\n",
    "Epoch #221: Loss:1.7439, Accuracy:0.3897, Validation Loss:1.8483, Validation Accuracy:0.3629\n",
    "Epoch #222: Loss:1.7460, Accuracy:0.3971, Validation Loss:1.8456, Validation Accuracy:0.3678\n",
    "Epoch #223: Loss:1.7389, Accuracy:0.3926, Validation Loss:1.8560, Validation Accuracy:0.3612\n",
    "Epoch #224: Loss:1.7402, Accuracy:0.3984, Validation Loss:1.8528, Validation Accuracy:0.3645\n",
    "Epoch #225: Loss:1.7363, Accuracy:0.3926, Validation Loss:1.8472, Validation Accuracy:0.3563\n",
    "Epoch #226: Loss:1.7366, Accuracy:0.4008, Validation Loss:1.8453, Validation Accuracy:0.3727\n",
    "Epoch #227: Loss:1.7346, Accuracy:0.3971, Validation Loss:1.8435, Validation Accuracy:0.3695\n",
    "Epoch #228: Loss:1.7328, Accuracy:0.4004, Validation Loss:1.8582, Validation Accuracy:0.3514\n",
    "Epoch #229: Loss:1.7386, Accuracy:0.3938, Validation Loss:1.8794, Validation Accuracy:0.3596\n",
    "Epoch #230: Loss:1.7491, Accuracy:0.3984, Validation Loss:1.8626, Validation Accuracy:0.3760\n",
    "Epoch #231: Loss:1.7534, Accuracy:0.3918, Validation Loss:1.8449, Validation Accuracy:0.3580\n",
    "Epoch #232: Loss:1.7333, Accuracy:0.4037, Validation Loss:1.8462, Validation Accuracy:0.3662\n",
    "Epoch #233: Loss:1.7316, Accuracy:0.3967, Validation Loss:1.8496, Validation Accuracy:0.3678\n",
    "Epoch #234: Loss:1.7299, Accuracy:0.3975, Validation Loss:1.8397, Validation Accuracy:0.3662\n",
    "Epoch #235: Loss:1.7294, Accuracy:0.3992, Validation Loss:1.8555, Validation Accuracy:0.3629\n",
    "Epoch #236: Loss:1.7354, Accuracy:0.3959, Validation Loss:1.8720, Validation Accuracy:0.3580\n",
    "Epoch #237: Loss:1.7302, Accuracy:0.3979, Validation Loss:1.8369, Validation Accuracy:0.3678\n",
    "Epoch #238: Loss:1.7205, Accuracy:0.4070, Validation Loss:1.8363, Validation Accuracy:0.3793\n",
    "Epoch #239: Loss:1.7195, Accuracy:0.4000, Validation Loss:1.8382, Validation Accuracy:0.3727\n",
    "Epoch #240: Loss:1.7225, Accuracy:0.4099, Validation Loss:1.8364, Validation Accuracy:0.3629\n",
    "Epoch #241: Loss:1.7218, Accuracy:0.4004, Validation Loss:1.8444, Validation Accuracy:0.3711\n",
    "Epoch #242: Loss:1.7266, Accuracy:0.4131, Validation Loss:1.8461, Validation Accuracy:0.3727\n",
    "Epoch #243: Loss:1.7296, Accuracy:0.4025, Validation Loss:1.8407, Validation Accuracy:0.3711\n",
    "Epoch #244: Loss:1.7169, Accuracy:0.4053, Validation Loss:1.8497, Validation Accuracy:0.3596\n",
    "Epoch #245: Loss:1.7271, Accuracy:0.4037, Validation Loss:1.8370, Validation Accuracy:0.3612\n",
    "Epoch #246: Loss:1.7265, Accuracy:0.4144, Validation Loss:1.8452, Validation Accuracy:0.3793\n",
    "Epoch #247: Loss:1.7271, Accuracy:0.4012, Validation Loss:1.8569, Validation Accuracy:0.3760\n",
    "Epoch #248: Loss:1.7345, Accuracy:0.3988, Validation Loss:1.8444, Validation Accuracy:0.3629\n",
    "Epoch #249: Loss:1.7201, Accuracy:0.4062, Validation Loss:1.8343, Validation Accuracy:0.3695\n",
    "Epoch #250: Loss:1.7389, Accuracy:0.3984, Validation Loss:1.8437, Validation Accuracy:0.3580\n",
    "Epoch #251: Loss:1.7243, Accuracy:0.4103, Validation Loss:1.8448, Validation Accuracy:0.3744\n",
    "Epoch #252: Loss:1.7120, Accuracy:0.4053, Validation Loss:1.8292, Validation Accuracy:0.3711\n",
    "Epoch #253: Loss:1.7078, Accuracy:0.4049, Validation Loss:1.8357, Validation Accuracy:0.3662\n",
    "Epoch #254: Loss:1.7147, Accuracy:0.4156, Validation Loss:1.8328, Validation Accuracy:0.3662\n",
    "Epoch #255: Loss:1.7074, Accuracy:0.4041, Validation Loss:1.8309, Validation Accuracy:0.3777\n",
    "Epoch #256: Loss:1.7065, Accuracy:0.4172, Validation Loss:1.8326, Validation Accuracy:0.3662\n",
    "Epoch #257: Loss:1.7121, Accuracy:0.4057, Validation Loss:1.8457, Validation Accuracy:0.3629\n",
    "Epoch #258: Loss:1.7274, Accuracy:0.4090, Validation Loss:1.8532, Validation Accuracy:0.3629\n",
    "Epoch #259: Loss:1.7468, Accuracy:0.3988, Validation Loss:1.8329, Validation Accuracy:0.3645\n",
    "Epoch #260: Loss:1.7191, Accuracy:0.4057, Validation Loss:1.8484, Validation Accuracy:0.3629\n",
    "Epoch #261: Loss:1.7316, Accuracy:0.3938, Validation Loss:1.8557, Validation Accuracy:0.3580\n",
    "Epoch #262: Loss:1.7190, Accuracy:0.4045, Validation Loss:1.8496, Validation Accuracy:0.3662\n",
    "Epoch #263: Loss:1.7041, Accuracy:0.4119, Validation Loss:1.8268, Validation Accuracy:0.3662\n",
    "Epoch #264: Loss:1.7098, Accuracy:0.4107, Validation Loss:1.8273, Validation Accuracy:0.3695\n",
    "Epoch #265: Loss:1.7034, Accuracy:0.4123, Validation Loss:1.8332, Validation Accuracy:0.3711\n",
    "Epoch #266: Loss:1.7051, Accuracy:0.4111, Validation Loss:1.8389, Validation Accuracy:0.3793\n",
    "Epoch #267: Loss:1.7085, Accuracy:0.4103, Validation Loss:1.8252, Validation Accuracy:0.3727\n",
    "Epoch #268: Loss:1.6927, Accuracy:0.4189, Validation Loss:1.8246, Validation Accuracy:0.3596\n",
    "Epoch #269: Loss:1.6935, Accuracy:0.4226, Validation Loss:1.8232, Validation Accuracy:0.3760\n",
    "Epoch #270: Loss:1.6951, Accuracy:0.4205, Validation Loss:1.8329, Validation Accuracy:0.3711\n",
    "Epoch #271: Loss:1.6987, Accuracy:0.4148, Validation Loss:1.8224, Validation Accuracy:0.3727\n",
    "Epoch #272: Loss:1.6915, Accuracy:0.4164, Validation Loss:1.8291, Validation Accuracy:0.3711\n",
    "Epoch #273: Loss:1.6920, Accuracy:0.4181, Validation Loss:1.8240, Validation Accuracy:0.3744\n",
    "Epoch #274: Loss:1.6875, Accuracy:0.4156, Validation Loss:1.8220, Validation Accuracy:0.3678\n",
    "Epoch #275: Loss:1.6867, Accuracy:0.4222, Validation Loss:1.8304, Validation Accuracy:0.3727\n",
    "Epoch #276: Loss:1.6871, Accuracy:0.4119, Validation Loss:1.8230, Validation Accuracy:0.3744\n",
    "Epoch #277: Loss:1.6873, Accuracy:0.4181, Validation Loss:1.8213, Validation Accuracy:0.3645\n",
    "Epoch #278: Loss:1.6873, Accuracy:0.4222, Validation Loss:1.8235, Validation Accuracy:0.3760\n",
    "Epoch #279: Loss:1.6831, Accuracy:0.4222, Validation Loss:1.8201, Validation Accuracy:0.3744\n",
    "Epoch #280: Loss:1.6857, Accuracy:0.4140, Validation Loss:1.8223, Validation Accuracy:0.3711\n",
    "Epoch #281: Loss:1.6865, Accuracy:0.4300, Validation Loss:1.8243, Validation Accuracy:0.3842\n",
    "Epoch #282: Loss:1.6880, Accuracy:0.4164, Validation Loss:1.8276, Validation Accuracy:0.3826\n",
    "Epoch #283: Loss:1.6924, Accuracy:0.4238, Validation Loss:1.8258, Validation Accuracy:0.3760\n",
    "Epoch #284: Loss:1.6926, Accuracy:0.4197, Validation Loss:1.8190, Validation Accuracy:0.3777\n",
    "Epoch #285: Loss:1.6895, Accuracy:0.4234, Validation Loss:1.8174, Validation Accuracy:0.3777\n",
    "Epoch #286: Loss:1.6838, Accuracy:0.4185, Validation Loss:1.8198, Validation Accuracy:0.3695\n",
    "Epoch #287: Loss:1.6941, Accuracy:0.4099, Validation Loss:1.8212, Validation Accuracy:0.3695\n",
    "Epoch #288: Loss:1.6994, Accuracy:0.4156, Validation Loss:1.8312, Validation Accuracy:0.3744\n",
    "Epoch #289: Loss:1.6847, Accuracy:0.4246, Validation Loss:1.8247, Validation Accuracy:0.3678\n",
    "Epoch #290: Loss:1.6937, Accuracy:0.4168, Validation Loss:1.8361, Validation Accuracy:0.3727\n",
    "Epoch #291: Loss:1.7013, Accuracy:0.4099, Validation Loss:1.8668, Validation Accuracy:0.3596\n",
    "Epoch #292: Loss:1.6992, Accuracy:0.4136, Validation Loss:1.8382, Validation Accuracy:0.3662\n",
    "Epoch #293: Loss:1.6889, Accuracy:0.4242, Validation Loss:1.8147, Validation Accuracy:0.3826\n",
    "Epoch #294: Loss:1.6783, Accuracy:0.4275, Validation Loss:1.8211, Validation Accuracy:0.3859\n",
    "Epoch #295: Loss:1.6821, Accuracy:0.4193, Validation Loss:1.8222, Validation Accuracy:0.3711\n",
    "Epoch #296: Loss:1.6816, Accuracy:0.4287, Validation Loss:1.8205, Validation Accuracy:0.3727\n",
    "Epoch #297: Loss:1.6779, Accuracy:0.4164, Validation Loss:1.8212, Validation Accuracy:0.3760\n",
    "Epoch #298: Loss:1.6775, Accuracy:0.4255, Validation Loss:1.8110, Validation Accuracy:0.3810\n",
    "Epoch #299: Loss:1.6785, Accuracy:0.4296, Validation Loss:1.8216, Validation Accuracy:0.3875\n",
    "Epoch #300: Loss:1.6747, Accuracy:0.4296, Validation Loss:1.8099, Validation Accuracy:0.3777\n",
    "\n",
    "Test:\n",
    "Test Loss:1.80991709, Accuracy:0.3777\n",
    "Labels: ['yd', 'eo', 'my', 'by', 'ek', 'eg', 'sk', 'eb', 'ib', 'sg', 'aa', 'mb', 'ce', 'ds', 'ck']\n",
    "Confusion Matrix:\n",
    "      yd  eo  my  by  ek  eg  sk  eb  ib  sg  aa  mb  ce  ds  ck\n",
    "t:yd  34   0   0   5   4   3   0   0   5   7   1   3   0   0   0\n",
    "t:eo   0  23   0   3   0   3   0   0   2   0   1   1   1   0   0\n",
    "t:my   1   0   2   0   4   0   0  10   0   0   0   1   0   2   0\n",
    "t:by   0   4   0  22   1   4   0   0   2   6   1   0   0   0   0\n",
    "t:ek   8   0   0   0  15   8   5   2   0   1   3   4   0   2   0\n",
    "t:eg   1   6   0   2   6  26   0   1   0   0   6   1   1   0   0\n",
    "t:sk   0   0   0   0  13   2   1  12   1   0   0   2   0   2   0\n",
    "t:eb   2   0   0   0   3   0   4  38   0   0   0   2   0   1   0\n",
    "t:ib  17   0   0   4   3   3   0   0  12   4   2   9   0   0   0\n",
    "t:sg  18   2   0  10   0   0   0   0   2  16   1   2   0   0   0\n",
    "t:aa   3   0   0   1   4  10   0   0   0   1  13   2   0   0   0\n",
    "t:mb   3   4   0   4  11   1   0   5   1   0   0  22   0   1   0\n",
    "t:ce   1   1   0   4   5   0   0   1   1   5   1   7   1   0   0\n",
    "t:ds   1   0   1   0   8   1   2  10   0   0   0   3   0   5   0\n",
    "t:ck   1   1   0   0   9   0   2   6   0   0   0   4   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.38      0.55      0.45        62\n",
    "          eo       0.56      0.68      0.61        34\n",
    "          my       0.67      0.10      0.17        20\n",
    "          by       0.40      0.55      0.46        40\n",
    "          ek       0.17      0.31      0.22        48\n",
    "          eg       0.43      0.52      0.47        50\n",
    "          sk       0.07      0.03      0.04        33\n",
    "          eb       0.45      0.76      0.56        50\n",
    "          ib       0.46      0.22      0.30        54\n",
    "          sg       0.40      0.31      0.35        51\n",
    "          aa       0.45      0.38      0.41        34\n",
    "          mb       0.35      0.42      0.38        52\n",
    "          ce       0.33      0.04      0.07        27\n",
    "          ds       0.38      0.16      0.23        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.37      0.34      0.32       609\n",
    "weighted avg       0.37      0.38      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 21:41:14 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.706820624998246, 2.6998491271571767, 2.6940128360867304, 2.688456975767765, 2.683260104534857, 2.678474505742391, 2.674279355846211, 2.6706102176252844, 2.6675321934454157, 2.665379534409747, 2.6637560397337614, 2.662570391186744, 2.661614982952625, 2.660712524783631, 2.6598055558447378, 2.658696950167075, 2.6572633968002495, 2.6550747009333717, 2.6510839943815334, 2.643732634671216, 2.6301915062276406, 2.606741832786397, 2.57294899685238, 2.535405097140859, 2.5079671087719144, 2.4840998907981837, 2.4638535518364364, 2.444746248827779, 2.4245283580178696, 2.405051573939707, 2.3860680653739643, 2.370232845957839, 2.3672658375331332, 2.341818585967391, 2.328988503157016, 2.31576641987893, 2.3081335449845137, 2.3183343982070146, 2.2886672579791942, 2.2813439095157317, 2.270022617380803, 2.2610670690270283, 2.254164320298995, 2.2586364491624003, 2.24308459590417, 2.2373186500593163, 2.2348846637556705, 2.229469763625823, 2.226037920010697, 2.22370367723537, 2.2158758170499, 2.2151821383897503, 2.2069870591750873, 2.2039546735572504, 2.2030377145275497, 2.1952011953238, 2.1939781240641776, 2.188271690472006, 2.186608431570244, 2.1909692487105947, 2.2201285162582773, 2.180775494411074, 2.1842339261999273, 2.1810266619245406, 2.1785236837828688, 2.169173386688107, 2.1740103687950345, 2.1636599417781985, 2.17833128742788, 2.163887606074266, 2.1674027415527695, 2.1606922474596497, 2.151298798951022, 2.1467559642979666, 2.1496178587081984, 2.1446581043437587, 2.146154157829598, 2.1365159094235775, 2.1491205226415877, 2.130076581034167, 2.1285857559229155, 2.1347415255404067, 2.118664244712867, 2.125997367555089, 2.1191421972315494, 2.132682696156118, 2.1285674380159927, 2.116371799572348, 2.137796046111384, 2.1045901148973036, 2.092411959699809, 2.0943682565673427, 2.0930303638596057, 2.1019014392188815, 2.081261031537612, 2.0798544695811905, 2.0750272426699183, 2.081223818664676, 2.0828303777917068, 2.0768103282439885, 2.0694623384961157, 2.0792774140149697, 2.082240836765183, 2.055645207466163, 2.0586636457928686, 2.0552217838995173, 2.049673350182269, 2.0510909929259853, 2.0483330584120476, 2.0528073295192373, 2.047592813745508, 2.0322489624931697, 2.0413254591436027, 2.0266915910153944, 2.07419256702041, 2.02296506438545, 2.0334581324619614, 2.0016934039753256, 2.022363997053826, 1.990795885596565, 1.9912509708764714, 1.9837407756517282, 1.9819245951124795, 1.9967742322505206, 1.978493723180298, 1.9740638499972465, 1.983903501812852, 1.9689531079654037, 1.972296299605534, 1.9719962838835317, 1.9715986841026394, 1.9605621629943597, 1.9493933634217737, 1.9525140420165163, 1.9400145933154378, 1.9510387726409486, 1.9796847294899826, 1.9728569252346144, 1.9301597727539113, 1.9762155395031757, 1.9424668685556046, 1.9330005547878972, 1.9222423204256005, 1.9190258488475005, 1.9173639339374986, 1.9245536556384835, 1.9123499046992787, 1.9205203334294711, 1.9218599199269988, 1.914526295192136, 1.9305869979028436, 1.9316389214229115, 1.9165625854078772, 1.9164995755663843, 1.916599046812073, 1.9513081991418046, 1.9066640451819634, 1.9177329955234121, 1.9060089415908839, 1.8975475633085654, 1.9216018639174588, 1.9035926971138013, 1.8914495291576792, 1.905485032422985, 1.894589402992737, 1.9005769902262195, 1.9001248203866392, 1.8863367014526342, 1.9202692667251737, 1.9537581403071462, 1.9027077051610586, 1.8891282011135457, 1.8836926249251968, 1.8863770681844751, 1.8818640448581214, 1.8849885385416216, 1.8844128851037112, 1.8820025856271754, 1.8772639897460812, 1.8812074688659317, 1.8854067697509365, 1.883301663477041, 1.8892615644019617, 1.9316657677855593, 1.8775093760983697, 1.870304719959378, 1.8844691986716635, 1.870330186508755, 1.8631045794839343, 1.8674249921134736, 1.8652507377962761, 1.907915838833513, 1.8696132923777664, 1.8845837270880763, 1.8692711920573795, 1.86702125471801, 1.928939578568407, 1.8849629827320868, 1.862800380866516, 1.862538096939989, 1.8585417597556153, 1.8578693199236014, 1.8772623264926604, 1.8826627359405919, 1.8566834384388915, 1.8634701464172263, 1.8550853672481717, 1.8692741264850634, 1.878621465271134, 1.8610665023033255, 1.8703337095445405, 1.8908828882552524, 1.867346552596695, 1.8547170181775523, 1.868735169150755, 1.8514734692565717, 1.8740808930498822, 1.8721722474043396, 1.850950276127394, 1.8707985470839126, 1.848341034550972, 1.8456243969536767, 1.8560102915724705, 1.852762008731197, 1.8471705244092518, 1.8452618241505865, 1.8434711626206322, 1.8581725963818028, 1.87942465322554, 1.8626343127346194, 1.8449451666943153, 1.8462237796955703, 1.8496063205800424, 1.8396819440406336, 1.8555144562901338, 1.8719708116966711, 1.836854509140666, 1.836294890820295, 1.8382478923045944, 1.8364221979244588, 1.8443563908387484, 1.8461025653801528, 1.8407107112051426, 1.849749232160634, 1.8369675252237931, 1.8452061173951098, 1.8568528638097452, 1.8443650073801552, 1.8342562217039036, 1.8437412311682364, 1.8448173958679726, 1.82923124870056, 1.8356619194419121, 1.8327908607930776, 1.8309264361173256, 1.832568514327502, 1.8457368778673495, 1.8532039385123793, 1.8328890794603696, 1.8483770666842783, 1.8557158530443565, 1.8496249168377203, 1.826808287983849, 1.8272728056743228, 1.8332201062360616, 1.8389459809254738, 1.8252068646435666, 1.8246085418660456, 1.8232419903838184, 1.8329294659625526, 1.822442508683416, 1.8290615578981848, 1.8239547983178952, 1.8219801516368472, 1.8303672846510688, 1.822956187775961, 1.8213110790268345, 1.8235409737416284, 1.8200526951960547, 1.8222954872206514, 1.8242680124069865, 1.8276325611058126, 1.8258156437787711, 1.81898678249522, 1.8174113041074405, 1.8197717535476183, 1.8211775233201402, 1.8312226161972447, 1.8246585162010882, 1.8360839923614352, 1.8667856396125455, 1.8381605007378339, 1.8147425238526318, 1.8211015026361876, 1.8221903048908377, 1.820467476774319, 1.8211531337650342, 1.8110420889846601, 1.821607651577403, 1.809917039276148], 'val_acc': [0.0886699504330632, 0.07881773358642175, 0.07881773358642175, 0.07881773358642175, 0.08538587738825575, 0.08538587738825575, 0.08538587738825575, 0.08538587738825575, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10509031177888363, 0.13136288997134551, 0.16420361176989545, 0.17733990106187233, 0.18226600924051062, 0.1740558285186639, 0.17898193699092113, 0.17733990096399938, 0.17898193699092113, 0.17241379249174216, 0.17898193689304814, 0.19704433436366334, 0.19047618976661138, 0.20525451498763705, 0.1937602622119468, 0.211822659780435, 0.22167487662707644, 0.2068965511124318, 0.2151067319321515, 0.22167487623558452, 0.21510673173640554, 0.224958948583047, 0.22331691245825225, 0.2216748763334575, 0.22824302093050947, 0.22167487643133046, 0.22331691245825225, 0.21674876795907325, 0.224958948583047, 0.23809523777715091, 0.21839080408386802, 0.2200328404044087, 0.22824302093050947, 0.224958948583047, 0.21839080408386802, 0.22495894877879294, 0.2446633822763299, 0.2298850571531772, 0.22824302093050947, 0.22167487652920345, 0.2216748763334575, 0.22495894877879294, 0.2380952378750239, 0.23809523748353198, 0.22660098480571472, 0.24794745472166535, 0.23973727370619968, 0.23481116552756143, 0.2216748763334575, 0.2413793099288674, 0.23645320155448318, 0.2315270933758449, 0.23481116542968844, 0.2413793100267404, 0.2315270933758449, 0.22824302093050947, 0.24466338256994882, 0.2298850570553042, 0.2446633821784569, 0.24302134605366216, 0.2446633821784569, 0.2413793100267404, 0.24630541840112463, 0.2643678156759939, 0.25123152677550886, 0.23973727380407267, 0.2643678158717398, 0.2528735626066847, 0.24302134615153514, 0.2594417073994826, 0.25451559912297134, 0.2643678158717398, 0.24794745452591938, 0.2725779966914595, 0.2610837434264044, 0.2676518881213293, 0.2692939241482511, 0.2610837433285314, 0.2758620667633752, 0.26600985170291563, 0.2594417070079907, 0.27586206686124815, 0.2610837433285314, 0.27093596027304584, 0.27093596056666475, 0.2676518878277104, 0.2824302111646812, 0.28407224956502275, 0.288998357841534, 0.275862068843176, 0.2742200326205083, 0.2972085384655077, 0.27422003281625423, 0.2906403918865279, 0.2709359585847369, 0.2906403940642017, 0.2955665003587851, 0.3037766810806318, 0.28407224946714976, 0.32512315119232843, 0.30541871740117255, 0.3103448257755568, 0.30377668098275884, 0.3070607535259673, 0.3185550063016575, 0.3103448255798108, 0.3185550063016575, 0.3185550064974035, 0.31691297017686276, 0.3103448257755568, 0.2988505726083746, 0.31691297017686276, 0.31691297017686276, 0.3201970426221982, 0.3333333316205562, 0.3234811147739147, 0.2955665004566581, 0.31034482587342976, 0.33169129530001545, 0.3070607533302213, 0.33333333171842916, 0.32183907864912, 0.3251231508008365, 0.3267651872192502, 0.33333333171842916, 0.3333333316205562, 0.33497536764747793, 0.3300492592730937, 0.3448275844941194, 0.3382594397991945, 0.33661740406589163, 0.3300492593709667, 0.32840722334404493, 0.3349753677453509, 0.33661740377227267, 0.321839078453374, 0.3382594400928134, 0.33990147621760813, 0.3349753677453509, 0.33990147602186216, 0.341543511950911, 0.3431855480757057, 0.3431855485650706, 0.3333333312290643, 0.3546798012428879, 0.3464696203252952, 0.35303776502022016, 0.34975369286850366, 0.3300492591752207, 0.3267651867298853, 0.33661740396801865, 0.3464696206189142, 0.34811165654796294, 0.3497536926727577, 0.3431855480757057, 0.3464696205210412, 0.34811165684158185, 0.3497536929663766, 0.34811165645008996, 0.3563218374655556, 0.3530377649223472, 0.3431855482714517, 0.3415435122445299, 0.3333333314248102, 0.3464696205210412, 0.3513957288954254, 0.35632183717193666, 0.35632183726980965, 0.3497536926727577, 0.3579638732967314, 0.35960590951939914, 0.3628899817689886, 0.36288998167111564, 0.3645320177959104, 0.3579638731988584, 0.3596059097151451, 0.3382594396034485, 0.3497536929663766, 0.3530377649223472, 0.35960590932365316, 0.35467980094926893, 0.3497536925748847, 0.35796387359035037, 0.3513957289932984, 0.35960590951939914, 0.3678160904369918, 0.35632183717193666, 0.3743842346425518, 0.36453201808952934, 0.3743842347404248, 0.36124794593781284, 0.34811165664583593, 0.35960590932365316, 0.3711001625887083, 0.37110016249083533, 0.3596059096172721, 0.35303776502022016, 0.3628899819647346, 0.35632183726980965, 0.3628899818668616, 0.36288998157324265, 0.36781609014337285, 0.3612479456441939, 0.36453201808952934, 0.3563218370740637, 0.37274219861563007, 0.3694581262681676, 0.3513957288954254, 0.3596059096172721, 0.37602627106096553, 0.3579638731988584, 0.3661740541164511, 0.36781609024124584, 0.3661740539207051, 0.3628899819647346, 0.3579638734924774, 0.36781609014337285, 0.37931034301693606, 0.37274219871350306, 0.36288998167111564, 0.37110016239296234, 0.3727421985177571, 0.37110016239296234, 0.35960590951939914, 0.3612479456441939, 0.3793103429190631, 0.3760262711588385, 0.36288998157324265, 0.3694581262681676, 0.3579638734924774, 0.3743842348382978, 0.37110016249083533, 0.3661740541164511, 0.3661740539207051, 0.3776683068921413, 0.3661740541164511, 0.3628899819647346, 0.3628899819647346, 0.36453201789378337, 0.3628899817689886, 0.35796387359035037, 0.3661740541164511, 0.3661740540185781, 0.3694581262681676, 0.37110016229508935, 0.379310343604174, 0.3727421984198841, 0.3596059096172721, 0.37602627096309255, 0.3711001625887083, 0.37274219871350306, 0.3711001625887083, 0.3743842346425518, 0.36781609014337285, 0.37274219861563007, 0.3743842348382978, 0.36453201808952934, 0.37602627096309255, 0.3743842350340438, 0.3711001628823273, 0.3842364514891933, 0.3825944157558905, 0.3760262711588385, 0.3776683070878873, 0.3776683071857603, 0.36945812656178656, 0.36945812656178656, 0.3743842347404248, 0.3678160905348648, 0.37274219881137605, 0.3596059098130181, 0.366174054605816, 0.3825944155601445, 0.385878487809734, 0.3711001626865813, 0.37274219881137605, 0.37602627096309255, 0.38095237953322275, 0.3875205240324017, 0.37766830738150625], 'loss': [2.712031887297268, 2.7044587883371594, 2.6975226054690946, 2.692278148799951, 2.6867451645265614, 2.681538796375909, 2.6768576884416584, 2.673174917820298, 2.669801015677638, 2.6668298637108148, 2.66487938275817, 2.6633411898015704, 2.6624939456367884, 2.661559975955031, 2.660793034300912, 2.6597791146938317, 2.6587762166832016, 2.6573751372233554, 2.654909036683351, 2.650447035129555, 2.6419388602646467, 2.6267329024093597, 2.602098750186897, 2.5693952524196932, 2.5376686642546917, 2.5159257566659603, 2.495410003211709, 2.475524381739403, 2.4581067002039916, 2.439288139441175, 2.42196530913915, 2.4033844634492785, 2.39075876653072, 2.3764896990092628, 2.3608040387625566, 2.3443101087879596, 2.335292573827003, 2.3289177068205094, 2.3209419494292085, 2.303735068103861, 2.2965420137440646, 2.287897939995329, 2.2793690031314044, 2.271591016448254, 2.269039501840329, 2.258895225054919, 2.255610125265572, 2.2472276281282397, 2.2383859358284264, 2.237086520498538, 2.2315552304168014, 2.2245661039372, 2.2215504389768754, 2.213361284913958, 2.2086032448363255, 2.2068030893925035, 2.199937419871775, 2.1959105271333543, 2.1906559461440884, 2.1884484789454715, 2.1919558989193897, 2.200578562385982, 2.1892998588648176, 2.180983348940432, 2.1720158332182398, 2.1710759755277538, 2.175133215867029, 2.1687372975280885, 2.1612464268349525, 2.1623746795575967, 2.1621803973000153, 2.1580889012534517, 2.147089397735909, 2.144368634625382, 2.140693564287691, 2.140651234562147, 2.135324902211371, 2.130864133874004, 2.127060383790817, 2.1270627231323744, 2.120977302351527, 2.1159653032095282, 2.1111087078431305, 2.1024659773652314, 2.1147810646151126, 2.1036907214648424, 2.111437546497008, 2.107131891426854, 2.096095188148702, 2.0944294458542028, 2.0778096388987204, 2.0719681816179403, 2.06821027604951, 2.067395212420203, 2.067852960817623, 2.055271250609255, 2.050163142147495, 2.05026084882278, 2.042861656584534, 2.0470261165493566, 2.0389686928147897, 2.0384900354506788, 2.0354378552407453, 2.0334359062280987, 2.0237401290595898, 2.0174257938866744, 2.0108067109599497, 2.0028068479326473, 2.005285811864865, 1.9992492755580487, 1.9957094929791084, 1.9895657296543003, 1.9838363301827433, 1.9805843368937592, 1.9759342023234592, 1.9945908617434804, 1.97481837086609, 1.9766416038575847, 1.9561159296936568, 1.946912501186316, 1.9440986211295002, 1.934629950631081, 1.9334887838951127, 1.9247926504460204, 1.93419931551759, 1.9253633992627905, 1.9237003331311675, 1.9150925622094093, 1.9053693424260103, 1.9094820639925572, 1.90718782731395, 1.8967448276171204, 1.895212378345231, 1.8867806202087558, 1.8826207750876582, 1.8824157432853807, 1.8818280172054282, 1.8899439937035405, 1.894320388643159, 1.8874569623132507, 1.8833560062874515, 1.8889833243720586, 1.8662732318686264, 1.8571406909817298, 1.8534253952194777, 1.8546082494684804, 1.8540836570199266, 1.84704313640477, 1.8500221757673385, 1.84987416267395, 1.841898564293644, 1.8570536618360014, 1.84007462539712, 1.837837460887995, 1.8400971555611925, 1.8424062761193183, 1.8446813471753005, 1.8321999862698315, 1.8297697142653886, 1.8264366528826328, 1.819796800417577, 1.8309982413873536, 1.8251994302385397, 1.8129817476018009, 1.8127215399144856, 1.817478862827074, 1.809253108476956, 1.8135363858338498, 1.8138133464652655, 1.814363669761642, 1.8349268975933475, 1.8134009145368541, 1.8177552159562003, 1.805342327006299, 1.809174972344228, 1.7994827107971942, 1.7978784830418455, 1.79767200041111, 1.7907955989700568, 1.7897018078171496, 1.7960325626866773, 1.7933823413183068, 1.7866715520314367, 1.7954377073526873, 1.8040177434866433, 1.788050437756877, 1.7893732103723765, 1.788315044291455, 1.780108385898739, 1.7734865756005478, 1.7754829739153508, 1.7794289633478717, 1.8011741727284583, 1.788327450282275, 1.7901068361877661, 1.7934059052007154, 1.7897595065330334, 1.7890321447619177, 1.7752281491516553, 1.7730505772929417, 1.7618991029825544, 1.7676826678262354, 1.7588080809591242, 1.7640941748139305, 1.7748149208464417, 1.7751720727101978, 1.7641351109412662, 1.7595567545117294, 1.7710756524142788, 1.7738658171659623, 1.7647359335446995, 1.7598218275536257, 1.767408586968142, 1.7550186374104244, 1.7520387159480697, 1.7602704475058177, 1.7511702201204868, 1.7515656499647263, 1.7473972294120084, 1.7498647121922926, 1.7439031996031806, 1.7460130416881867, 1.7389465677664755, 1.7401620130519357, 1.7363346119925716, 1.736565980235654, 1.7346484092226753, 1.7328106376914274, 1.738570155940751, 1.7491004418543477, 1.753435613340421, 1.7332933411216345, 1.7316351635500147, 1.7299411192077385, 1.729429881577619, 1.7353687595293017, 1.7301937455269345, 1.720465407870878, 1.7194934788181062, 1.7224852277023346, 1.721823818287076, 1.7265652533674143, 1.7295540540370118, 1.716897641755717, 1.7271224454198284, 1.7264848190166622, 1.7270989840035567, 1.734453238794691, 1.7200993083096139, 1.7389199694079296, 1.7243134777648739, 1.7119772329467522, 1.7077995693414363, 1.7147085477194501, 1.7074418114440886, 1.706520077629011, 1.712079926976433, 1.7274335128815512, 1.7467713255657062, 1.719098465104857, 1.7315656247814577, 1.7189518565269957, 1.7041444407841018, 1.709750592830979, 1.7034298179085985, 1.7050577976375634, 1.7085439652632883, 1.6926553228797365, 1.6935078830934402, 1.6950687364386336, 1.6986697603789689, 1.691490898925421, 1.692031435114647, 1.6875166233070578, 1.6866505252262405, 1.687132995035614, 1.6872837481312684, 1.6872683192180657, 1.683147527158138, 1.6856648505835563, 1.6865180016053531, 1.688015576995129, 1.6924025674620204, 1.6926184978328447, 1.6895470800341033, 1.6837582584279274, 1.6940624529820938, 1.699402885074733, 1.6847040476005914, 1.6937101698998798, 1.70134136867719, 1.699230910424579, 1.6888664149650559, 1.6783113329807102, 1.6821300339649834, 1.6815753841791798, 1.6778664417090603, 1.6774621140540749, 1.6785293522801488, 1.6747195189004072], 'acc': [0.0891170430752531, 0.07679671413232658, 0.07843942520126425, 0.07843942480961155, 0.07843942500543791, 0.08501026669872859, 0.0850102670903813, 0.085010267482034, 0.08501026747285463, 0.10225872742260751, 0.10225872743178686, 0.10225872644347576, 0.10225872723596052, 0.10225872743178686, 0.10225872703095482, 0.10225872684430783, 0.10225872742260751, 0.10225872704013417, 0.10225872744096623, 0.10225872723596052, 0.10225872683512846, 0.11252566708186813, 0.14620123094357015, 0.1544147850857623, 0.16591375880172854, 0.16509240137478165, 0.16303901506522842, 0.1687885009165417, 0.16960985732763945, 0.17125256603748157, 0.17248459928334373, 0.17905544193740744, 0.18069815125308733, 0.18028747365337622, 0.18110882965446254, 0.19425051361016424, 0.20410677665802487, 0.21232032921524752, 0.21149897415657554, 0.21560574973143593, 0.21519507136677815, 0.214784394513655, 0.21889117149601728, 0.22340862349563065, 0.22217659162055295, 0.2246406579348096, 0.21806981586822494, 0.22997946677389086, 0.2291581099527817, 0.22751540028216657, 0.23367556570981318, 0.2320328542767609, 0.22751540104711326, 0.2394250523811493, 0.23531827600462482, 0.23449692096431152, 0.236960984928652, 0.24188911630877236, 0.24229979510180025, 0.24065708482534734, 0.24147843728320065, 0.23942505140201756, 0.23039014302117625, 0.23285420851541005, 0.24476386159352453, 0.24271047134908563, 0.24229979666841103, 0.2451745374491572, 0.2464065712824984, 0.2398357295892077, 0.2406570850211737, 0.2439425055740795, 0.24887063718674365, 0.24394250578826457, 0.24763860550749228, 0.24312114957299322, 0.2459958928811232, 0.2509240254912778, 0.2517453814923641, 0.2443531808055157, 0.2554414794675134, 0.2558521582605413, 0.2616016405686216, 0.26078028948155274, 0.2574948677536888, 0.26735113058736437, 0.26036961127600383, 0.26036960970939305, 0.2624229995621793, 0.26119096435805367, 0.2665297755470511, 0.26858316109165764, 0.2743326495070722, 0.27802874885300594, 0.27186858419030596, 0.2739219703223916, 0.27145790559310445, 0.2767967152154911, 0.2735112912968199, 0.27885010332419885, 0.2870636540639082, 0.2825461996776612, 0.28254620226012117, 0.2862423000394441, 0.29363449775217987, 0.2956878854141588, 0.29979466081155154, 0.30184805046851143, 0.29445585435910393, 0.3043121144144932, 0.3071868571167854, 0.3121149875544914, 0.31457905329962776, 0.31498973526259466, 0.31457905643284934, 0.30841889259017224, 0.30800821458044975, 0.3334702272557135, 0.32607803013045683, 0.3305954841250512, 0.3215605729659235, 0.33552361632519434, 0.3301848070944604, 0.3326488688496349, 0.3330595496376437, 0.3330595484626856, 0.3318275169792606, 0.3396303895317798, 0.3470225890436701, 0.3375770038280644, 0.3351129385112982, 0.35195071924883237, 0.34825461911959327, 0.34702258646121015, 0.3503080074057687, 0.3519507208521606, 0.35523613784347474, 0.34537987661312736, 0.3490759775256719, 0.34250513446159675, 0.34455852372690393, 0.34702258884784376, 0.3503080066224633, 0.35893223934349827, 0.35934291559078363, 0.3601642692603125, 0.36468172384238584, 0.35770020750513803, 0.36139630368113274, 0.35687885105732287, 0.3609856248881048, 0.3568788507023876, 0.3630390161116755, 0.36262833892197577, 0.3683778216217088, 0.36550308221174704, 0.36221765852561966, 0.36139630368113274, 0.3659137586915762, 0.36427104544101063, 0.3728952795328301, 0.3618069795367654, 0.3675564695555082, 0.37166324393705175, 0.37084188889673847, 0.3741273078463161, 0.3724846019514777, 0.3704312118661477, 0.37166324671533807, 0.37207392230170955, 0.35934291559078363, 0.37166324432870446, 0.3687884996314313, 0.37289527855369836, 0.3663244347063178, 0.3765913771163267, 0.37700205453857016, 0.3778234089914044, 0.3798767972775798, 0.38069814973543314, 0.37084188948421754, 0.37453798984600045, 0.38193018595540795, 0.37700205316778573, 0.3757700197260972, 0.3802874721173633, 0.3774127329399454, 0.3802874727415598, 0.38193018556375524, 0.38275154001658945, 0.3831622172430066, 0.38357289544855544, 0.37453798745936684, 0.37125256690646097, 0.37864476187762786, 0.37905544184561385, 0.3774127313733346, 0.3683778216217088, 0.3823408618110406, 0.37905544243309286, 0.3901437369460198, 0.3761806965241442, 0.3856262843222099, 0.3872689940111838, 0.3864476385792178, 0.38562628314725184, 0.38521560376674485, 0.3880903472523425, 0.3835728970151662, 0.38685831835137746, 0.38480492748274203, 0.38521560572500835, 0.3843942499013897, 0.3876796704542955, 0.3913757685885537, 0.3938398366836062, 0.39301847827752756, 0.39548254793919085, 0.3963039005928705, 0.39219712480382507, 0.3897330575287954, 0.39712525801981746, 0.39260780183441585, 0.39835729146150595, 0.39260780144276314, 0.40082135619079307, 0.3971252554373575, 0.40041067935602864, 0.393839833746211, 0.39835728871993703, 0.39178644898490983, 0.403696098697259, 0.3967145809892267, 0.3975359362253663, 0.3991786451310348, 0.39589322238732166, 0.39794661208099896, 0.4069815200701876, 0.4000000007588271, 0.4098562614016954, 0.400410679551855, 0.41314168215042757, 0.4024640648639178, 0.40533880803129757, 0.40369609971310816, 0.4143737187620551, 0.4012320327930137, 0.3987679692754021, 0.4061601624474144, 0.39835729146150595, 0.4102669384322862, 0.40533880819040646, 0.40492813315479664, 0.4156057500129363, 0.4041067751403707, 0.41724846087686823, 0.40574948799928356, 0.40903490694886113, 0.398767964930505, 0.4057494889784153, 0.39383983707525894, 0.40451745530418304, 0.4119096491003918, 0.4106776188286423, 0.41232032773431077, 0.41108829726673496, 0.4102669407822023, 0.4188911727566494, 0.42258727112345135, 0.42053388084229504, 0.41478439598847217, 0.4164271030949861, 0.4180698137630917, 0.4156057510287855, 0.42217659151040066, 0.411909652037787, 0.4180698129797863, 0.42217659170622696, 0.4221765909229216, 0.4139630373865672, 0.42997946566624806, 0.4164271046615969, 0.4238193035860081, 0.41971252384371827, 0.4234086245971539, 0.4184804925561196, 0.4098562614016954, 0.41560574903380454, 0.42464065823466873, 0.4168377830629721, 0.4098562649632871, 0.41355236074762913, 0.4242299801882288, 0.4275154021486365, 0.4193018486122821, 0.42874743559032497, 0.41642710250750703, 0.4254620122591328, 0.4295687898106154, 0.42956878726487285]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
