{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf74.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 00:18:51 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '03', '04', '05', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000024C0A6BBE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000024C07106EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6161, Accuracy:0.1943, Validation Loss:1.6111, Validation Accuracy:0.1970\n",
    "Epoch #2: Loss:1.6093, Accuracy:0.2111, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6026, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6025, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6022, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6019, Accuracy:0.2324, Validation Loss:1.6047, Validation Accuracy:0.2381\n",
    "Epoch #22: Loss:1.6016, Accuracy:0.2402, Validation Loss:1.6046, Validation Accuracy:0.2381\n",
    "Epoch #23: Loss:1.6016, Accuracy:0.2398, Validation Loss:1.6045, Validation Accuracy:0.2381\n",
    "Epoch #24: Loss:1.6014, Accuracy:0.2402, Validation Loss:1.6046, Validation Accuracy:0.2381\n",
    "Epoch #25: Loss:1.6012, Accuracy:0.2398, Validation Loss:1.6047, Validation Accuracy:0.2397\n",
    "Epoch #26: Loss:1.6010, Accuracy:0.2398, Validation Loss:1.6048, Validation Accuracy:0.2447\n",
    "Epoch #27: Loss:1.6010, Accuracy:0.2444, Validation Loss:1.6050, Validation Accuracy:0.2365\n",
    "Epoch #28: Loss:1.6009, Accuracy:0.2444, Validation Loss:1.6050, Validation Accuracy:0.2381\n",
    "Epoch #29: Loss:1.6008, Accuracy:0.2452, Validation Loss:1.6052, Validation Accuracy:0.2365\n",
    "Epoch #30: Loss:1.6008, Accuracy:0.2448, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #31: Loss:1.6007, Accuracy:0.2435, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #32: Loss:1.6004, Accuracy:0.2431, Validation Loss:1.6054, Validation Accuracy:0.2348\n",
    "Epoch #33: Loss:1.6004, Accuracy:0.2427, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #34: Loss:1.6005, Accuracy:0.2427, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #35: Loss:1.6004, Accuracy:0.2448, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #36: Loss:1.6000, Accuracy:0.2439, Validation Loss:1.6058, Validation Accuracy:0.2348\n",
    "Epoch #37: Loss:1.6001, Accuracy:0.2427, Validation Loss:1.6059, Validation Accuracy:0.2348\n",
    "Epoch #38: Loss:1.6001, Accuracy:0.2423, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #39: Loss:1.6000, Accuracy:0.2439, Validation Loss:1.6063, Validation Accuracy:0.2348\n",
    "Epoch #40: Loss:1.5997, Accuracy:0.2435, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #41: Loss:1.5998, Accuracy:0.2435, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #42: Loss:1.5999, Accuracy:0.2431, Validation Loss:1.6056, Validation Accuracy:0.2365\n",
    "Epoch #43: Loss:1.5994, Accuracy:0.2448, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #44: Loss:1.5993, Accuracy:0.2427, Validation Loss:1.6059, Validation Accuracy:0.2348\n",
    "Epoch #45: Loss:1.5992, Accuracy:0.2427, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #46: Loss:1.5993, Accuracy:0.2435, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #47: Loss:1.5991, Accuracy:0.2419, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #48: Loss:1.5990, Accuracy:0.2415, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #49: Loss:1.5989, Accuracy:0.2419, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.5987, Accuracy:0.2423, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #51: Loss:1.5988, Accuracy:0.2402, Validation Loss:1.6058, Validation Accuracy:0.2315\n",
    "Epoch #52: Loss:1.5986, Accuracy:0.2398, Validation Loss:1.6054, Validation Accuracy:0.2365\n",
    "Epoch #53: Loss:1.5986, Accuracy:0.2382, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #54: Loss:1.5986, Accuracy:0.2419, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #55: Loss:1.5985, Accuracy:0.2411, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #56: Loss:1.5985, Accuracy:0.2411, Validation Loss:1.6056, Validation Accuracy:0.2365\n",
    "Epoch #57: Loss:1.5985, Accuracy:0.2411, Validation Loss:1.6056, Validation Accuracy:0.2365\n",
    "Epoch #58: Loss:1.5982, Accuracy:0.2402, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #59: Loss:1.5980, Accuracy:0.2407, Validation Loss:1.6061, Validation Accuracy:0.2381\n",
    "Epoch #60: Loss:1.5980, Accuracy:0.2411, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #61: Loss:1.5974, Accuracy:0.2435, Validation Loss:1.6062, Validation Accuracy:0.2365\n",
    "Epoch #62: Loss:1.5976, Accuracy:0.2411, Validation Loss:1.6063, Validation Accuracy:0.2365\n",
    "Epoch #63: Loss:1.5974, Accuracy:0.2411, Validation Loss:1.6066, Validation Accuracy:0.2348\n",
    "Epoch #64: Loss:1.5973, Accuracy:0.2390, Validation Loss:1.6068, Validation Accuracy:0.2348\n",
    "Epoch #65: Loss:1.5972, Accuracy:0.2378, Validation Loss:1.6067, Validation Accuracy:0.2397\n",
    "Epoch #66: Loss:1.5972, Accuracy:0.2423, Validation Loss:1.6053, Validation Accuracy:0.2397\n",
    "Epoch #67: Loss:1.5980, Accuracy:0.2444, Validation Loss:1.6061, Validation Accuracy:0.2348\n",
    "Epoch #68: Loss:1.5977, Accuracy:0.2427, Validation Loss:1.6066, Validation Accuracy:0.2348\n",
    "Epoch #69: Loss:1.5974, Accuracy:0.2427, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #70: Loss:1.5972, Accuracy:0.2448, Validation Loss:1.6075, Validation Accuracy:0.2151\n",
    "Epoch #71: Loss:1.5973, Accuracy:0.2423, Validation Loss:1.6074, Validation Accuracy:0.2135\n",
    "Epoch #72: Loss:1.5970, Accuracy:0.2390, Validation Loss:1.6075, Validation Accuracy:0.2332\n",
    "Epoch #73: Loss:1.5975, Accuracy:0.2427, Validation Loss:1.6080, Validation Accuracy:0.2348\n",
    "Epoch #74: Loss:1.5975, Accuracy:0.2407, Validation Loss:1.6086, Validation Accuracy:0.2069\n",
    "Epoch #75: Loss:1.5977, Accuracy:0.2345, Validation Loss:1.6083, Validation Accuracy:0.2118\n",
    "Epoch #76: Loss:1.5972, Accuracy:0.2398, Validation Loss:1.6088, Validation Accuracy:0.2102\n",
    "Epoch #77: Loss:1.5970, Accuracy:0.2415, Validation Loss:1.6088, Validation Accuracy:0.2118\n",
    "Epoch #78: Loss:1.5969, Accuracy:0.2386, Validation Loss:1.6091, Validation Accuracy:0.2135\n",
    "Epoch #79: Loss:1.5966, Accuracy:0.2427, Validation Loss:1.6087, Validation Accuracy:0.2167\n",
    "Epoch #80: Loss:1.5967, Accuracy:0.2374, Validation Loss:1.6095, Validation Accuracy:0.2003\n",
    "Epoch #81: Loss:1.5965, Accuracy:0.2472, Validation Loss:1.6091, Validation Accuracy:0.2167\n",
    "Epoch #82: Loss:1.5963, Accuracy:0.2427, Validation Loss:1.6094, Validation Accuracy:0.2184\n",
    "Epoch #83: Loss:1.5961, Accuracy:0.2357, Validation Loss:1.6093, Validation Accuracy:0.1987\n",
    "Epoch #84: Loss:1.5964, Accuracy:0.2394, Validation Loss:1.6099, Validation Accuracy:0.2332\n",
    "Epoch #85: Loss:1.5963, Accuracy:0.2386, Validation Loss:1.6100, Validation Accuracy:0.2069\n",
    "Epoch #86: Loss:1.5956, Accuracy:0.2394, Validation Loss:1.6100, Validation Accuracy:0.2184\n",
    "Epoch #87: Loss:1.5954, Accuracy:0.2431, Validation Loss:1.6107, Validation Accuracy:0.2217\n",
    "Epoch #88: Loss:1.5950, Accuracy:0.2460, Validation Loss:1.6105, Validation Accuracy:0.2184\n",
    "Epoch #89: Loss:1.5955, Accuracy:0.2283, Validation Loss:1.6108, Validation Accuracy:0.2151\n",
    "Epoch #90: Loss:1.5957, Accuracy:0.2439, Validation Loss:1.6118, Validation Accuracy:0.2332\n",
    "Epoch #91: Loss:1.5948, Accuracy:0.2407, Validation Loss:1.6112, Validation Accuracy:0.2053\n",
    "Epoch #92: Loss:1.5950, Accuracy:0.2419, Validation Loss:1.6115, Validation Accuracy:0.2036\n",
    "Epoch #93: Loss:1.5947, Accuracy:0.2382, Validation Loss:1.6110, Validation Accuracy:0.2184\n",
    "Epoch #94: Loss:1.5946, Accuracy:0.2419, Validation Loss:1.6110, Validation Accuracy:0.2184\n",
    "Epoch #95: Loss:1.5947, Accuracy:0.2341, Validation Loss:1.6114, Validation Accuracy:0.2102\n",
    "Epoch #96: Loss:1.5944, Accuracy:0.2382, Validation Loss:1.6112, Validation Accuracy:0.2135\n",
    "Epoch #97: Loss:1.5943, Accuracy:0.2439, Validation Loss:1.6117, Validation Accuracy:0.2200\n",
    "Epoch #98: Loss:1.5950, Accuracy:0.2402, Validation Loss:1.6122, Validation Accuracy:0.2085\n",
    "Epoch #99: Loss:1.5962, Accuracy:0.2423, Validation Loss:1.6130, Validation Accuracy:0.2102\n",
    "Epoch #100: Loss:1.5940, Accuracy:0.2382, Validation Loss:1.6122, Validation Accuracy:0.2217\n",
    "Epoch #101: Loss:1.5941, Accuracy:0.2415, Validation Loss:1.6122, Validation Accuracy:0.2217\n",
    "Epoch #102: Loss:1.5937, Accuracy:0.2439, Validation Loss:1.6121, Validation Accuracy:0.2085\n",
    "Epoch #103: Loss:1.5940, Accuracy:0.2439, Validation Loss:1.6124, Validation Accuracy:0.2200\n",
    "Epoch #104: Loss:1.5935, Accuracy:0.2452, Validation Loss:1.6127, Validation Accuracy:0.2118\n",
    "Epoch #105: Loss:1.5933, Accuracy:0.2415, Validation Loss:1.6128, Validation Accuracy:0.2020\n",
    "Epoch #106: Loss:1.5933, Accuracy:0.2366, Validation Loss:1.6127, Validation Accuracy:0.2085\n",
    "Epoch #107: Loss:1.5931, Accuracy:0.2402, Validation Loss:1.6127, Validation Accuracy:0.2151\n",
    "Epoch #108: Loss:1.5932, Accuracy:0.2390, Validation Loss:1.6133, Validation Accuracy:0.2151\n",
    "Epoch #109: Loss:1.5933, Accuracy:0.2419, Validation Loss:1.6134, Validation Accuracy:0.2053\n",
    "Epoch #110: Loss:1.5927, Accuracy:0.2411, Validation Loss:1.6131, Validation Accuracy:0.2102\n",
    "Epoch #111: Loss:1.5932, Accuracy:0.2505, Validation Loss:1.6136, Validation Accuracy:0.2118\n",
    "Epoch #112: Loss:1.5933, Accuracy:0.2394, Validation Loss:1.6140, Validation Accuracy:0.2085\n",
    "Epoch #113: Loss:1.5930, Accuracy:0.2493, Validation Loss:1.6140, Validation Accuracy:0.2332\n",
    "Epoch #114: Loss:1.5925, Accuracy:0.2398, Validation Loss:1.6146, Validation Accuracy:0.2135\n",
    "Epoch #115: Loss:1.5926, Accuracy:0.2456, Validation Loss:1.6140, Validation Accuracy:0.2135\n",
    "Epoch #116: Loss:1.5923, Accuracy:0.2448, Validation Loss:1.6142, Validation Accuracy:0.2020\n",
    "Epoch #117: Loss:1.5932, Accuracy:0.2374, Validation Loss:1.6144, Validation Accuracy:0.2085\n",
    "Epoch #118: Loss:1.5920, Accuracy:0.2452, Validation Loss:1.6143, Validation Accuracy:0.2102\n",
    "Epoch #119: Loss:1.5928, Accuracy:0.2452, Validation Loss:1.6146, Validation Accuracy:0.2135\n",
    "Epoch #120: Loss:1.5920, Accuracy:0.2456, Validation Loss:1.6150, Validation Accuracy:0.2299\n",
    "Epoch #121: Loss:1.5924, Accuracy:0.2439, Validation Loss:1.6144, Validation Accuracy:0.2003\n",
    "Epoch #122: Loss:1.5923, Accuracy:0.2489, Validation Loss:1.6142, Validation Accuracy:0.2053\n",
    "Epoch #123: Loss:1.5924, Accuracy:0.2464, Validation Loss:1.6148, Validation Accuracy:0.2315\n",
    "Epoch #124: Loss:1.5923, Accuracy:0.2501, Validation Loss:1.6139, Validation Accuracy:0.2053\n",
    "Epoch #125: Loss:1.5918, Accuracy:0.2468, Validation Loss:1.6141, Validation Accuracy:0.2069\n",
    "Epoch #126: Loss:1.5921, Accuracy:0.2472, Validation Loss:1.6150, Validation Accuracy:0.2299\n",
    "Epoch #127: Loss:1.5919, Accuracy:0.2505, Validation Loss:1.6142, Validation Accuracy:0.2233\n",
    "Epoch #128: Loss:1.5918, Accuracy:0.2526, Validation Loss:1.6142, Validation Accuracy:0.2118\n",
    "Epoch #129: Loss:1.5913, Accuracy:0.2489, Validation Loss:1.6142, Validation Accuracy:0.2315\n",
    "Epoch #130: Loss:1.5913, Accuracy:0.2423, Validation Loss:1.6142, Validation Accuracy:0.2315\n",
    "Epoch #131: Loss:1.5910, Accuracy:0.2522, Validation Loss:1.6145, Validation Accuracy:0.2003\n",
    "Epoch #132: Loss:1.5914, Accuracy:0.2509, Validation Loss:1.6143, Validation Accuracy:0.2299\n",
    "Epoch #133: Loss:1.5922, Accuracy:0.2439, Validation Loss:1.6147, Validation Accuracy:0.2315\n",
    "Epoch #134: Loss:1.5916, Accuracy:0.2456, Validation Loss:1.6143, Validation Accuracy:0.2118\n",
    "Epoch #135: Loss:1.5918, Accuracy:0.2464, Validation Loss:1.6145, Validation Accuracy:0.2020\n",
    "Epoch #136: Loss:1.5911, Accuracy:0.2456, Validation Loss:1.6144, Validation Accuracy:0.2397\n",
    "Epoch #137: Loss:1.5907, Accuracy:0.2456, Validation Loss:1.6141, Validation Accuracy:0.2365\n",
    "Epoch #138: Loss:1.5915, Accuracy:0.2505, Validation Loss:1.6144, Validation Accuracy:0.2118\n",
    "Epoch #139: Loss:1.5909, Accuracy:0.2456, Validation Loss:1.6149, Validation Accuracy:0.2332\n",
    "Epoch #140: Loss:1.5915, Accuracy:0.2407, Validation Loss:1.6141, Validation Accuracy:0.2332\n",
    "Epoch #141: Loss:1.5907, Accuracy:0.2505, Validation Loss:1.6141, Validation Accuracy:0.2299\n",
    "Epoch #142: Loss:1.5905, Accuracy:0.2497, Validation Loss:1.6141, Validation Accuracy:0.2036\n",
    "Epoch #143: Loss:1.5909, Accuracy:0.2522, Validation Loss:1.6140, Validation Accuracy:0.2282\n",
    "Epoch #144: Loss:1.5911, Accuracy:0.2485, Validation Loss:1.6136, Validation Accuracy:0.2365\n",
    "Epoch #145: Loss:1.5906, Accuracy:0.2517, Validation Loss:1.6139, Validation Accuracy:0.2299\n",
    "Epoch #146: Loss:1.5910, Accuracy:0.2505, Validation Loss:1.6145, Validation Accuracy:0.2299\n",
    "Epoch #147: Loss:1.5903, Accuracy:0.2489, Validation Loss:1.6131, Validation Accuracy:0.2332\n",
    "Epoch #148: Loss:1.5908, Accuracy:0.2444, Validation Loss:1.6133, Validation Accuracy:0.2053\n",
    "Epoch #149: Loss:1.5911, Accuracy:0.2398, Validation Loss:1.6135, Validation Accuracy:0.2381\n",
    "Epoch #150: Loss:1.5902, Accuracy:0.2456, Validation Loss:1.6128, Validation Accuracy:0.2299\n",
    "Epoch #151: Loss:1.5906, Accuracy:0.2485, Validation Loss:1.6134, Validation Accuracy:0.2299\n",
    "Epoch #152: Loss:1.5901, Accuracy:0.2497, Validation Loss:1.6140, Validation Accuracy:0.2365\n",
    "Epoch #153: Loss:1.5901, Accuracy:0.2489, Validation Loss:1.6136, Validation Accuracy:0.2299\n",
    "Epoch #154: Loss:1.5902, Accuracy:0.2501, Validation Loss:1.6135, Validation Accuracy:0.2299\n",
    "Epoch #155: Loss:1.5909, Accuracy:0.2493, Validation Loss:1.6141, Validation Accuracy:0.2282\n",
    "Epoch #156: Loss:1.5901, Accuracy:0.2505, Validation Loss:1.6136, Validation Accuracy:0.2233\n",
    "Epoch #157: Loss:1.5908, Accuracy:0.2517, Validation Loss:1.6130, Validation Accuracy:0.2381\n",
    "Epoch #158: Loss:1.5897, Accuracy:0.2522, Validation Loss:1.6138, Validation Accuracy:0.2414\n",
    "Epoch #159: Loss:1.5897, Accuracy:0.2476, Validation Loss:1.6138, Validation Accuracy:0.2299\n",
    "Epoch #160: Loss:1.5897, Accuracy:0.2476, Validation Loss:1.6134, Validation Accuracy:0.2299\n",
    "Epoch #161: Loss:1.5893, Accuracy:0.2522, Validation Loss:1.6134, Validation Accuracy:0.2282\n",
    "Epoch #162: Loss:1.5899, Accuracy:0.2517, Validation Loss:1.6134, Validation Accuracy:0.2299\n",
    "Epoch #163: Loss:1.5894, Accuracy:0.2522, Validation Loss:1.6138, Validation Accuracy:0.2299\n",
    "Epoch #164: Loss:1.5905, Accuracy:0.2439, Validation Loss:1.6144, Validation Accuracy:0.2053\n",
    "Epoch #165: Loss:1.5892, Accuracy:0.2522, Validation Loss:1.6134, Validation Accuracy:0.2299\n",
    "Epoch #166: Loss:1.5895, Accuracy:0.2509, Validation Loss:1.6137, Validation Accuracy:0.2381\n",
    "Epoch #167: Loss:1.5903, Accuracy:0.2464, Validation Loss:1.6137, Validation Accuracy:0.2282\n",
    "Epoch #168: Loss:1.5892, Accuracy:0.2522, Validation Loss:1.6132, Validation Accuracy:0.2299\n",
    "Epoch #169: Loss:1.5895, Accuracy:0.2522, Validation Loss:1.6132, Validation Accuracy:0.2299\n",
    "Epoch #170: Loss:1.5889, Accuracy:0.2522, Validation Loss:1.6130, Validation Accuracy:0.2282\n",
    "Epoch #171: Loss:1.5892, Accuracy:0.2534, Validation Loss:1.6129, Validation Accuracy:0.2299\n",
    "Epoch #172: Loss:1.5887, Accuracy:0.2513, Validation Loss:1.6135, Validation Accuracy:0.2299\n",
    "Epoch #173: Loss:1.5894, Accuracy:0.2517, Validation Loss:1.6136, Validation Accuracy:0.2282\n",
    "Epoch #174: Loss:1.5891, Accuracy:0.2497, Validation Loss:1.6127, Validation Accuracy:0.2282\n",
    "Epoch #175: Loss:1.5890, Accuracy:0.2497, Validation Loss:1.6134, Validation Accuracy:0.2299\n",
    "Epoch #176: Loss:1.5890, Accuracy:0.2517, Validation Loss:1.6137, Validation Accuracy:0.2299\n",
    "Epoch #177: Loss:1.5893, Accuracy:0.2534, Validation Loss:1.6133, Validation Accuracy:0.2282\n",
    "Epoch #178: Loss:1.5887, Accuracy:0.2522, Validation Loss:1.6139, Validation Accuracy:0.2299\n",
    "Epoch #179: Loss:1.5889, Accuracy:0.2522, Validation Loss:1.6140, Validation Accuracy:0.2282\n",
    "Epoch #180: Loss:1.5888, Accuracy:0.2546, Validation Loss:1.6130, Validation Accuracy:0.2315\n",
    "Epoch #181: Loss:1.5888, Accuracy:0.2476, Validation Loss:1.6133, Validation Accuracy:0.2299\n",
    "Epoch #182: Loss:1.5893, Accuracy:0.2542, Validation Loss:1.6135, Validation Accuracy:0.2233\n",
    "Epoch #183: Loss:1.5891, Accuracy:0.2505, Validation Loss:1.6128, Validation Accuracy:0.2299\n",
    "Epoch #184: Loss:1.5892, Accuracy:0.2571, Validation Loss:1.6131, Validation Accuracy:0.2282\n",
    "Epoch #185: Loss:1.5884, Accuracy:0.2501, Validation Loss:1.6128, Validation Accuracy:0.2282\n",
    "Epoch #186: Loss:1.5884, Accuracy:0.2530, Validation Loss:1.6126, Validation Accuracy:0.2299\n",
    "Epoch #187: Loss:1.5887, Accuracy:0.2448, Validation Loss:1.6135, Validation Accuracy:0.2036\n",
    "Epoch #188: Loss:1.5889, Accuracy:0.2497, Validation Loss:1.6128, Validation Accuracy:0.2315\n",
    "Epoch #189: Loss:1.5884, Accuracy:0.2538, Validation Loss:1.6129, Validation Accuracy:0.2332\n",
    "Epoch #190: Loss:1.5881, Accuracy:0.2509, Validation Loss:1.6134, Validation Accuracy:0.2266\n",
    "Epoch #191: Loss:1.5880, Accuracy:0.2530, Validation Loss:1.6136, Validation Accuracy:0.2282\n",
    "Epoch #192: Loss:1.5882, Accuracy:0.2538, Validation Loss:1.6137, Validation Accuracy:0.2266\n",
    "Epoch #193: Loss:1.5882, Accuracy:0.2526, Validation Loss:1.6137, Validation Accuracy:0.2266\n",
    "Epoch #194: Loss:1.5881, Accuracy:0.2538, Validation Loss:1.6131, Validation Accuracy:0.2266\n",
    "Epoch #195: Loss:1.5879, Accuracy:0.2501, Validation Loss:1.6138, Validation Accuracy:0.2266\n",
    "Epoch #196: Loss:1.5878, Accuracy:0.2489, Validation Loss:1.6132, Validation Accuracy:0.2282\n",
    "Epoch #197: Loss:1.5886, Accuracy:0.2431, Validation Loss:1.6136, Validation Accuracy:0.2266\n",
    "Epoch #198: Loss:1.5881, Accuracy:0.2497, Validation Loss:1.6141, Validation Accuracy:0.2053\n",
    "Epoch #199: Loss:1.5877, Accuracy:0.2534, Validation Loss:1.6136, Validation Accuracy:0.2282\n",
    "Epoch #200: Loss:1.5880, Accuracy:0.2554, Validation Loss:1.6140, Validation Accuracy:0.2365\n",
    "Epoch #201: Loss:1.5878, Accuracy:0.2571, Validation Loss:1.6134, Validation Accuracy:0.2315\n",
    "Epoch #202: Loss:1.5881, Accuracy:0.2505, Validation Loss:1.6142, Validation Accuracy:0.2250\n",
    "Epoch #203: Loss:1.5887, Accuracy:0.2530, Validation Loss:1.6136, Validation Accuracy:0.2315\n",
    "Epoch #204: Loss:1.5886, Accuracy:0.2538, Validation Loss:1.6129, Validation Accuracy:0.2332\n",
    "Epoch #205: Loss:1.5879, Accuracy:0.2505, Validation Loss:1.6142, Validation Accuracy:0.2184\n",
    "Epoch #206: Loss:1.5878, Accuracy:0.2489, Validation Loss:1.6133, Validation Accuracy:0.2266\n",
    "Epoch #207: Loss:1.5878, Accuracy:0.2571, Validation Loss:1.6127, Validation Accuracy:0.2348\n",
    "Epoch #208: Loss:1.5879, Accuracy:0.2579, Validation Loss:1.6137, Validation Accuracy:0.2414\n",
    "Epoch #209: Loss:1.5876, Accuracy:0.2624, Validation Loss:1.6132, Validation Accuracy:0.2250\n",
    "Epoch #210: Loss:1.5877, Accuracy:0.2575, Validation Loss:1.6132, Validation Accuracy:0.2315\n",
    "Epoch #211: Loss:1.5874, Accuracy:0.2554, Validation Loss:1.6127, Validation Accuracy:0.2414\n",
    "Epoch #212: Loss:1.5876, Accuracy:0.2546, Validation Loss:1.6128, Validation Accuracy:0.2315\n",
    "Epoch #213: Loss:1.5874, Accuracy:0.2575, Validation Loss:1.6133, Validation Accuracy:0.2365\n",
    "Epoch #214: Loss:1.5872, Accuracy:0.2546, Validation Loss:1.6136, Validation Accuracy:0.2365\n",
    "Epoch #215: Loss:1.5876, Accuracy:0.2526, Validation Loss:1.6128, Validation Accuracy:0.2348\n",
    "Epoch #216: Loss:1.5870, Accuracy:0.2517, Validation Loss:1.6138, Validation Accuracy:0.2365\n",
    "Epoch #217: Loss:1.5874, Accuracy:0.2513, Validation Loss:1.6138, Validation Accuracy:0.2365\n",
    "Epoch #218: Loss:1.5868, Accuracy:0.2595, Validation Loss:1.6136, Validation Accuracy:0.2315\n",
    "Epoch #219: Loss:1.5876, Accuracy:0.2522, Validation Loss:1.6138, Validation Accuracy:0.2365\n",
    "Epoch #220: Loss:1.5877, Accuracy:0.2583, Validation Loss:1.6137, Validation Accuracy:0.2315\n",
    "Epoch #221: Loss:1.5867, Accuracy:0.2538, Validation Loss:1.6149, Validation Accuracy:0.2020\n",
    "Epoch #222: Loss:1.5871, Accuracy:0.2575, Validation Loss:1.6136, Validation Accuracy:0.2332\n",
    "Epoch #223: Loss:1.5871, Accuracy:0.2538, Validation Loss:1.6130, Validation Accuracy:0.2365\n",
    "Epoch #224: Loss:1.5876, Accuracy:0.2505, Validation Loss:1.6144, Validation Accuracy:0.2135\n",
    "Epoch #225: Loss:1.5868, Accuracy:0.2595, Validation Loss:1.6144, Validation Accuracy:0.2282\n",
    "Epoch #226: Loss:1.5867, Accuracy:0.2522, Validation Loss:1.6128, Validation Accuracy:0.2414\n",
    "Epoch #227: Loss:1.5868, Accuracy:0.2559, Validation Loss:1.6129, Validation Accuracy:0.2315\n",
    "Epoch #228: Loss:1.5869, Accuracy:0.2550, Validation Loss:1.6149, Validation Accuracy:0.1987\n",
    "Epoch #229: Loss:1.5865, Accuracy:0.2550, Validation Loss:1.6137, Validation Accuracy:0.2365\n",
    "Epoch #230: Loss:1.5870, Accuracy:0.2550, Validation Loss:1.6140, Validation Accuracy:0.2315\n",
    "Epoch #231: Loss:1.5863, Accuracy:0.2563, Validation Loss:1.6134, Validation Accuracy:0.2365\n",
    "Epoch #232: Loss:1.5863, Accuracy:0.2571, Validation Loss:1.6142, Validation Accuracy:0.2365\n",
    "Epoch #233: Loss:1.5863, Accuracy:0.2591, Validation Loss:1.6145, Validation Accuracy:0.2365\n",
    "Epoch #234: Loss:1.5864, Accuracy:0.2583, Validation Loss:1.6150, Validation Accuracy:0.2365\n",
    "Epoch #235: Loss:1.5868, Accuracy:0.2559, Validation Loss:1.6141, Validation Accuracy:0.2315\n",
    "Epoch #236: Loss:1.5866, Accuracy:0.2583, Validation Loss:1.6150, Validation Accuracy:0.2250\n",
    "Epoch #237: Loss:1.5862, Accuracy:0.2546, Validation Loss:1.6137, Validation Accuracy:0.2365\n",
    "Epoch #238: Loss:1.5873, Accuracy:0.2550, Validation Loss:1.6142, Validation Accuracy:0.2397\n",
    "Epoch #239: Loss:1.5859, Accuracy:0.2497, Validation Loss:1.6153, Validation Accuracy:0.2365\n",
    "Epoch #240: Loss:1.5861, Accuracy:0.2554, Validation Loss:1.6151, Validation Accuracy:0.2118\n",
    "Epoch #241: Loss:1.5861, Accuracy:0.2583, Validation Loss:1.6145, Validation Accuracy:0.2365\n",
    "Epoch #242: Loss:1.5864, Accuracy:0.2505, Validation Loss:1.6154, Validation Accuracy:0.2365\n",
    "Epoch #243: Loss:1.5864, Accuracy:0.2550, Validation Loss:1.6150, Validation Accuracy:0.2315\n",
    "Epoch #244: Loss:1.5860, Accuracy:0.2571, Validation Loss:1.6148, Validation Accuracy:0.2365\n",
    "Epoch #245: Loss:1.5868, Accuracy:0.2583, Validation Loss:1.6153, Validation Accuracy:0.2414\n",
    "Epoch #246: Loss:1.5860, Accuracy:0.2526, Validation Loss:1.6146, Validation Accuracy:0.2135\n",
    "Epoch #247: Loss:1.5877, Accuracy:0.2546, Validation Loss:1.6155, Validation Accuracy:0.2036\n",
    "Epoch #248: Loss:1.5868, Accuracy:0.2542, Validation Loss:1.6140, Validation Accuracy:0.2397\n",
    "Epoch #249: Loss:1.5864, Accuracy:0.2513, Validation Loss:1.6159, Validation Accuracy:0.2282\n",
    "Epoch #250: Loss:1.5859, Accuracy:0.2538, Validation Loss:1.6160, Validation Accuracy:0.2118\n",
    "Epoch #251: Loss:1.5861, Accuracy:0.2567, Validation Loss:1.6150, Validation Accuracy:0.2463\n",
    "Epoch #252: Loss:1.5858, Accuracy:0.2554, Validation Loss:1.6146, Validation Accuracy:0.2397\n",
    "Epoch #253: Loss:1.5853, Accuracy:0.2595, Validation Loss:1.6155, Validation Accuracy:0.2365\n",
    "Epoch #254: Loss:1.5853, Accuracy:0.2600, Validation Loss:1.6151, Validation Accuracy:0.2332\n",
    "Epoch #255: Loss:1.5868, Accuracy:0.2485, Validation Loss:1.6148, Validation Accuracy:0.2365\n",
    "Epoch #256: Loss:1.5864, Accuracy:0.2559, Validation Loss:1.6165, Validation Accuracy:0.1987\n",
    "Epoch #257: Loss:1.5859, Accuracy:0.2559, Validation Loss:1.6148, Validation Accuracy:0.2184\n",
    "Epoch #258: Loss:1.5853, Accuracy:0.2542, Validation Loss:1.6144, Validation Accuracy:0.2315\n",
    "Epoch #259: Loss:1.5855, Accuracy:0.2546, Validation Loss:1.6153, Validation Accuracy:0.2348\n",
    "Epoch #260: Loss:1.5855, Accuracy:0.2517, Validation Loss:1.6154, Validation Accuracy:0.2365\n",
    "Epoch #261: Loss:1.5853, Accuracy:0.2554, Validation Loss:1.6157, Validation Accuracy:0.2151\n",
    "Epoch #262: Loss:1.5866, Accuracy:0.2485, Validation Loss:1.6149, Validation Accuracy:0.2397\n",
    "Epoch #263: Loss:1.5857, Accuracy:0.2501, Validation Loss:1.6169, Validation Accuracy:0.2069\n",
    "Epoch #264: Loss:1.5857, Accuracy:0.2550, Validation Loss:1.6154, Validation Accuracy:0.2217\n",
    "Epoch #265: Loss:1.5854, Accuracy:0.2563, Validation Loss:1.6150, Validation Accuracy:0.2315\n",
    "Epoch #266: Loss:1.5847, Accuracy:0.2587, Validation Loss:1.6155, Validation Accuracy:0.2365\n",
    "Epoch #267: Loss:1.5859, Accuracy:0.2575, Validation Loss:1.6161, Validation Accuracy:0.2184\n",
    "Epoch #268: Loss:1.5849, Accuracy:0.2579, Validation Loss:1.6154, Validation Accuracy:0.2315\n",
    "Epoch #269: Loss:1.5852, Accuracy:0.2546, Validation Loss:1.6155, Validation Accuracy:0.2365\n",
    "Epoch #270: Loss:1.5858, Accuracy:0.2559, Validation Loss:1.6170, Validation Accuracy:0.2463\n",
    "Epoch #271: Loss:1.5850, Accuracy:0.2595, Validation Loss:1.6160, Validation Accuracy:0.2299\n",
    "Epoch #272: Loss:1.5851, Accuracy:0.2563, Validation Loss:1.6162, Validation Accuracy:0.2167\n",
    "Epoch #273: Loss:1.5852, Accuracy:0.2567, Validation Loss:1.6165, Validation Accuracy:0.2365\n",
    "Epoch #274: Loss:1.5848, Accuracy:0.2571, Validation Loss:1.6152, Validation Accuracy:0.2397\n",
    "Epoch #275: Loss:1.5847, Accuracy:0.2571, Validation Loss:1.6157, Validation Accuracy:0.2365\n",
    "Epoch #276: Loss:1.5844, Accuracy:0.2559, Validation Loss:1.6169, Validation Accuracy:0.2135\n",
    "Epoch #277: Loss:1.5845, Accuracy:0.2604, Validation Loss:1.6160, Validation Accuracy:0.2315\n",
    "Epoch #278: Loss:1.5846, Accuracy:0.2554, Validation Loss:1.6163, Validation Accuracy:0.2430\n",
    "Epoch #279: Loss:1.5846, Accuracy:0.2616, Validation Loss:1.6163, Validation Accuracy:0.2365\n",
    "Epoch #280: Loss:1.5857, Accuracy:0.2591, Validation Loss:1.6168, Validation Accuracy:0.2151\n",
    "Epoch #281: Loss:1.5853, Accuracy:0.2579, Validation Loss:1.6175, Validation Accuracy:0.2036\n",
    "Epoch #282: Loss:1.5845, Accuracy:0.2571, Validation Loss:1.6164, Validation Accuracy:0.2315\n",
    "Epoch #283: Loss:1.5853, Accuracy:0.2575, Validation Loss:1.6160, Validation Accuracy:0.2397\n",
    "Epoch #284: Loss:1.5853, Accuracy:0.2571, Validation Loss:1.6175, Validation Accuracy:0.2184\n",
    "Epoch #285: Loss:1.5848, Accuracy:0.2517, Validation Loss:1.6163, Validation Accuracy:0.2184\n",
    "Epoch #286: Loss:1.5845, Accuracy:0.2608, Validation Loss:1.6159, Validation Accuracy:0.2299\n",
    "Epoch #287: Loss:1.5846, Accuracy:0.2559, Validation Loss:1.6167, Validation Accuracy:0.2365\n",
    "Epoch #288: Loss:1.5848, Accuracy:0.2591, Validation Loss:1.6159, Validation Accuracy:0.2315\n",
    "Epoch #289: Loss:1.5844, Accuracy:0.2571, Validation Loss:1.6163, Validation Accuracy:0.2167\n",
    "Epoch #290: Loss:1.5837, Accuracy:0.2583, Validation Loss:1.6163, Validation Accuracy:0.2365\n",
    "Epoch #291: Loss:1.5842, Accuracy:0.2563, Validation Loss:1.6164, Validation Accuracy:0.2315\n",
    "Epoch #292: Loss:1.5834, Accuracy:0.2579, Validation Loss:1.6164, Validation Accuracy:0.2315\n",
    "Epoch #293: Loss:1.5844, Accuracy:0.2559, Validation Loss:1.6170, Validation Accuracy:0.2365\n",
    "Epoch #294: Loss:1.5849, Accuracy:0.2546, Validation Loss:1.6178, Validation Accuracy:0.2151\n",
    "Epoch #295: Loss:1.5842, Accuracy:0.2513, Validation Loss:1.6180, Validation Accuracy:0.2447\n",
    "Epoch #296: Loss:1.5838, Accuracy:0.2587, Validation Loss:1.6167, Validation Accuracy:0.2447\n",
    "Epoch #297: Loss:1.5849, Accuracy:0.2554, Validation Loss:1.6173, Validation Accuracy:0.2315\n",
    "Epoch #298: Loss:1.5842, Accuracy:0.2546, Validation Loss:1.6190, Validation Accuracy:0.2069\n",
    "Epoch #299: Loss:1.5844, Accuracy:0.2591, Validation Loss:1.6176, Validation Accuracy:0.2184\n",
    "Epoch #300: Loss:1.5842, Accuracy:0.2530, Validation Loss:1.6168, Validation Accuracy:0.2365\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61681092, Accuracy:0.2365\n",
    "Labels: ['01', '03', '04', '05', '02']\n",
    "Confusion Matrix:\n",
    "      01  03  04  05  02\n",
    "t:01  48  12  17  49   0\n",
    "t:03  50   7   9  49   0\n",
    "t:04  45   6  18  43   0\n",
    "t:05  50   8  13  71   0\n",
    "t:02  46   6  18  44   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.20      0.38      0.26       126\n",
    "          03       0.18      0.06      0.09       115\n",
    "          04       0.24      0.16      0.19       112\n",
    "          05       0.28      0.50      0.36       142\n",
    "          02       0.00      0.00      0.00       114\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.18      0.22      0.18       609\n",
    "weighted avg       0.18      0.24      0.19       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 00:59:27 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6111401554398936, 1.606450456116587, 1.6050155998646527, 1.604667944665417, 1.6046737379628448, 1.604489447056562, 1.604245911873816, 1.6041629122591567, 1.6043156266016718, 1.6042015718904818, 1.6040690503096933, 1.6055326397195826, 1.6047975602016855, 1.6042561581960844, 1.6047983412280655, 1.6047346439267614, 1.6045504700765625, 1.6043736948364082, 1.6044326462769156, 1.6043313876748673, 1.6046702842211293, 1.604592375371648, 1.6045149302443455, 1.604576601770711, 1.6046971752138561, 1.6047748147168965, 1.6050118045462372, 1.605012596142899, 1.6052350886349607, 1.605295357250032, 1.6054067075350407, 1.6054173028723555, 1.6053277684745726, 1.6056994861374152, 1.6056257429577054, 1.6057876113600331, 1.6058581368676548, 1.6061672122999169, 1.6062802297532657, 1.606209567419218, 1.6061715508133712, 1.6056356715842812, 1.6057901429425319, 1.6059431969042874, 1.6057208048299028, 1.6054526804311717, 1.6054819354478558, 1.605517941351203, 1.605534012290253, 1.6055785693558566, 1.6057594476270753, 1.6054301222752663, 1.6053862984740284, 1.605495688167503, 1.605340833930155, 1.6055939810225137, 1.605573740303027, 1.6056256685742407, 1.6060857467463452, 1.6062034454643237, 1.6061576919994136, 1.6063312542653827, 1.6066263669425827, 1.6067594516844976, 1.6066722398125284, 1.6053489384001307, 1.6060766380995952, 1.6065550880087616, 1.6071559020451136, 1.6075374396955242, 1.6073663185774203, 1.6075337236542224, 1.6079511472157069, 1.6086087542018672, 1.608300999859088, 1.608796897780132, 1.6088296083002451, 1.609065240044116, 1.608679049707986, 1.6095138005239427, 1.6091233239385294, 1.6094364832187522, 1.6092921346670692, 1.6099278965998558, 1.6099516570274466, 1.609986658362528, 1.610724038678437, 1.6104600321874634, 1.610793860675079, 1.6117540676214033, 1.6111589777841553, 1.6114539475668044, 1.611008647823177, 1.6110152644281122, 1.611441902143419, 1.6112206913959022, 1.6117049686622933, 1.6121697664652357, 1.6129975655591742, 1.6122019388797053, 1.6122129425430924, 1.6121352893378347, 1.6124072172763118, 1.612664794099742, 1.6127674160724008, 1.6127286985002716, 1.6127107566213372, 1.6132813660773542, 1.613400696925146, 1.6130896194032065, 1.613565483899735, 1.6140494223298698, 1.6139573545878745, 1.614610410284722, 1.6139967016790104, 1.6141630528595647, 1.6143726498035376, 1.614276004346525, 1.6145515555427188, 1.615013784375684, 1.6143841105337409, 1.6142396664580296, 1.6147528814369039, 1.6138720909754436, 1.6140979070381578, 1.6150366884147005, 1.614236665476719, 1.6142366449233934, 1.6142018549939485, 1.614223067004888, 1.6144506858878926, 1.6142918435223583, 1.6146605053950218, 1.6143225312037226, 1.6145069916260066, 1.614367230380893, 1.6141158757342886, 1.6143955330934823, 1.6148724755630117, 1.6141206998934692, 1.6141003383987251, 1.614129935383601, 1.61397440249501, 1.613608268877164, 1.6139343896718643, 1.614477423220041, 1.6131140077838366, 1.6133232470999406, 1.6134826485159361, 1.612823329927103, 1.6134369905750543, 1.6139584016330137, 1.6136412495267018, 1.6135263906911088, 1.6140720748353279, 1.6136237578634753, 1.6130008948064594, 1.6137747911396871, 1.613784791996522, 1.6134118068785894, 1.613414539883681, 1.6133576516055905, 1.6138051840276357, 1.6144015818393875, 1.6133850640655543, 1.613680815657567, 1.6136519000643776, 1.613186507780955, 1.6131771973201208, 1.6129648951669828, 1.612918588915482, 1.6135236285198693, 1.613570949126934, 1.6127289999490497, 1.6134034681006997, 1.613716740913579, 1.6132770880494016, 1.6138890257414142, 1.6140052479476177, 1.6130177845508593, 1.61331978826883, 1.6134550923784379, 1.6128482914519036, 1.6130640637894178, 1.6127629219409085, 1.6126499927689877, 1.613501338535929, 1.61284685702551, 1.6129234913730466, 1.6133869514480992, 1.6135650123477179, 1.613665350748009, 1.6136510926122931, 1.6131057674661646, 1.6137934001208527, 1.613203500879222, 1.6135875992782793, 1.6140899421350514, 1.613582285754199, 1.6140087091276798, 1.6134293975892717, 1.6142275544810178, 1.6135743494300028, 1.6129151336077987, 1.6142228123393945, 1.6133201889608098, 1.6126974265172171, 1.6137091175871725, 1.6132467572129223, 1.613176044963655, 1.6126997692048648, 1.6128130307534254, 1.6133167093806275, 1.6135544761256828, 1.6127514432020766, 1.6137513131735164, 1.613820698265176, 1.6135547499742806, 1.6138419402252473, 1.6136745419995537, 1.6148994611010372, 1.6135750570516476, 1.6129863990351485, 1.6143587181720827, 1.614427236892124, 1.6127677862280108, 1.6129135367122582, 1.6148533748679952, 1.6137361164359232, 1.6140074624216616, 1.61336416720561, 1.6141861082102082, 1.6144802325660568, 1.6149670382830115, 1.6141496935892967, 1.6149843161916497, 1.6137042145423701, 1.6142045183134783, 1.615297867355284, 1.6150740623865614, 1.6144774265477222, 1.6154277222888616, 1.6149751214166777, 1.6148170346305484, 1.6153130192670524, 1.6146093096051897, 1.6155474232922633, 1.6139701965015705, 1.6159079329329367, 1.6160271563161965, 1.6149931427684716, 1.61457919487225, 1.615468223302431, 1.6151306658542801, 1.6148093046225938, 1.6165406200881858, 1.6147700060764556, 1.6144118545873607, 1.6152743044353666, 1.6153554998595139, 1.6156745739954055, 1.614933425961261, 1.6168753264964313, 1.61542950436008, 1.614957795550279, 1.615537351183899, 1.6161377197024467, 1.615380648517452, 1.6155111378636853, 1.6170022614875255, 1.6159689224803782, 1.6161704419673175, 1.6165253840056546, 1.6152033144225824, 1.615714732845037, 1.6168546815615374, 1.6159560488558364, 1.6163174910302625, 1.6163304991322784, 1.6167938746450765, 1.6175424420383373, 1.6164046580764069, 1.6160267517092977, 1.6174776969089102, 1.6163042672161985, 1.6159049417389242, 1.6166689072923708, 1.615896068574564, 1.6163494999968555, 1.616341204283077, 1.6164357587817464, 1.616374924656597, 1.617045316790125, 1.6177906197280132, 1.6179534650984264, 1.6167049014509605, 1.6172649674423418, 1.618951010978085, 1.6176264145104169, 1.6168110364763608], 'val_acc': [0.19704433485302825, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.238095236088842, 0.238095236088842, 0.238095236088842, 0.238095236088842, 0.23973727221363675, 0.24466338058802098, 0.23645319986617427, 0.23809523599096902, 0.23645319986617427, 0.23645319986617427, 0.2331691276165848, 0.23481116374137953, 0.2331691276165848, 0.2331691276165848, 0.23481116374137953, 0.23481116383925252, 0.23481116374137953, 0.23481116374137953, 0.23481116374137953, 0.2331691276165848, 0.23481116374137953, 0.23645319986617427, 0.2331691276165848, 0.23481116374137953, 0.2331691275187118, 0.23645319986617427, 0.2331691275187118, 0.2331691275187118, 0.23645319986617427, 0.2331691275187118, 0.2315270912960441, 0.23645319967042833, 0.23316912742083884, 0.23645319967042833, 0.23645319967042833, 0.23645319967042833, 0.23645319967042833, 0.23481116354563358, 0.23809523579522307, 0.23481116354563358, 0.23645319967042833, 0.23645319967042833, 0.23481116354563358, 0.23481116354563358, 0.23973727211576376, 0.23973727211576376, 0.23481116374137953, 0.23481116374137953, 0.2331691276165848, 0.2151067323236434, 0.21346469619884867, 0.2331691276165848, 0.23481116374137953, 0.2068965516996697, 0.2118226578963801, 0.21018062177158536, 0.21182265779850715, 0.2134646939233019, 0.21674876617289138, 0.20032840700474475, 0.21674876627076436, 0.2183908023955591, 0.19868637078207702, 0.2331691276165848, 0.2068965516017967, 0.2183908023955591, 0.22167487484089454, 0.21839080229768612, 0.21510673014596962, 0.23316912781233076, 0.20525451557487495, 0.2036124771745334, 0.2183908023955591, 0.21839080229768612, 0.2101806238513862, 0.21346469590522973, 0.22003283852035385, 0.20853858782446444, 0.2101806237535132, 0.2216748745472756, 0.2216748746451486, 0.2085385875308455, 0.22003283852035385, 0.21182266007405393, 0.20197044283592056, 0.2085385875308455, 0.21510673222577043, 0.21510673212789747, 0.205254515281256, 0.2101806238513862, 0.21182265997618094, 0.2085385875308455, 0.23316912742083884, 0.21346469610097568, 0.21346469610097568, 0.2019704430316665, 0.20853858782446444, 0.2101806238513862, 0.21346469610097568, 0.2298850552691223, 0.20032840700474475, 0.20525451537912898, 0.23152709139391706, 0.20525451537912898, 0.2068965516017967, 0.2298850552691223, 0.22331691067207035, 0.21182265997618094, 0.23152709139391706, 0.23152709139391706, 0.20032840680899877, 0.2298850552691223, 0.23152709139391706, 0.21182265997618094, 0.2019704430316665, 0.23973727231150974, 0.23645319996404726, 0.21182266007405393, 0.2331691275187118, 0.23316912771445777, 0.2298850553669953, 0.20361247915646125, 0.22824301914432757, 0.23645319996404726, 0.2298850553669953, 0.2298850552691223, 0.23316912771445777, 0.20525451537912898, 0.238095236088842, 0.2298850553669953, 0.2298850553669953, 0.23645319996404726, 0.2298850553669953, 0.2298850553669953, 0.22824301914432757, 0.22331691076994334, 0.238095236186715, 0.24137930843630448, 0.2298850553669953, 0.2298850553669953, 0.22824301924220056, 0.2298850552691223, 0.2298850553669953, 0.20525451537912898, 0.2298850553669953, 0.238095236088842, 0.22824301924220056, 0.2298850553669953, 0.2298850553669953, 0.22824301904645458, 0.2298850553669953, 0.2298850553669953, 0.22824301924220056, 0.22824301904645458, 0.2298850553669953, 0.2298850553669953, 0.22824301904645458, 0.2298850553669953, 0.22824301924220056, 0.23152709139391706, 0.2298850553669953, 0.22331691067207035, 0.2298850553669953, 0.22824301904645458, 0.22824301904645458, 0.2298850552691223, 0.2036124788628423, 0.23152709139391706, 0.23316912781233076, 0.22660098292165984, 0.22824301904645458, 0.22660098292165984, 0.22660098292165984, 0.22660098292165984, 0.22660098509933366, 0.22824301904645458, 0.22660098292165984, 0.20525451518338303, 0.22824301904645458, 0.2364532001597932, 0.23152709168753602, 0.2249589467968651, 0.2315270934737179, 0.23316912781233076, 0.21839080427961396, 0.22660098292165984, 0.23481116403499847, 0.24137930843630448, 0.2249589467968651, 0.23152709149179004, 0.24137930853417747, 0.23152709168753602, 0.23645319986617427, 0.23645319986617427, 0.23481116403499847, 0.23645319986617427, 0.23645319986617427, 0.23152709168753602, 0.2364532001597932, 0.23152709168753602, 0.20197044293379354, 0.23316912781233076, 0.2364532001597932, 0.21346469590522973, 0.22824301904645458, 0.24137930863205043, 0.23152709149179004, 0.19868637068420403, 0.23645319986617427, 0.23152709149179004, 0.23645319986617427, 0.23645319986617427, 0.23645319986617427, 0.23645319986617427, 0.23152709149179004, 0.2249589467968651, 0.23645319986617427, 0.23973727231150974, 0.23645319986617427, 0.21182265987830795, 0.23645319986617427, 0.23645319986617427, 0.23152709149179004, 0.23645319986617427, 0.2413793083384315, 0.21346469619884867, 0.20361247905858829, 0.23973727231150974, 0.2282430212241284, 0.21182265997618094, 0.2463054168106887, 0.23973727250725568, 0.23645319986617427, 0.2331691275187118, 0.23645319986617427, 0.19868637068420403, 0.2183908045732329, 0.23152709149179004, 0.23481116364350654, 0.23645319986617427, 0.21510673212789747, 0.23973727231150974, 0.20689655130817777, 0.2216748768228224, 0.23152709149179004, 0.23645319986617427, 0.2183908045732329, 0.23152709149179004, 0.23645319986617427, 0.2463054168106887, 0.22988505517124935, 0.21674876835056517, 0.23645319986617427, 0.23973727231150974, 0.23645319986617427, 0.21346469600310272, 0.23152709149179004, 0.24302134446322624, 0.23645319986617427, 0.21510673212789747, 0.20361247905858829, 0.23152709149179004, 0.23973727231150974, 0.2183908045732329, 0.2183908045732329, 0.2298850552691223, 0.23645319996404726, 0.23152709149179004, 0.21674876844843816, 0.23645319986617427, 0.23152709149179004, 0.23152709149179004, 0.23645319986617427, 0.21510673222577043, 0.24466338068589397, 0.24466338068589397, 0.23152709149179004, 0.20689655150392372, 0.2183908045732329, 0.23645319986617427], 'loss': [1.616092520474898, 1.6093449960743866, 1.605481159417781, 1.6043756979446882, 1.6041322386485106, 1.604144207799704, 1.6038455372718325, 1.6034153905002977, 1.6031071470013878, 1.602803457982731, 1.6027506387209256, 1.6025923490524292, 1.6045093104090289, 1.603578558594784, 1.6027540290624944, 1.6027369656846753, 1.6025259983368234, 1.602109012858334, 1.6023967063402493, 1.602218465687558, 1.6018969763965332, 1.601583607681478, 1.6016144022070162, 1.6014044911954437, 1.6011820553754144, 1.6009979207902474, 1.601030308607912, 1.6008600325555038, 1.6008482183519086, 1.600815939805346, 1.6006607105110215, 1.6004427568134096, 1.600373653513695, 1.6004717217823319, 1.6003688767705364, 1.5999778121403845, 1.6001044686080494, 1.6000572983489145, 1.6000113544522858, 1.5997463282128868, 1.5998309797819634, 1.5998783491964947, 1.599448581104161, 1.5993283909204314, 1.5992002004960235, 1.5992587080236822, 1.5990533396448687, 1.5989922735970123, 1.5989269054890658, 1.5987339010963204, 1.5988384758422507, 1.598609822731488, 1.5985736360295353, 1.5986384593975373, 1.5984871799696152, 1.5984803881733325, 1.5984927131899573, 1.5981945457889315, 1.5979584719366118, 1.598010069633656, 1.5974304370566805, 1.597627807985341, 1.5974471853988617, 1.597292801584796, 1.5971868455777178, 1.597234110176196, 1.5979864757898161, 1.597728063389506, 1.597363832452214, 1.5972025380242287, 1.5973077838670546, 1.597001876282741, 1.597547947356833, 1.5975126301728235, 1.597661533884444, 1.5972175264750172, 1.5970293084698781, 1.5969059890300585, 1.596587950982597, 1.5967296690421917, 1.5965445265388096, 1.5963088566762467, 1.5960917468433262, 1.5963749817998991, 1.5962835570869993, 1.595593194990922, 1.5953780494921017, 1.5949526398578464, 1.5954845740320256, 1.595689668302908, 1.5948406403069624, 1.59501916015907, 1.5946635455321483, 1.5945747931145544, 1.5946856336182393, 1.59438839722463, 1.5943441043399442, 1.594978540633983, 1.5962364528703004, 1.5939533018233596, 1.5940555610695903, 1.5936919270110081, 1.5940462532474275, 1.5934612642813022, 1.593254097286436, 1.5933296229070706, 1.5930819929502829, 1.5932479548013676, 1.593264025535427, 1.5927399450748607, 1.5932381578049866, 1.5932756013693996, 1.5929898128862008, 1.5925412251963997, 1.5926026302686218, 1.5922768515972632, 1.5932362046329882, 1.5919682025909423, 1.5928291788336189, 1.5920279343270178, 1.5924238411552851, 1.5922668264631863, 1.5923708831015553, 1.5923260831245407, 1.591816206732325, 1.5920615912462897, 1.5919144165344552, 1.5918451440407755, 1.5912677564170572, 1.5913039500708452, 1.5909976805015267, 1.5913971849045958, 1.5921629695187358, 1.5915795125021337, 1.5918071451617952, 1.5911211511682435, 1.5906786621963218, 1.5915247422713763, 1.590861142930064, 1.5915350791609997, 1.5907408276622546, 1.5905270886861813, 1.590932647942028, 1.5911210224124195, 1.5906046808133134, 1.5910169858462513, 1.5902697437842523, 1.5907917053547729, 1.5911028110026335, 1.590192217601643, 1.5905534142096673, 1.5901228110648278, 1.590126774247422, 1.590210720792688, 1.5908829956328845, 1.5900790898951662, 1.5908062055125618, 1.5896937251825352, 1.5896535249216601, 1.5896783440509616, 1.5893184242307283, 1.5899331089407511, 1.5893706292342356, 1.590526310717056, 1.5891933515086556, 1.5894546625305739, 1.5903488310455542, 1.5891715848715153, 1.5895031642130513, 1.588940438597599, 1.589163163848971, 1.588675658462963, 1.5894305364796757, 1.5890524784887106, 1.589018305567011, 1.5889555733796263, 1.58933585091538, 1.5887093529808938, 1.5888885853961263, 1.5888012712741044, 1.5887789891730588, 1.5892921999984209, 1.589095335819393, 1.5891614353387506, 1.5883807118178883, 1.5883876535192407, 1.588671800781814, 1.588906312772136, 1.58838763143982, 1.5881457356212076, 1.5879944771467047, 1.5882029167190959, 1.5882225152648206, 1.588097646936499, 1.5879441695046865, 1.5877982774554338, 1.5885526926855287, 1.5881342656313762, 1.5876998308013353, 1.588031442747958, 1.5877962493798572, 1.588120760712046, 1.5886674209297071, 1.588625674472942, 1.5878618017114408, 1.587789477900558, 1.587809308304679, 1.5879477324671813, 1.5875952289334558, 1.5876627790854452, 1.5873814405601863, 1.587613603368677, 1.5874411931028112, 1.5872160464586418, 1.587623143440889, 1.5870390953714109, 1.587397908870689, 1.5867540441744137, 1.5875723568076225, 1.587687349564241, 1.5866969750891966, 1.5871214603251744, 1.5870661320872375, 1.5876272077188354, 1.5867557291132715, 1.5866538210815961, 1.5868448038610345, 1.5868546870210087, 1.5864534747184424, 1.5870132806119979, 1.5863032681251699, 1.5862727576947064, 1.5862901241137997, 1.5864331451529594, 1.586810471438774, 1.586646627645473, 1.5862143521925751, 1.587253402145981, 1.5858790149923712, 1.5861400985130294, 1.5861059372919541, 1.586371093364222, 1.5864197413289816, 1.585972595900236, 1.5867538676369606, 1.5860488591497683, 1.5877260845055081, 1.5868186205074772, 1.5864255235914821, 1.5858575399896202, 1.5861348434150586, 1.5858402935631222, 1.5852970023909143, 1.5853368935888552, 1.5867546786028257, 1.5864458419948633, 1.5858963966859194, 1.585293658362277, 1.585512453966317, 1.5855401911529916, 1.585267321187123, 1.586554326658621, 1.5856538053900309, 1.5856654068772553, 1.58537302790726, 1.5847491503740974, 1.5858607197199515, 1.584901681621951, 1.5851575296762297, 1.5858190984451794, 1.5849567825544542, 1.5851086699742312, 1.585154242828886, 1.5848110624896916, 1.5847252676863934, 1.5844004235473257, 1.5844728144776894, 1.5845660576340599, 1.58464783623478, 1.5856728578250265, 1.5852584168651511, 1.5845089797366572, 1.5853329667810052, 1.585349907963183, 1.5848215373389776, 1.5844802980795043, 1.5846409137733664, 1.5847500602322682, 1.5844117796151789, 1.5837317148518024, 1.5842472729007322, 1.5834477961675342, 1.5844213271777488, 1.5848532069145531, 1.584157625752553, 1.5837898211312735, 1.5848725376677464, 1.5841898483906929, 1.5844375146732683, 1.5842049975170003], 'acc': [0.19425051382434932, 0.21108829538190635, 0.23285421049203225, 0.23285420990455322, 0.23285420828286627, 0.23285421049203225, 0.2328542087112364, 0.23285420851541005, 0.23285420951290053, 0.23285421029620593, 0.2328542083195837, 0.23285420951290053, 0.23285421047367355, 0.23285420872959514, 0.23285420929871545, 0.2328542094945418, 0.23285421047367355, 0.23285420951290053, 0.23285421049203225, 0.23285420990455322, 0.23244353248230976, 0.2402464058364931, 0.23983573017668675, 0.24024640857806195, 0.23983572821842328, 0.23983572782677057, 0.2443531811971684, 0.24435318319214933, 0.24517453686167817, 0.2447638594394347, 0.24353182777854207, 0.24312115016047225, 0.24271047293405512, 0.2427104727382288, 0.2447638614160569, 0.24394250479077412, 0.24271047215074973, 0.24229979627675835, 0.24394250576990587, 0.24353182756435698, 0.24353182758271572, 0.2431211507479513, 0.2447638594394347, 0.24271047332570783, 0.24271047291569642, 0.24353182776018334, 0.24188911826703582, 0.24147843986566062, 0.241889116112946, 0.24229979572599675, 0.24024640720727752, 0.239835729197555, 0.23819301796032907, 0.24188911807120947, 0.24106776126845905, 0.24106776126845905, 0.24106776205176444, 0.24024640564066674, 0.24065708366874797, 0.24106776187429682, 0.2435318291309678, 0.24106776246177586, 0.2410677614642854, 0.23901437280481602, 0.23778234151721736, 0.24229979392684217, 0.24435318319214933, 0.2427104736990018, 0.24271047311152277, 0.2447638610244042, 0.2422997941226685, 0.23901437376558904, 0.24271047234657608, 0.24065708404204195, 0.23449692057265883, 0.23983572861007596, 0.24147844084479236, 0.23860369616587793, 0.24271047136744434, 0.23737166389914754, 0.2472279271244758, 0.24271047271987006, 0.23572895262520416, 0.23942505218532295, 0.23860369634334555, 0.23942505120619123, 0.24312114955463449, 0.2459958914736213, 0.22833675590995892, 0.24394250400746872, 0.24065708325873655, 0.2418891180895682, 0.23819301913528718, 0.24188911787538314, 0.23408624316877408, 0.2381930189211021, 0.24394250578826457, 0.24024640644233083, 0.2422997945143212, 0.23819301990023384, 0.24147843849487619, 0.24394250598409092, 0.24394250576990587, 0.2451745384466477, 0.24147843910071395, 0.2365503086630079, 0.24024640544484038, 0.23901437259063094, 0.24188911787538314, 0.24106776087680637, 0.2505133468757175, 0.23942505099200614, 0.24928131519646615, 0.23983572999921912, 0.24558521526305338, 0.24476386043692516, 0.23737166446826785, 0.24517453881994164, 0.2451745384466477, 0.2455852164563702, 0.24394250559243824, 0.24887063601178555, 0.2464065712824984, 0.25010266984512675, 0.24681724909639458, 0.247227925931159, 0.2505133474815553, 0.2525667371201564, 0.24887063738256998, 0.24229979373101582, 0.2521560579354758, 0.2509240250996251, 0.24394250500495918, 0.24558521624218513, 0.24640657149668346, 0.24558521604635877, 0.24558521643801146, 0.25051334767738165, 0.2455852144981067, 0.2406570834729216, 0.2505133464840648, 0.24969199203123058, 0.2521560573663555, 0.2484599582162481, 0.2517453807090587, 0.25051334787320795, 0.24887063640343823, 0.24435318319214933, 0.23983572921591373, 0.24558521506722703, 0.2484599585895421, 0.24969199183540422, 0.2488706365992646, 0.25010266867016867, 0.24928131323820266, 0.25051334488073657, 0.25174537972992694, 0.2521560569563441, 0.24763860492001324, 0.2476386039408815, 0.25215605795383456, 0.2517453809232438, 0.25215605656469137, 0.24394250617991728, 0.25215605754382314, 0.25092402429796096, 0.24640657169250982, 0.25215605656469137, 0.2521560579354758, 0.2521560551571895, 0.25338809196464335, 0.2513347023076835, 0.251745380317406, 0.24969199224541563, 0.2496919928328947, 0.25174537972992694, 0.2533880887947043, 0.2521560573479968, 0.2521560583454872, 0.2546201244272001, 0.24763860452836053, 0.2542094470049566, 0.2505133464840648, 0.2570841893523136, 0.2501026704509645, 0.2529774129757891, 0.2447638606143928, 0.24969199224541563, 0.2537987691910605, 0.2509240228904591, 0.25297741454239986, 0.2537987695827132, 0.25256673457441386, 0.2537987686035814, 0.25010266886599497, 0.2488706354243065, 0.24312115016047225, 0.249691993402015, 0.2533880882072253, 0.25544147731342354, 0.2570841893523136, 0.2505133466982499, 0.2529774103933291, 0.2537987676244497, 0.2505133450765629, 0.24887063563849157, 0.2570841889606609, 0.2579055436093215, 0.2624229995621793, 0.2574948662054367, 0.25544147927168703, 0.25462012423137376, 0.25749486438792346, 0.2546201228605893, 0.25256673457441386, 0.251745380904885, 0.25133470269933617, 0.2595482528699252, 0.2521560579354758, 0.2583162196240631, 0.25379876840775506, 0.2574948657954253, 0.2537987689952341, 0.2505133464840648, 0.25954825566657025, 0.2521560573479968, 0.25585215767306224, 0.25503079969535375, 0.25503080224109625, 0.2550307990711572, 0.2562628339203476, 0.2570841897439663, 0.25913757763848905, 0.25831622281236083, 0.2558521574772359, 0.2583162224023494, 0.25462012423137376, 0.2550308010661382, 0.24969199144375154, 0.25544147770507625, 0.2583162235773075, 0.2505133452723893, 0.25503080028283276, 0.25708418837318187, 0.25831622142321764, 0.2525667371201564, 0.2546201240355474, 0.25420944543834584, 0.25133470172020445, 0.2537987682119287, 0.2566735113425911, 0.2554414782925553, 0.25954825408160076, 0.2599589326788023, 0.2484599597828589, 0.2558521555189724, 0.25585215571479875, 0.254209447200783, 0.25462012209564266, 0.251745380904885, 0.25544147929004574, 0.2484599597828589, 0.25010267043260576, 0.2550308028285753, 0.25626283450782666, 0.25872689943294014, 0.25749486755786244, 0.25790554302184243, 0.25462012540633183, 0.25585215587390764, 0.2595482548832649, 0.25626283411617395, 0.2566735095434365, 0.25708418995815135, 0.257084190135619, 0.25585215532314604, 0.26036961108017753, 0.25544147770507625, 0.26160164393438695, 0.25913757466437637, 0.257905545567585, 0.2570841885690082, 0.2574948667745571, 0.25708418954813994, 0.2517453809232438, 0.2607802877191156, 0.2558521563022778, 0.2591375764635309, 0.2570841881773555, 0.2583162225981757, 0.2562628354869584, 0.2579055440009742, 0.2558521559106251, 0.2546201210614347, 0.2513347011327254, 0.25872690139120363, 0.25544147850674037, 0.2546201236438947, 0.25913757705101, 0.25297741317161543]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
