{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf12.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 02:10:36 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '3', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000232AA484E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000232A6C36EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0802, Accuracy:0.3786, Validation Loss:1.0764, Validation Accuracy:0.3629\n",
    "Epoch #2: Loss:1.0751, Accuracy:0.3832, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #3: Loss:1.0760, Accuracy:0.3910, Validation Loss:1.0757, Validation Accuracy:0.3924\n",
    "Epoch #4: Loss:1.0748, Accuracy:0.3943, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #10: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0735, Accuracy:0.3938, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #13: Loss:1.0733, Accuracy:0.3938, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0734, Accuracy:0.3955, Validation Loss:1.0746, Validation Accuracy:0.4007\n",
    "Epoch #20: Loss:1.0731, Accuracy:0.3938, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #21: Loss:1.0730, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0731, Accuracy:0.3934, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0730, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #26: Loss:1.0728, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0727, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3924\n",
    "Epoch #31: Loss:1.0732, Accuracy:0.3967, Validation Loss:1.0729, Validation Accuracy:0.3892\n",
    "Epoch #32: Loss:1.0734, Accuracy:0.3979, Validation Loss:1.0724, Validation Accuracy:0.3842\n",
    "Epoch #33: Loss:1.0731, Accuracy:0.3947, Validation Loss:1.0723, Validation Accuracy:0.3974\n",
    "Epoch #34: Loss:1.0729, Accuracy:0.3947, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #35: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #36: Loss:1.0725, Accuracy:0.3979, Validation Loss:1.0727, Validation Accuracy:0.3990\n",
    "Epoch #37: Loss:1.0728, Accuracy:0.3963, Validation Loss:1.0719, Validation Accuracy:0.3990\n",
    "Epoch #38: Loss:1.0721, Accuracy:0.3959, Validation Loss:1.0723, Validation Accuracy:0.3990\n",
    "Epoch #39: Loss:1.0732, Accuracy:0.3988, Validation Loss:1.0724, Validation Accuracy:0.3859\n",
    "Epoch #40: Loss:1.0723, Accuracy:0.3975, Validation Loss:1.0717, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0718, Validation Accuracy:0.3892\n",
    "Epoch #42: Loss:1.0729, Accuracy:0.3959, Validation Loss:1.0728, Validation Accuracy:0.3924\n",
    "Epoch #43: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0725, Validation Accuracy:0.3875\n",
    "Epoch #44: Loss:1.0726, Accuracy:0.3959, Validation Loss:1.0719, Validation Accuracy:0.4154\n",
    "Epoch #45: Loss:1.0727, Accuracy:0.3918, Validation Loss:1.0719, Validation Accuracy:0.4220\n",
    "Epoch #46: Loss:1.0723, Accuracy:0.3918, Validation Loss:1.0721, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0729, Accuracy:0.3959, Validation Loss:1.0714, Validation Accuracy:0.3859\n",
    "Epoch #48: Loss:1.0719, Accuracy:0.3922, Validation Loss:1.0712, Validation Accuracy:0.4023\n",
    "Epoch #49: Loss:1.0723, Accuracy:0.4012, Validation Loss:1.0708, Validation Accuracy:0.4023\n",
    "Epoch #50: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0704, Validation Accuracy:0.4220\n",
    "Epoch #51: Loss:1.0719, Accuracy:0.3955, Validation Loss:1.0707, Validation Accuracy:0.4105\n",
    "Epoch #52: Loss:1.0726, Accuracy:0.4066, Validation Loss:1.0732, Validation Accuracy:0.4056\n",
    "Epoch #53: Loss:1.0718, Accuracy:0.4053, Validation Loss:1.0732, Validation Accuracy:0.4072\n",
    "Epoch #54: Loss:1.0720, Accuracy:0.3971, Validation Loss:1.0718, Validation Accuracy:0.4089\n",
    "Epoch #55: Loss:1.0719, Accuracy:0.3979, Validation Loss:1.0713, Validation Accuracy:0.4138\n",
    "Epoch #56: Loss:1.0718, Accuracy:0.3959, Validation Loss:1.0695, Validation Accuracy:0.4072\n",
    "Epoch #57: Loss:1.0704, Accuracy:0.4140, Validation Loss:1.0711, Validation Accuracy:0.4187\n",
    "Epoch #58: Loss:1.0716, Accuracy:0.4070, Validation Loss:1.0701, Validation Accuracy:0.4154\n",
    "Epoch #59: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0710, Validation Accuracy:0.4039\n",
    "Epoch #60: Loss:1.0719, Accuracy:0.4107, Validation Loss:1.0712, Validation Accuracy:0.3859\n",
    "Epoch #61: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0705, Validation Accuracy:0.3924\n",
    "Epoch #62: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0692, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0730, Accuracy:0.3996, Validation Loss:1.0710, Validation Accuracy:0.4171\n",
    "Epoch #64: Loss:1.0722, Accuracy:0.4033, Validation Loss:1.0694, Validation Accuracy:0.3957\n",
    "Epoch #65: Loss:1.0726, Accuracy:0.3951, Validation Loss:1.0690, Validation Accuracy:0.4236\n",
    "Epoch #66: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0693, Validation Accuracy:0.4056\n",
    "Epoch #67: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0689, Validation Accuracy:0.4007\n",
    "Epoch #68: Loss:1.0715, Accuracy:0.4025, Validation Loss:1.0689, Validation Accuracy:0.4072\n",
    "Epoch #69: Loss:1.0710, Accuracy:0.4037, Validation Loss:1.0701, Validation Accuracy:0.3924\n",
    "Epoch #70: Loss:1.0715, Accuracy:0.4008, Validation Loss:1.0701, Validation Accuracy:0.4039\n",
    "Epoch #71: Loss:1.0708, Accuracy:0.4049, Validation Loss:1.0700, Validation Accuracy:0.4023\n",
    "Epoch #72: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0698, Validation Accuracy:0.4319\n",
    "Epoch #73: Loss:1.0712, Accuracy:0.4029, Validation Loss:1.0683, Validation Accuracy:0.3842\n",
    "Epoch #74: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0681, Validation Accuracy:0.4023\n",
    "Epoch #75: Loss:1.0705, Accuracy:0.4066, Validation Loss:1.0690, Validation Accuracy:0.4335\n",
    "Epoch #76: Loss:1.0708, Accuracy:0.4107, Validation Loss:1.0687, Validation Accuracy:0.4286\n",
    "Epoch #77: Loss:1.0702, Accuracy:0.4000, Validation Loss:1.0689, Validation Accuracy:0.4171\n",
    "Epoch #78: Loss:1.0710, Accuracy:0.4066, Validation Loss:1.0690, Validation Accuracy:0.4187\n",
    "Epoch #79: Loss:1.0700, Accuracy:0.4049, Validation Loss:1.0691, Validation Accuracy:0.4154\n",
    "Epoch #80: Loss:1.0704, Accuracy:0.4049, Validation Loss:1.0691, Validation Accuracy:0.4171\n",
    "Epoch #81: Loss:1.0707, Accuracy:0.4033, Validation Loss:1.0680, Validation Accuracy:0.4335\n",
    "Epoch #82: Loss:1.0727, Accuracy:0.3918, Validation Loss:1.0687, Validation Accuracy:0.4187\n",
    "Epoch #83: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0686, Validation Accuracy:0.4138\n",
    "Epoch #84: Loss:1.0711, Accuracy:0.4008, Validation Loss:1.0674, Validation Accuracy:0.4351\n",
    "Epoch #85: Loss:1.0709, Accuracy:0.4066, Validation Loss:1.0686, Validation Accuracy:0.3859\n",
    "Epoch #86: Loss:1.0707, Accuracy:0.4053, Validation Loss:1.0691, Validation Accuracy:0.4171\n",
    "Epoch #87: Loss:1.0714, Accuracy:0.4033, Validation Loss:1.0695, Validation Accuracy:0.4319\n",
    "Epoch #88: Loss:1.0710, Accuracy:0.3979, Validation Loss:1.0692, Validation Accuracy:0.4039\n",
    "Epoch #89: Loss:1.0714, Accuracy:0.3979, Validation Loss:1.0698, Validation Accuracy:0.4039\n",
    "Epoch #90: Loss:1.0702, Accuracy:0.4094, Validation Loss:1.0708, Validation Accuracy:0.4171\n",
    "Epoch #91: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0692, Validation Accuracy:0.4171\n",
    "Epoch #92: Loss:1.0700, Accuracy:0.4000, Validation Loss:1.0696, Validation Accuracy:0.3875\n",
    "Epoch #93: Loss:1.0713, Accuracy:0.4029, Validation Loss:1.0699, Validation Accuracy:0.4072\n",
    "Epoch #94: Loss:1.0705, Accuracy:0.3951, Validation Loss:1.0701, Validation Accuracy:0.4072\n",
    "Epoch #95: Loss:1.0708, Accuracy:0.4033, Validation Loss:1.0698, Validation Accuracy:0.4253\n",
    "Epoch #96: Loss:1.0709, Accuracy:0.3926, Validation Loss:1.0690, Validation Accuracy:0.4253\n",
    "Epoch #97: Loss:1.0706, Accuracy:0.4004, Validation Loss:1.0695, Validation Accuracy:0.4072\n",
    "Epoch #98: Loss:1.0704, Accuracy:0.3975, Validation Loss:1.0688, Validation Accuracy:0.4154\n",
    "Epoch #99: Loss:1.0706, Accuracy:0.3992, Validation Loss:1.0692, Validation Accuracy:0.3924\n",
    "Epoch #100: Loss:1.0711, Accuracy:0.3951, Validation Loss:1.0702, Validation Accuracy:0.3957\n",
    "Epoch #101: Loss:1.0706, Accuracy:0.3967, Validation Loss:1.0709, Validation Accuracy:0.3941\n",
    "Epoch #102: Loss:1.0720, Accuracy:0.4053, Validation Loss:1.0707, Validation Accuracy:0.3924\n",
    "Epoch #103: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0715, Validation Accuracy:0.4056\n",
    "Epoch #104: Loss:1.0705, Accuracy:0.3996, Validation Loss:1.0702, Validation Accuracy:0.4023\n",
    "Epoch #105: Loss:1.0700, Accuracy:0.4115, Validation Loss:1.0689, Validation Accuracy:0.3990\n",
    "Epoch #106: Loss:1.0711, Accuracy:0.3963, Validation Loss:1.0692, Validation Accuracy:0.4302\n",
    "Epoch #107: Loss:1.0699, Accuracy:0.4037, Validation Loss:1.0690, Validation Accuracy:0.4138\n",
    "Epoch #108: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0693, Validation Accuracy:0.4187\n",
    "Epoch #109: Loss:1.0705, Accuracy:0.4004, Validation Loss:1.0695, Validation Accuracy:0.4253\n",
    "Epoch #110: Loss:1.0709, Accuracy:0.3988, Validation Loss:1.0688, Validation Accuracy:0.4089\n",
    "Epoch #111: Loss:1.0708, Accuracy:0.4012, Validation Loss:1.0691, Validation Accuracy:0.4138\n",
    "Epoch #112: Loss:1.0703, Accuracy:0.4086, Validation Loss:1.0686, Validation Accuracy:0.4187\n",
    "Epoch #113: Loss:1.0691, Accuracy:0.4078, Validation Loss:1.0673, Validation Accuracy:0.4220\n",
    "Epoch #114: Loss:1.0697, Accuracy:0.3992, Validation Loss:1.0676, Validation Accuracy:0.4269\n",
    "Epoch #115: Loss:1.0698, Accuracy:0.3943, Validation Loss:1.0670, Validation Accuracy:0.4335\n",
    "Epoch #116: Loss:1.0699, Accuracy:0.4000, Validation Loss:1.0672, Validation Accuracy:0.4253\n",
    "Epoch #117: Loss:1.0695, Accuracy:0.4012, Validation Loss:1.0697, Validation Accuracy:0.3974\n",
    "Epoch #118: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0702, Validation Accuracy:0.4236\n",
    "Epoch #119: Loss:1.0709, Accuracy:0.4004, Validation Loss:1.0689, Validation Accuracy:0.4286\n",
    "Epoch #120: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0690, Validation Accuracy:0.3941\n",
    "Epoch #121: Loss:1.0712, Accuracy:0.4066, Validation Loss:1.0702, Validation Accuracy:0.4220\n",
    "Epoch #122: Loss:1.0721, Accuracy:0.3984, Validation Loss:1.0689, Validation Accuracy:0.4154\n",
    "Epoch #123: Loss:1.0704, Accuracy:0.4090, Validation Loss:1.0689, Validation Accuracy:0.4187\n",
    "Epoch #124: Loss:1.0699, Accuracy:0.4074, Validation Loss:1.0692, Validation Accuracy:0.4138\n",
    "Epoch #125: Loss:1.0706, Accuracy:0.4000, Validation Loss:1.0675, Validation Accuracy:0.4335\n",
    "Epoch #126: Loss:1.0702, Accuracy:0.3959, Validation Loss:1.0680, Validation Accuracy:0.4236\n",
    "Epoch #127: Loss:1.0693, Accuracy:0.4037, Validation Loss:1.0675, Validation Accuracy:0.4499\n",
    "Epoch #128: Loss:1.0697, Accuracy:0.4016, Validation Loss:1.0673, Validation Accuracy:0.4384\n",
    "Epoch #129: Loss:1.0697, Accuracy:0.4041, Validation Loss:1.0667, Validation Accuracy:0.4401\n",
    "Epoch #130: Loss:1.0697, Accuracy:0.4033, Validation Loss:1.0675, Validation Accuracy:0.4286\n",
    "Epoch #131: Loss:1.0701, Accuracy:0.4140, Validation Loss:1.0672, Validation Accuracy:0.4105\n",
    "Epoch #132: Loss:1.0690, Accuracy:0.4136, Validation Loss:1.0672, Validation Accuracy:0.4253\n",
    "Epoch #133: Loss:1.0689, Accuracy:0.4090, Validation Loss:1.0679, Validation Accuracy:0.4154\n",
    "Epoch #134: Loss:1.0687, Accuracy:0.4082, Validation Loss:1.0678, Validation Accuracy:0.4171\n",
    "Epoch #135: Loss:1.0684, Accuracy:0.4053, Validation Loss:1.0675, Validation Accuracy:0.4253\n",
    "Epoch #136: Loss:1.0691, Accuracy:0.3996, Validation Loss:1.0680, Validation Accuracy:0.4236\n",
    "Epoch #137: Loss:1.0687, Accuracy:0.4053, Validation Loss:1.0678, Validation Accuracy:0.4039\n",
    "Epoch #138: Loss:1.0692, Accuracy:0.4082, Validation Loss:1.0682, Validation Accuracy:0.4072\n",
    "Epoch #139: Loss:1.0692, Accuracy:0.4099, Validation Loss:1.0672, Validation Accuracy:0.4056\n",
    "Epoch #140: Loss:1.0690, Accuracy:0.4074, Validation Loss:1.0668, Validation Accuracy:0.3810\n",
    "Epoch #141: Loss:1.0693, Accuracy:0.4082, Validation Loss:1.0683, Validation Accuracy:0.4122\n",
    "Epoch #142: Loss:1.0685, Accuracy:0.4094, Validation Loss:1.0669, Validation Accuracy:0.4072\n",
    "Epoch #143: Loss:1.0687, Accuracy:0.4070, Validation Loss:1.0659, Validation Accuracy:0.4089\n",
    "Epoch #144: Loss:1.0685, Accuracy:0.4057, Validation Loss:1.0658, Validation Accuracy:0.4072\n",
    "Epoch #145: Loss:1.0690, Accuracy:0.4074, Validation Loss:1.0656, Validation Accuracy:0.3974\n",
    "Epoch #146: Loss:1.0680, Accuracy:0.4041, Validation Loss:1.0651, Validation Accuracy:0.4056\n",
    "Epoch #147: Loss:1.0678, Accuracy:0.4057, Validation Loss:1.0651, Validation Accuracy:0.4072\n",
    "Epoch #148: Loss:1.0685, Accuracy:0.4037, Validation Loss:1.0661, Validation Accuracy:0.4023\n",
    "Epoch #149: Loss:1.0674, Accuracy:0.4111, Validation Loss:1.0657, Validation Accuracy:0.4072\n",
    "Epoch #150: Loss:1.0664, Accuracy:0.4119, Validation Loss:1.0648, Validation Accuracy:0.4039\n",
    "Epoch #151: Loss:1.0675, Accuracy:0.4078, Validation Loss:1.0668, Validation Accuracy:0.4072\n",
    "Epoch #152: Loss:1.0670, Accuracy:0.4062, Validation Loss:1.0660, Validation Accuracy:0.4204\n",
    "Epoch #153: Loss:1.0666, Accuracy:0.4000, Validation Loss:1.0656, Validation Accuracy:0.4236\n",
    "Epoch #154: Loss:1.0656, Accuracy:0.4082, Validation Loss:1.0667, Validation Accuracy:0.4236\n",
    "Epoch #155: Loss:1.0673, Accuracy:0.4094, Validation Loss:1.0664, Validation Accuracy:0.4138\n",
    "Epoch #156: Loss:1.0664, Accuracy:0.4074, Validation Loss:1.0668, Validation Accuracy:0.4154\n",
    "Epoch #157: Loss:1.0657, Accuracy:0.4086, Validation Loss:1.0662, Validation Accuracy:0.4122\n",
    "Epoch #158: Loss:1.0656, Accuracy:0.4103, Validation Loss:1.0654, Validation Accuracy:0.4154\n",
    "Epoch #159: Loss:1.0665, Accuracy:0.4156, Validation Loss:1.0673, Validation Accuracy:0.4253\n",
    "Epoch #160: Loss:1.0675, Accuracy:0.4131, Validation Loss:1.0695, Validation Accuracy:0.4319\n",
    "Epoch #161: Loss:1.0679, Accuracy:0.4123, Validation Loss:1.0711, Validation Accuracy:0.4269\n",
    "Epoch #162: Loss:1.0684, Accuracy:0.4127, Validation Loss:1.0718, Validation Accuracy:0.4269\n",
    "Epoch #163: Loss:1.0651, Accuracy:0.4115, Validation Loss:1.0720, Validation Accuracy:0.4171\n",
    "Epoch #164: Loss:1.0661, Accuracy:0.4119, Validation Loss:1.0698, Validation Accuracy:0.4204\n",
    "Epoch #165: Loss:1.0664, Accuracy:0.4090, Validation Loss:1.0716, Validation Accuracy:0.4154\n",
    "Epoch #166: Loss:1.0667, Accuracy:0.4041, Validation Loss:1.0702, Validation Accuracy:0.4171\n",
    "Epoch #167: Loss:1.0660, Accuracy:0.4099, Validation Loss:1.0708, Validation Accuracy:0.4089\n",
    "Epoch #168: Loss:1.0674, Accuracy:0.4160, Validation Loss:1.0718, Validation Accuracy:0.4138\n",
    "Epoch #169: Loss:1.0666, Accuracy:0.4156, Validation Loss:1.0710, Validation Accuracy:0.4089\n",
    "Epoch #170: Loss:1.0679, Accuracy:0.4074, Validation Loss:1.0707, Validation Accuracy:0.4138\n",
    "Epoch #171: Loss:1.0677, Accuracy:0.4086, Validation Loss:1.0693, Validation Accuracy:0.4122\n",
    "Epoch #172: Loss:1.0684, Accuracy:0.3996, Validation Loss:1.0703, Validation Accuracy:0.4187\n",
    "Epoch #173: Loss:1.0658, Accuracy:0.4111, Validation Loss:1.0691, Validation Accuracy:0.4007\n",
    "Epoch #174: Loss:1.0664, Accuracy:0.4045, Validation Loss:1.0684, Validation Accuracy:0.4220\n",
    "Epoch #175: Loss:1.0670, Accuracy:0.4131, Validation Loss:1.0664, Validation Accuracy:0.4154\n",
    "Epoch #176: Loss:1.0653, Accuracy:0.4066, Validation Loss:1.0652, Validation Accuracy:0.4220\n",
    "Epoch #177: Loss:1.0667, Accuracy:0.4094, Validation Loss:1.0657, Validation Accuracy:0.4384\n",
    "Epoch #178: Loss:1.0672, Accuracy:0.4131, Validation Loss:1.0656, Validation Accuracy:0.4154\n",
    "Epoch #179: Loss:1.0653, Accuracy:0.4074, Validation Loss:1.0693, Validation Accuracy:0.4269\n",
    "Epoch #180: Loss:1.0663, Accuracy:0.4086, Validation Loss:1.0654, Validation Accuracy:0.4269\n",
    "Epoch #181: Loss:1.0676, Accuracy:0.4049, Validation Loss:1.0655, Validation Accuracy:0.4384\n",
    "Epoch #182: Loss:1.0670, Accuracy:0.4074, Validation Loss:1.0668, Validation Accuracy:0.4335\n",
    "Epoch #183: Loss:1.0650, Accuracy:0.4062, Validation Loss:1.0662, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0656, Accuracy:0.4057, Validation Loss:1.0657, Validation Accuracy:0.4105\n",
    "Epoch #185: Loss:1.0647, Accuracy:0.4115, Validation Loss:1.0635, Validation Accuracy:0.4154\n",
    "Epoch #186: Loss:1.0657, Accuracy:0.4123, Validation Loss:1.0630, Validation Accuracy:0.4056\n",
    "Epoch #187: Loss:1.0662, Accuracy:0.4049, Validation Loss:1.0642, Validation Accuracy:0.4351\n",
    "Epoch #188: Loss:1.0641, Accuracy:0.4094, Validation Loss:1.0644, Validation Accuracy:0.3908\n",
    "Epoch #189: Loss:1.0652, Accuracy:0.4078, Validation Loss:1.0632, Validation Accuracy:0.4384\n",
    "Epoch #190: Loss:1.0640, Accuracy:0.4070, Validation Loss:1.0642, Validation Accuracy:0.4516\n",
    "Epoch #191: Loss:1.0642, Accuracy:0.4049, Validation Loss:1.0657, Validation Accuracy:0.4351\n",
    "Epoch #192: Loss:1.0642, Accuracy:0.4021, Validation Loss:1.0667, Validation Accuracy:0.4335\n",
    "Epoch #193: Loss:1.0645, Accuracy:0.4025, Validation Loss:1.0672, Validation Accuracy:0.4450\n",
    "Epoch #194: Loss:1.0645, Accuracy:0.4107, Validation Loss:1.0680, Validation Accuracy:0.4286\n",
    "Epoch #195: Loss:1.0636, Accuracy:0.4045, Validation Loss:1.0707, Validation Accuracy:0.4253\n",
    "Epoch #196: Loss:1.0642, Accuracy:0.4094, Validation Loss:1.0729, Validation Accuracy:0.4187\n",
    "Epoch #197: Loss:1.0658, Accuracy:0.4115, Validation Loss:1.0706, Validation Accuracy:0.4187\n",
    "Epoch #198: Loss:1.0659, Accuracy:0.4144, Validation Loss:1.0740, Validation Accuracy:0.4220\n",
    "Epoch #199: Loss:1.0672, Accuracy:0.4181, Validation Loss:1.0714, Validation Accuracy:0.4187\n",
    "Epoch #200: Loss:1.0684, Accuracy:0.4103, Validation Loss:1.0696, Validation Accuracy:0.4204\n",
    "Epoch #201: Loss:1.0683, Accuracy:0.4025, Validation Loss:1.0687, Validation Accuracy:0.4154\n",
    "Epoch #202: Loss:1.0666, Accuracy:0.4131, Validation Loss:1.0687, Validation Accuracy:0.4154\n",
    "Epoch #203: Loss:1.0643, Accuracy:0.4201, Validation Loss:1.0697, Validation Accuracy:0.4122\n",
    "Epoch #204: Loss:1.0647, Accuracy:0.4090, Validation Loss:1.0699, Validation Accuracy:0.4122\n",
    "Epoch #205: Loss:1.0653, Accuracy:0.4090, Validation Loss:1.0690, Validation Accuracy:0.4138\n",
    "Epoch #206: Loss:1.0652, Accuracy:0.4008, Validation Loss:1.0700, Validation Accuracy:0.4154\n",
    "Epoch #207: Loss:1.0642, Accuracy:0.4172, Validation Loss:1.0696, Validation Accuracy:0.4089\n",
    "Epoch #208: Loss:1.0636, Accuracy:0.4172, Validation Loss:1.0691, Validation Accuracy:0.4220\n",
    "Epoch #209: Loss:1.0662, Accuracy:0.4053, Validation Loss:1.0691, Validation Accuracy:0.4269\n",
    "Epoch #210: Loss:1.0682, Accuracy:0.4070, Validation Loss:1.0694, Validation Accuracy:0.4187\n",
    "Epoch #211: Loss:1.0671, Accuracy:0.4078, Validation Loss:1.0725, Validation Accuracy:0.4089\n",
    "Epoch #212: Loss:1.0647, Accuracy:0.4086, Validation Loss:1.0713, Validation Accuracy:0.4056\n",
    "Epoch #213: Loss:1.0685, Accuracy:0.4000, Validation Loss:1.0706, Validation Accuracy:0.4269\n",
    "Epoch #214: Loss:1.0678, Accuracy:0.3938, Validation Loss:1.0718, Validation Accuracy:0.4220\n",
    "Epoch #215: Loss:1.0659, Accuracy:0.4049, Validation Loss:1.0707, Validation Accuracy:0.4335\n",
    "Epoch #216: Loss:1.0673, Accuracy:0.3955, Validation Loss:1.0705, Validation Accuracy:0.4417\n",
    "Epoch #217: Loss:1.0679, Accuracy:0.3864, Validation Loss:1.0720, Validation Accuracy:0.4368\n",
    "Epoch #218: Loss:1.0693, Accuracy:0.4000, Validation Loss:1.0702, Validation Accuracy:0.4138\n",
    "Epoch #219: Loss:1.0672, Accuracy:0.4012, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #220: Loss:1.0681, Accuracy:0.3955, Validation Loss:1.0764, Validation Accuracy:0.3842\n",
    "Epoch #221: Loss:1.0673, Accuracy:0.3910, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #222: Loss:1.0670, Accuracy:0.4078, Validation Loss:1.0748, Validation Accuracy:0.4007\n",
    "Epoch #223: Loss:1.0659, Accuracy:0.4066, Validation Loss:1.0759, Validation Accuracy:0.3957\n",
    "Epoch #224: Loss:1.0658, Accuracy:0.3988, Validation Loss:1.0759, Validation Accuracy:0.4105\n",
    "Epoch #225: Loss:1.0651, Accuracy:0.4066, Validation Loss:1.0756, Validation Accuracy:0.4204\n",
    "Epoch #226: Loss:1.0650, Accuracy:0.4082, Validation Loss:1.0752, Validation Accuracy:0.4204\n",
    "Epoch #227: Loss:1.0654, Accuracy:0.4074, Validation Loss:1.0760, Validation Accuracy:0.4122\n",
    "Epoch #228: Loss:1.0641, Accuracy:0.4066, Validation Loss:1.0748, Validation Accuracy:0.4023\n",
    "Epoch #229: Loss:1.0637, Accuracy:0.4049, Validation Loss:1.0741, Validation Accuracy:0.4056\n",
    "Epoch #230: Loss:1.0624, Accuracy:0.4099, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #231: Loss:1.0631, Accuracy:0.4049, Validation Loss:1.0707, Validation Accuracy:0.4105\n",
    "Epoch #232: Loss:1.0649, Accuracy:0.4078, Validation Loss:1.0713, Validation Accuracy:0.4154\n",
    "Epoch #233: Loss:1.0638, Accuracy:0.3967, Validation Loss:1.0715, Validation Accuracy:0.4056\n",
    "Epoch #234: Loss:1.0630, Accuracy:0.4070, Validation Loss:1.0715, Validation Accuracy:0.4072\n",
    "Epoch #235: Loss:1.0639, Accuracy:0.3725, Validation Loss:1.0721, Validation Accuracy:0.4056\n",
    "Epoch #236: Loss:1.0629, Accuracy:0.4082, Validation Loss:1.0711, Validation Accuracy:0.4023\n",
    "Epoch #237: Loss:1.0619, Accuracy:0.4033, Validation Loss:1.0697, Validation Accuracy:0.4072\n",
    "Epoch #238: Loss:1.0629, Accuracy:0.3955, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #239: Loss:1.0617, Accuracy:0.4078, Validation Loss:1.0723, Validation Accuracy:0.3974\n",
    "Epoch #240: Loss:1.0628, Accuracy:0.4053, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #241: Loss:1.0629, Accuracy:0.4115, Validation Loss:1.0738, Validation Accuracy:0.4056\n",
    "Epoch #242: Loss:1.0631, Accuracy:0.4119, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #243: Loss:1.0630, Accuracy:0.4103, Validation Loss:1.0728, Validation Accuracy:0.4072\n",
    "Epoch #244: Loss:1.0620, Accuracy:0.4181, Validation Loss:1.0751, Validation Accuracy:0.4089\n",
    "Epoch #245: Loss:1.0606, Accuracy:0.4152, Validation Loss:1.0772, Validation Accuracy:0.3990\n",
    "Epoch #246: Loss:1.0632, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #247: Loss:1.0618, Accuracy:0.4119, Validation Loss:1.0721, Validation Accuracy:0.4039\n",
    "Epoch #248: Loss:1.0626, Accuracy:0.3930, Validation Loss:1.0720, Validation Accuracy:0.4023\n",
    "Epoch #249: Loss:1.0616, Accuracy:0.4103, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #250: Loss:1.0627, Accuracy:0.3988, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #251: Loss:1.0658, Accuracy:0.4127, Validation Loss:1.0734, Validation Accuracy:0.4154\n",
    "Epoch #252: Loss:1.0668, Accuracy:0.3848, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #253: Loss:1.0635, Accuracy:0.4107, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #254: Loss:1.0628, Accuracy:0.4127, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #255: Loss:1.0632, Accuracy:0.3955, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #256: Loss:1.0654, Accuracy:0.4107, Validation Loss:1.0686, Validation Accuracy:0.4056\n",
    "Epoch #257: Loss:1.0611, Accuracy:0.4103, Validation Loss:1.0714, Validation Accuracy:0.4220\n",
    "Epoch #258: Loss:1.0609, Accuracy:0.4021, Validation Loss:1.0686, Validation Accuracy:0.4122\n",
    "Epoch #259: Loss:1.0597, Accuracy:0.4119, Validation Loss:1.0701, Validation Accuracy:0.4253\n",
    "Epoch #260: Loss:1.0600, Accuracy:0.4136, Validation Loss:1.0682, Validation Accuracy:0.4171\n",
    "Epoch #261: Loss:1.0587, Accuracy:0.4049, Validation Loss:1.0704, Validation Accuracy:0.4138\n",
    "Epoch #262: Loss:1.0576, Accuracy:0.4152, Validation Loss:1.0702, Validation Accuracy:0.4089\n",
    "Epoch #263: Loss:1.0600, Accuracy:0.4127, Validation Loss:1.0706, Validation Accuracy:0.4220\n",
    "Epoch #264: Loss:1.0594, Accuracy:0.4160, Validation Loss:1.0698, Validation Accuracy:0.4039\n",
    "Epoch #265: Loss:1.0593, Accuracy:0.4000, Validation Loss:1.0712, Validation Accuracy:0.4056\n",
    "Epoch #266: Loss:1.0648, Accuracy:0.4115, Validation Loss:1.0729, Validation Accuracy:0.4171\n",
    "Epoch #267: Loss:1.0613, Accuracy:0.4000, Validation Loss:1.0702, Validation Accuracy:0.4023\n",
    "Epoch #268: Loss:1.0599, Accuracy:0.4090, Validation Loss:1.0719, Validation Accuracy:0.4187\n",
    "Epoch #269: Loss:1.0574, Accuracy:0.4062, Validation Loss:1.0738, Validation Accuracy:0.4105\n",
    "Epoch #270: Loss:1.0586, Accuracy:0.4148, Validation Loss:1.0763, Validation Accuracy:0.4138\n",
    "Epoch #271: Loss:1.0584, Accuracy:0.4004, Validation Loss:1.0781, Validation Accuracy:0.4072\n",
    "Epoch #272: Loss:1.0557, Accuracy:0.4140, Validation Loss:1.0787, Validation Accuracy:0.3810\n",
    "Epoch #273: Loss:1.0553, Accuracy:0.4053, Validation Loss:1.0761, Validation Accuracy:0.4204\n",
    "Epoch #274: Loss:1.0558, Accuracy:0.4119, Validation Loss:1.0753, Validation Accuracy:0.4171\n",
    "Epoch #275: Loss:1.0564, Accuracy:0.4140, Validation Loss:1.0757, Validation Accuracy:0.4187\n",
    "Epoch #276: Loss:1.0583, Accuracy:0.4148, Validation Loss:1.0737, Validation Accuracy:0.4056\n",
    "Epoch #277: Loss:1.0573, Accuracy:0.4140, Validation Loss:1.0743, Validation Accuracy:0.4089\n",
    "Epoch #278: Loss:1.0563, Accuracy:0.3971, Validation Loss:1.0736, Validation Accuracy:0.4171\n",
    "Epoch #279: Loss:1.0553, Accuracy:0.4144, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #280: Loss:1.0541, Accuracy:0.4045, Validation Loss:1.0756, Validation Accuracy:0.4039\n",
    "Epoch #281: Loss:1.0539, Accuracy:0.4107, Validation Loss:1.0697, Validation Accuracy:0.4138\n",
    "Epoch #282: Loss:1.0529, Accuracy:0.4218, Validation Loss:1.0691, Validation Accuracy:0.4269\n",
    "Epoch #283: Loss:1.0539, Accuracy:0.4144, Validation Loss:1.0701, Validation Accuracy:0.4302\n",
    "Epoch #284: Loss:1.0565, Accuracy:0.4119, Validation Loss:1.0698, Validation Accuracy:0.4138\n",
    "Epoch #285: Loss:1.0592, Accuracy:0.4189, Validation Loss:1.0708, Validation Accuracy:0.4105\n",
    "Epoch #286: Loss:1.0581, Accuracy:0.3951, Validation Loss:1.0699, Validation Accuracy:0.4220\n",
    "Epoch #287: Loss:1.0551, Accuracy:0.4148, Validation Loss:1.0669, Validation Accuracy:0.4204\n",
    "Epoch #288: Loss:1.0564, Accuracy:0.4160, Validation Loss:1.0679, Validation Accuracy:0.4089\n",
    "Epoch #289: Loss:1.0558, Accuracy:0.3959, Validation Loss:1.0639, Validation Accuracy:0.3892\n",
    "Epoch #290: Loss:1.0596, Accuracy:0.3992, Validation Loss:1.0675, Validation Accuracy:0.4154\n",
    "Epoch #291: Loss:1.0590, Accuracy:0.3992, Validation Loss:1.0707, Validation Accuracy:0.3842\n",
    "Epoch #292: Loss:1.0583, Accuracy:0.4201, Validation Loss:1.0712, Validation Accuracy:0.4007\n",
    "Epoch #293: Loss:1.0571, Accuracy:0.4185, Validation Loss:1.0712, Validation Accuracy:0.4023\n",
    "Epoch #294: Loss:1.0563, Accuracy:0.4172, Validation Loss:1.0718, Validation Accuracy:0.4072\n",
    "Epoch #295: Loss:1.0544, Accuracy:0.4140, Validation Loss:1.0715, Validation Accuracy:0.4072\n",
    "Epoch #296: Loss:1.0547, Accuracy:0.4136, Validation Loss:1.0711, Validation Accuracy:0.3990\n",
    "Epoch #297: Loss:1.0563, Accuracy:0.4094, Validation Loss:1.0716, Validation Accuracy:0.4072\n",
    "Epoch #298: Loss:1.0578, Accuracy:0.4148, Validation Loss:1.0716, Validation Accuracy:0.3990\n",
    "Epoch #299: Loss:1.0570, Accuracy:0.4090, Validation Loss:1.0731, Validation Accuracy:0.4072\n",
    "Epoch #300: Loss:1.0578, Accuracy:0.4107, Validation Loss:1.0694, Validation Accuracy:0.3990\n",
    "\n",
    "Test:\n",
    "Test Loss:1.06944823, Accuracy:0.3990\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03   01  02\n",
    "t:03   2  135   5\n",
    "t:01   0  222  18\n",
    "t:02   1  207  19\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.67      0.01      0.03       142\n",
    "          01       0.39      0.93      0.55       240\n",
    "          02       0.45      0.08      0.14       227\n",
    "\n",
    "    accuracy                           0.40       609\n",
    "   macro avg       0.50      0.34      0.24       609\n",
    "weighted avg       0.48      0.40      0.28       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 02:26:24 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0764331161877987, 1.075534789628779, 1.0757456017636704, 1.075056176271736, 1.074727563043729, 1.0750223940424928, 1.0745541137231787, 1.0744054182409652, 1.0745862217372275, 1.0746373589990175, 1.0746926807221913, 1.0747369429943792, 1.0748060254627847, 1.0752661208605336, 1.0753347253173051, 1.074261145051477, 1.0748654678341594, 1.0747775264169979, 1.0746353631732108, 1.0744151477939, 1.074015868512672, 1.074220719204356, 1.0733573368225975, 1.073792937158168, 1.0741586358284911, 1.073873807644022, 1.0736842380564398, 1.0727699206184675, 1.0734072729871778, 1.0730827025004797, 1.072918176651001, 1.072382492972125, 1.07231227674312, 1.0730707780480973, 1.0729513189867017, 1.0726957875128058, 1.0719496728164222, 1.0722915863951634, 1.0723983460459217, 1.0717392844715337, 1.0718212732540562, 1.0727767730972841, 1.0725161659306492, 1.071901763992748, 1.0718610772162627, 1.0720964572308294, 1.0713874428534547, 1.0712432847625908, 1.0707905652683551, 1.0703596092014283, 1.0706758579401352, 1.0731762238519729, 1.0732098453737833, 1.071809094723418, 1.0712608586391206, 1.0694859084628878, 1.0710751681492245, 1.0700994095778817, 1.0710307389057327, 1.071198733960858, 1.0705409228116616, 1.0692495779059399, 1.07097328629204, 1.06936412922463, 1.0689544840203522, 1.069265564674227, 1.068927921489346, 1.0688956836957257, 1.070080085340979, 1.0701090984156567, 1.069988151684966, 1.0697680476851064, 1.068335961825742, 1.0681492998486473, 1.06903737421302, 1.068711607522761, 1.0689266851578636, 1.068972676845607, 1.0690878653173963, 1.0691478420752414, 1.0680480420295828, 1.068662333175271, 1.0685774408929258, 1.067410628783879, 1.0686358945514574, 1.069146776825728, 1.0695108539365195, 1.0691554832145302, 1.069777469916884, 1.0708119884891854, 1.069180721524118, 1.0696367320951765, 1.0699079541737222, 1.0700999265429618, 1.0697587785266696, 1.0689650178934358, 1.069484463270466, 1.0687596934965287, 1.0692085143184817, 1.070206295876276, 1.0709346695290802, 1.0707169089998518, 1.0715315461354498, 1.0702283721056283, 1.0689466968545773, 1.0691871108679936, 1.0689519305143058, 1.0692885714798726, 1.0695199766769785, 1.0687909034281138, 1.0691306303287376, 1.068634287281381, 1.0672848663110843, 1.0675663703376632, 1.0670138159017453, 1.067194299940601, 1.0696530021078676, 1.07022619854249, 1.0689153855265852, 1.068971511374162, 1.0702386816538418, 1.0688966945278624, 1.0689234807965007, 1.069163465734773, 1.067499894813951, 1.0680407383563288, 1.067548062413784, 1.0673001714919392, 1.066726502135078, 1.0674540614846892, 1.0672128527426759, 1.0671883228377168, 1.067917057641817, 1.0677560145044562, 1.0675159146633055, 1.0680488811925126, 1.067816567538407, 1.0682017382338325, 1.0671969307663014, 1.066820009588608, 1.068264263016837, 1.066903492695788, 1.0658886332817266, 1.065772352938973, 1.0656247644001626, 1.0651481112431618, 1.0651427219653953, 1.066110210857172, 1.0656615239254554, 1.0647786873314768, 1.0668264977841933, 1.065966722021745, 1.0656281296647045, 1.0666822582630102, 1.0664190159642637, 1.0668418231268821, 1.0662418390533999, 1.0654088119763654, 1.06732279091633, 1.0695061942039452, 1.0710870154776988, 1.071760107535251, 1.0719963192744013, 1.0697930736103276, 1.0715552654563891, 1.070247486894354, 1.0707748445188274, 1.071804462395278, 1.0710397821733322, 1.070655169745384, 1.0692898350200435, 1.0703233961988552, 1.0691039403671114, 1.068407338632543, 1.066426981258862, 1.0652343383172072, 1.0656906618860555, 1.0655885694062182, 1.0693449619759872, 1.0654152786398952, 1.0654907441883057, 1.0667741605996695, 1.066241579494257, 1.0656666342652295, 1.06347266128302, 1.0630076366105103, 1.064244611314169, 1.0644415834267151, 1.063199875585747, 1.064249443107442, 1.0656882964918766, 1.0666790928550933, 1.0671821987296168, 1.068028108826999, 1.07070372335625, 1.0729483111542826, 1.0706009600550084, 1.0739589937410527, 1.0714494318797672, 1.0695632242021107, 1.0687101476493923, 1.0687031438589487, 1.0696645996644971, 1.069944626004825, 1.06900297988616, 1.0700434037225783, 1.0695882781190043, 1.069112411077778, 1.0691111070181936, 1.0694227199053334, 1.072461948997673, 1.071344032271938, 1.0706488696616663, 1.071769488856123, 1.070689033209201, 1.070477531265547, 1.0719543922515142, 1.070200435437984, 1.073931994696556, 1.0763557385928526, 1.074897835407351, 1.0747569879679062, 1.0759068200936655, 1.0759043241369313, 1.075574034932016, 1.0752295189107384, 1.0759586726111927, 1.0747954663384724, 1.074094534116034, 1.0728279044866953, 1.0707239411734595, 1.0712843685118827, 1.0715102912561452, 1.071462193146128, 1.0720948192286375, 1.0711040442017303, 1.0697170527306292, 1.073844899489179, 1.0723494528158153, 1.0748206568860459, 1.0737713189743618, 1.0741198924178952, 1.0727997667879503, 1.075092154966395, 1.077235645848542, 1.0746888088671054, 1.0720543385726478, 1.0719907919957328, 1.0754616170485423, 1.0754540915951156, 1.0733890439489204, 1.073672438098488, 1.0733944520182994, 1.074754837894283, 1.0730572811684194, 1.0686249288627863, 1.0714388428063228, 1.06857599687498, 1.0701177359018812, 1.0682037037190153, 1.070398869968596, 1.0701644248367335, 1.070637728976107, 1.0698373219845525, 1.071232444742826, 1.0729166003087862, 1.0702132982964978, 1.0719362569755717, 1.0738298969315778, 1.076302283503152, 1.0780971555287027, 1.078682614860472, 1.0760983029022593, 1.0753403757201823, 1.0757427008085454, 1.0737088863877045, 1.0742841262144016, 1.0735835623858598, 1.0745290952363038, 1.0756301932734222, 1.069703971811116, 1.0691010446971274, 1.0700988209697804, 1.069793160325788, 1.0707773289265499, 1.0699271255330303, 1.066866085838606, 1.0679031209209673, 1.0638924218555195, 1.0675347148882737, 1.0706715730610739, 1.071208641055378, 1.0712103099854318, 1.0717993085999011, 1.0715169626699488, 1.0710603993122996, 1.071558625827282, 1.0715819158773312, 1.0730787345341273, 1.0694480942583633], 'val_acc': [0.3628899818668616, 0.39737274146628104, 0.3924466329940238, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.3957307052436133, 0.39408866911881857, 0.3940886690209456, 0.3908045968692291, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.4006568138137435, 0.3957307052436133, 0.39408866911881857, 0.39408866911881857, 0.39408866921669156, 0.39408866911881857, 0.3957307053414863, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.3924466329940238, 0.38916256074443434, 0.38423645217430413, 0.39737274146628104, 0.38916256074443434, 0.3957307054393593, 0.3990147776889488, 0.39901477778682176, 0.3990147776889488, 0.3858784886905908, 0.3940886695103105, 0.3891625610380533, 0.3924466333855158, 0.38752052491325856, 0.4154351391326422, 0.42200328343607524, 0.39408866941243753, 0.3858784886905908, 0.40229884993853826, 0.40229885052577613, 0.42200328343607524, 0.41050902650078336, 0.4055829224817467, 0.40722495860654145, 0.4088669947313362, 0.4137931032035934, 0.40722495850866847, 0.41871921138223167, 0.4154351348751676, 0.40394088596546, 0.3858784888863368, 0.3924466330918968, 0.39408866931456454, 0.4170771753553099, 0.3957307054393593, 0.42364531965874297, 0.40558292228600074, 0.4006568138137435, 0.4072249584107955, 0.3924466333855158, 0.403940886259079, 0.40229884993853826, 0.4318555001848437, 0.3842364525657961, 0.40229885013428424, 0.4334975360160195, 0.42857142764163525, 0.41707717515956394, 0.4187192107949938, 0.4154351386432773, 0.4170771752574369, 0.43349753630963844, 0.41871921118648575, 0.41379310300784744, 0.4351395724344332, 0.3858784886905908, 0.41707717515956394, 0.4318555005763356, 0.40394088635695197, 0.40394088635695197, 0.417077175061691, 0.4170771752574369, 0.38752052481538557, 0.40722495860654145, 0.4072249580193036, 0.42528735558779174, 0.42528735558779174, 0.4072249580193036, 0.41543513923051517, 0.3924466332876428, 0.3957307053414863, 0.39408866931456454, 0.3924466332876428, 0.4055829225796197, 0.40229885003641125, 0.3990147776889488, 0.43021346435366786, 0.41379310290997445, 0.41871921118648575, 0.42528735597928363, 0.4088669945355902, 0.41379310310572043, 0.4187192112843587, 0.4220032835339482, 0.4269293921040784, 0.43349753611389247, 0.4252873556856647, 0.39737274156415403, 0.423645319462997, 0.4285714280331272, 0.39408866921669156, 0.42200328363182116, 0.4154351390347692, 0.41871921138223167, 0.41379310290997445, 0.4334975367990033, 0.4236453154991413, 0.4499178980469508, 0.43842364517338756, 0.44006568120030937, 0.4285714281310002, 0.41050903075825795, 0.4252873557835377, 0.41543513893689626, 0.417077175061691, 0.42528735588141064, 0.4236453198544889, 0.40394088596546, 0.4072249583129225, 0.40558292209025476, 0.3809523804140796, 0.4121510663936878, 0.40722495811717657, 0.40886699443771723, 0.4072249583129225, 0.397372741662027, 0.40558292218812775, 0.4072249584107955, 0.40229884974279234, 0.4072249582150495, 0.403940886259079, 0.4072249583129225, 0.4203612475070264, 0.42364531956087, 0.4236453198544889, 0.41379310281210147, 0.4154351391326422, 0.4121510666873067, 0.4154351388390233, 0.42528735597928363, 0.4318555004784626, 0.4269293921040784, 0.42692939181045947, 0.4170771754531829, 0.4203612472134075, 0.41543513893689626, 0.417077175061691, 0.40886699443771723, 0.41379310271422853, 0.40886699443771723, 0.41379310271422853, 0.4121510667851797, 0.41871921138223167, 0.4006568138137435, 0.42200328363182116, 0.41543513893689626, 0.42200328343607524, 0.43842364478189566, 0.4154351390347692, 0.42692939239769734, 0.4269293921040784, 0.43842364507551457, 0.43349753630963844, 0.39573070563510526, 0.4105090301710201, 0.4154351387411503, 0.4055829225796197, 0.4351395723365602, 0.39080459726072103, 0.43842364478189566, 0.4515599337802536, 0.43513957263017916, 0.43349753640751143, 0.44499178918320165, 0.4285714281310002, 0.4252873557835377, 0.41871921148010466, 0.41871921138223167, 0.4220032835339482, 0.41871921148010466, 0.4203612473112805, 0.41543513893689626, 0.41543513923051517, 0.4121510665894338, 0.4121510666873067, 0.41379310290997445, 0.41543513942626115, 0.4088669945355902, 0.42200328382756713, 0.4269293916147135, 0.41871921118648575, 0.4088669937526064, 0.40558292218812775, 0.42692939190833246, 0.42200328382756713, 0.43349753611389247, 0.4417077172272311, 0.4367816089507198, 0.41379310271422853, 0.40065681361799754, 0.38423645158706626, 0.39408866931456454, 0.4006568138137435, 0.3957307051457404, 0.41050903036676606, 0.42036124789851836, 0.42036124789851836, 0.4121510664915608, 0.40229884993853826, 0.40558292218812775, 0.403940886063333, 0.41050903075825795, 0.4154351387411503, 0.40558292218812775, 0.4072249584107955, 0.40558292228600074, 0.4022988504279032, 0.4072249584107955, 0.3990147776889488, 0.39737274156415403, 0.3957307053414863, 0.40558292199238183, 0.4039408858675871, 0.4072249582150495, 0.40886699443771723, 0.39901477798256774, 0.4055829223838737, 0.403940886161206, 0.40229884993853826, 0.39080459706497506, 0.4088669948292092, 0.41543513942626115, 0.3990147776889488, 0.39737274156415403, 0.39737274117266214, 0.3990147776889488, 0.40558292218812775, 0.42200327937434656, 0.4121510666873067, 0.4252873562729026, 0.4170771752574369, 0.41379310281210147, 0.40886699443771723, 0.4220032840233131, 0.403940886063333, 0.40558292218812775, 0.4170771755510559, 0.40229884993853826, 0.41871921118648575, 0.41050903095400393, 0.41379310300784744, 0.4072249582150495, 0.3809523793374768, 0.4203612478006454, 0.4170771756489289, 0.4187192071247571, 0.40558292218812775, 0.4088669945355902, 0.4170771756489289, 0.40229884993853826, 0.4039408866505709, 0.41379310290997445, 0.4269293877487308, 0.43021346425579493, 0.41379310271422853, 0.41050903026889307, 0.42200328343607524, 0.4203612475070264, 0.4088669942419713, 0.38916256015719647, 0.41543513893689626, 0.3842364514891933, 0.40065681371587053, 0.40229884974279234, 0.40722495850866847, 0.40722495870441444, 0.3990147775910758, 0.40722495880228743, 0.3990147776889488, 0.4072249589001604, 0.3990147776889488], 'loss': [1.080241095603614, 1.075050199301091, 1.07598184888123, 1.074847533276928, 1.0743511661122223, 1.0745699776761095, 1.0743821899993709, 1.0741673159158696, 1.0739254856501272, 1.07404942429286, 1.0735251897659146, 1.0732643013862124, 1.0732679719552856, 1.0733548432160207, 1.0733442363308197, 1.0731603764900193, 1.0733829279455072, 1.0735478407058872, 1.0733944579071577, 1.0731358338675214, 1.0730101366062674, 1.0733864869425185, 1.0734829359720375, 1.0731093621107097, 1.072964774118067, 1.072770642744687, 1.072664666322712, 1.0732680108268158, 1.0728904441641587, 1.0731824613449754, 1.0731938166295234, 1.073420144351356, 1.0730506923409213, 1.072904885720913, 1.073023566426193, 1.0725466906901993, 1.0728053898781966, 1.0721445484572611, 1.0732295950824964, 1.0723289345813727, 1.0713000547714546, 1.0728774091301512, 1.072962621839629, 1.0725681547756312, 1.0727026420452266, 1.0723417814262595, 1.0729457293202989, 1.0719451425501454, 1.072280072284675, 1.072328397331786, 1.0719254051635398, 1.072618254401111, 1.0717778809995866, 1.0719625574852163, 1.071931328421011, 1.0717819269188131, 1.0703611424816217, 1.0716116298150722, 1.0719448276613772, 1.0718601705112496, 1.0712262220206448, 1.07259924020121, 1.073023171836101, 1.0722167533036375, 1.0725793938372414, 1.0713183988046353, 1.0712516811594093, 1.0715230082584357, 1.0709678635215367, 1.071524198490981, 1.0707849483959972, 1.0708652392549927, 1.0712117071758795, 1.0710304190735553, 1.0705361090646388, 1.0707626984594294, 1.070204538241549, 1.0709646401708866, 1.070027405035814, 1.0704153830510634, 1.0706723351253375, 1.0727012176533255, 1.0722990921635402, 1.0710576126952436, 1.0709471712367, 1.0707098226527658, 1.0714395700294135, 1.0710186450143615, 1.071438890952594, 1.0702255111455428, 1.070901391491508, 1.0699532506891833, 1.0712733429804964, 1.0704935618249787, 1.0708423755496923, 1.0709162870226945, 1.0705559805433364, 1.0703564815207918, 1.0705757664459197, 1.0711398883521923, 1.0705961527031305, 1.0719697507744697, 1.070906741075692, 1.0704978916434538, 1.0699953382264906, 1.0711204549859925, 1.0699307782938838, 1.0699878695564349, 1.070500270688803, 1.070939039939238, 1.070809966337999, 1.0702694690692596, 1.0690922654874515, 1.0697244276011504, 1.0697965127486713, 1.069883959053478, 1.0694855794279972, 1.0710277067807177, 1.0708509092213436, 1.071077459548778, 1.0712042253365017, 1.0721406798098367, 1.0704447187682198, 1.0698855259580045, 1.0706123375060377, 1.0701921137451391, 1.06931272708415, 1.0697288385896466, 1.0697438111295445, 1.069685053531639, 1.070131760456234, 1.0689521077722004, 1.0688502179523758, 1.0686505231524395, 1.0684183090864021, 1.0691439521875714, 1.068704287227419, 1.0692363482970721, 1.069247142783915, 1.068994883198513, 1.0692550164228591, 1.068503465348935, 1.0687234422264646, 1.0685318588965727, 1.0689559461154976, 1.0680050852851946, 1.0678459219864016, 1.068487368374145, 1.0674424558204791, 1.0664180305704198, 1.0674555668351098, 1.066963666273583, 1.0666416269553025, 1.0655568626137484, 1.0673040320006728, 1.066419774012399, 1.0657361554414095, 1.0656028761756005, 1.0664942997436992, 1.0675030560464096, 1.0678920933842904, 1.0684156059974028, 1.0651197291497576, 1.0660944054503705, 1.066388445370496, 1.0667298560269804, 1.066040870198479, 1.067365987295977, 1.0666261976994038, 1.0679222567125513, 1.0676854263340914, 1.0683790710182894, 1.0658261105754783, 1.0663876938868841, 1.0669837905640964, 1.0653169786660823, 1.066652044231642, 1.0671929046113877, 1.065334943873192, 1.0662634144573486, 1.0676215357848995, 1.0670260336854374, 1.0649959114297949, 1.0655628835396111, 1.0646629164106303, 1.0657250284414272, 1.0661633715737282, 1.0641486762240682, 1.0651709068971982, 1.0639845249344435, 1.064178934626021, 1.0642228001197016, 1.0644892529050918, 1.0645188442735456, 1.0635675326999452, 1.0641849879611445, 1.0658487918195783, 1.0658566362803967, 1.0672437909202654, 1.0683951726439553, 1.068300026300262, 1.0665784525430668, 1.064278009148349, 1.0647434063760652, 1.0653493103794982, 1.0652229957267245, 1.0642384912443847, 1.0635980180156794, 1.0662072595384822, 1.068243156370441, 1.0670585678832976, 1.0646552466758712, 1.0684604202207844, 1.0677882321316603, 1.0659450466383165, 1.067344609765791, 1.067904453649658, 1.069271672090221, 1.0671532011130018, 1.0680980572710292, 1.0672845798351436, 1.0669582806573512, 1.0659489210626183, 1.0657632410648668, 1.065066906950557, 1.0649739127873885, 1.0654251520149027, 1.0641153804085828, 1.0637423125135825, 1.0624371724451838, 1.0630621346115332, 1.0649461514651164, 1.063792172545525, 1.0629723568471794, 1.063874180605769, 1.062888079795994, 1.0619400580560892, 1.062896826820452, 1.0617337806513667, 1.062834279003574, 1.0628530731436163, 1.0630720919896934, 1.0630407513534263, 1.0620406129276974, 1.0605571818792354, 1.0632384942542357, 1.0617527826610778, 1.0625572805287167, 1.0616357742148992, 1.0626573834820694, 1.0658336446515344, 1.0667847793449856, 1.0635412047775863, 1.0627972199441962, 1.06323414637078, 1.0653897005919313, 1.061079065412956, 1.0609335514064686, 1.0597194710306563, 1.0600349165820488, 1.0587153447971696, 1.0575997759429336, 1.0599707624016357, 1.0593764444641018, 1.0593410569784334, 1.0647909120367782, 1.0613056995051109, 1.0599008463736188, 1.0574043484928672, 1.0585964607262268, 1.0583831891386906, 1.0557333204046166, 1.0552929140459095, 1.055793913042276, 1.056437822974438, 1.0583305001014067, 1.0572748687967384, 1.0563332681048823, 1.055327246957736, 1.05411398885676, 1.0538830983810112, 1.052853809049242, 1.0538753538405872, 1.0565239254209295, 1.059196799981276, 1.0581256756792323, 1.0551366713012758, 1.0563618765719374, 1.0558011282640807, 1.0596002252194916, 1.0590304773201442, 1.0582640801611867, 1.0571237880836033, 1.05627924514747, 1.0543644338173053, 1.0546717547782882, 1.0563403201054253, 1.057825969964327, 1.057006762551576, 1.0578442245048665], 'acc': [0.3786447616818015, 0.383162218026312, 0.39096509237798577, 0.39425051250252147, 0.39425051391002336, 0.3942505119150424, 0.394250514301676, 0.3942505125392389, 0.3946611893372859, 0.39466119152809315, 0.3938398378585643, 0.39548254437759917, 0.3938398335136672, 0.3942505126983478, 0.39425051312671794, 0.3942505123434126, 0.3942505123066951, 0.3942505156724605, 0.395482545197622, 0.3938398345295164, 0.39630389984628256, 0.3946611907447878, 0.3942505121475862, 0.3934291573030993, 0.3942505128941742, 0.3946611905489614, 0.3946611897289386, 0.3942505156724605, 0.3942505141058497, 0.3942505123434126, 0.39671458079340033, 0.39794661361089234, 0.3946611905489614, 0.39466119309470393, 0.3991786427444012, 0.39794661047767077, 0.3963038994546299, 0.3958932247739553, 0.3987679663380069, 0.3975359338754501, 0.4004106799435077, 0.3958932232073445, 0.39835728750826155, 0.3958932221914953, 0.39178644542331814, 0.3917864499640416, 0.39589322179984265, 0.3921971246447162, 0.40123203165477306, 0.39917864509431733, 0.3954825475475382, 0.4065708412771597, 0.4053388103444963, 0.3971252554373575, 0.3979466138434361, 0.3958932247739553, 0.4139630409481589, 0.4069815216367984, 0.4008213547832912, 0.4106776174211404, 0.40492812939737854, 0.40082135716992484, 0.39958932036247097, 0.4032854189250993, 0.3950718685586839, 0.40328542189921196, 0.40328542029588377, 0.4024640656839406, 0.4036961004964135, 0.4008213565824458, 0.4049281291648348, 0.39999999935132524, 0.402874743852772, 0.403285422486691, 0.4065708412771597, 0.4106776182044458, 0.3999999999755217, 0.4065708422562914, 0.40492813135564204, 0.4049281305723367, 0.403285418729273, 0.3917864465982762, 0.40082135497911753, 0.40082135619079307, 0.406570844055446, 0.4053388083862328, 0.403285418729273, 0.39794661306013074, 0.39794661403926246, 0.40944558714939094, 0.40369609932145545, 0.3999999985680198, 0.4028747433020104, 0.39507186953781565, 0.4032854216666682, 0.39260780484524593, 0.4004106760269807, 0.3975359344629292, 0.3991786437235329, 0.3950718671878995, 0.3967145809892267, 0.40533880959790836, 0.39958932353241, 0.3995893247073681, 0.41149897249817113, 0.3963039031753305, 0.4036961006922399, 0.403285419316752, 0.40041067818107057, 0.3987679692386846, 0.4012320318505994, 0.4086242287433123, 0.40780287569797996, 0.3991786437235329, 0.3942505152808078, 0.39999999798054076, 0.4012320330255575, 0.40164271123110634, 0.4004106765777423, 0.4053388113603455, 0.40657083967383145, 0.398357288132458, 0.4090349089438421, 0.4073921953016238, 0.4000000005630007, 0.3958932216040163, 0.40369610030058717, 0.4016427127977171, 0.404106778506136, 0.40328542229086467, 0.4139630403606799, 0.41355236352591546, 0.40903490616555577, 0.4082135527285707, 0.4053388103812137, 0.3995893221616255, 0.4053388076029274, 0.40821355233691803, 0.4098562633966763, 0.40739219788408376, 0.4082135531202234, 0.4094455830003202, 0.40698152065766663, 0.40574948482934453, 0.4073921986673892, 0.404106774748718, 0.4057494854168236, 0.4036960977181272, 0.41108829464755753, 0.41190964988369716, 0.4078028770687644, 0.40616016362237245, 0.4000000015421325, 0.40821355092941614, 0.40944558476275733, 0.40739219510579744, 0.408624229134965, 0.4102669384322862, 0.41560575181209086, 0.4131416855161929, 0.41232032832178983, 0.4127310078981231, 0.4114989736731292, 0.41190965070372, 0.40903490734051384, 0.4041067741612389, 0.4098562649632871, 0.4160164250852636, 0.4156057492296309, 0.4073921980799101, 0.40862422893913863, 0.3995893219657992, 0.41108829503921024, 0.4045174515834824, 0.4131416823462539, 0.4065708442145549, 0.4094455833919729, 0.41314168610367197, 0.40739219690495204, 0.40862423210907767, 0.4049281315514684, 0.40739219945069455, 0.40616016600900606, 0.4057494850251709, 0.41149897265728, 0.4123203281259635, 0.40492813174729475, 0.40944558714939094, 0.4078028740946517, 0.4069815216367984, 0.4049281310007068, 0.4020533894366552, 0.40246406408061236, 0.41067761663783503, 0.4045174515834824, 0.40944558358779926, 0.41149897304893274, 0.41437371461298433, 0.4180698137630917, 0.41026694062309343, 0.4024640672505514, 0.4131416846961701, 0.4201232022450935, 0.40903490816053667, 0.40903490913966845, 0.4008213569740985, 0.4172484618927174, 0.41724846087686823, 0.4053388085820592, 0.4069815192868822, 0.4078028766771117, 0.40862422893913863, 0.399999999583869, 0.3938398378585643, 0.40492813374227565, 0.39548254696005913, 0.3864476385792178, 0.39999999938804265, 0.40123203122640294, 0.39548254594420995, 0.3909650917537893, 0.4078028766771117, 0.4065708438229022, 0.39876796790461766, 0.40657084124044224, 0.40821355370770246, 0.40739219886321554, 0.40657084065296323, 0.4049281325673176, 0.40985626316413254, 0.404928133350623, 0.4078028760896326, 0.3967145809892267, 0.40698152049855774, 0.37248459936901773, 0.4082135507335898, 0.4032854189250993, 0.3954825441817728, 0.4078028740946517, 0.40533880916953824, 0.411498974652261, 0.41190965129119905, 0.4102669407822023, 0.41806981356726536, 0.4151950700449503, 0.40041067736104774, 0.4119096492962181, 0.39301848262242467, 0.41026693862811253, 0.39876796712131224, 0.4127310064906212, 0.3848049275194595, 0.4106776182044458, 0.41273100731064405, 0.3954825455892747, 0.4106776168336614, 0.4102669398030706, 0.40205338724584794, 0.41190965324946255, 0.4135523599643237, 0.4049281313923595, 0.415195070436603, 0.4127310075064704, 0.4160164282552026, 0.3999999982130846, 0.4114989756313927, 0.4000000019337852, 0.40903490675303483, 0.40616016323071974, 0.41478439540099316, 0.4004106791602023, 0.41396304114398524, 0.4053388095611909, 0.41190964949204445, 0.4139630393448307, 0.41478439500934045, 0.4139630393815481, 0.3971252578239911, 0.41437371856622873, 0.40451745589166205, 0.4106776172253141, 0.42176591408815717, 0.41437371461298433, 0.4119096491003918, 0.4188911721691703, 0.39507186793448745, 0.414784394972623, 0.4160164260643953, 0.39589322418647627, 0.39917864689347193, 0.3991786447393821, 0.4201232016576144, 0.4184804947469269, 0.4172484614643473, 0.41396303797404627, 0.4135523599643237, 0.40944558714939094, 0.41478439540099316, 0.4090349099229738, 0.41067761902446864]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
