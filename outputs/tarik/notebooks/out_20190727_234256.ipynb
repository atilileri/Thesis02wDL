{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf53.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 23:42:56 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '2Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '05', '03', '04', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F01224CE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F00E5A7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6085, Accuracy:0.2066, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6066, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6063, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6002, Accuracy:0.2329, Validation Loss:1.5983, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.5970, Accuracy:0.2361, Validation Loss:1.5942, Validation Accuracy:0.2447\n",
    "Epoch #15: Loss:1.5915, Accuracy:0.2513, Validation Loss:1.5867, Validation Accuracy:0.2660\n",
    "Epoch #16: Loss:1.5820, Accuracy:0.2735, Validation Loss:1.5739, Validation Accuracy:0.2775\n",
    "Epoch #17: Loss:1.5652, Accuracy:0.2858, Validation Loss:1.5536, Validation Accuracy:0.2906\n",
    "Epoch #18: Loss:1.5397, Accuracy:0.3064, Validation Loss:1.5247, Validation Accuracy:0.3021\n",
    "Epoch #19: Loss:1.5072, Accuracy:0.3261, Validation Loss:1.4957, Validation Accuracy:0.3153\n",
    "Epoch #20: Loss:1.4799, Accuracy:0.3359, Validation Loss:1.4827, Validation Accuracy:0.3186\n",
    "Epoch #21: Loss:1.4698, Accuracy:0.3388, Validation Loss:1.4815, Validation Accuracy:0.3251\n",
    "Epoch #22: Loss:1.4674, Accuracy:0.3372, Validation Loss:1.4688, Validation Accuracy:0.3235\n",
    "Epoch #23: Loss:1.4547, Accuracy:0.3384, Validation Loss:1.4635, Validation Accuracy:0.3383\n",
    "Epoch #24: Loss:1.4508, Accuracy:0.3433, Validation Loss:1.4624, Validation Accuracy:0.3465\n",
    "Epoch #25: Loss:1.4507, Accuracy:0.3388, Validation Loss:1.4651, Validation Accuracy:0.3432\n",
    "Epoch #26: Loss:1.4465, Accuracy:0.3396, Validation Loss:1.4629, Validation Accuracy:0.3629\n",
    "Epoch #27: Loss:1.4476, Accuracy:0.3441, Validation Loss:1.4652, Validation Accuracy:0.3317\n",
    "Epoch #28: Loss:1.4428, Accuracy:0.3532, Validation Loss:1.4572, Validation Accuracy:0.3514\n",
    "Epoch #29: Loss:1.4416, Accuracy:0.3478, Validation Loss:1.4609, Validation Accuracy:0.3399\n",
    "Epoch #30: Loss:1.4386, Accuracy:0.3536, Validation Loss:1.4559, Validation Accuracy:0.3563\n",
    "Epoch #31: Loss:1.4419, Accuracy:0.3421, Validation Loss:1.4531, Validation Accuracy:0.3580\n",
    "Epoch #32: Loss:1.4372, Accuracy:0.3528, Validation Loss:1.4517, Validation Accuracy:0.3563\n",
    "Epoch #33: Loss:1.4354, Accuracy:0.3491, Validation Loss:1.4507, Validation Accuracy:0.3530\n",
    "Epoch #34: Loss:1.4343, Accuracy:0.3524, Validation Loss:1.4538, Validation Accuracy:0.3448\n",
    "Epoch #35: Loss:1.4323, Accuracy:0.3511, Validation Loss:1.4510, Validation Accuracy:0.3498\n",
    "Epoch #36: Loss:1.4324, Accuracy:0.3548, Validation Loss:1.4522, Validation Accuracy:0.3448\n",
    "Epoch #37: Loss:1.4317, Accuracy:0.3483, Validation Loss:1.4489, Validation Accuracy:0.3481\n",
    "Epoch #38: Loss:1.4286, Accuracy:0.3474, Validation Loss:1.4501, Validation Accuracy:0.3415\n",
    "Epoch #39: Loss:1.4263, Accuracy:0.3626, Validation Loss:1.4492, Validation Accuracy:0.3415\n",
    "Epoch #40: Loss:1.4264, Accuracy:0.3606, Validation Loss:1.4491, Validation Accuracy:0.3300\n",
    "Epoch #41: Loss:1.4247, Accuracy:0.3626, Validation Loss:1.4486, Validation Accuracy:0.3350\n",
    "Epoch #42: Loss:1.4251, Accuracy:0.3593, Validation Loss:1.4501, Validation Accuracy:0.3317\n",
    "Epoch #43: Loss:1.4252, Accuracy:0.3581, Validation Loss:1.4488, Validation Accuracy:0.3383\n",
    "Epoch #44: Loss:1.4218, Accuracy:0.3626, Validation Loss:1.4479, Validation Accuracy:0.3383\n",
    "Epoch #45: Loss:1.4253, Accuracy:0.3581, Validation Loss:1.4498, Validation Accuracy:0.3498\n",
    "Epoch #46: Loss:1.4233, Accuracy:0.3556, Validation Loss:1.4521, Validation Accuracy:0.3498\n",
    "Epoch #47: Loss:1.4214, Accuracy:0.3589, Validation Loss:1.4473, Validation Accuracy:0.3432\n",
    "Epoch #48: Loss:1.4191, Accuracy:0.3589, Validation Loss:1.4470, Validation Accuracy:0.3383\n",
    "Epoch #49: Loss:1.4181, Accuracy:0.3634, Validation Loss:1.4466, Validation Accuracy:0.3465\n",
    "Epoch #50: Loss:1.4191, Accuracy:0.3634, Validation Loss:1.4475, Validation Accuracy:0.3465\n",
    "Epoch #51: Loss:1.4175, Accuracy:0.3581, Validation Loss:1.4466, Validation Accuracy:0.3415\n",
    "Epoch #52: Loss:1.4194, Accuracy:0.3803, Validation Loss:1.4502, Validation Accuracy:0.3366\n",
    "Epoch #53: Loss:1.4142, Accuracy:0.3667, Validation Loss:1.4452, Validation Accuracy:0.3383\n",
    "Epoch #54: Loss:1.4145, Accuracy:0.3667, Validation Loss:1.4467, Validation Accuracy:0.3432\n",
    "Epoch #55: Loss:1.4153, Accuracy:0.3684, Validation Loss:1.4423, Validation Accuracy:0.3333\n",
    "Epoch #56: Loss:1.4129, Accuracy:0.3622, Validation Loss:1.4414, Validation Accuracy:0.3383\n",
    "Epoch #57: Loss:1.4139, Accuracy:0.3643, Validation Loss:1.4419, Validation Accuracy:0.3448\n",
    "Epoch #58: Loss:1.4118, Accuracy:0.3733, Validation Loss:1.4443, Validation Accuracy:0.3399\n",
    "Epoch #59: Loss:1.4128, Accuracy:0.3593, Validation Loss:1.4414, Validation Accuracy:0.3448\n",
    "Epoch #60: Loss:1.4099, Accuracy:0.3684, Validation Loss:1.4408, Validation Accuracy:0.3530\n",
    "Epoch #61: Loss:1.4109, Accuracy:0.3655, Validation Loss:1.4427, Validation Accuracy:0.3415\n",
    "Epoch #62: Loss:1.4089, Accuracy:0.3729, Validation Loss:1.4402, Validation Accuracy:0.3530\n",
    "Epoch #63: Loss:1.4069, Accuracy:0.3639, Validation Loss:1.4384, Validation Accuracy:0.3563\n",
    "Epoch #64: Loss:1.4070, Accuracy:0.3708, Validation Loss:1.4497, Validation Accuracy:0.3448\n",
    "Epoch #65: Loss:1.4069, Accuracy:0.3671, Validation Loss:1.4443, Validation Accuracy:0.3366\n",
    "Epoch #66: Loss:1.4131, Accuracy:0.3602, Validation Loss:1.4370, Validation Accuracy:0.3498\n",
    "Epoch #67: Loss:1.4068, Accuracy:0.3692, Validation Loss:1.4370, Validation Accuracy:0.3547\n",
    "Epoch #68: Loss:1.4062, Accuracy:0.3630, Validation Loss:1.4385, Validation Accuracy:0.3514\n",
    "Epoch #69: Loss:1.4034, Accuracy:0.3721, Validation Loss:1.4372, Validation Accuracy:0.3465\n",
    "Epoch #70: Loss:1.4028, Accuracy:0.3680, Validation Loss:1.4362, Validation Accuracy:0.3498\n",
    "Epoch #71: Loss:1.4040, Accuracy:0.3655, Validation Loss:1.4351, Validation Accuracy:0.3547\n",
    "Epoch #72: Loss:1.4019, Accuracy:0.3688, Validation Loss:1.4353, Validation Accuracy:0.3547\n",
    "Epoch #73: Loss:1.4006, Accuracy:0.3704, Validation Loss:1.4335, Validation Accuracy:0.3498\n",
    "Epoch #74: Loss:1.4010, Accuracy:0.3614, Validation Loss:1.4435, Validation Accuracy:0.3498\n",
    "Epoch #75: Loss:1.4108, Accuracy:0.3655, Validation Loss:1.4388, Validation Accuracy:0.3547\n",
    "Epoch #76: Loss:1.4034, Accuracy:0.3725, Validation Loss:1.4363, Validation Accuracy:0.3366\n",
    "Epoch #77: Loss:1.3999, Accuracy:0.3676, Validation Loss:1.4318, Validation Accuracy:0.3530\n",
    "Epoch #78: Loss:1.3980, Accuracy:0.3700, Validation Loss:1.4316, Validation Accuracy:0.3481\n",
    "Epoch #79: Loss:1.3978, Accuracy:0.3754, Validation Loss:1.4311, Validation Accuracy:0.3530\n",
    "Epoch #80: Loss:1.3967, Accuracy:0.3766, Validation Loss:1.4363, Validation Accuracy:0.3432\n",
    "Epoch #81: Loss:1.3982, Accuracy:0.3684, Validation Loss:1.4325, Validation Accuracy:0.3399\n",
    "Epoch #82: Loss:1.3953, Accuracy:0.3704, Validation Loss:1.4303, Validation Accuracy:0.3530\n",
    "Epoch #83: Loss:1.3940, Accuracy:0.3713, Validation Loss:1.4307, Validation Accuracy:0.3432\n",
    "Epoch #84: Loss:1.3975, Accuracy:0.3655, Validation Loss:1.4387, Validation Accuracy:0.3465\n",
    "Epoch #85: Loss:1.3993, Accuracy:0.3676, Validation Loss:1.4344, Validation Accuracy:0.3465\n",
    "Epoch #86: Loss:1.3981, Accuracy:0.3593, Validation Loss:1.4301, Validation Accuracy:0.3465\n",
    "Epoch #87: Loss:1.3945, Accuracy:0.3704, Validation Loss:1.4301, Validation Accuracy:0.3547\n",
    "Epoch #88: Loss:1.3950, Accuracy:0.3659, Validation Loss:1.4329, Validation Accuracy:0.3432\n",
    "Epoch #89: Loss:1.3972, Accuracy:0.3696, Validation Loss:1.4279, Validation Accuracy:0.3514\n",
    "Epoch #90: Loss:1.3962, Accuracy:0.3782, Validation Loss:1.4358, Validation Accuracy:0.3333\n",
    "Epoch #91: Loss:1.3974, Accuracy:0.3692, Validation Loss:1.4294, Validation Accuracy:0.3498\n",
    "Epoch #92: Loss:1.3959, Accuracy:0.3659, Validation Loss:1.4342, Validation Accuracy:0.3415\n",
    "Epoch #93: Loss:1.3985, Accuracy:0.3659, Validation Loss:1.4280, Validation Accuracy:0.3596\n",
    "Epoch #94: Loss:1.3901, Accuracy:0.3737, Validation Loss:1.4291, Validation Accuracy:0.3629\n",
    "Epoch #95: Loss:1.3919, Accuracy:0.3708, Validation Loss:1.4266, Validation Accuracy:0.3530\n",
    "Epoch #96: Loss:1.3903, Accuracy:0.3696, Validation Loss:1.4270, Validation Accuracy:0.3465\n",
    "Epoch #97: Loss:1.3905, Accuracy:0.3782, Validation Loss:1.4348, Validation Accuracy:0.3415\n",
    "Epoch #98: Loss:1.3929, Accuracy:0.3634, Validation Loss:1.4258, Validation Accuracy:0.3530\n",
    "Epoch #99: Loss:1.3971, Accuracy:0.3626, Validation Loss:1.4352, Validation Accuracy:0.3399\n",
    "Epoch #100: Loss:1.3955, Accuracy:0.3659, Validation Loss:1.4284, Validation Accuracy:0.3432\n",
    "Epoch #101: Loss:1.3879, Accuracy:0.3717, Validation Loss:1.4272, Validation Accuracy:0.3563\n",
    "Epoch #102: Loss:1.3880, Accuracy:0.3696, Validation Loss:1.4252, Validation Accuracy:0.3612\n",
    "Epoch #103: Loss:1.3882, Accuracy:0.3634, Validation Loss:1.4259, Validation Accuracy:0.3563\n",
    "Epoch #104: Loss:1.3890, Accuracy:0.3786, Validation Loss:1.4385, Validation Accuracy:0.3432\n",
    "Epoch #105: Loss:1.3953, Accuracy:0.3651, Validation Loss:1.4262, Validation Accuracy:0.3465\n",
    "Epoch #106: Loss:1.3867, Accuracy:0.3643, Validation Loss:1.4241, Validation Accuracy:0.3514\n",
    "Epoch #107: Loss:1.3870, Accuracy:0.3713, Validation Loss:1.4238, Validation Accuracy:0.3547\n",
    "Epoch #108: Loss:1.3852, Accuracy:0.3708, Validation Loss:1.4279, Validation Accuracy:0.3481\n",
    "Epoch #109: Loss:1.3858, Accuracy:0.3745, Validation Loss:1.4238, Validation Accuracy:0.3596\n",
    "Epoch #110: Loss:1.3838, Accuracy:0.3663, Validation Loss:1.4240, Validation Accuracy:0.3547\n",
    "Epoch #111: Loss:1.3845, Accuracy:0.3721, Validation Loss:1.4259, Validation Accuracy:0.3465\n",
    "Epoch #112: Loss:1.3875, Accuracy:0.3766, Validation Loss:1.4262, Validation Accuracy:0.3563\n",
    "Epoch #113: Loss:1.3936, Accuracy:0.3655, Validation Loss:1.4307, Validation Accuracy:0.3481\n",
    "Epoch #114: Loss:1.3975, Accuracy:0.3618, Validation Loss:1.4235, Validation Accuracy:0.3645\n",
    "Epoch #115: Loss:1.3916, Accuracy:0.3737, Validation Loss:1.4323, Validation Accuracy:0.3498\n",
    "Epoch #116: Loss:1.3944, Accuracy:0.3536, Validation Loss:1.4273, Validation Accuracy:0.3498\n",
    "Epoch #117: Loss:1.3841, Accuracy:0.3791, Validation Loss:1.4253, Validation Accuracy:0.3530\n",
    "Epoch #118: Loss:1.3871, Accuracy:0.3725, Validation Loss:1.4234, Validation Accuracy:0.3629\n",
    "Epoch #119: Loss:1.3861, Accuracy:0.3680, Validation Loss:1.4240, Validation Accuracy:0.3366\n",
    "Epoch #120: Loss:1.3887, Accuracy:0.3733, Validation Loss:1.4214, Validation Accuracy:0.3465\n",
    "Epoch #121: Loss:1.3831, Accuracy:0.3713, Validation Loss:1.4238, Validation Accuracy:0.3612\n",
    "Epoch #122: Loss:1.3832, Accuracy:0.3721, Validation Loss:1.4202, Validation Accuracy:0.3580\n",
    "Epoch #123: Loss:1.3826, Accuracy:0.3700, Validation Loss:1.4201, Validation Accuracy:0.3580\n",
    "Epoch #124: Loss:1.3794, Accuracy:0.3774, Validation Loss:1.4249, Validation Accuracy:0.3547\n",
    "Epoch #125: Loss:1.3813, Accuracy:0.3749, Validation Loss:1.4196, Validation Accuracy:0.3547\n",
    "Epoch #126: Loss:1.3797, Accuracy:0.3700, Validation Loss:1.4191, Validation Accuracy:0.3530\n",
    "Epoch #127: Loss:1.3814, Accuracy:0.3667, Validation Loss:1.4212, Validation Accuracy:0.3481\n",
    "Epoch #128: Loss:1.3843, Accuracy:0.3700, Validation Loss:1.4259, Validation Accuracy:0.3547\n",
    "Epoch #129: Loss:1.3808, Accuracy:0.3778, Validation Loss:1.4179, Validation Accuracy:0.3547\n",
    "Epoch #130: Loss:1.3779, Accuracy:0.3721, Validation Loss:1.4184, Validation Accuracy:0.3547\n",
    "Epoch #131: Loss:1.3785, Accuracy:0.3782, Validation Loss:1.4204, Validation Accuracy:0.3727\n",
    "Epoch #132: Loss:1.3762, Accuracy:0.3733, Validation Loss:1.4190, Validation Accuracy:0.3514\n",
    "Epoch #133: Loss:1.3806, Accuracy:0.3659, Validation Loss:1.4185, Validation Accuracy:0.3514\n",
    "Epoch #134: Loss:1.3797, Accuracy:0.3795, Validation Loss:1.4253, Validation Accuracy:0.3415\n",
    "Epoch #135: Loss:1.3774, Accuracy:0.3832, Validation Loss:1.4162, Validation Accuracy:0.3645\n",
    "Epoch #136: Loss:1.3756, Accuracy:0.3671, Validation Loss:1.4171, Validation Accuracy:0.3530\n",
    "Epoch #137: Loss:1.3761, Accuracy:0.3741, Validation Loss:1.4174, Validation Accuracy:0.3580\n",
    "Epoch #138: Loss:1.3763, Accuracy:0.3799, Validation Loss:1.4205, Validation Accuracy:0.3629\n",
    "Epoch #139: Loss:1.3877, Accuracy:0.3630, Validation Loss:1.4176, Validation Accuracy:0.3514\n",
    "Epoch #140: Loss:1.3757, Accuracy:0.3766, Validation Loss:1.4159, Validation Accuracy:0.3662\n",
    "Epoch #141: Loss:1.3735, Accuracy:0.3811, Validation Loss:1.4163, Validation Accuracy:0.3612\n",
    "Epoch #142: Loss:1.3738, Accuracy:0.3733, Validation Loss:1.4143, Validation Accuracy:0.3596\n",
    "Epoch #143: Loss:1.3733, Accuracy:0.3754, Validation Loss:1.4160, Validation Accuracy:0.3547\n",
    "Epoch #144: Loss:1.3758, Accuracy:0.3725, Validation Loss:1.4165, Validation Accuracy:0.3580\n",
    "Epoch #145: Loss:1.3745, Accuracy:0.3725, Validation Loss:1.4145, Validation Accuracy:0.3629\n",
    "Epoch #146: Loss:1.3747, Accuracy:0.3791, Validation Loss:1.4158, Validation Accuracy:0.3530\n",
    "Epoch #147: Loss:1.3720, Accuracy:0.3791, Validation Loss:1.4155, Validation Accuracy:0.3514\n",
    "Epoch #148: Loss:1.3736, Accuracy:0.3782, Validation Loss:1.4174, Validation Accuracy:0.3498\n",
    "Epoch #149: Loss:1.3721, Accuracy:0.3774, Validation Loss:1.4139, Validation Accuracy:0.3530\n",
    "Epoch #150: Loss:1.3704, Accuracy:0.3782, Validation Loss:1.4163, Validation Accuracy:0.3465\n",
    "Epoch #151: Loss:1.3707, Accuracy:0.3811, Validation Loss:1.4135, Validation Accuracy:0.3629\n",
    "Epoch #152: Loss:1.3706, Accuracy:0.3733, Validation Loss:1.4162, Validation Accuracy:0.3465\n",
    "Epoch #153: Loss:1.3760, Accuracy:0.3791, Validation Loss:1.4133, Validation Accuracy:0.3580\n",
    "Epoch #154: Loss:1.3729, Accuracy:0.3770, Validation Loss:1.4178, Validation Accuracy:0.3415\n",
    "Epoch #155: Loss:1.3728, Accuracy:0.3700, Validation Loss:1.4123, Validation Accuracy:0.3514\n",
    "Epoch #156: Loss:1.3706, Accuracy:0.3782, Validation Loss:1.4120, Validation Accuracy:0.3547\n",
    "Epoch #157: Loss:1.3731, Accuracy:0.3713, Validation Loss:1.4230, Validation Accuracy:0.3514\n",
    "Epoch #158: Loss:1.3717, Accuracy:0.3741, Validation Loss:1.4102, Validation Accuracy:0.3662\n",
    "Epoch #159: Loss:1.3688, Accuracy:0.3782, Validation Loss:1.4129, Validation Accuracy:0.3547\n",
    "Epoch #160: Loss:1.3675, Accuracy:0.3795, Validation Loss:1.4106, Validation Accuracy:0.3596\n",
    "Epoch #161: Loss:1.3661, Accuracy:0.3795, Validation Loss:1.4123, Validation Accuracy:0.3547\n",
    "Epoch #162: Loss:1.3674, Accuracy:0.3717, Validation Loss:1.4098, Validation Accuracy:0.3645\n",
    "Epoch #163: Loss:1.3681, Accuracy:0.3778, Validation Loss:1.4172, Validation Accuracy:0.3481\n",
    "Epoch #164: Loss:1.3701, Accuracy:0.3778, Validation Loss:1.4100, Validation Accuracy:0.3514\n",
    "Epoch #165: Loss:1.3670, Accuracy:0.3799, Validation Loss:1.4109, Validation Accuracy:0.3399\n",
    "Epoch #166: Loss:1.3698, Accuracy:0.3770, Validation Loss:1.4085, Validation Accuracy:0.3547\n",
    "Epoch #167: Loss:1.3666, Accuracy:0.3766, Validation Loss:1.4103, Validation Accuracy:0.3580\n",
    "Epoch #168: Loss:1.3661, Accuracy:0.3811, Validation Loss:1.4160, Validation Accuracy:0.3465\n",
    "Epoch #169: Loss:1.3685, Accuracy:0.3807, Validation Loss:1.4086, Validation Accuracy:0.3530\n",
    "Epoch #170: Loss:1.3729, Accuracy:0.3725, Validation Loss:1.4087, Validation Accuracy:0.3547\n",
    "Epoch #171: Loss:1.3696, Accuracy:0.3704, Validation Loss:1.4130, Validation Accuracy:0.3317\n",
    "Epoch #172: Loss:1.3661, Accuracy:0.3774, Validation Loss:1.4090, Validation Accuracy:0.3514\n",
    "Epoch #173: Loss:1.3626, Accuracy:0.3864, Validation Loss:1.4065, Validation Accuracy:0.3498\n",
    "Epoch #174: Loss:1.3684, Accuracy:0.3770, Validation Loss:1.4097, Validation Accuracy:0.3530\n",
    "Epoch #175: Loss:1.3620, Accuracy:0.3836, Validation Loss:1.4076, Validation Accuracy:0.3563\n",
    "Epoch #176: Loss:1.3628, Accuracy:0.3770, Validation Loss:1.4066, Validation Accuracy:0.3481\n",
    "Epoch #177: Loss:1.3631, Accuracy:0.3774, Validation Loss:1.4098, Validation Accuracy:0.3399\n",
    "Epoch #178: Loss:1.3666, Accuracy:0.3807, Validation Loss:1.4074, Validation Accuracy:0.3530\n",
    "Epoch #179: Loss:1.3641, Accuracy:0.3749, Validation Loss:1.4175, Validation Accuracy:0.3481\n",
    "Epoch #180: Loss:1.3670, Accuracy:0.3799, Validation Loss:1.4065, Validation Accuracy:0.3547\n",
    "Epoch #181: Loss:1.3587, Accuracy:0.3823, Validation Loss:1.4102, Validation Accuracy:0.3498\n",
    "Epoch #182: Loss:1.3615, Accuracy:0.3782, Validation Loss:1.4064, Validation Accuracy:0.3432\n",
    "Epoch #183: Loss:1.3628, Accuracy:0.3836, Validation Loss:1.4069, Validation Accuracy:0.3498\n",
    "Epoch #184: Loss:1.3580, Accuracy:0.3770, Validation Loss:1.4046, Validation Accuracy:0.3547\n",
    "Epoch #185: Loss:1.3616, Accuracy:0.3795, Validation Loss:1.4131, Validation Accuracy:0.3383\n",
    "Epoch #186: Loss:1.3624, Accuracy:0.3922, Validation Loss:1.4123, Validation Accuracy:0.3481\n",
    "Epoch #187: Loss:1.3574, Accuracy:0.3869, Validation Loss:1.4140, Validation Accuracy:0.3530\n",
    "Epoch #188: Loss:1.3671, Accuracy:0.3721, Validation Loss:1.4048, Validation Accuracy:0.3399\n",
    "Epoch #189: Loss:1.3557, Accuracy:0.3819, Validation Loss:1.4075, Validation Accuracy:0.3547\n",
    "Epoch #190: Loss:1.3547, Accuracy:0.3799, Validation Loss:1.4079, Validation Accuracy:0.3580\n",
    "Epoch #191: Loss:1.3617, Accuracy:0.3795, Validation Loss:1.4039, Validation Accuracy:0.3465\n",
    "Epoch #192: Loss:1.3577, Accuracy:0.3869, Validation Loss:1.4096, Validation Accuracy:0.3448\n",
    "Epoch #193: Loss:1.3570, Accuracy:0.3852, Validation Loss:1.4102, Validation Accuracy:0.3415\n",
    "Epoch #194: Loss:1.3561, Accuracy:0.3786, Validation Loss:1.4056, Validation Accuracy:0.3448\n",
    "Epoch #195: Loss:1.3562, Accuracy:0.3856, Validation Loss:1.4031, Validation Accuracy:0.3514\n",
    "Epoch #196: Loss:1.3526, Accuracy:0.3778, Validation Loss:1.4021, Validation Accuracy:0.3547\n",
    "Epoch #197: Loss:1.3517, Accuracy:0.3811, Validation Loss:1.4017, Validation Accuracy:0.3514\n",
    "Epoch #198: Loss:1.3508, Accuracy:0.3860, Validation Loss:1.4068, Validation Accuracy:0.3432\n",
    "Epoch #199: Loss:1.3533, Accuracy:0.3918, Validation Loss:1.4023, Validation Accuracy:0.3481\n",
    "Epoch #200: Loss:1.3583, Accuracy:0.3885, Validation Loss:1.4367, Validation Accuracy:0.3498\n",
    "Epoch #201: Loss:1.3641, Accuracy:0.3828, Validation Loss:1.4272, Validation Accuracy:0.3333\n",
    "Epoch #202: Loss:1.3547, Accuracy:0.3864, Validation Loss:1.4013, Validation Accuracy:0.3481\n",
    "Epoch #203: Loss:1.3536, Accuracy:0.3914, Validation Loss:1.4033, Validation Accuracy:0.3563\n",
    "Epoch #204: Loss:1.3460, Accuracy:0.3918, Validation Loss:1.4056, Validation Accuracy:0.3530\n",
    "Epoch #205: Loss:1.3503, Accuracy:0.3819, Validation Loss:1.4058, Validation Accuracy:0.3415\n",
    "Epoch #206: Loss:1.3489, Accuracy:0.3885, Validation Loss:1.3981, Validation Accuracy:0.3481\n",
    "Epoch #207: Loss:1.3442, Accuracy:0.3930, Validation Loss:1.4010, Validation Accuracy:0.3514\n",
    "Epoch #208: Loss:1.3441, Accuracy:0.3943, Validation Loss:1.3991, Validation Accuracy:0.3547\n",
    "Epoch #209: Loss:1.3424, Accuracy:0.3975, Validation Loss:1.4028, Validation Accuracy:0.3596\n",
    "Epoch #210: Loss:1.3483, Accuracy:0.3860, Validation Loss:1.4004, Validation Accuracy:0.3465\n",
    "Epoch #211: Loss:1.3452, Accuracy:0.3947, Validation Loss:1.4048, Validation Accuracy:0.3481\n",
    "Epoch #212: Loss:1.3457, Accuracy:0.3918, Validation Loss:1.3957, Validation Accuracy:0.3563\n",
    "Epoch #213: Loss:1.3438, Accuracy:0.3971, Validation Loss:1.4076, Validation Accuracy:0.3514\n",
    "Epoch #214: Loss:1.3493, Accuracy:0.3906, Validation Loss:1.4011, Validation Accuracy:0.3514\n",
    "Epoch #215: Loss:1.3467, Accuracy:0.3893, Validation Loss:1.4028, Validation Accuracy:0.3612\n",
    "Epoch #216: Loss:1.3391, Accuracy:0.3930, Validation Loss:1.3952, Validation Accuracy:0.3498\n",
    "Epoch #217: Loss:1.3464, Accuracy:0.3926, Validation Loss:1.4098, Validation Accuracy:0.3448\n",
    "Epoch #218: Loss:1.3588, Accuracy:0.3848, Validation Loss:1.3949, Validation Accuracy:0.3563\n",
    "Epoch #219: Loss:1.3415, Accuracy:0.3906, Validation Loss:1.3992, Validation Accuracy:0.3727\n",
    "Epoch #220: Loss:1.3382, Accuracy:0.4012, Validation Loss:1.3950, Validation Accuracy:0.3596\n",
    "Epoch #221: Loss:1.3335, Accuracy:0.3975, Validation Loss:1.3907, Validation Accuracy:0.3678\n",
    "Epoch #222: Loss:1.3360, Accuracy:0.3971, Validation Loss:1.3901, Validation Accuracy:0.3415\n",
    "Epoch #223: Loss:1.3271, Accuracy:0.4086, Validation Loss:1.3916, Validation Accuracy:0.3547\n",
    "Epoch #224: Loss:1.3278, Accuracy:0.4086, Validation Loss:1.3933, Validation Accuracy:0.3662\n",
    "Epoch #225: Loss:1.3261, Accuracy:0.4082, Validation Loss:1.3883, Validation Accuracy:0.3481\n",
    "Epoch #226: Loss:1.3252, Accuracy:0.4045, Validation Loss:1.3878, Validation Accuracy:0.3563\n",
    "Epoch #227: Loss:1.3226, Accuracy:0.4086, Validation Loss:1.3875, Validation Accuracy:0.3629\n",
    "Epoch #228: Loss:1.3214, Accuracy:0.4103, Validation Loss:1.3869, Validation Accuracy:0.3744\n",
    "Epoch #229: Loss:1.3211, Accuracy:0.4086, Validation Loss:1.3845, Validation Accuracy:0.3793\n",
    "Epoch #230: Loss:1.3205, Accuracy:0.4057, Validation Loss:1.3896, Validation Accuracy:0.3892\n",
    "Epoch #231: Loss:1.3289, Accuracy:0.4049, Validation Loss:1.3873, Validation Accuracy:0.3563\n",
    "Epoch #232: Loss:1.3294, Accuracy:0.4111, Validation Loss:1.3814, Validation Accuracy:0.3547\n",
    "Epoch #233: Loss:1.3243, Accuracy:0.4099, Validation Loss:1.3880, Validation Accuracy:0.3563\n",
    "Epoch #234: Loss:1.3207, Accuracy:0.4094, Validation Loss:1.3867, Validation Accuracy:0.3498\n",
    "Epoch #235: Loss:1.3186, Accuracy:0.4156, Validation Loss:1.3800, Validation Accuracy:0.3957\n",
    "Epoch #236: Loss:1.3111, Accuracy:0.4263, Validation Loss:1.3812, Validation Accuracy:0.3596\n",
    "Epoch #237: Loss:1.3164, Accuracy:0.4201, Validation Loss:1.3967, Validation Accuracy:0.3957\n",
    "Epoch #238: Loss:1.3137, Accuracy:0.4119, Validation Loss:1.3790, Validation Accuracy:0.3760\n",
    "Epoch #239: Loss:1.3104, Accuracy:0.4119, Validation Loss:1.3809, Validation Accuracy:0.3547\n",
    "Epoch #240: Loss:1.3146, Accuracy:0.4025, Validation Loss:1.3967, Validation Accuracy:0.3629\n",
    "Epoch #241: Loss:1.3164, Accuracy:0.4136, Validation Loss:1.3733, Validation Accuracy:0.3629\n",
    "Epoch #242: Loss:1.3026, Accuracy:0.4172, Validation Loss:1.3734, Validation Accuracy:0.3629\n",
    "Epoch #243: Loss:1.3063, Accuracy:0.4160, Validation Loss:1.3742, Validation Accuracy:0.3563\n",
    "Epoch #244: Loss:1.3069, Accuracy:0.4172, Validation Loss:1.3758, Validation Accuracy:0.3678\n",
    "Epoch #245: Loss:1.3031, Accuracy:0.4177, Validation Loss:1.3793, Validation Accuracy:0.3826\n",
    "Epoch #246: Loss:1.3061, Accuracy:0.4086, Validation Loss:1.3701, Validation Accuracy:0.3760\n",
    "Epoch #247: Loss:1.2983, Accuracy:0.4189, Validation Loss:1.3716, Validation Accuracy:0.3711\n",
    "Epoch #248: Loss:1.2981, Accuracy:0.4193, Validation Loss:1.3735, Validation Accuracy:0.3941\n",
    "Epoch #249: Loss:1.2999, Accuracy:0.4189, Validation Loss:1.3720, Validation Accuracy:0.3695\n",
    "Epoch #250: Loss:1.2962, Accuracy:0.4255, Validation Loss:1.3691, Validation Accuracy:0.3957\n",
    "Epoch #251: Loss:1.2983, Accuracy:0.4218, Validation Loss:1.3683, Validation Accuracy:0.3957\n",
    "Epoch #252: Loss:1.2969, Accuracy:0.4242, Validation Loss:1.3786, Validation Accuracy:0.3662\n",
    "Epoch #253: Loss:1.3004, Accuracy:0.4160, Validation Loss:1.3691, Validation Accuracy:0.3842\n",
    "Epoch #254: Loss:1.2932, Accuracy:0.4234, Validation Loss:1.3709, Validation Accuracy:0.3941\n",
    "Epoch #255: Loss:1.2907, Accuracy:0.4320, Validation Loss:1.3694, Validation Accuracy:0.3957\n",
    "Epoch #256: Loss:1.2897, Accuracy:0.4320, Validation Loss:1.3698, Validation Accuracy:0.3990\n",
    "Epoch #257: Loss:1.2906, Accuracy:0.4242, Validation Loss:1.3786, Validation Accuracy:0.4023\n",
    "Epoch #258: Loss:1.3036, Accuracy:0.4214, Validation Loss:1.3754, Validation Accuracy:0.3744\n",
    "Epoch #259: Loss:1.3049, Accuracy:0.4238, Validation Loss:1.3691, Validation Accuracy:0.4105\n",
    "Epoch #260: Loss:1.2944, Accuracy:0.4242, Validation Loss:1.3767, Validation Accuracy:0.3777\n",
    "Epoch #261: Loss:1.2926, Accuracy:0.4292, Validation Loss:1.3747, Validation Accuracy:0.4105\n",
    "Epoch #262: Loss:1.2942, Accuracy:0.4251, Validation Loss:1.3675, Validation Accuracy:0.4023\n",
    "Epoch #263: Loss:1.2918, Accuracy:0.4255, Validation Loss:1.3811, Validation Accuracy:0.3924\n",
    "Epoch #264: Loss:1.2943, Accuracy:0.4275, Validation Loss:1.3727, Validation Accuracy:0.3974\n",
    "Epoch #265: Loss:1.2880, Accuracy:0.4349, Validation Loss:1.3722, Validation Accuracy:0.3760\n",
    "Epoch #266: Loss:1.2935, Accuracy:0.4226, Validation Loss:1.3765, Validation Accuracy:0.3695\n",
    "Epoch #267: Loss:1.2843, Accuracy:0.4337, Validation Loss:1.3800, Validation Accuracy:0.4122\n",
    "Epoch #268: Loss:1.2954, Accuracy:0.4181, Validation Loss:1.3885, Validation Accuracy:0.4154\n",
    "Epoch #269: Loss:1.2937, Accuracy:0.4238, Validation Loss:1.3677, Validation Accuracy:0.3941\n",
    "Epoch #270: Loss:1.2896, Accuracy:0.4312, Validation Loss:1.3662, Validation Accuracy:0.4023\n",
    "Epoch #271: Loss:1.2825, Accuracy:0.4398, Validation Loss:1.3665, Validation Accuracy:0.3662\n",
    "Epoch #272: Loss:1.2856, Accuracy:0.4329, Validation Loss:1.3791, Validation Accuracy:0.3760\n",
    "Epoch #273: Loss:1.2903, Accuracy:0.4361, Validation Loss:1.3713, Validation Accuracy:0.3662\n",
    "Epoch #274: Loss:1.2886, Accuracy:0.4337, Validation Loss:1.3714, Validation Accuracy:0.3974\n",
    "Epoch #275: Loss:1.2974, Accuracy:0.4333, Validation Loss:1.3791, Validation Accuracy:0.4187\n",
    "Epoch #276: Loss:1.2908, Accuracy:0.4312, Validation Loss:1.3833, Validation Accuracy:0.4072\n",
    "Epoch #277: Loss:1.2873, Accuracy:0.4283, Validation Loss:1.3672, Validation Accuracy:0.4105\n",
    "Epoch #278: Loss:1.2775, Accuracy:0.4407, Validation Loss:1.3836, Validation Accuracy:0.3826\n",
    "Epoch #279: Loss:1.2881, Accuracy:0.4246, Validation Loss:1.3687, Validation Accuracy:0.3842\n",
    "Epoch #280: Loss:1.2914, Accuracy:0.4287, Validation Loss:1.3621, Validation Accuracy:0.4122\n",
    "Epoch #281: Loss:1.2816, Accuracy:0.4353, Validation Loss:1.3682, Validation Accuracy:0.4122\n",
    "Epoch #282: Loss:1.2792, Accuracy:0.4423, Validation Loss:1.3636, Validation Accuracy:0.4171\n",
    "Epoch #283: Loss:1.2762, Accuracy:0.4353, Validation Loss:1.3662, Validation Accuracy:0.3760\n",
    "Epoch #284: Loss:1.2764, Accuracy:0.4345, Validation Loss:1.3633, Validation Accuracy:0.4154\n",
    "Epoch #285: Loss:1.2752, Accuracy:0.4398, Validation Loss:1.3600, Validation Accuracy:0.3859\n",
    "Epoch #286: Loss:1.2746, Accuracy:0.4398, Validation Loss:1.3606, Validation Accuracy:0.4039\n",
    "Epoch #287: Loss:1.2738, Accuracy:0.4361, Validation Loss:1.3602, Validation Accuracy:0.4105\n",
    "Epoch #288: Loss:1.2773, Accuracy:0.4320, Validation Loss:1.3617, Validation Accuracy:0.4122\n",
    "Epoch #289: Loss:1.2803, Accuracy:0.4271, Validation Loss:1.3652, Validation Accuracy:0.4105\n",
    "Epoch #290: Loss:1.2800, Accuracy:0.4333, Validation Loss:1.3988, Validation Accuracy:0.3990\n",
    "Epoch #291: Loss:1.2933, Accuracy:0.4357, Validation Loss:1.3600, Validation Accuracy:0.4171\n",
    "Epoch #292: Loss:1.2826, Accuracy:0.4370, Validation Loss:1.3594, Validation Accuracy:0.4138\n",
    "Epoch #293: Loss:1.2813, Accuracy:0.4411, Validation Loss:1.3612, Validation Accuracy:0.4105\n",
    "Epoch #294: Loss:1.2725, Accuracy:0.4407, Validation Loss:1.3632, Validation Accuracy:0.3892\n",
    "Epoch #295: Loss:1.2703, Accuracy:0.4349, Validation Loss:1.3601, Validation Accuracy:0.4154\n",
    "Epoch #296: Loss:1.2690, Accuracy:0.4427, Validation Loss:1.3630, Validation Accuracy:0.4105\n",
    "Epoch #297: Loss:1.2691, Accuracy:0.4444, Validation Loss:1.3573, Validation Accuracy:0.4187\n",
    "Epoch #298: Loss:1.2690, Accuracy:0.4456, Validation Loss:1.3624, Validation Accuracy:0.4171\n",
    "Epoch #299: Loss:1.2706, Accuracy:0.4419, Validation Loss:1.3617, Validation Accuracy:0.4122\n",
    "Epoch #300: Loss:1.2733, Accuracy:0.4361, Validation Loss:1.3735, Validation Accuracy:0.3892\n",
    "\n",
    "Test:\n",
    "Test Loss:1.37347567, Accuracy:0.3892\n",
    "Labels: ['01', '05', '03', '04', '02']\n",
    "Confusion Matrix:\n",
    "      01  05  03  04  02\n",
    "t:01  56  36  19   5  10\n",
    "t:05  41  80   6   1  14\n",
    "t:03  11  20  42  29  13\n",
    "t:04   9  16  38  39  10\n",
    "t:02  42  17  17  18  20\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.35      0.44      0.39       126\n",
    "          05       0.47      0.56      0.51       142\n",
    "          03       0.34      0.37      0.35       115\n",
    "          04       0.42      0.35      0.38       112\n",
    "          02       0.30      0.18      0.22       114\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.38      0.38      0.37       609\n",
    "weighted avg       0.38      0.39      0.38       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 00:23:24 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 27 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6064109594755376, 1.605896839758837, 1.605644381496511, 1.6053842858140692, 1.6052809626793823, 1.6051193841768212, 1.6048862135469033, 1.6044815657369804, 1.604026749607769, 1.6034038994699864, 1.6024110575614892, 1.6008886128223587, 1.5983405737649827, 1.5941653108753397, 1.5866949454512698, 1.5739335923750803, 1.553637951465663, 1.524714670549277, 1.4956614464178852, 1.4827060200310693, 1.481513583992894, 1.468798406997142, 1.4635126287322522, 1.4623646826188161, 1.4650549929717491, 1.4629220929247602, 1.4652222062175106, 1.4571862242296216, 1.4609230887909437, 1.455941042485104, 1.453139090381428, 1.4517082935092094, 1.4507140171743182, 1.4537550426273316, 1.4509959152375145, 1.452183470350181, 1.4488667768406358, 1.4501375509991825, 1.4492088297905006, 1.4490729304174288, 1.4485768122821803, 1.4500535824420222, 1.4487833412997242, 1.4479168201314991, 1.4497964793238147, 1.452100182205977, 1.4472636489445352, 1.4469915139068328, 1.4466376590415566, 1.4474504136882589, 1.44660458384672, 1.4501698338143736, 1.445182443056592, 1.4467028974508025, 1.4422809284896099, 1.4414095128894049, 1.4419367272278358, 1.4442844214697776, 1.4413845746583736, 1.4408211510365427, 1.4427290238770358, 1.4402318758330321, 1.4383879903893557, 1.4496740551026193, 1.444275560050175, 1.4369539909174878, 1.4370092296443746, 1.438540580824678, 1.437189406949311, 1.4362450871365802, 1.435149088673208, 1.4352777826374974, 1.4334924283677526, 1.4434861778625714, 1.4388437233926432, 1.4363449974404572, 1.4317825881914161, 1.4315538363307958, 1.431067349679756, 1.4362619500637837, 1.4325250532043783, 1.430318144741904, 1.430744819844689, 1.4386762713368106, 1.4344405072858963, 1.4300723009313072, 1.4301105369683753, 1.4329250369753157, 1.4278831231378766, 1.435760444412482, 1.4293736103916013, 1.434221718111649, 1.4279724594407481, 1.4290757263431018, 1.426561147904357, 1.4270443252741998, 1.4348338204260138, 1.4258475689269443, 1.4352341768972587, 1.4284168851786647, 1.4271878636333546, 1.4252244623619543, 1.4259188126264926, 1.4384639255323237, 1.426178885210911, 1.4240775836512374, 1.4238342573294303, 1.4278901496348515, 1.423762155479594, 1.4240140427509551, 1.4258746253250072, 1.4261583299276668, 1.4307311301552408, 1.4234602570729498, 1.4322586404083202, 1.4273047909165055, 1.425346510750907, 1.423403326317986, 1.424034084005309, 1.42144364872198, 1.4238330812877036, 1.4201931027551786, 1.4200646708947293, 1.424870178421534, 1.4195541824613298, 1.4190716686702909, 1.4212484941106711, 1.4259425076749328, 1.4179444626242852, 1.4183516966298295, 1.4204337166252199, 1.4189669126751778, 1.4185059636292983, 1.4253023338239572, 1.4161853481004587, 1.4171198530150164, 1.4173625243708419, 1.4204532064632047, 1.417550801447851, 1.415898936918412, 1.4162967543688119, 1.414295898869707, 1.4160016178106047, 1.416537972898123, 1.4145339731316653, 1.4157926785730572, 1.4154972219702058, 1.4173921802752516, 1.4139454730821557, 1.416307928918422, 1.4134866179308085, 1.416213738507238, 1.413302339748013, 1.4177738132539446, 1.4122603119691997, 1.412016013964448, 1.4230490819182497, 1.410210136709542, 1.4128916666817, 1.4106180765750178, 1.412296887297544, 1.409813934554803, 1.4172168764574775, 1.409970662863971, 1.4108698886799302, 1.4085484320307013, 1.4103074668859221, 1.4159678941094034, 1.4086419909654189, 1.4087179143636293, 1.413048384029094, 1.409023876652146, 1.4065340363920615, 1.4097075074764307, 1.4076115679858354, 1.406609111427282, 1.4098343612329518, 1.4074368447505783, 1.417536886063311, 1.4065302696525561, 1.4102162594474204, 1.4064493948602912, 1.40689057141102, 1.4046230564759479, 1.4131400062532848, 1.4122530370706017, 1.413972469190463, 1.4048407522132635, 1.4074573906380163, 1.40794903754405, 1.4039208458366457, 1.4095998740157079, 1.410183861337859, 1.4056468951486798, 1.403051447007065, 1.4021402818620303, 1.40166530311597, 1.4067569949552539, 1.4022513680857391, 1.4366775101237306, 1.4271923493477707, 1.401268994083937, 1.4033228366441524, 1.4056309201251502, 1.4057619354407775, 1.3981071350413983, 1.4009727223949089, 1.3990626121780947, 1.4027796085244917, 1.4004021593306843, 1.4047531363216332, 1.3956557193217412, 1.4075773727326166, 1.401085259291926, 1.4027787789531139, 1.3951569269051889, 1.4097677874447676, 1.3949069775188303, 1.3991934241136699, 1.3949969985410693, 1.3906766158606618, 1.3901149864462992, 1.3915601958977961, 1.3932693642739984, 1.3882745625741768, 1.387751667174604, 1.387539448213499, 1.3868673554390718, 1.3844676176315458, 1.389616911634436, 1.3873015411185905, 1.3813588722977537, 1.3879818747979276, 1.3866889061794687, 1.3800292105118825, 1.3812236783931213, 1.396659235844667, 1.3789750701688193, 1.3808794522716104, 1.3966520960107813, 1.3732857232414835, 1.3734321866325165, 1.374209248960899, 1.37576769412249, 1.3793105967526365, 1.3700692724124552, 1.3716059285040167, 1.373501985922627, 1.3720396602486546, 1.3691495903607072, 1.368332959944, 1.3785560215243762, 1.3690830306662323, 1.3708790409545397, 1.3693532618787292, 1.3697774333906878, 1.3785944891289146, 1.3754187203784687, 1.3690962294248132, 1.3767162048562211, 1.3746806854880698, 1.3674950341285743, 1.38107748865494, 1.3727298430817076, 1.3721999278405226, 1.3765119509939685, 1.3799748495098796, 1.3884860656923066, 1.3676721057281118, 1.366229002307397, 1.3664945839661096, 1.379076196446599, 1.371338767762646, 1.3713709348919747, 1.3790733986495947, 1.3832837865857655, 1.3672086207933223, 1.3835587824506712, 1.368651997083905, 1.3620964558840973, 1.3682120480560904, 1.363621034841428, 1.3661550443943693, 1.3633356990876848, 1.3599595988325297, 1.360577063215973, 1.3601643448001255, 1.3616663176437904, 1.3651798082689934, 1.3988472340729436, 1.3599711014523685, 1.3593969785521183, 1.3611943003383569, 1.3631945355185147, 1.3600803877919765, 1.3629550231109895, 1.357265387458363, 1.362408941993964, 1.3617012242378272, 1.3734757536150553], 'val_acc': [0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.24466338247207586, 0.2660098501124797, 0.2775041030839159, 0.2906403920822739, 0.30213464554307495, 0.31527093483505186, 0.31855500708464135, 0.3251231516816933, 0.3234811154590256, 0.33825944068005126, 0.34646962140189796, 0.34318554895656256, 0.36288998274771844, 0.33169129598512637, 0.35139572967840926, 0.3399014766091001, 0.3563218383464124, 0.3579638742754612, 0.3563218383464124, 0.353037765803204, 0.3448275851792303, 0.34975369365148745, 0.3448275854728492, 0.3481116576245657, 0.34154351292964075, 0.3415435128317678, 0.33004926015395053, 0.33497536852833476, 0.33169129598512637, 0.33825944087579723, 0.33825944077792425, 0.34975369374936044, 0.3497536935536145, 0.3431855490544355, 0.33825944058217833, 0.34646962149977095, 0.34646962130402503, 0.34154351292964075, 0.3366174042616376, 0.33825944058217833, 0.3431855490544355, 0.3333333321099211, 0.33825944058217833, 0.3448275850813573, 0.3399014764133541, 0.3448275853749762, 0.353037765803204, 0.34154351263602184, 0.3530377660968229, 0.3563218385421584, 0.3448275850813573, 0.3366174043595106, 0.34975369374936044, 0.35467980192799875, 0.3513957294826633, 0.34646962149977095, 0.34975369365148745, 0.35467980192799875, 0.35467980231949064, 0.34975369345574153, 0.34975369345574153, 0.35467980183012576, 0.33661740445738353, 0.353037765901077, 0.3481116574288198, 0.353037765901077, 0.3431855486629436, 0.339901476902719, 0.3530377659989499, 0.34318554895656256, 0.34646962130402503, 0.34646962149977095, 0.34646962130402503, 0.3546798017322528, 0.34318554925018147, 0.35139572967840926, 0.33333333191417513, 0.34975369374936044, 0.34154351302751373, 0.35960591040025597, 0.36288998255197247, 0.3530377660968229, 0.3464696216955169, 0.3415435128317678, 0.353037765705331, 0.339901476902719, 0.3431855486629436, 0.3563218382485394, 0.36124794672079663, 0.3563218383464124, 0.3431855490544355, 0.3464696216955169, 0.3513957294826633, 0.3546798016343798, 0.3481116574288198, 0.3596059105960019, 0.3546798015365068, 0.34646962120615205, 0.3563218381506665, 0.3481116577224387, 0.3645320185788942, 0.34975369316212257, 0.3497536938472334, 0.353037765803204, 0.3628899824540995, 0.3366174047510025, 0.34646962159764394, 0.36124794623143175, 0.3579638743733342, 0.3579638743733342, 0.3546798016343798, 0.35467980202587174, 0.3530377659989499, 0.3481116576245657, 0.35467980192799875, 0.35467980183012576, 0.35467980222161766, 0.3727421993986139, 0.35139572977628225, 0.35139572977628225, 0.34154351292964075, 0.3645320188725132, 0.353037765705331, 0.35796387417758824, 0.36288998264984546, 0.35139572997202817, 0.36617405499730793, 0.3612479469165426, 0.359605910302383, 0.35467980212374467, 0.3579638742754612, 0.36288998264984546, 0.3530377660968229, 0.3513957294826633, 0.34975369335786854, 0.353037765705331, 0.34646962140189796, 0.36288998255197247, 0.34646962140189796, 0.3579638743733342, 0.34154351244027586, 0.35139573006990116, 0.35467980183012576, 0.3513957294826633, 0.36617405519305385, 0.35467980202587174, 0.3596059105960019, 0.35467980183012576, 0.3645320188725132, 0.3481116575266927, 0.3513957294826633, 0.339901476902719, 0.35467980202587174, 0.3579638743733342, 0.34646962130402503, 0.3530377659989499, 0.3546798017322528, 0.3316912961808723, 0.35139572977628225, 0.34975369335786854, 0.3530377659989499, 0.3563218379549205, 0.3481116576245657, 0.339901476804846, 0.35303776560745803, 0.3481116571352008, 0.35467980212374467, 0.34975369325999556, 0.3431855491523085, 0.34975369374936044, 0.35467980192799875, 0.33825944038643235, 0.3481116573309468, 0.35303776560745803, 0.339901476902719, 0.35467980202587174, 0.3579638742754612, 0.34646962130402503, 0.3448275848856113, 0.34154351253814885, 0.3448275853749762, 0.35139572977628225, 0.35467980192799875, 0.35139572967840926, 0.3431855487608166, 0.3481116574288198, 0.3497536930642496, 0.3333333320120481, 0.3481116576245657, 0.3563218382485394, 0.35303776560745803, 0.34154351253814885, 0.3481116577224387, 0.35139572977628225, 0.3546798016343798, 0.35960591020451, 0.34646962130402503, 0.3481116571352008, 0.3563218380527935, 0.3513957292869173, 0.3513957291890443, 0.36124794632930474, 0.3497536930642496, 0.3448275849834843, 0.35632183756342856, 0.3727421993986139, 0.3596059098130181, 0.3678160909263567, 0.3415435122445299, 0.35467980192799875, 0.366174054507943, 0.3481116567437089, 0.3563218374655556, 0.3628899824540995, 0.37438423571915463, 0.3793103438977929, 0.38916256045081543, 0.35632183775917453, 0.3546798014386338, 0.3563218374655556, 0.3497536929663766, 0.3957307049499944, 0.35960590991089103, 0.3957307052436133, 0.3760262712567115, 0.3546798014386338, 0.3628899821604805, 0.3628899818668616, 0.3628899819647346, 0.3563218374655556, 0.3678160905348648, 0.3825944156580175, 0.37602627135458444, 0.3711001632738192, 0.3940886685315807, 0.36945812695327845, 0.3957307053414863, 0.3957307048521214, 0.3661740543121971, 0.3842364518806852, 0.39408866911881857, 0.3957307049499944, 0.3990147771995839, 0.40229884974279234, 0.37438423513191676, 0.4105090301710201, 0.37766830767512516, 0.41050903046463905, 0.40229884954704637, 0.3924466331897698, 0.39737274107478915, 0.3760262716482034, 0.36945812705115144, 0.4121510666873067, 0.4154351390347692, 0.39408866892307265, 0.40229884974279234, 0.366174054507943, 0.3760262712567115, 0.36617405499730793, 0.3973727412705351, 0.41871921118648575, 0.4072249582150495, 0.410509030562512, 0.38259441624525536, 0.3842364522721771, 0.4121510665894338, 0.4121510667851797, 0.417077174963818, 0.3760262718439494, 0.41543513893689626, 0.38587848839697186, 0.4039408857697141, 0.410509030562512, 0.4121510667851797, 0.41050903075825795, 0.3990147774932028, 0.4170771752574369, 0.41379310281210147, 0.410509030562512, 0.3891625608423073, 0.41543513893689626, 0.41050903066038497, 0.41871921118648575, 0.417077175061691, 0.4121510668830527, 0.38916256025506946], 'loss': [1.608532773053132, 1.6065514722643937, 1.6062754196797553, 1.6057466766917485, 1.6054914138155552, 1.6053410991261383, 1.605290322675842, 1.6049390667028252, 1.6046272744877872, 1.6040275408257205, 1.603352294222775, 1.6020737997560286, 1.6001696968470267, 1.5969863709483059, 1.591452200016202, 1.5820240759017286, 1.565208578599307, 1.5396885987424753, 1.5071921781837574, 1.4799119583145548, 1.4698115907410576, 1.4674162957213008, 1.4546705355634435, 1.4508143289378046, 1.4507037473655089, 1.4465331962221213, 1.4476302078372398, 1.4427728775835136, 1.4415946715666283, 1.4385872481050432, 1.441902924709986, 1.4372048016201544, 1.4353728274300357, 1.4343421674606982, 1.432257111606167, 1.4324155634678364, 1.431733210914189, 1.4286433797597395, 1.4263290375899484, 1.4263917837299604, 1.4247283779864928, 1.4250996949001993, 1.4251504907862607, 1.42176453381838, 1.425328887708378, 1.42327415380145, 1.421378806435352, 1.419123983725875, 1.418108541960589, 1.4190823442882092, 1.4175402175719243, 1.4193551097807209, 1.4142190867625712, 1.414543393798922, 1.4152596852617831, 1.4128626825873123, 1.4139343179471684, 1.411778033391651, 1.412759408128335, 1.4099117263386627, 1.4108748014457906, 1.4089016830651913, 1.4069006045985761, 1.4069760082193958, 1.4069473339546876, 1.413149949069875, 1.4067646019757405, 1.4061953517690575, 1.4033620691886917, 1.4028104691534806, 1.404030371201846, 1.4018702841392532, 1.4005940022654602, 1.4009868938085726, 1.410827914645295, 1.4034249030589079, 1.399941366702869, 1.398002683993972, 1.3977885712833131, 1.3967202656567708, 1.3981990271280433, 1.395330673370518, 1.3939958155767138, 1.397451793290751, 1.3993128481342072, 1.3980827890137628, 1.3945115761590445, 1.3949872746359886, 1.3972288072965964, 1.3961893566824817, 1.3973976158263501, 1.395942780271448, 1.398465165073622, 1.3901212566442314, 1.3919165133940365, 1.390307485102628, 1.390458773195866, 1.3929381915920813, 1.3971177840379718, 1.395469355093625, 1.3878982495967858, 1.3879706033201433, 1.3882023242954356, 1.389034543389902, 1.395264189052386, 1.386703485336147, 1.3869801692159758, 1.3851731506461236, 1.385820813296512, 1.3838067770983404, 1.38453088418659, 1.3874654271029838, 1.3935679775488694, 1.3975468139139289, 1.3915995633577665, 1.394413297866649, 1.3841411540150887, 1.3870607745720864, 1.386077797388394, 1.3887185454613373, 1.3830513702036173, 1.3831516621293962, 1.382642949386299, 1.3794332364256623, 1.3812744076491872, 1.3797117545619393, 1.3814206389186319, 1.3842791230771576, 1.3808341387116199, 1.3778850521639876, 1.3784849226597153, 1.3761698213201283, 1.3805677376243857, 1.3796776676080065, 1.377409491989402, 1.3756015547981497, 1.3761353252359974, 1.3763460294421939, 1.3877211270635867, 1.3756740965148018, 1.373470085455407, 1.373754787249242, 1.3733099340168602, 1.3757704780331872, 1.374495679641896, 1.3747407194035743, 1.3719947840398832, 1.3735641081475134, 1.3721182710581001, 1.3704098114977137, 1.3707008400492111, 1.370572341002478, 1.3760419390284795, 1.3729038656125079, 1.372790909694695, 1.3705565101067387, 1.3730638171613094, 1.3716860570946758, 1.3688161438740254, 1.3675455804723, 1.3660994448945751, 1.3673932356021732, 1.3681296272689067, 1.3700879950298177, 1.3669535069005445, 1.3698414315922793, 1.3665725445600505, 1.366123373356688, 1.368457262667787, 1.3728837722625575, 1.3696240394267214, 1.3660931850605678, 1.3625585505115423, 1.3683689582519218, 1.3620178403795622, 1.3627601502612385, 1.3630557154238347, 1.366618754046164, 1.3640994970313822, 1.3670235785125953, 1.3587164269335705, 1.3614571378461144, 1.362832945181359, 1.3580492180230925, 1.3615769502318615, 1.3624220119119914, 1.357421766022637, 1.3670644979457347, 1.355747710018432, 1.354736260662823, 1.3617016056969424, 1.3577056712438438, 1.3569631728793072, 1.3561125876722395, 1.356228848796116, 1.3525770817449205, 1.3517184916462985, 1.350822815415306, 1.3533457489229082, 1.358251318451805, 1.3641077125831307, 1.3546665362019314, 1.35362528330002, 1.34603518783679, 1.3503399905727629, 1.3488500769867788, 1.3442185883159754, 1.3440765513531725, 1.3423881240449158, 1.3483014168435787, 1.34517237856158, 1.3456986328414824, 1.3438379123225594, 1.3493123755073155, 1.3466969830299549, 1.3391112455841943, 1.3464212149320443, 1.358844652068199, 1.3415272492402877, 1.3382429567940182, 1.3335246717171012, 1.3360001655085132, 1.3271201817651548, 1.3277688843024096, 1.3260731825838343, 1.3251605942508768, 1.3225878381631213, 1.3214353264234884, 1.3211342854176704, 1.3204510277546406, 1.328930098271223, 1.3293751299503647, 1.3242701607318386, 1.3206910848127988, 1.3185905825675635, 1.3110891267749074, 1.316419561687681, 1.3136618414453902, 1.3103541751172263, 1.3146124773201757, 1.316409805814833, 1.3026450863609078, 1.3062708611850622, 1.3069293879385602, 1.3030654968911861, 1.3061431210143855, 1.2982678238616097, 1.2981111197500992, 1.2998530568528224, 1.2961684874685393, 1.2983353998626772, 1.2968717834543153, 1.3004254742079937, 1.2931685071706283, 1.2907396222531673, 1.2897443002743887, 1.2905567009101413, 1.3036308607771168, 1.3049319629062128, 1.2943500176102718, 1.292641172761545, 1.294194086327445, 1.291839119198386, 1.2942944286295521, 1.2879665820260802, 1.2934614470362418, 1.2843390648369917, 1.2953764736774767, 1.2937213916798147, 1.2895919589780929, 1.282480437995472, 1.2855857491248441, 1.2903180096428497, 1.2886453635883528, 1.2974115640475765, 1.2908472586950972, 1.2872621323783295, 1.2774910017205459, 1.2880539829970874, 1.2914398488567596, 1.2815661180190727, 1.279169952600154, 1.2762113335685807, 1.2764445672045008, 1.2752354666437702, 1.2745961032608941, 1.273846290782247, 1.2773063375719764, 1.280256350769889, 1.2800123064914524, 1.293345866262056, 1.2826279655863861, 1.281297240805577, 1.2724713146808946, 1.2703230778049883, 1.269041564968822, 1.2691117273953416, 1.2689503426914097, 1.270554283463245, 1.2733252867536133], 'acc': [0.2065708429539228, 0.23285421049203225, 0.23285420929871545, 0.2328542094945418, 0.23285420833794243, 0.2328542094945418, 0.2328542094945418, 0.2328542102778472, 0.23285420892542147, 0.23285420912124782, 0.23285420990455322, 0.2328542091028891, 0.2328542083195837, 0.23613963124076442, 0.2513347001168762, 0.2735112932550834, 0.2858316241838115, 0.30636550109734034, 0.3260780303262832, 0.33593429116497786, 0.33880903629062115, 0.3371663234317082, 0.338398359651683, 0.34332648691945006, 0.33880903726975287, 0.33963038976432364, 0.344147844933876, 0.35318275253141196, 0.34784394173406724, 0.3535934311286135, 0.34209445367358793, 0.3527720753049948, 0.34907597376825383, 0.35236139588777043, 0.35112936244608195, 0.3548254631628001, 0.34825462072292146, 0.3474332645076501, 0.3626283353603841, 0.36057494824916675, 0.36262833594786315, 0.35934291778159094, 0.3581108839115323, 0.3626283345770787, 0.3581108853190342, 0.3556468188273099, 0.35893223679775577, 0.3589322405551738, 0.36344969333809257, 0.36344969337481003, 0.3581108817574425, 0.3802874748956496, 0.3667351117369086, 0.3667351107577768, 0.3683778220133615, 0.3622176611080796, 0.3642710489658849, 0.37330595578011544, 0.35934291778159094, 0.368377824559104, 0.3655030798618309, 0.3728952775745666, 0.3638603691937253, 0.37084188850508576, 0.36714579248819995, 0.3601642720385988, 0.36919918041944016, 0.3630390151325437, 0.372073922925906, 0.367967144003639, 0.3655030779035674, 0.368788500023084, 0.3704312112786687, 0.36139630466026446, 0.3655030798618309, 0.3724846013639987, 0.3675564665446781, 0.3700205346397306, 0.37535934347881184, 0.3765913769572178, 0.36837782279666687, 0.37043121206197405, 0.3712525657315029, 0.36550308244429086, 0.36755646638556916, 0.3593429150033046, 0.37043121151121244, 0.36591376006236065, 0.36960985819661885, 0.3782340873927796, 0.3691991764661957, 0.3659137598665343, 0.3659137559132899, 0.3737166322232272, 0.37084188987587025, 0.3696098564341817, 0.3782340872336707, 0.36344969000904465, 0.36262833810195294, 0.3659137559132899, 0.37166324432870446, 0.3696098558467027, 0.36344969376646274, 0.37864476305258593, 0.3650924018521084, 0.36427104739927413, 0.3712525671022873, 0.37084188713430133, 0.37453798961345663, 0.3663244358812759, 0.3720739219467743, 0.3765913739831051, 0.3655030812326153, 0.3618069838816625, 0.37371663101155167, 0.35359343054113446, 0.37905543988735035, 0.372484601559825, 0.3679671461577288, 0.3733059530018291, 0.3712525661231556, 0.37207392273007966, 0.3700205362430588, 0.377412733331598, 0.3749486666440474, 0.37002053248564076, 0.36673511193273495, 0.37002053209398805, 0.3778234093830571, 0.37207392429669045, 0.37823408801697606, 0.37330595617176815, 0.365913757088248, 0.37946611789707285, 0.38316221704718023, 0.3671457929165701, 0.37412731042877606, 0.37987679809760266, 0.3630390137617593, 0.3765913751580632, 0.3811088289568312, 0.37330595578011544, 0.37535934269550647, 0.372484598977365, 0.37248459819405966, 0.3790554398506329, 0.3790554392998713, 0.37823408758860594, 0.377412728986701, 0.3782340882128024, 0.3811088315025737, 0.37330595264689387, 0.37905544106230843, 0.377002052776133, 0.37002053561886233, 0.3782340862178215, 0.37125256631898196, 0.3741273102329497, 0.37823408563034244, 0.3794661200511627, 0.3794661190720309, 0.37166324550366253, 0.3778234093830571, 0.3778234097747098, 0.37987679747340614, 0.37700205375526474, 0.37659137731215303, 0.381108831110921, 0.3806981515345877, 0.37248459976067044, 0.3704312097120579, 0.3774127313733346, 0.3864476374042597, 0.3770020519928276, 0.38357289349029194, 0.37700205218865396, 0.3774127309816819, 0.380698151142935, 0.3749486672682439, 0.37987679629844806, 0.3823408627901724, 0.3782340878211497, 0.38357289544855544, 0.37700205571352824, 0.3794661210302944, 0.39219712581967425, 0.3868583152181559, 0.37207392331755873, 0.38193018517210253, 0.3798767961026217, 0.3794661210302944, 0.38685831795972475, 0.3852156067041401, 0.3786447618409104, 0.38562628373473085, 0.3778234093830571, 0.3811088287610048, 0.38603695919871084, 0.3917864462066235, 0.388501025262065, 0.38275153841326126, 0.3864476363884105, 0.39137577097518733, 0.3917864485932571, 0.38193018454790606, 0.38850102784452495, 0.3930184810558139, 0.39425051508498143, 0.39753593325125364, 0.38603695959036355, 0.3946611907447878, 0.39178644937656254, 0.3971252574323384, 0.3905544149557423, 0.3893223828848383, 0.3930184804683349, 0.3926078050043549, 0.3848049259528487, 0.39055441632652677, 0.401232034983821, 0.39753593364290635, 0.3971252562573803, 0.4086242287433123, 0.408624229134965, 0.4082135501461107, 0.40451745416594237, 0.408624232304904, 0.41026693960724425, 0.40862422776418056, 0.4057494887458715, 0.40492812978903125, 0.41108829742584385, 0.40985626320084995, 0.4094455839794519, 0.4156057510287855, 0.4262833672994461, 0.42012320283257254, 0.41190964988369716, 0.4119096496878708, 0.4024640666263549, 0.4135523613718256, 0.4172484622843701, 0.41601642845102893, 0.41724845774364666, 0.41765913673250094, 0.4086242293675088, 0.41889117197334397, 0.41930184661730113, 0.41889116899923134, 0.4254620144132226, 0.42176591608313807, 0.4242299772141161, 0.416016426456048, 0.42340862593122086, 0.43203285493155524, 0.43203285591068696, 0.4242299796007497, 0.42135523529512925, 0.42381930280270275, 0.42422997881744434, 0.4291581102342821, 0.4250513330744522, 0.42546201323826455, 0.42751540054530823, 0.4349075998246547, 0.42258727092762505, 0.43367556520800815, 0.4180698129797863, 0.4238193011993745, 0.4312114987162839, 0.43983572849992364, 0.43285421079189135, 0.4361396301331217, 0.43367556599131357, 0.43326488641498023, 0.4312114977371521, 0.42833675441066343, 0.44065708252438773, 0.4246406583937776, 0.4287474310496015, 0.4353182747011557, 0.4422997957749533, 0.43531827450532934, 0.4344969196608424, 0.43983572709242175, 0.4398357290874027, 0.43613962837068454, 0.4320328551273816, 0.4271047227314121, 0.4332648858275012, 0.43572895134009376, 0.4369609853692613, 0.44106776233326483, 0.44065708275693155, 0.43490759786639127, 0.44271047401721964, 0.4443531834736497, 0.44558521554455377, 0.44188911756940447, 0.436139630328948]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
