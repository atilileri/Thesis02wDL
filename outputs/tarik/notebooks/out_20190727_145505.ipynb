{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf43.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 14:55:06 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'AllShfRnd', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['by', 'ck', 'ce', 'eg', 'yd', 'mb', 'aa', 'ib', 'eo', 'ds', 'eb', 'my', 'sg', 'ek', 'sk'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000020C01A9F278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000020C39FE6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7081, Accuracy:0.0534, Validation Loss:2.7026, Validation Accuracy:0.0542\n",
    "Epoch #2: Loss:2.6996, Accuracy:0.0723, Validation Loss:2.6942, Validation Accuracy:0.0788\n",
    "Epoch #3: Loss:2.6912, Accuracy:0.0784, Validation Loss:2.6863, Validation Accuracy:0.0788\n",
    "Epoch #4: Loss:2.6838, Accuracy:0.0784, Validation Loss:2.6795, Validation Accuracy:0.0788\n",
    "Epoch #5: Loss:2.6769, Accuracy:0.0879, Validation Loss:2.6743, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6727, Accuracy:0.1023, Validation Loss:2.6702, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6692, Accuracy:0.1023, Validation Loss:2.6670, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6663, Accuracy:0.1023, Validation Loss:2.6647, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6644, Accuracy:0.1023, Validation Loss:2.6627, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6619, Accuracy:0.1023, Validation Loss:2.6605, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6598, Accuracy:0.1023, Validation Loss:2.6582, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6570, Accuracy:0.1023, Validation Loss:2.6556, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6540, Accuracy:0.1023, Validation Loss:2.6525, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6501, Accuracy:0.1023, Validation Loss:2.6484, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6453, Accuracy:0.1023, Validation Loss:2.6427, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6385, Accuracy:0.1023, Validation Loss:2.6352, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6292, Accuracy:0.1035, Validation Loss:2.6256, Validation Accuracy:0.1084\n",
    "Epoch #18: Loss:2.6182, Accuracy:0.1092, Validation Loss:2.6134, Validation Accuracy:0.1100\n",
    "Epoch #19: Loss:2.6046, Accuracy:0.1154, Validation Loss:2.5986, Validation Accuracy:0.1232\n",
    "Epoch #20: Loss:2.5882, Accuracy:0.1318, Validation Loss:2.5824, Validation Accuracy:0.1445\n",
    "Epoch #21: Loss:2.5712, Accuracy:0.1429, Validation Loss:2.5656, Validation Accuracy:0.1478\n",
    "Epoch #22: Loss:2.5544, Accuracy:0.1540, Validation Loss:2.5501, Validation Accuracy:0.1576\n",
    "Epoch #23: Loss:2.5410, Accuracy:0.1503, Validation Loss:2.5371, Validation Accuracy:0.1691\n",
    "Epoch #24: Loss:2.5284, Accuracy:0.1725, Validation Loss:2.5249, Validation Accuracy:0.1823\n",
    "Epoch #25: Loss:2.5160, Accuracy:0.1770, Validation Loss:2.5119, Validation Accuracy:0.1987\n",
    "Epoch #26: Loss:2.5034, Accuracy:0.1934, Validation Loss:2.4987, Validation Accuracy:0.2118\n",
    "Epoch #27: Loss:2.4912, Accuracy:0.1988, Validation Loss:2.4865, Validation Accuracy:0.2085\n",
    "Epoch #28: Loss:2.4802, Accuracy:0.2074, Validation Loss:2.4774, Validation Accuracy:0.1938\n",
    "Epoch #29: Loss:2.4689, Accuracy:0.2033, Validation Loss:2.4618, Validation Accuracy:0.2085\n",
    "Epoch #30: Loss:2.4551, Accuracy:0.2021, Validation Loss:2.4449, Validation Accuracy:0.2036\n",
    "Epoch #31: Loss:2.4376, Accuracy:0.2053, Validation Loss:2.4284, Validation Accuracy:0.2053\n",
    "Epoch #32: Loss:2.4197, Accuracy:0.2066, Validation Loss:2.4098, Validation Accuracy:0.2085\n",
    "Epoch #33: Loss:2.3984, Accuracy:0.2136, Validation Loss:2.3876, Validation Accuracy:0.2102\n",
    "Epoch #34: Loss:2.3757, Accuracy:0.2255, Validation Loss:2.3598, Validation Accuracy:0.2217\n",
    "Epoch #35: Loss:2.3457, Accuracy:0.2271, Validation Loss:2.3370, Validation Accuracy:0.2299\n",
    "Epoch #36: Loss:2.3188, Accuracy:0.2415, Validation Loss:2.3061, Validation Accuracy:0.2315\n",
    "Epoch #37: Loss:2.2937, Accuracy:0.2464, Validation Loss:2.2922, Validation Accuracy:0.2365\n",
    "Epoch #38: Loss:2.2790, Accuracy:0.2653, Validation Loss:2.2882, Validation Accuracy:0.2512\n",
    "Epoch #39: Loss:2.2543, Accuracy:0.2604, Validation Loss:2.2442, Validation Accuracy:0.2759\n",
    "Epoch #40: Loss:2.2298, Accuracy:0.2764, Validation Loss:2.2416, Validation Accuracy:0.2479\n",
    "Epoch #41: Loss:2.2047, Accuracy:0.2612, Validation Loss:2.2012, Validation Accuracy:0.2644\n",
    "Epoch #42: Loss:2.1836, Accuracy:0.2764, Validation Loss:2.1878, Validation Accuracy:0.2759\n",
    "Epoch #43: Loss:2.1635, Accuracy:0.2916, Validation Loss:2.1710, Validation Accuracy:0.2742\n",
    "Epoch #44: Loss:2.1437, Accuracy:0.2850, Validation Loss:2.1531, Validation Accuracy:0.2841\n",
    "Epoch #45: Loss:2.1272, Accuracy:0.2928, Validation Loss:2.1484, Validation Accuracy:0.2906\n",
    "Epoch #46: Loss:2.1159, Accuracy:0.2982, Validation Loss:2.1259, Validation Accuracy:0.3054\n",
    "Epoch #47: Loss:2.1024, Accuracy:0.3055, Validation Loss:2.1147, Validation Accuracy:0.2956\n",
    "Epoch #48: Loss:2.0856, Accuracy:0.3076, Validation Loss:2.1055, Validation Accuracy:0.3087\n",
    "Epoch #49: Loss:2.0711, Accuracy:0.3133, Validation Loss:2.0973, Validation Accuracy:0.3038\n",
    "Epoch #50: Loss:2.0616, Accuracy:0.3154, Validation Loss:2.0839, Validation Accuracy:0.3087\n",
    "Epoch #51: Loss:2.0528, Accuracy:0.3236, Validation Loss:2.0784, Validation Accuracy:0.3005\n",
    "Epoch #52: Loss:2.0440, Accuracy:0.3199, Validation Loss:2.0689, Validation Accuracy:0.3038\n",
    "Epoch #53: Loss:2.0370, Accuracy:0.3138, Validation Loss:2.0690, Validation Accuracy:0.3218\n",
    "Epoch #54: Loss:2.0281, Accuracy:0.3224, Validation Loss:2.0545, Validation Accuracy:0.3120\n",
    "Epoch #55: Loss:2.0207, Accuracy:0.3269, Validation Loss:2.0498, Validation Accuracy:0.3120\n",
    "Epoch #56: Loss:2.0110, Accuracy:0.3244, Validation Loss:2.0442, Validation Accuracy:0.3087\n",
    "Epoch #57: Loss:2.0032, Accuracy:0.3314, Validation Loss:2.0388, Validation Accuracy:0.3202\n",
    "Epoch #58: Loss:1.9969, Accuracy:0.3335, Validation Loss:2.0411, Validation Accuracy:0.3136\n",
    "Epoch #59: Loss:1.9932, Accuracy:0.3318, Validation Loss:2.0382, Validation Accuracy:0.3218\n",
    "Epoch #60: Loss:1.9978, Accuracy:0.3211, Validation Loss:2.0273, Validation Accuracy:0.3235\n",
    "Epoch #61: Loss:1.9851, Accuracy:0.3310, Validation Loss:2.0203, Validation Accuracy:0.3235\n",
    "Epoch #62: Loss:1.9772, Accuracy:0.3339, Validation Loss:2.0239, Validation Accuracy:0.3300\n",
    "Epoch #63: Loss:1.9772, Accuracy:0.3396, Validation Loss:2.0128, Validation Accuracy:0.3218\n",
    "Epoch #64: Loss:1.9671, Accuracy:0.3339, Validation Loss:2.0086, Validation Accuracy:0.3284\n",
    "Epoch #65: Loss:1.9636, Accuracy:0.3437, Validation Loss:2.0053, Validation Accuracy:0.3366\n",
    "Epoch #66: Loss:1.9589, Accuracy:0.3392, Validation Loss:2.0012, Validation Accuracy:0.3350\n",
    "Epoch #67: Loss:1.9553, Accuracy:0.3441, Validation Loss:2.0040, Validation Accuracy:0.3186\n",
    "Epoch #68: Loss:1.9539, Accuracy:0.3429, Validation Loss:2.0012, Validation Accuracy:0.3333\n",
    "Epoch #69: Loss:1.9546, Accuracy:0.3396, Validation Loss:2.0094, Validation Accuracy:0.3251\n",
    "Epoch #70: Loss:1.9498, Accuracy:0.3495, Validation Loss:1.9903, Validation Accuracy:0.3432\n",
    "Epoch #71: Loss:1.9415, Accuracy:0.3409, Validation Loss:1.9883, Validation Accuracy:0.3333\n",
    "Epoch #72: Loss:1.9377, Accuracy:0.3380, Validation Loss:1.9853, Validation Accuracy:0.3415\n",
    "Epoch #73: Loss:1.9322, Accuracy:0.3478, Validation Loss:1.9814, Validation Accuracy:0.3383\n",
    "Epoch #74: Loss:1.9275, Accuracy:0.3487, Validation Loss:1.9764, Validation Accuracy:0.3383\n",
    "Epoch #75: Loss:1.9240, Accuracy:0.3540, Validation Loss:1.9768, Validation Accuracy:0.3399\n",
    "Epoch #76: Loss:1.9200, Accuracy:0.3561, Validation Loss:1.9758, Validation Accuracy:0.3350\n",
    "Epoch #77: Loss:1.9216, Accuracy:0.3515, Validation Loss:1.9747, Validation Accuracy:0.3448\n",
    "Epoch #78: Loss:1.9207, Accuracy:0.3528, Validation Loss:1.9663, Validation Accuracy:0.3317\n",
    "Epoch #79: Loss:1.9123, Accuracy:0.3544, Validation Loss:1.9679, Validation Accuracy:0.3383\n",
    "Epoch #80: Loss:1.9091, Accuracy:0.3548, Validation Loss:1.9661, Validation Accuracy:0.3399\n",
    "Epoch #81: Loss:1.9107, Accuracy:0.3446, Validation Loss:1.9615, Validation Accuracy:0.3448\n",
    "Epoch #82: Loss:1.9065, Accuracy:0.3548, Validation Loss:1.9589, Validation Accuracy:0.3399\n",
    "Epoch #83: Loss:1.9043, Accuracy:0.3577, Validation Loss:1.9646, Validation Accuracy:0.3350\n",
    "Epoch #84: Loss:1.9041, Accuracy:0.3577, Validation Loss:1.9560, Validation Accuracy:0.3366\n",
    "Epoch #85: Loss:1.8996, Accuracy:0.3581, Validation Loss:1.9553, Validation Accuracy:0.3448\n",
    "Epoch #86: Loss:1.8933, Accuracy:0.3602, Validation Loss:1.9518, Validation Accuracy:0.3547\n",
    "Epoch #87: Loss:1.8898, Accuracy:0.3602, Validation Loss:1.9510, Validation Accuracy:0.3465\n",
    "Epoch #88: Loss:1.8924, Accuracy:0.3614, Validation Loss:1.9509, Validation Accuracy:0.3366\n",
    "Epoch #89: Loss:1.8868, Accuracy:0.3544, Validation Loss:1.9481, Validation Accuracy:0.3383\n",
    "Epoch #90: Loss:1.8879, Accuracy:0.3528, Validation Loss:1.9504, Validation Accuracy:0.3383\n",
    "Epoch #91: Loss:1.8902, Accuracy:0.3556, Validation Loss:1.9431, Validation Accuracy:0.3317\n",
    "Epoch #92: Loss:1.8832, Accuracy:0.3532, Validation Loss:1.9441, Validation Accuracy:0.3448\n",
    "Epoch #93: Loss:1.8833, Accuracy:0.3643, Validation Loss:1.9379, Validation Accuracy:0.3432\n",
    "Epoch #94: Loss:1.8748, Accuracy:0.3602, Validation Loss:1.9385, Validation Accuracy:0.3399\n",
    "Epoch #95: Loss:1.8773, Accuracy:0.3630, Validation Loss:1.9352, Validation Accuracy:0.3448\n",
    "Epoch #96: Loss:1.8671, Accuracy:0.3651, Validation Loss:1.9358, Validation Accuracy:0.3366\n",
    "Epoch #97: Loss:1.8676, Accuracy:0.3614, Validation Loss:1.9321, Validation Accuracy:0.3366\n",
    "Epoch #98: Loss:1.8628, Accuracy:0.3680, Validation Loss:1.9345, Validation Accuracy:0.3366\n",
    "Epoch #99: Loss:1.8653, Accuracy:0.3634, Validation Loss:1.9240, Validation Accuracy:0.3448\n",
    "Epoch #100: Loss:1.8663, Accuracy:0.3626, Validation Loss:1.9309, Validation Accuracy:0.3530\n",
    "Epoch #101: Loss:1.8598, Accuracy:0.3667, Validation Loss:1.9316, Validation Accuracy:0.3498\n",
    "Epoch #102: Loss:1.8626, Accuracy:0.3684, Validation Loss:1.9319, Validation Accuracy:0.3465\n",
    "Epoch #103: Loss:1.8620, Accuracy:0.3692, Validation Loss:1.9251, Validation Accuracy:0.3465\n",
    "Epoch #104: Loss:1.8560, Accuracy:0.3676, Validation Loss:1.9209, Validation Accuracy:0.3415\n",
    "Epoch #105: Loss:1.8478, Accuracy:0.3733, Validation Loss:1.9186, Validation Accuracy:0.3399\n",
    "Epoch #106: Loss:1.8483, Accuracy:0.3696, Validation Loss:1.9175, Validation Accuracy:0.3498\n",
    "Epoch #107: Loss:1.8406, Accuracy:0.3713, Validation Loss:1.9107, Validation Accuracy:0.3530\n",
    "Epoch #108: Loss:1.8400, Accuracy:0.3708, Validation Loss:1.9133, Validation Accuracy:0.3465\n",
    "Epoch #109: Loss:1.8410, Accuracy:0.3708, Validation Loss:1.9098, Validation Accuracy:0.3448\n",
    "Epoch #110: Loss:1.8355, Accuracy:0.3766, Validation Loss:1.9232, Validation Accuracy:0.3465\n",
    "Epoch #111: Loss:1.8362, Accuracy:0.3721, Validation Loss:1.9306, Validation Accuracy:0.3481\n",
    "Epoch #112: Loss:1.8380, Accuracy:0.3713, Validation Loss:1.9168, Validation Accuracy:0.3448\n",
    "Epoch #113: Loss:1.8340, Accuracy:0.3721, Validation Loss:1.9138, Validation Accuracy:0.3432\n",
    "Epoch #114: Loss:1.8294, Accuracy:0.3758, Validation Loss:1.9008, Validation Accuracy:0.3563\n",
    "Epoch #115: Loss:1.8238, Accuracy:0.3832, Validation Loss:1.9023, Validation Accuracy:0.3465\n",
    "Epoch #116: Loss:1.8224, Accuracy:0.3791, Validation Loss:1.9001, Validation Accuracy:0.3645\n",
    "Epoch #117: Loss:1.8191, Accuracy:0.3766, Validation Loss:1.9175, Validation Accuracy:0.3366\n",
    "Epoch #118: Loss:1.8263, Accuracy:0.3799, Validation Loss:1.8930, Validation Accuracy:0.3711\n",
    "Epoch #119: Loss:1.8225, Accuracy:0.3754, Validation Loss:1.9172, Validation Accuracy:0.3530\n",
    "Epoch #120: Loss:1.8267, Accuracy:0.3717, Validation Loss:1.8965, Validation Accuracy:0.3530\n",
    "Epoch #121: Loss:1.8192, Accuracy:0.3844, Validation Loss:1.9023, Validation Accuracy:0.3514\n",
    "Epoch #122: Loss:1.8125, Accuracy:0.3803, Validation Loss:1.8905, Validation Accuracy:0.3695\n",
    "Epoch #123: Loss:1.8065, Accuracy:0.3848, Validation Loss:1.8941, Validation Accuracy:0.3563\n",
    "Epoch #124: Loss:1.8060, Accuracy:0.3901, Validation Loss:1.8842, Validation Accuracy:0.3662\n",
    "Epoch #125: Loss:1.8114, Accuracy:0.3819, Validation Loss:1.9030, Validation Accuracy:0.3465\n",
    "Epoch #126: Loss:1.8041, Accuracy:0.3803, Validation Loss:1.8855, Validation Accuracy:0.3645\n",
    "Epoch #127: Loss:1.8055, Accuracy:0.3889, Validation Loss:1.8989, Validation Accuracy:0.3530\n",
    "Epoch #128: Loss:1.8084, Accuracy:0.3828, Validation Loss:1.8884, Validation Accuracy:0.3596\n",
    "Epoch #129: Loss:1.8067, Accuracy:0.3922, Validation Loss:1.8976, Validation Accuracy:0.3432\n",
    "Epoch #130: Loss:1.8106, Accuracy:0.3791, Validation Loss:1.8963, Validation Accuracy:0.3662\n",
    "Epoch #131: Loss:1.8063, Accuracy:0.3840, Validation Loss:1.8994, Validation Accuracy:0.3530\n",
    "Epoch #132: Loss:1.8012, Accuracy:0.3856, Validation Loss:1.8761, Validation Accuracy:0.3711\n",
    "Epoch #133: Loss:1.7931, Accuracy:0.3996, Validation Loss:1.8731, Validation Accuracy:0.3629\n",
    "Epoch #134: Loss:1.7903, Accuracy:0.3967, Validation Loss:1.8792, Validation Accuracy:0.3678\n",
    "Epoch #135: Loss:1.7889, Accuracy:0.3938, Validation Loss:1.8796, Validation Accuracy:0.3629\n",
    "Epoch #136: Loss:1.7893, Accuracy:0.3906, Validation Loss:1.8979, Validation Accuracy:0.3498\n",
    "Epoch #137: Loss:1.8022, Accuracy:0.3795, Validation Loss:1.9042, Validation Accuracy:0.3580\n",
    "Epoch #138: Loss:1.8044, Accuracy:0.3844, Validation Loss:1.8713, Validation Accuracy:0.3662\n",
    "Epoch #139: Loss:1.7942, Accuracy:0.3906, Validation Loss:1.8693, Validation Accuracy:0.3711\n",
    "Epoch #140: Loss:1.7889, Accuracy:0.3889, Validation Loss:1.8919, Validation Accuracy:0.3547\n",
    "Epoch #141: Loss:1.7922, Accuracy:0.3959, Validation Loss:1.8764, Validation Accuracy:0.3711\n",
    "Epoch #142: Loss:1.7829, Accuracy:0.3934, Validation Loss:1.8865, Validation Accuracy:0.3563\n",
    "Epoch #143: Loss:1.7833, Accuracy:0.3955, Validation Loss:1.8624, Validation Accuracy:0.3727\n",
    "Epoch #144: Loss:1.7808, Accuracy:0.3988, Validation Loss:1.8632, Validation Accuracy:0.3744\n",
    "Epoch #145: Loss:1.7731, Accuracy:0.3955, Validation Loss:1.8777, Validation Accuracy:0.3580\n",
    "Epoch #146: Loss:1.7760, Accuracy:0.4016, Validation Loss:1.8561, Validation Accuracy:0.3695\n",
    "Epoch #147: Loss:1.7648, Accuracy:0.4012, Validation Loss:1.8585, Validation Accuracy:0.3760\n",
    "Epoch #148: Loss:1.7607, Accuracy:0.4037, Validation Loss:1.8545, Validation Accuracy:0.3711\n",
    "Epoch #149: Loss:1.7641, Accuracy:0.4025, Validation Loss:1.8663, Validation Accuracy:0.3744\n",
    "Epoch #150: Loss:1.7636, Accuracy:0.4000, Validation Loss:1.8629, Validation Accuracy:0.3612\n",
    "Epoch #151: Loss:1.7640, Accuracy:0.3975, Validation Loss:1.8721, Validation Accuracy:0.3563\n",
    "Epoch #152: Loss:1.7636, Accuracy:0.4057, Validation Loss:1.8638, Validation Accuracy:0.3612\n",
    "Epoch #153: Loss:1.7646, Accuracy:0.4041, Validation Loss:1.8537, Validation Accuracy:0.3678\n",
    "Epoch #154: Loss:1.7566, Accuracy:0.4053, Validation Loss:1.8550, Validation Accuracy:0.3695\n",
    "Epoch #155: Loss:1.7491, Accuracy:0.4057, Validation Loss:1.8504, Validation Accuracy:0.3695\n",
    "Epoch #156: Loss:1.7477, Accuracy:0.4119, Validation Loss:1.8456, Validation Accuracy:0.3760\n",
    "Epoch #157: Loss:1.7453, Accuracy:0.4107, Validation Loss:1.8510, Validation Accuracy:0.3645\n",
    "Epoch #158: Loss:1.7465, Accuracy:0.4074, Validation Loss:1.8412, Validation Accuracy:0.3744\n",
    "Epoch #159: Loss:1.7408, Accuracy:0.4140, Validation Loss:1.8391, Validation Accuracy:0.3760\n",
    "Epoch #160: Loss:1.7405, Accuracy:0.4140, Validation Loss:1.8449, Validation Accuracy:0.3662\n",
    "Epoch #161: Loss:1.7369, Accuracy:0.4057, Validation Loss:1.8402, Validation Accuracy:0.3744\n",
    "Epoch #162: Loss:1.7340, Accuracy:0.4140, Validation Loss:1.8564, Validation Accuracy:0.3662\n",
    "Epoch #163: Loss:1.7353, Accuracy:0.4103, Validation Loss:1.8454, Validation Accuracy:0.3629\n",
    "Epoch #164: Loss:1.7284, Accuracy:0.4164, Validation Loss:1.8337, Validation Accuracy:0.3826\n",
    "Epoch #165: Loss:1.7344, Accuracy:0.4066, Validation Loss:1.8703, Validation Accuracy:0.3399\n",
    "Epoch #166: Loss:1.7526, Accuracy:0.4053, Validation Loss:1.8379, Validation Accuracy:0.3645\n",
    "Epoch #167: Loss:1.7524, Accuracy:0.4086, Validation Loss:1.8628, Validation Accuracy:0.3695\n",
    "Epoch #168: Loss:1.7466, Accuracy:0.4066, Validation Loss:1.8458, Validation Accuracy:0.3678\n",
    "Epoch #169: Loss:1.7247, Accuracy:0.4111, Validation Loss:1.8379, Validation Accuracy:0.3924\n",
    "Epoch #170: Loss:1.7321, Accuracy:0.4160, Validation Loss:1.8323, Validation Accuracy:0.3727\n",
    "Epoch #171: Loss:1.7277, Accuracy:0.4160, Validation Loss:1.8451, Validation Accuracy:0.3760\n",
    "Epoch #172: Loss:1.7243, Accuracy:0.4136, Validation Loss:1.8308, Validation Accuracy:0.3859\n",
    "Epoch #173: Loss:1.7319, Accuracy:0.4136, Validation Loss:1.8367, Validation Accuracy:0.3727\n",
    "Epoch #174: Loss:1.7212, Accuracy:0.4259, Validation Loss:1.8344, Validation Accuracy:0.3678\n",
    "Epoch #175: Loss:1.7145, Accuracy:0.4226, Validation Loss:1.8175, Validation Accuracy:0.3908\n",
    "Epoch #176: Loss:1.7169, Accuracy:0.4226, Validation Loss:1.8383, Validation Accuracy:0.3678\n",
    "Epoch #177: Loss:1.7164, Accuracy:0.4168, Validation Loss:1.8316, Validation Accuracy:0.3695\n",
    "Epoch #178: Loss:1.7199, Accuracy:0.4111, Validation Loss:1.8303, Validation Accuracy:0.3727\n",
    "Epoch #179: Loss:1.7075, Accuracy:0.4201, Validation Loss:1.8267, Validation Accuracy:0.3727\n",
    "Epoch #180: Loss:1.7151, Accuracy:0.4201, Validation Loss:1.8348, Validation Accuracy:0.3695\n",
    "Epoch #181: Loss:1.7102, Accuracy:0.4193, Validation Loss:1.8273, Validation Accuracy:0.3810\n",
    "Epoch #182: Loss:1.7091, Accuracy:0.4168, Validation Loss:1.8238, Validation Accuracy:0.3826\n",
    "Epoch #183: Loss:1.7070, Accuracy:0.4259, Validation Loss:1.8139, Validation Accuracy:0.3793\n",
    "Epoch #184: Loss:1.6970, Accuracy:0.4275, Validation Loss:1.8167, Validation Accuracy:0.3777\n",
    "Epoch #185: Loss:1.6973, Accuracy:0.4304, Validation Loss:1.8310, Validation Accuracy:0.3727\n",
    "Epoch #186: Loss:1.6897, Accuracy:0.4246, Validation Loss:1.8104, Validation Accuracy:0.3941\n",
    "Epoch #187: Loss:1.6936, Accuracy:0.4300, Validation Loss:1.8056, Validation Accuracy:0.3842\n",
    "Epoch #188: Loss:1.6895, Accuracy:0.4279, Validation Loss:1.8313, Validation Accuracy:0.3662\n",
    "Epoch #189: Loss:1.6855, Accuracy:0.4271, Validation Loss:1.8084, Validation Accuracy:0.3990\n",
    "Epoch #190: Loss:1.6847, Accuracy:0.4279, Validation Loss:1.8036, Validation Accuracy:0.3793\n",
    "Epoch #191: Loss:1.6842, Accuracy:0.4296, Validation Loss:1.7998, Validation Accuracy:0.3908\n",
    "Epoch #192: Loss:1.6799, Accuracy:0.4329, Validation Loss:1.8218, Validation Accuracy:0.3760\n",
    "Epoch #193: Loss:1.6873, Accuracy:0.4246, Validation Loss:1.7937, Validation Accuracy:0.3941\n",
    "Epoch #194: Loss:1.6726, Accuracy:0.4353, Validation Loss:1.8099, Validation Accuracy:0.3777\n",
    "Epoch #195: Loss:1.6859, Accuracy:0.4271, Validation Loss:1.8351, Validation Accuracy:0.3662\n",
    "Epoch #196: Loss:1.7000, Accuracy:0.4172, Validation Loss:1.7962, Validation Accuracy:0.3810\n",
    "Epoch #197: Loss:1.6882, Accuracy:0.4218, Validation Loss:1.8339, Validation Accuracy:0.3793\n",
    "Epoch #198: Loss:1.6956, Accuracy:0.4242, Validation Loss:1.8162, Validation Accuracy:0.3810\n",
    "Epoch #199: Loss:1.6832, Accuracy:0.4349, Validation Loss:1.8119, Validation Accuracy:0.3810\n",
    "Epoch #200: Loss:1.6808, Accuracy:0.4312, Validation Loss:1.7970, Validation Accuracy:0.3859\n",
    "Epoch #201: Loss:1.6664, Accuracy:0.4333, Validation Loss:1.7934, Validation Accuracy:0.3859\n",
    "Epoch #202: Loss:1.6616, Accuracy:0.4382, Validation Loss:1.7883, Validation Accuracy:0.3875\n",
    "Epoch #203: Loss:1.6622, Accuracy:0.4329, Validation Loss:1.7890, Validation Accuracy:0.3777\n",
    "Epoch #204: Loss:1.6548, Accuracy:0.4374, Validation Loss:1.7851, Validation Accuracy:0.3842\n",
    "Epoch #205: Loss:1.6528, Accuracy:0.4374, Validation Loss:1.7801, Validation Accuracy:0.3908\n",
    "Epoch #206: Loss:1.6458, Accuracy:0.4411, Validation Loss:1.7818, Validation Accuracy:0.3842\n",
    "Epoch #207: Loss:1.6455, Accuracy:0.4390, Validation Loss:1.7859, Validation Accuracy:0.3875\n",
    "Epoch #208: Loss:1.6469, Accuracy:0.4485, Validation Loss:1.7741, Validation Accuracy:0.3892\n",
    "Epoch #209: Loss:1.6444, Accuracy:0.4386, Validation Loss:1.7709, Validation Accuracy:0.3875\n",
    "Epoch #210: Loss:1.6505, Accuracy:0.4370, Validation Loss:1.7790, Validation Accuracy:0.3826\n",
    "Epoch #211: Loss:1.6443, Accuracy:0.4398, Validation Loss:1.7754, Validation Accuracy:0.3842\n",
    "Epoch #212: Loss:1.6485, Accuracy:0.4353, Validation Loss:1.8104, Validation Accuracy:0.3695\n",
    "Epoch #213: Loss:1.6528, Accuracy:0.4386, Validation Loss:1.7724, Validation Accuracy:0.3974\n",
    "Epoch #214: Loss:1.6521, Accuracy:0.4324, Validation Loss:1.7893, Validation Accuracy:0.3875\n",
    "Epoch #215: Loss:1.6431, Accuracy:0.4333, Validation Loss:1.7803, Validation Accuracy:0.3941\n",
    "Epoch #216: Loss:1.6418, Accuracy:0.4452, Validation Loss:1.7702, Validation Accuracy:0.3793\n",
    "Epoch #217: Loss:1.6324, Accuracy:0.4439, Validation Loss:1.7836, Validation Accuracy:0.3957\n",
    "Epoch #218: Loss:1.6425, Accuracy:0.4472, Validation Loss:1.7605, Validation Accuracy:0.3859\n",
    "Epoch #219: Loss:1.6289, Accuracy:0.4526, Validation Loss:1.8015, Validation Accuracy:0.3908\n",
    "Epoch #220: Loss:1.6320, Accuracy:0.4419, Validation Loss:1.7708, Validation Accuracy:0.3990\n",
    "Epoch #221: Loss:1.6537, Accuracy:0.4308, Validation Loss:1.7893, Validation Accuracy:0.3892\n",
    "Epoch #222: Loss:1.6316, Accuracy:0.4468, Validation Loss:1.7779, Validation Accuracy:0.3974\n",
    "Epoch #223: Loss:1.6306, Accuracy:0.4419, Validation Loss:1.7739, Validation Accuracy:0.3957\n",
    "Epoch #224: Loss:1.6156, Accuracy:0.4485, Validation Loss:1.7599, Validation Accuracy:0.4007\n",
    "Epoch #225: Loss:1.6188, Accuracy:0.4517, Validation Loss:1.7634, Validation Accuracy:0.3842\n",
    "Epoch #226: Loss:1.6128, Accuracy:0.4480, Validation Loss:1.7728, Validation Accuracy:0.3957\n",
    "Epoch #227: Loss:1.6145, Accuracy:0.4517, Validation Loss:1.7720, Validation Accuracy:0.4056\n",
    "Epoch #228: Loss:1.6046, Accuracy:0.4522, Validation Loss:1.7476, Validation Accuracy:0.4007\n",
    "Epoch #229: Loss:1.6084, Accuracy:0.4485, Validation Loss:1.7702, Validation Accuracy:0.3826\n",
    "Epoch #230: Loss:1.5969, Accuracy:0.4571, Validation Loss:1.7420, Validation Accuracy:0.3957\n",
    "Epoch #231: Loss:1.5944, Accuracy:0.4559, Validation Loss:1.7482, Validation Accuracy:0.3924\n",
    "Epoch #232: Loss:1.5986, Accuracy:0.4567, Validation Loss:1.7643, Validation Accuracy:0.3908\n",
    "Epoch #233: Loss:1.6050, Accuracy:0.4567, Validation Loss:1.7560, Validation Accuracy:0.3957\n",
    "Epoch #234: Loss:1.6080, Accuracy:0.4505, Validation Loss:1.7535, Validation Accuracy:0.3810\n",
    "Epoch #235: Loss:1.5967, Accuracy:0.4542, Validation Loss:1.7411, Validation Accuracy:0.3908\n",
    "Epoch #236: Loss:1.5880, Accuracy:0.4657, Validation Loss:1.7528, Validation Accuracy:0.3941\n",
    "Epoch #237: Loss:1.5955, Accuracy:0.4591, Validation Loss:1.7381, Validation Accuracy:0.3941\n",
    "Epoch #238: Loss:1.5858, Accuracy:0.4587, Validation Loss:1.7472, Validation Accuracy:0.3924\n",
    "Epoch #239: Loss:1.5911, Accuracy:0.4608, Validation Loss:1.7495, Validation Accuracy:0.3875\n",
    "Epoch #240: Loss:1.5859, Accuracy:0.4575, Validation Loss:1.7453, Validation Accuracy:0.3892\n",
    "Epoch #241: Loss:1.5791, Accuracy:0.4682, Validation Loss:1.7552, Validation Accuracy:0.4007\n",
    "Epoch #242: Loss:1.5893, Accuracy:0.4534, Validation Loss:1.7437, Validation Accuracy:0.4105\n",
    "Epoch #243: Loss:1.5887, Accuracy:0.4563, Validation Loss:1.7267, Validation Accuracy:0.3990\n",
    "Epoch #244: Loss:1.5785, Accuracy:0.4674, Validation Loss:1.7617, Validation Accuracy:0.3826\n",
    "Epoch #245: Loss:1.5793, Accuracy:0.4653, Validation Loss:1.7285, Validation Accuracy:0.3957\n",
    "Epoch #246: Loss:1.5688, Accuracy:0.4608, Validation Loss:1.7266, Validation Accuracy:0.4105\n",
    "Epoch #247: Loss:1.5834, Accuracy:0.4624, Validation Loss:1.7204, Validation Accuracy:0.3990\n",
    "Epoch #248: Loss:1.5647, Accuracy:0.4719, Validation Loss:1.7449, Validation Accuracy:0.3908\n",
    "Epoch #249: Loss:1.5616, Accuracy:0.4657, Validation Loss:1.7216, Validation Accuracy:0.4039\n",
    "Epoch #250: Loss:1.5678, Accuracy:0.4665, Validation Loss:1.7281, Validation Accuracy:0.3957\n",
    "Epoch #251: Loss:1.5624, Accuracy:0.4686, Validation Loss:1.7356, Validation Accuracy:0.3842\n",
    "Epoch #252: Loss:1.5631, Accuracy:0.4678, Validation Loss:1.7224, Validation Accuracy:0.4023\n",
    "Epoch #253: Loss:1.5537, Accuracy:0.4747, Validation Loss:1.7129, Validation Accuracy:0.4039\n",
    "Epoch #254: Loss:1.5648, Accuracy:0.4682, Validation Loss:1.7204, Validation Accuracy:0.4089\n",
    "Epoch #255: Loss:1.5607, Accuracy:0.4682, Validation Loss:1.7240, Validation Accuracy:0.4007\n",
    "Epoch #256: Loss:1.5540, Accuracy:0.4686, Validation Loss:1.7211, Validation Accuracy:0.3974\n",
    "Epoch #257: Loss:1.5564, Accuracy:0.4731, Validation Loss:1.7434, Validation Accuracy:0.4072\n",
    "Epoch #258: Loss:1.5667, Accuracy:0.4682, Validation Loss:1.7092, Validation Accuracy:0.4154\n",
    "Epoch #259: Loss:1.5523, Accuracy:0.4723, Validation Loss:1.7073, Validation Accuracy:0.4039\n",
    "Epoch #260: Loss:1.5452, Accuracy:0.4706, Validation Loss:1.7297, Validation Accuracy:0.3974\n",
    "Epoch #261: Loss:1.5482, Accuracy:0.4731, Validation Loss:1.7026, Validation Accuracy:0.4056\n",
    "Epoch #262: Loss:1.5528, Accuracy:0.4698, Validation Loss:1.7055, Validation Accuracy:0.4105\n",
    "Epoch #263: Loss:1.5441, Accuracy:0.4710, Validation Loss:1.7093, Validation Accuracy:0.4007\n",
    "Epoch #264: Loss:1.5498, Accuracy:0.4764, Validation Loss:1.7313, Validation Accuracy:0.3924\n",
    "Epoch #265: Loss:1.5528, Accuracy:0.4727, Validation Loss:1.7056, Validation Accuracy:0.4023\n",
    "Epoch #266: Loss:1.5377, Accuracy:0.4817, Validation Loss:1.6997, Validation Accuracy:0.4089\n",
    "Epoch #267: Loss:1.5328, Accuracy:0.4817, Validation Loss:1.7331, Validation Accuracy:0.4056\n",
    "Epoch #268: Loss:1.5466, Accuracy:0.4760, Validation Loss:1.7042, Validation Accuracy:0.3974\n",
    "Epoch #269: Loss:1.5527, Accuracy:0.4760, Validation Loss:1.7237, Validation Accuracy:0.3957\n",
    "Epoch #270: Loss:1.5422, Accuracy:0.4805, Validation Loss:1.7038, Validation Accuracy:0.4122\n",
    "Epoch #271: Loss:1.5329, Accuracy:0.4760, Validation Loss:1.6925, Validation Accuracy:0.4171\n",
    "Epoch #272: Loss:1.5298, Accuracy:0.4747, Validation Loss:1.6923, Validation Accuracy:0.4105\n",
    "Epoch #273: Loss:1.5267, Accuracy:0.4768, Validation Loss:1.6951, Validation Accuracy:0.4105\n",
    "Epoch #274: Loss:1.5264, Accuracy:0.4850, Validation Loss:1.6820, Validation Accuracy:0.4089\n",
    "Epoch #275: Loss:1.5265, Accuracy:0.4793, Validation Loss:1.6980, Validation Accuracy:0.4138\n",
    "Epoch #276: Loss:1.5189, Accuracy:0.4780, Validation Loss:1.7054, Validation Accuracy:0.4171\n",
    "Epoch #277: Loss:1.5325, Accuracy:0.4768, Validation Loss:1.7032, Validation Accuracy:0.4138\n",
    "Epoch #278: Loss:1.5296, Accuracy:0.4752, Validation Loss:1.6812, Validation Accuracy:0.4154\n",
    "Epoch #279: Loss:1.5237, Accuracy:0.4743, Validation Loss:1.6861, Validation Accuracy:0.4138\n",
    "Epoch #280: Loss:1.5257, Accuracy:0.4789, Validation Loss:1.6833, Validation Accuracy:0.4204\n",
    "Epoch #281: Loss:1.5239, Accuracy:0.4784, Validation Loss:1.6960, Validation Accuracy:0.4171\n",
    "Epoch #282: Loss:1.5150, Accuracy:0.4825, Validation Loss:1.6853, Validation Accuracy:0.4220\n",
    "Epoch #283: Loss:1.5071, Accuracy:0.4830, Validation Loss:1.6880, Validation Accuracy:0.4171\n",
    "Epoch #284: Loss:1.5064, Accuracy:0.4842, Validation Loss:1.6761, Validation Accuracy:0.4171\n",
    "Epoch #285: Loss:1.5088, Accuracy:0.4899, Validation Loss:1.6767, Validation Accuracy:0.4138\n",
    "Epoch #286: Loss:1.5072, Accuracy:0.4867, Validation Loss:1.6979, Validation Accuracy:0.4023\n",
    "Epoch #287: Loss:1.5290, Accuracy:0.4760, Validation Loss:1.6862, Validation Accuracy:0.4171\n",
    "Epoch #288: Loss:1.5086, Accuracy:0.4883, Validation Loss:1.6912, Validation Accuracy:0.4171\n",
    "Epoch #289: Loss:1.5168, Accuracy:0.4834, Validation Loss:1.6889, Validation Accuracy:0.4154\n",
    "Epoch #290: Loss:1.5132, Accuracy:0.4867, Validation Loss:1.7355, Validation Accuracy:0.3924\n",
    "Epoch #291: Loss:1.5276, Accuracy:0.4772, Validation Loss:1.6748, Validation Accuracy:0.4171\n",
    "Epoch #292: Loss:1.5097, Accuracy:0.4854, Validation Loss:1.6865, Validation Accuracy:0.4236\n",
    "Epoch #293: Loss:1.4978, Accuracy:0.4908, Validation Loss:1.6862, Validation Accuracy:0.4089\n",
    "Epoch #294: Loss:1.4982, Accuracy:0.4895, Validation Loss:1.6822, Validation Accuracy:0.4253\n",
    "Epoch #295: Loss:1.4925, Accuracy:0.4912, Validation Loss:1.6662, Validation Accuracy:0.4171\n",
    "Epoch #296: Loss:1.4924, Accuracy:0.4912, Validation Loss:1.6723, Validation Accuracy:0.4253\n",
    "Epoch #297: Loss:1.4932, Accuracy:0.4953, Validation Loss:1.6662, Validation Accuracy:0.4122\n",
    "Epoch #298: Loss:1.4976, Accuracy:0.4871, Validation Loss:1.6789, Validation Accuracy:0.4072\n",
    "Epoch #299: Loss:1.4957, Accuracy:0.4867, Validation Loss:1.7059, Validation Accuracy:0.4154\n",
    "Epoch #300: Loss:1.5036, Accuracy:0.4854, Validation Loss:1.6698, Validation Accuracy:0.4286\n",
    "\n",
    "Test:\n",
    "Test Loss:1.66975665, Accuracy:0.4286\n",
    "Labels: ['by', 'ck', 'ce', 'eg', 'yd', 'mb', 'aa', 'ib', 'eo', 'ds', 'eb', 'my', 'sg', 'ek', 'sk']\n",
    "Confusion Matrix:\n",
    "      by  ck  ce  eg  yd  mb  aa  ib  eo  ds  eb  my  sg  ek  sk\n",
    "t:by  29   0   1   4   1   0   0   0   0   0   0   0   5   0   0\n",
    "t:ck   0   3   2   0   0   9   1   1   0   1   2   0   0   4   0\n",
    "t:ce   6   1   3   3   0   5   0   3   1   0   1   0   1   3   0\n",
    "t:eg   6   0   1  18   1   4   8   1   1   4   3   0   3   0   0\n",
    "t:yd   0   1   0   2  32   6   0   1   0   0   4   0  11   5   0\n",
    "t:mb   0   2   1   4   5  22   1   7   0   3   2   0   1   4   0\n",
    "t:aa   1   0   0   9   2   3  12   2   0   1   1   0   2   1   0\n",
    "t:ib   0   0   1   2   2   1   3  38   0   0   0   0   5   2   0\n",
    "t:eo   4   0   1   0   0   0   1   0  28   0   0   0   0   0   0\n",
    "t:ds   0   3   0   3   1   3   1   0   0   4   9   1   0   5   1\n",
    "t:eb   0   1   0   0   4   2   0   0   0   2  34   0   0   7   0\n",
    "t:my   0   2   0   0   0   6   0   1   0   1   6   1   0   3   0\n",
    "t:sg   5   0   0   4  11   3   0   3   0   0   0   0  25   0   0\n",
    "t:ek   0   1   0   9   5   1   0   1   0   4  16   1   0  10   0\n",
    "t:sk   1   2   0   2   0   5   1   0   0   2  13   0   0   5   2\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          by       0.56      0.72      0.63        40\n",
    "          ck       0.19      0.13      0.15        23\n",
    "          ce       0.30      0.11      0.16        27\n",
    "          eg       0.30      0.36      0.33        50\n",
    "          yd       0.50      0.52      0.51        62\n",
    "          mb       0.31      0.42      0.36        52\n",
    "          aa       0.43      0.35      0.39        34\n",
    "          ib       0.66      0.70      0.68        54\n",
    "          eo       0.93      0.82      0.87        34\n",
    "          ds       0.18      0.13      0.15        31\n",
    "          eb       0.37      0.68      0.48        50\n",
    "          my       0.33      0.05      0.09        20\n",
    "          sg       0.47      0.49      0.48        51\n",
    "          ek       0.20      0.21      0.21        48\n",
    "          sk       0.67      0.06      0.11        33\n",
    "\n",
    "    accuracy                           0.43       609\n",
    "   macro avg       0.43      0.38      0.37       609\n",
    "weighted avg       0.44      0.43      0.41       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 15:36:01 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 55 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7026388265424957, 2.6941545498977937, 2.686326207394279, 2.679493078848803, 2.6743020197049345, 2.670206454391354, 2.6669733031042693, 2.664694099786442, 2.662676917312572, 2.6605244587207664, 2.6582292218513675, 2.65564060485226, 2.6524950937293044, 2.6483745641504797, 2.6427052095410075, 2.635216440081792, 2.6256281519171054, 2.6133793829305616, 2.598645333586068, 2.582357290343111, 2.565598775209073, 2.550095718678191, 2.5370542231843194, 2.524904060442068, 2.5119155443752144, 2.4987409987864626, 2.486531138615851, 2.477354654537633, 2.461824197095799, 2.4448680450959355, 2.4283928068596348, 2.409758225646121, 2.3876359474482793, 2.3597902789687484, 2.336998916416137, 2.306087281707864, 2.292239542273661, 2.2881775419113084, 2.244163317829126, 2.241576139954315, 2.201229843600043, 2.1877680549089153, 2.171000161194449, 2.153060101131696, 2.148410435380607, 2.1258961445788054, 2.114700563239738, 2.1055087268058887, 2.097302151039512, 2.083883587362731, 2.078439863640295, 2.068904504008677, 2.068955982455675, 2.0544645140323734, 2.049799197217318, 2.0441984366900816, 2.0387838548431647, 2.041091864136444, 2.038209956855022, 2.0272615768247833, 2.020331971751058, 2.0239257101942165, 2.0128137519206906, 2.0085613024841584, 2.005337972946355, 2.001217772807981, 2.0039949389709824, 2.001220221785685, 2.0094303061026464, 1.9902909557611876, 1.9882706023985137, 1.9852813813095218, 1.981357537858396, 1.9764385344751167, 1.9768285590831087, 1.9757634963112316, 1.9747386873257766, 1.966344371609304, 1.9678562634879928, 1.9661329839812907, 1.9615262709619181, 1.9589307584198825, 1.9646430056670616, 1.9559985191755498, 1.9553320593826093, 1.9517738916995295, 1.9509555418503108, 1.9508651449958287, 1.9480796735274968, 1.9504405336426984, 1.9431441292191178, 1.9440622329711914, 1.9379401197182917, 1.9384695814160877, 1.9351693601248103, 1.9357740595227195, 1.9320519833729184, 1.9344952276774816, 1.9239674105824311, 1.9309001321275834, 1.9315649656630893, 1.931897096250249, 1.9251305350333403, 1.9208588782202434, 1.9186260582778254, 1.9174922248608568, 1.910664248740536, 1.913349118138769, 1.9097879058230296, 1.9232484429144898, 1.9306031819830582, 1.9168356670730415, 1.9138449933532815, 1.9007737395798632, 1.9023198740822929, 1.900098584164148, 1.9175012452261788, 1.8930355678442468, 1.9171811128876284, 1.8964742081505912, 1.9022848022786658, 1.8904730073728389, 1.8941224161627257, 1.884210213260306, 1.9029959859127676, 1.8854503445633135, 1.898925571606077, 1.88842868981103, 1.8976107397298703, 1.8963263573121947, 1.89944427314846, 1.8761317500927177, 1.8731299427342532, 1.879228134656383, 1.8795733686738414, 1.8979093558682596, 1.9041600532719654, 1.8713270506052353, 1.8693059905996463, 1.8919441641257901, 1.8764058680369937, 1.8864793658060786, 1.8623882152372588, 1.8631886725355251, 1.8777299500842792, 1.8560853485990627, 1.8584921176014666, 1.8545327576118933, 1.8662518720908705, 1.86289934845785, 1.872089888270461, 1.8637561257836854, 1.85365861272577, 1.8549506278656582, 1.8503555214072291, 1.8456144712632905, 1.8510040130912768, 1.8412108707114785, 1.839125882228607, 1.8448864744214588, 1.840182699397671, 1.8564344071010845, 1.8453712439889391, 1.8336990425739381, 1.8702891663769, 1.8379350967203651, 1.862753572330882, 1.845810919168156, 1.8379248271043274, 1.8322548921080841, 1.8450560119547477, 1.8307999164991582, 1.8366869785907038, 1.8344497054276991, 1.8174597659134513, 1.8382958302944166, 1.8315797429562397, 1.8302708296548753, 1.8267415926374238, 1.83482630793097, 1.8272948695716795, 1.8237833630275258, 1.8138869538878768, 1.8166584085752615, 1.8309545143092991, 1.8104267617556067, 1.8055657152276126, 1.8312888307915924, 1.808443833845981, 1.8035503390974599, 1.799841690337521, 1.821783165234846, 1.793738579123674, 1.8099034548980262, 1.8351279904298203, 1.7961843264318256, 1.8339017463239347, 1.8162413158244493, 1.8118566548687287, 1.796967732691021, 1.7933501987817448, 1.7883113683346652, 1.789008685716463, 1.7850502688309242, 1.7800581016759762, 1.7817649391092887, 1.7858892199636875, 1.7740762061477686, 1.7709234265858316, 1.7790234470602326, 1.7754080931737113, 1.8104167877159683, 1.7724328714442763, 1.7892912005751787, 1.7802922760911764, 1.7701823619394663, 1.7835601108219041, 1.7604584368970397, 1.8015478669324727, 1.7707576614686813, 1.7892753633568048, 1.7779067198826957, 1.7739049395904165, 1.7599161090130484, 1.7634478196721945, 1.7728401463607262, 1.7719803655088828, 1.7476041947288075, 1.7701855498581685, 1.7420281222692655, 1.7481873301645414, 1.7642688248153586, 1.7560497492992233, 1.753496092724291, 1.741095557001424, 1.7528059441467811, 1.7380921498112294, 1.7471778520026622, 1.7494819608619452, 1.7453343907404808, 1.7551509507966943, 1.7436885712377739, 1.7266638496239197, 1.7617295823856722, 1.7284866561639094, 1.7266339817266354, 1.7204327516759361, 1.7448578288010972, 1.7216323308756787, 1.7281354168561487, 1.7356281204176653, 1.7223646848268306, 1.7128577610150542, 1.7204042072170864, 1.723994966993974, 1.7211286994232529, 1.7434251455250631, 1.7092183407500068, 1.7073288903447794, 1.729732335103165, 1.7026273985018676, 1.7055271715170448, 1.709281764202713, 1.7313149323800123, 1.7056126101263638, 1.6997074996700818, 1.7331363352257239, 1.704150705697697, 1.7236732313002663, 1.7038490875992673, 1.692511796559802, 1.6923495681806542, 1.6951067416343, 1.682016197683776, 1.697980913622626, 1.7053739143709832, 1.7031638808242597, 1.6812180946221689, 1.6860717950000357, 1.6833002183629178, 1.6959890718334805, 1.685300939580294, 1.687988500876967, 1.6760719517377405, 1.6767093342513286, 1.697893183219609, 1.6862401016827286, 1.6911711557745346, 1.6888810603685176, 1.7355283522253553, 1.6748484144069877, 1.686473622306423, 1.6862072699958663, 1.6822310509940086, 1.6662175205149283, 1.6722541156856494, 1.6662158527593502, 1.678865778818115, 1.7059090703187514, 1.6697565783029316], 'val_acc': [0.05418719211516807, 0.07881773378828476, 0.07881773378828476, 0.07881773378828476, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10837438402847312, 0.11001642025114085, 0.12315270845428085, 0.14449917807661253, 0.147783250521948, 0.15763546727071645, 0.16912972043789862, 0.1822660093383836, 0.19868637068420403, 0.211822659682562, 0.20853858743297252, 0.19376026211407385, 0.20853858733509953, 0.2036124789607153, 0.20525451518338303, 0.20853858743297252, 0.2101806237535132, 0.22167487643133046, 0.2298850571531772, 0.23152709357159088, 0.23645320175022916, 0.2512315270691278, 0.27586206686124815, 0.24794745462379236, 0.26436781398768494, 0.27586206874530306, 0.27422003281625423, 0.2840722475830949, 0.29064039218014687, 0.30541871749904553, 0.2955665006524041, 0.30870278935714307, 0.3037766812763778, 0.30870278965076203, 0.3004926090267883, 0.3037766816678697, 0.32183907913848486, 0.3119868619982245, 0.3119868619003515, 0.308702789846508, 0.3201970430136901, 0.31362889812301925, 0.32183907923635785, 0.3234811153611526, 0.3234811149696607, 0.33004925966458565, 0.32183907894273894, 0.3284072235397909, 0.3366174041637646, 0.3349753680389699, 0.31855500698676836, 0.33333333230566703, 0.3251231512902014, 0.34318554895656256, 0.3333333320120481, 0.3415435128317678, 0.33825944058217833, 0.33825944048430534, 0.339901476804846, 0.3349753679410969, 0.34482758478773834, 0.3316912957893804, 0.33825944058217833, 0.33990147670697307, 0.3448275849834843, 0.33990147670697307, 0.33497536813684287, 0.3366174041637646, 0.3448275851792303, 0.35467980212374467, 0.34646962110827906, 0.3366174042616376, 0.33825944028855937, 0.33825944068005126, 0.3316912957893804, 0.3448275851792303, 0.3431855487608166, 0.3399014765112271, 0.3448275851792303, 0.3366174042616376, 0.33661740406589163, 0.33661740445738353, 0.3448275849834843, 0.353037765901077, 0.34975369365148745, 0.3464696209125331, 0.34646962101040607, 0.3415435128317678, 0.339901476902719, 0.3497536935536145, 0.353037765803204, 0.34646962130402503, 0.3448275850813573, 0.3464696209125331, 0.3481116571352008, 0.34482758468986535, 0.3431855490544355, 0.3563218378570475, 0.34646962110827906, 0.36453201838314825, 0.33661740445738353, 0.3711001629802002, 0.353037765803204, 0.353037765705331, 0.35139572967840926, 0.36945812695327845, 0.3563218379549205, 0.36617405470368897, 0.34646962130402503, 0.3645320185788942, 0.35303776521596614, 0.35960591020451, 0.34318554885868957, 0.3661740543121971, 0.353037765705331, 0.3711001630780732, 0.3628899823562265, 0.3678160910242297, 0.3628899824540995, 0.3497536930642496, 0.35796387417758824, 0.36617405470368897, 0.3711001629802002, 0.35467980183012576, 0.3711001630780732, 0.3563218379549205, 0.37274219910499495, 0.3743842352297897, 0.35796387407971525, 0.36945812685540547, 0.3760262715503304, 0.3711001629802002, 0.37438423542553567, 0.36124794632930474, 0.3563218379549205, 0.36124794593781284, 0.3678160908284837, 0.36945812695327845, 0.36945812685540547, 0.3760262716482034, 0.3645320185788942, 0.3743842353276627, 0.3760262715503304, 0.36617405480156195, 0.37438423542553567, 0.36617405470368897, 0.36288998255197247, 0.3825944157558905, 0.33990147670697307, 0.36453201828527526, 0.3694581273447704, 0.3678160909263567, 0.39244663250465894, 0.37274219910499495, 0.37602627203969535, 0.38587848810335296, 0.37274219910499495, 0.36781609063273774, 0.3908045967713561, 0.3678160909263567, 0.36945812685540547, 0.372742199007122, 0.372742199007122, 0.36945812705115144, 0.38095237992471465, 0.38259441634312835, 0.3793103437999199, 0.37766830728363326, 0.3727421994964869, 0.39408866882519966, 0.3842364522721771, 0.36617405470368897, 0.3990147772974569, 0.3793103437999199, 0.39080459696710207, 0.3760262716482034, 0.3940886690209456, 0.37766830777299815, 0.366174054507943, 0.3809523802183336, 0.37931034409353886, 0.38095237992471465, 0.38095237963109574, 0.3858784886905908, 0.3858784886905908, 0.3875205246196396, 0.3776683079687441, 0.3842364523700501, 0.3908045967713561, 0.3842364525657961, 0.38752052481538557, 0.3891625610380533, 0.3875205246196396, 0.3825944161473824, 0.3842364519785582, 0.36945812705115144, 0.39737274117266214, 0.3875205247175126, 0.39408866931456454, 0.3793103437999199, 0.3957307050478674, 0.38587848859271784, 0.390804597358594, 0.3990147772974569, 0.38916256064656135, 0.3973727412705351, 0.3957307053414863, 0.4006568140094895, 0.3842364525657961, 0.3957307052436133, 0.40558292209025476, 0.4006568141073625, 0.3825944165388743, 0.3957307054393593, 0.39244663260253193, 0.3908045965756102, 0.39573070563510526, 0.3809523803162066, 0.39080459716284804, 0.39408866882519966, 0.39408866892307265, 0.3924466333855158, 0.38752052491325856, 0.3891625608423073, 0.40065681361799754, 0.41050903036676606, 0.3990147776889488, 0.38259441644100134, 0.3957307054393593, 0.41050903036676606, 0.3990147774932028, 0.39080459706497506, 0.403940886063333, 0.3957307055372323, 0.3842364524679231, 0.4022988498406653, 0.4039408858675871, 0.4088669947313362, 0.4006568141073625, 0.39737274156415403, 0.4072249580193036, 0.4154351390347692, 0.403940886063333, 0.397372741662027, 0.40558292199238183, 0.41050903066038497, 0.4006568134222516, 0.3924466332876428, 0.40229885003641125, 0.40886699443771723, 0.40558292228600074, 0.39737274156415403, 0.3957307052436133, 0.4121510667851797, 0.41707717515956394, 0.410509030562512, 0.41050903026889307, 0.40886699443771723, 0.41379310310572043, 0.4170771756489289, 0.4137931032035934, 0.41543513893689626, 0.41379310251848256, 0.4203612477027724, 0.4170771753553099, 0.42200328363182116, 0.4170771756489289, 0.41707717515956394, 0.41379310281210147, 0.40229885013428424, 0.4170771754531829, 0.4170771752574369, 0.41543513952413413, 0.3924466331897698, 0.41707717515956394, 0.4236453198544889, 0.4088669945355902, 0.4252873561750296, 0.4170771752574369, 0.4252873560771566, 0.4121510665894338, 0.4072249584107955, 0.41543513932838816, 0.4285714283267461], 'loss': [2.708068884961169, 2.6996281304643386, 2.691213679852182, 2.683799782621787, 2.676936430216325, 2.6727497504232356, 2.6691658461607948, 2.6662875966614523, 2.664432188030141, 2.6618757608245285, 2.6598042318708353, 2.65698414436356, 2.653989009739682, 2.650113334597014, 2.6452522885872845, 2.6384818146605755, 2.6292248432151593, 2.6182230319820143, 2.6046239904799258, 2.5882482714721555, 2.5711754723496014, 2.554350529461181, 2.5409548452992214, 2.5283992183771464, 2.5159717505962207, 2.503408178313802, 2.491218737850933, 2.4802431656839423, 2.468937087401717, 2.455137189555707, 2.437622906099355, 2.4197076259942025, 2.3984073086685713, 2.375665781287442, 2.345680494034315, 2.318812631238902, 2.29367087469943, 2.2790242178484155, 2.254303083086895, 2.2297516741547008, 2.2046512749650393, 2.1835835282562694, 2.163465823970536, 2.1436531849220795, 2.1272460552701227, 2.1158612956256593, 2.1024435933365715, 2.085614090142064, 2.0710828627404245, 2.061620633655995, 2.0527571113202607, 2.044029286363042, 2.0370346141791686, 2.0280866237146897, 2.0207447047106295, 2.0110474592361607, 2.003212149872672, 1.996922806694767, 1.9932410673928702, 1.9977797771626185, 1.9850685679202698, 1.9772481694113793, 1.977243594663099, 1.9671440621421077, 1.9635763289747297, 1.9588622556329998, 1.955323373365696, 1.953929017358737, 1.9545796360078533, 1.9497730034332745, 1.9414702912865234, 1.9376899278628998, 1.9322021909318177, 1.9275135503902083, 1.9239812824026026, 1.920029083659272, 1.9215982548754809, 1.9206557475076318, 1.9123138378777789, 1.909136530506048, 1.910683139589533, 1.906534202289777, 1.9043203386193184, 1.9040923185172267, 1.8995627104624095, 1.8933091645857636, 1.8897586258040318, 1.8923883014635874, 1.886846401804037, 1.8878917464485403, 1.890207199345379, 1.8831764049843351, 1.8832911731281319, 1.8747723835939254, 1.8772503138078067, 1.8670799677376875, 1.8676281803687251, 1.8628246210439003, 1.8653290514583705, 1.8662698009420469, 1.85976226824265, 1.8625529152167162, 1.8620012123237155, 1.8560366440113076, 1.847789674617916, 1.848337319793153, 1.8406209851192499, 1.840012020839558, 1.8410365862523261, 1.8355317288600443, 1.836243353144589, 1.8380451287087474, 1.8339658273563737, 1.8294186488803652, 1.823762277362283, 1.8224366071043074, 1.8190870137674853, 1.8263265269003366, 1.822531666990668, 1.82665929500572, 1.8191919754662798, 1.812488863943049, 1.8065048999610134, 1.8060115962547443, 1.8114322552201194, 1.8041383780982705, 1.8054545161660447, 1.8084104178132951, 1.8067143598376358, 1.8105547623467886, 1.8062859392753616, 1.8011935781404467, 1.7930833060638616, 1.79031429383789, 1.7888761952672405, 1.7893214623786096, 1.8021735053777206, 1.8044259564832494, 1.7942008805226006, 1.7888750011181684, 1.7922184862884898, 1.7828552146711878, 1.7832620709338962, 1.7807763557904066, 1.7730903172150285, 1.7759892394165728, 1.7648257708892197, 1.7607061764053251, 1.7640957778973745, 1.7636321492753235, 1.764044525099486, 1.763585947475394, 1.764596450255392, 1.7566322490664723, 1.7490805425683087, 1.7476666596880683, 1.745279644353189, 1.7464572031639929, 1.7407778994013887, 1.7404503696997797, 1.7368970735851499, 1.7340070597689745, 1.735287021415679, 1.7283642513306479, 1.7344304700651698, 1.7526282975805858, 1.7523685808788825, 1.7465650369009689, 1.7247040251197268, 1.7320615299918078, 1.727676492305262, 1.7242707420913101, 1.7318502410481353, 1.7211702450589723, 1.71447125401585, 1.7168930974094774, 1.7164489669231908, 1.7199008527477664, 1.7074602266111902, 1.715052066495531, 1.7102216298085708, 1.7091142410125575, 1.7069553515749545, 1.6970336108726642, 1.6973433045146402, 1.6897332523392945, 1.6935885265377757, 1.6895296274514169, 1.6855397888767156, 1.6847044701938512, 1.684248337021109, 1.6799152499596441, 1.6873358135105894, 1.672612109174474, 1.6859053331234126, 1.700003116674247, 1.6881816130154432, 1.6956358425426288, 1.6831827766351877, 1.6808447156843465, 1.6663543227272113, 1.6616297448685038, 1.6621866041629956, 1.654794362340375, 1.652773947490559, 1.6458370804052334, 1.6454623454405297, 1.6468894801345448, 1.6444418490544972, 1.6504614055034317, 1.6443369460546506, 1.6485391343643534, 1.6527840230499204, 1.6521416162318519, 1.6430637928494682, 1.6418012272405917, 1.6324310495623329, 1.6424678615476072, 1.628920904419995, 1.6319538717152402, 1.6537194419445689, 1.6316074830550678, 1.6305505202291437, 1.6156182382630617, 1.6188055975481226, 1.6128000690706947, 1.6145270235974196, 1.604601307277562, 1.608378354675716, 1.5968649517584141, 1.5944230465428786, 1.5985866874639993, 1.6049567086985468, 1.6079674147972092, 1.596688208883548, 1.5879967578872274, 1.5954565786483106, 1.5858231033877426, 1.5910704741487758, 1.5859228858712762, 1.5790799495865433, 1.589299725311248, 1.5887260622067618, 1.5784963544144524, 1.5792860712603622, 1.5687700276012537, 1.5834310010228558, 1.5646507644065841, 1.5616204870799728, 1.567772014283057, 1.5623735521363527, 1.5630719995107005, 1.553652763758352, 1.564754825451046, 1.5606795701647687, 1.5539611761574872, 1.5564173928031686, 1.5667410235630168, 1.552291438613829, 1.545247610344779, 1.5482463254086536, 1.5528314274194548, 1.5440554588482365, 1.5498451126674362, 1.552750775848326, 1.5377407087192887, 1.5328212946102604, 1.5465909963270967, 1.5527337800305971, 1.542156898363415, 1.5328912472088478, 1.5297797662766317, 1.5266826331003491, 1.5264485266663945, 1.5265120384384718, 1.5189136105641203, 1.5324997071612787, 1.529636715814563, 1.5236691624721708, 1.5256526459413877, 1.5239058291398035, 1.5150254449805196, 1.507094247238347, 1.5064125342535533, 1.5087951107925948, 1.50715052126859, 1.5289988638194434, 1.508627757840088, 1.5167734971036657, 1.5131706378298373, 1.527603239934792, 1.50973409950366, 1.4977934505905215, 1.4981626629584623, 1.49249412558162, 1.4924350989183117, 1.4932482270979048, 1.4976260376172388, 1.4957375864718239, 1.5035525726341858], 'acc': [0.05338809051736424, 0.07227926053856433, 0.07843942480961155, 0.07843942501461726, 0.08788500980185288, 0.10225872742260751, 0.10225872663930212, 0.10225872645265513, 0.10225872724513987, 0.10225872743178686, 0.10225872664848147, 0.10225872645265513, 0.10225872663930212, 0.10225872742260751, 0.10225872704013417, 0.10225872683512846, 0.10349075988516426, 0.10924024653814167, 0.11540041080918889, 0.13182751611028118, 0.14291581080067575, 0.15400410785934518, 0.15030800831758512, 0.17248459930170243, 0.17700205407960215, 0.19342915880239475, 0.19876796683981188, 0.207392197192572, 0.20328542101187383, 0.2020533869827063, 0.20533880951223432, 0.20657084256227012, 0.2135523602703024, 0.2254620131525905, 0.227104722058259, 0.24147844045313965, 0.2464065708908457, 0.2652977399145553, 0.26036961108017753, 0.27638603661828953, 0.2611909667079698, 0.27638603661828953, 0.2915811104451362, 0.28501026558190645, 0.2928131413043647, 0.2981519527259059, 0.30554415001027146, 0.30759753672983614, 0.3133470212287237, 0.31540041245229433, 0.3236139638345589, 0.31991786625106233, 0.31375769802677067, 0.3223819307845231, 0.32689938282085396, 0.3244353200498303, 0.33141683838205904, 0.3334702276473662, 0.3318275160001289, 0.3211498973061172, 0.33100616193894733, 0.3338809038946516, 0.33963039231006614, 0.3338809036988252, 0.3437371680991116, 0.33921971488782265, 0.3441478461088341, 0.3429158110638174, 0.339630390547629, 0.3494866507988446, 0.3408624251642756, 0.3379876778845425, 0.34784394071821806, 0.3486652995159494, 0.35400410557674433, 0.35605749425457245, 0.3515400400641518, 0.35277207510916847, 0.35441478299898777, 0.35482546159618933, 0.34455852255194586, 0.3548254602254049, 0.35770020413937265, 0.3577002031602409, 0.35811088410735864, 0.3601642690644862, 0.3601642730177306, 0.36139630606776635, 0.3544147855447303, 0.35277207213505585, 0.3556468160490236, 0.35318275370637003, 0.3642710479867532, 0.3601642725893604, 0.36303901239097486, 0.3650924018521084, 0.3613963054435699, 0.36796714596190244, 0.36344969235896085, 0.3626283353603841, 0.3667351121285613, 0.36837782220918786, 0.3691991792077646, 0.36755646795217994, 0.3733059562084856, 0.36960985565087634, 0.37125256749394003, 0.37084188713430133, 0.37084189046334926, 0.376591373554735, 0.3720739215551216, 0.37125256631898196, 0.37207391979268445, 0.37577002270020987, 0.3831622172430066, 0.3790554430205719, 0.3765913751580632, 0.37987679786505885, 0.3753593432829855, 0.3716632431537464, 0.3843942484938878, 0.3802874725457334, 0.38480492591613125, 0.39014373812097786, 0.3819301857595816, 0.38028747669480417, 0.38891170330850494, 0.3827515402124158, 0.39219712382469335, 0.3790554392998713, 0.3839835750248887, 0.385626282914708, 0.39958932094995003, 0.39671457938589844, 0.3938398362919535, 0.3905544149557423, 0.37946612024698906, 0.3843942522880233, 0.39055441515156863, 0.3889117034676139, 0.39589322536143434, 0.39342915926136274, 0.3954825475475382, 0.398767968687923, 0.3954825446101429, 0.4016427114269327, 0.4012320334172102, 0.4036960968981044, 0.40246406505974414, 0.3999999991554989, 0.3975359358337136, 0.4057494854168236, 0.40410677733117795, 0.4053388103812137, 0.4057494860043026, 0.41190964890456544, 0.41067761980777406, 0.4073921986673892, 0.41396303758239356, 0.4139630397732008, 0.40574948799928356, 0.41396304114398524, 0.41026694199387786, 0.41642710489414064, 0.40657084343124955, 0.40533880881460294, 0.408624229134965, 0.4065708438229022, 0.4110882976216702, 0.41601642488943724, 0.416016429038508, 0.4135523629384364, 0.4135523625467837, 0.4258726894855499, 0.42258726873681773, 0.4225872681493387, 0.4168377820838403, 0.41108829347259945, 0.4201232010701354, 0.4201232022450935, 0.4193018462256484, 0.41683778172890507, 0.4258726879189391, 0.4275153987828711, 0.43039014230518613, 0.42464065526055605, 0.42997946429546363, 0.42792607640094094, 0.42710472194810667, 0.42792607777172537, 0.4295687908264646, 0.43285421118354406, 0.4246406570229932, 0.43531827450532934, 0.4271047227314121, 0.4172484615010647, 0.42176591549565906, 0.42422997881744434, 0.4349075996655458, 0.43121149989124197, 0.43326488860578755, 0.43819301861512344, 0.43285420723029966, 0.4373716609923502, 0.43737166357481017, 0.4410677609624804, 0.43901437404708943, 0.44845995789191073, 0.43860369662484594, 0.43696098337428035, 0.43983572732496556, 0.4353182737220239, 0.43860369544988786, 0.43244353176631967, 0.43326488915654915, 0.4451745361273294, 0.4439425038973164, 0.4472279244135048, 0.4525667362266987, 0.4418891149869445, 0.4308008232890213, 0.44681724663632605, 0.4418891189401889, 0.44845996043765324, 0.4517453787997518, 0.44804928203627803, 0.4517453807947327, 0.4521560578253235, 0.44845996082930595, 0.45708419022129304, 0.4558521556046464, 0.4566735114282651, 0.4566735133865286, 0.45051334477058425, 0.45420944454488815, 0.4657084196132801, 0.45913757651248754, 0.4587269006935723, 0.4607802862014614, 0.45749486666440475, 0.46817248473421996, 0.4533880884887257, 0.4562628330268899, 0.46735112969390663, 0.4652977396085767, 0.46078028698476675, 0.4624229964779143, 0.47186858525511177, 0.4657084170308201, 0.466529773870288, 0.46858316235228975, 0.46776180492534286, 0.4747433259991405, 0.4681724845383936, 0.46817248493004626, 0.46858316196063704, 0.4731006169343631, 0.46817248254341265, 0.4722792597032426, 0.47063654864348425, 0.4731006149393822, 0.4698151930156919, 0.47104723001897214, 0.4763860351006353, 0.47268993634218065, 0.48172484472302196, 0.4817248455063273, 0.4759753608116134, 0.47597535767839183, 0.4804928117097036, 0.4759753612399836, 0.47474332760246873, 0.4767967164761232, 0.4850102652759278, 0.47926078159706303, 0.4780287453770882, 0.4767967140894896, 0.4751540028339049, 0.47433264900526717, 0.47885010061322786, 0.47843942655674976, 0.4825462009382933, 0.48295687761394884, 0.4841889136013799, 0.48993839825937635, 0.486652978330667, 0.47597535830258836, 0.4882956901737307, 0.4833675548036485, 0.48665297653151246, 0.4772073929192349, 0.4854209450480874, 0.49075975251638426, 0.4895277210329592, 0.4911704326801966, 0.49117042958369245, 0.49527720611932585, 0.48706365692786857, 0.48665297711899147, 0.48542094524391377]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
