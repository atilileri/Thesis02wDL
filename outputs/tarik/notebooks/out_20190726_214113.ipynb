{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf17.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 21:41:13 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '0Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '02', '03', '01', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023D81ACBE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023DB3837EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6136, Accuracy:0.1893, Validation Loss:1.6102, Validation Accuracy:0.1938\n",
    "Epoch #2: Loss:1.6078, Accuracy:0.2197, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6059, Accuracy:0.2333, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2315\n",
    "Epoch #16: Loss:1.6028, Accuracy:0.2341, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6026, Accuracy:0.2374, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #18: Loss:1.6020, Accuracy:0.2411, Validation Loss:1.6052, Validation Accuracy:0.2315\n",
    "Epoch #19: Loss:1.6020, Accuracy:0.2448, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6015, Accuracy:0.2480, Validation Loss:1.6054, Validation Accuracy:0.2348\n",
    "Epoch #21: Loss:1.6012, Accuracy:0.2464, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #22: Loss:1.6007, Accuracy:0.2456, Validation Loss:1.6054, Validation Accuracy:0.2348\n",
    "Epoch #23: Loss:1.6004, Accuracy:0.2476, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6000, Accuracy:0.2444, Validation Loss:1.6059, Validation Accuracy:0.2315\n",
    "Epoch #25: Loss:1.5996, Accuracy:0.2456, Validation Loss:1.6060, Validation Accuracy:0.2315\n",
    "Epoch #26: Loss:1.5994, Accuracy:0.2464, Validation Loss:1.6064, Validation Accuracy:0.2315\n",
    "Epoch #27: Loss:1.5990, Accuracy:0.2464, Validation Loss:1.6067, Validation Accuracy:0.2315\n",
    "Epoch #28: Loss:1.5989, Accuracy:0.2464, Validation Loss:1.6072, Validation Accuracy:0.2315\n",
    "Epoch #29: Loss:1.5987, Accuracy:0.2485, Validation Loss:1.6068, Validation Accuracy:0.2348\n",
    "Epoch #30: Loss:1.5988, Accuracy:0.2485, Validation Loss:1.6068, Validation Accuracy:0.2348\n",
    "Epoch #31: Loss:1.5986, Accuracy:0.2493, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #32: Loss:1.5982, Accuracy:0.2489, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #33: Loss:1.5985, Accuracy:0.2493, Validation Loss:1.6072, Validation Accuracy:0.2365\n",
    "Epoch #34: Loss:1.5978, Accuracy:0.2497, Validation Loss:1.6078, Validation Accuracy:0.2332\n",
    "Epoch #35: Loss:1.5975, Accuracy:0.2472, Validation Loss:1.6071, Validation Accuracy:0.2332\n",
    "Epoch #36: Loss:1.5973, Accuracy:0.2480, Validation Loss:1.6071, Validation Accuracy:0.2332\n",
    "Epoch #37: Loss:1.5969, Accuracy:0.2480, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #38: Loss:1.5973, Accuracy:0.2464, Validation Loss:1.6065, Validation Accuracy:0.2348\n",
    "Epoch #39: Loss:1.5973, Accuracy:0.2489, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #40: Loss:1.5968, Accuracy:0.2439, Validation Loss:1.6073, Validation Accuracy:0.2447\n",
    "Epoch #41: Loss:1.5965, Accuracy:0.2452, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #42: Loss:1.5968, Accuracy:0.2456, Validation Loss:1.6051, Validation Accuracy:0.2348\n",
    "Epoch #43: Loss:1.5979, Accuracy:0.2431, Validation Loss:1.6080, Validation Accuracy:0.2430\n",
    "Epoch #44: Loss:1.5968, Accuracy:0.2464, Validation Loss:1.6087, Validation Accuracy:0.2365\n",
    "Epoch #45: Loss:1.5970, Accuracy:0.2460, Validation Loss:1.6082, Validation Accuracy:0.2348\n",
    "Epoch #46: Loss:1.5971, Accuracy:0.2476, Validation Loss:1.6053, Validation Accuracy:0.2299\n",
    "Epoch #47: Loss:1.6008, Accuracy:0.2398, Validation Loss:1.6061, Validation Accuracy:0.2315\n",
    "Epoch #48: Loss:1.5970, Accuracy:0.2444, Validation Loss:1.6097, Validation Accuracy:0.2348\n",
    "Epoch #49: Loss:1.5988, Accuracy:0.2419, Validation Loss:1.6089, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.5971, Accuracy:0.2439, Validation Loss:1.6064, Validation Accuracy:0.2348\n",
    "Epoch #51: Loss:1.5971, Accuracy:0.2472, Validation Loss:1.6068, Validation Accuracy:0.2348\n",
    "Epoch #52: Loss:1.5962, Accuracy:0.2509, Validation Loss:1.6070, Validation Accuracy:0.2365\n",
    "Epoch #53: Loss:1.5958, Accuracy:0.2476, Validation Loss:1.6085, Validation Accuracy:0.2414\n",
    "Epoch #54: Loss:1.5962, Accuracy:0.2464, Validation Loss:1.6086, Validation Accuracy:0.2414\n",
    "Epoch #55: Loss:1.5963, Accuracy:0.2464, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #56: Loss:1.5970, Accuracy:0.2464, Validation Loss:1.6060, Validation Accuracy:0.2414\n",
    "Epoch #57: Loss:1.5971, Accuracy:0.2435, Validation Loss:1.6088, Validation Accuracy:0.2348\n",
    "Epoch #58: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6075, Validation Accuracy:0.2299\n",
    "Epoch #59: Loss:1.5970, Accuracy:0.2444, Validation Loss:1.6131, Validation Accuracy:0.2250\n",
    "Epoch #60: Loss:1.5981, Accuracy:0.2415, Validation Loss:1.6105, Validation Accuracy:0.2348\n",
    "Epoch #61: Loss:1.5973, Accuracy:0.2509, Validation Loss:1.6077, Validation Accuracy:0.2332\n",
    "Epoch #62: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.6082, Validation Accuracy:0.2365\n",
    "Epoch #63: Loss:1.5962, Accuracy:0.2431, Validation Loss:1.6106, Validation Accuracy:0.2332\n",
    "Epoch #64: Loss:1.5953, Accuracy:0.2493, Validation Loss:1.6066, Validation Accuracy:0.2315\n",
    "Epoch #65: Loss:1.5987, Accuracy:0.2370, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #66: Loss:1.5967, Accuracy:0.2439, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #67: Loss:1.5960, Accuracy:0.2431, Validation Loss:1.6102, Validation Accuracy:0.2332\n",
    "Epoch #68: Loss:1.5947, Accuracy:0.2460, Validation Loss:1.6063, Validation Accuracy:0.2348\n",
    "Epoch #69: Loss:1.5944, Accuracy:0.2464, Validation Loss:1.6050, Validation Accuracy:0.2348\n",
    "Epoch #70: Loss:1.5942, Accuracy:0.2485, Validation Loss:1.6067, Validation Accuracy:0.2381\n",
    "Epoch #71: Loss:1.5935, Accuracy:0.2448, Validation Loss:1.6082, Validation Accuracy:0.2348\n",
    "Epoch #72: Loss:1.5935, Accuracy:0.2444, Validation Loss:1.6075, Validation Accuracy:0.2348\n",
    "Epoch #73: Loss:1.5931, Accuracy:0.2468, Validation Loss:1.6079, Validation Accuracy:0.2365\n",
    "Epoch #74: Loss:1.5927, Accuracy:0.2497, Validation Loss:1.6086, Validation Accuracy:0.2348\n",
    "Epoch #75: Loss:1.5927, Accuracy:0.2468, Validation Loss:1.6077, Validation Accuracy:0.2315\n",
    "Epoch #76: Loss:1.5942, Accuracy:0.2411, Validation Loss:1.6086, Validation Accuracy:0.2381\n",
    "Epoch #77: Loss:1.5933, Accuracy:0.2456, Validation Loss:1.6054, Validation Accuracy:0.2282\n",
    "Epoch #78: Loss:1.5932, Accuracy:0.2480, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #79: Loss:1.5931, Accuracy:0.2427, Validation Loss:1.6106, Validation Accuracy:0.2365\n",
    "Epoch #80: Loss:1.5925, Accuracy:0.2456, Validation Loss:1.6087, Validation Accuracy:0.2348\n",
    "Epoch #81: Loss:1.5914, Accuracy:0.2456, Validation Loss:1.6089, Validation Accuracy:0.2266\n",
    "Epoch #82: Loss:1.5926, Accuracy:0.2468, Validation Loss:1.6102, Validation Accuracy:0.2266\n",
    "Epoch #83: Loss:1.5912, Accuracy:0.2513, Validation Loss:1.6071, Validation Accuracy:0.2200\n",
    "Epoch #84: Loss:1.5909, Accuracy:0.2476, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #85: Loss:1.5920, Accuracy:0.2415, Validation Loss:1.6116, Validation Accuracy:0.2282\n",
    "Epoch #86: Loss:1.5924, Accuracy:0.2423, Validation Loss:1.6105, Validation Accuracy:0.2217\n",
    "Epoch #87: Loss:1.5908, Accuracy:0.2501, Validation Loss:1.6135, Validation Accuracy:0.2085\n",
    "Epoch #88: Loss:1.5892, Accuracy:0.2530, Validation Loss:1.6139, Validation Accuracy:0.2085\n",
    "Epoch #89: Loss:1.5882, Accuracy:0.2579, Validation Loss:1.6133, Validation Accuracy:0.1938\n",
    "Epoch #90: Loss:1.5888, Accuracy:0.2571, Validation Loss:1.6170, Validation Accuracy:0.2085\n",
    "Epoch #91: Loss:1.5897, Accuracy:0.2542, Validation Loss:1.6119, Validation Accuracy:0.2020\n",
    "Epoch #92: Loss:1.5885, Accuracy:0.2497, Validation Loss:1.6155, Validation Accuracy:0.2118\n",
    "Epoch #93: Loss:1.5883, Accuracy:0.2595, Validation Loss:1.6121, Validation Accuracy:0.2036\n",
    "Epoch #94: Loss:1.5870, Accuracy:0.2587, Validation Loss:1.6167, Validation Accuracy:0.2167\n",
    "Epoch #95: Loss:1.5874, Accuracy:0.2501, Validation Loss:1.6152, Validation Accuracy:0.2118\n",
    "Epoch #96: Loss:1.5869, Accuracy:0.2554, Validation Loss:1.6160, Validation Accuracy:0.2020\n",
    "Epoch #97: Loss:1.5855, Accuracy:0.2604, Validation Loss:1.6181, Validation Accuracy:0.2151\n",
    "Epoch #98: Loss:1.5884, Accuracy:0.2522, Validation Loss:1.6170, Validation Accuracy:0.2020\n",
    "Epoch #99: Loss:1.5906, Accuracy:0.2608, Validation Loss:1.6118, Validation Accuracy:0.2332\n",
    "Epoch #100: Loss:1.5885, Accuracy:0.2534, Validation Loss:1.6182, Validation Accuracy:0.2151\n",
    "Epoch #101: Loss:1.5895, Accuracy:0.2563, Validation Loss:1.6180, Validation Accuracy:0.2282\n",
    "Epoch #102: Loss:1.5899, Accuracy:0.2595, Validation Loss:1.6149, Validation Accuracy:0.2085\n",
    "Epoch #103: Loss:1.5907, Accuracy:0.2526, Validation Loss:1.6148, Validation Accuracy:0.2102\n",
    "Epoch #104: Loss:1.5938, Accuracy:0.2517, Validation Loss:1.6119, Validation Accuracy:0.2167\n",
    "Epoch #105: Loss:1.5923, Accuracy:0.2587, Validation Loss:1.6140, Validation Accuracy:0.2151\n",
    "Epoch #106: Loss:1.5921, Accuracy:0.2628, Validation Loss:1.6106, Validation Accuracy:0.2167\n",
    "Epoch #107: Loss:1.5907, Accuracy:0.2522, Validation Loss:1.6122, Validation Accuracy:0.2233\n",
    "Epoch #108: Loss:1.5902, Accuracy:0.2554, Validation Loss:1.6131, Validation Accuracy:0.2102\n",
    "Epoch #109: Loss:1.5899, Accuracy:0.2620, Validation Loss:1.6137, Validation Accuracy:0.2085\n",
    "Epoch #110: Loss:1.5902, Accuracy:0.2616, Validation Loss:1.6137, Validation Accuracy:0.2102\n",
    "Epoch #111: Loss:1.5890, Accuracy:0.2632, Validation Loss:1.6132, Validation Accuracy:0.2250\n",
    "Epoch #112: Loss:1.5874, Accuracy:0.2575, Validation Loss:1.6139, Validation Accuracy:0.2085\n",
    "Epoch #113: Loss:1.5875, Accuracy:0.2674, Validation Loss:1.6185, Validation Accuracy:0.2151\n",
    "Epoch #114: Loss:1.5887, Accuracy:0.2583, Validation Loss:1.6186, Validation Accuracy:0.2003\n",
    "Epoch #115: Loss:1.5862, Accuracy:0.2583, Validation Loss:1.6160, Validation Accuracy:0.2151\n",
    "Epoch #116: Loss:1.5843, Accuracy:0.2694, Validation Loss:1.6234, Validation Accuracy:0.1987\n",
    "Epoch #117: Loss:1.5844, Accuracy:0.2665, Validation Loss:1.6211, Validation Accuracy:0.2020\n",
    "Epoch #118: Loss:1.5839, Accuracy:0.2710, Validation Loss:1.6207, Validation Accuracy:0.2069\n",
    "Epoch #119: Loss:1.5829, Accuracy:0.2706, Validation Loss:1.6222, Validation Accuracy:0.2085\n",
    "Epoch #120: Loss:1.5816, Accuracy:0.2731, Validation Loss:1.6241, Validation Accuracy:0.2053\n",
    "Epoch #121: Loss:1.5819, Accuracy:0.2739, Validation Loss:1.6245, Validation Accuracy:0.2053\n",
    "Epoch #122: Loss:1.5821, Accuracy:0.2723, Validation Loss:1.6220, Validation Accuracy:0.2020\n",
    "Epoch #123: Loss:1.5824, Accuracy:0.2653, Validation Loss:1.6266, Validation Accuracy:0.2003\n",
    "Epoch #124: Loss:1.5816, Accuracy:0.2821, Validation Loss:1.6215, Validation Accuracy:0.2135\n",
    "Epoch #125: Loss:1.5833, Accuracy:0.2694, Validation Loss:1.6216, Validation Accuracy:0.2003\n",
    "Epoch #126: Loss:1.5805, Accuracy:0.2645, Validation Loss:1.6203, Validation Accuracy:0.2036\n",
    "Epoch #127: Loss:1.5804, Accuracy:0.2649, Validation Loss:1.6191, Validation Accuracy:0.2085\n",
    "Epoch #128: Loss:1.5820, Accuracy:0.2575, Validation Loss:1.6142, Validation Accuracy:0.2167\n",
    "Epoch #129: Loss:1.5842, Accuracy:0.2669, Validation Loss:1.6174, Validation Accuracy:0.1987\n",
    "Epoch #130: Loss:1.5840, Accuracy:0.2616, Validation Loss:1.6175, Validation Accuracy:0.2118\n",
    "Epoch #131: Loss:1.5818, Accuracy:0.2657, Validation Loss:1.6150, Validation Accuracy:0.2217\n",
    "Epoch #132: Loss:1.5842, Accuracy:0.2637, Validation Loss:1.6189, Validation Accuracy:0.2135\n",
    "Epoch #133: Loss:1.5825, Accuracy:0.2554, Validation Loss:1.6176, Validation Accuracy:0.2036\n",
    "Epoch #134: Loss:1.5831, Accuracy:0.2616, Validation Loss:1.6152, Validation Accuracy:0.2085\n",
    "Epoch #135: Loss:1.5825, Accuracy:0.2595, Validation Loss:1.6171, Validation Accuracy:0.2266\n",
    "Epoch #136: Loss:1.5823, Accuracy:0.2608, Validation Loss:1.6171, Validation Accuracy:0.2282\n",
    "Epoch #137: Loss:1.5835, Accuracy:0.2628, Validation Loss:1.6165, Validation Accuracy:0.2184\n",
    "Epoch #138: Loss:1.5819, Accuracy:0.2513, Validation Loss:1.6176, Validation Accuracy:0.2315\n",
    "Epoch #139: Loss:1.5817, Accuracy:0.2600, Validation Loss:1.6189, Validation Accuracy:0.2217\n",
    "Epoch #140: Loss:1.5818, Accuracy:0.2583, Validation Loss:1.6160, Validation Accuracy:0.2266\n",
    "Epoch #141: Loss:1.5825, Accuracy:0.2686, Validation Loss:1.6150, Validation Accuracy:0.2200\n",
    "Epoch #142: Loss:1.5822, Accuracy:0.2571, Validation Loss:1.6161, Validation Accuracy:0.2315\n",
    "Epoch #143: Loss:1.5789, Accuracy:0.2702, Validation Loss:1.6147, Validation Accuracy:0.2266\n",
    "Epoch #144: Loss:1.5802, Accuracy:0.2612, Validation Loss:1.6152, Validation Accuracy:0.2151\n",
    "Epoch #145: Loss:1.5779, Accuracy:0.2715, Validation Loss:1.6157, Validation Accuracy:0.2233\n",
    "Epoch #146: Loss:1.5786, Accuracy:0.2694, Validation Loss:1.6186, Validation Accuracy:0.2102\n",
    "Epoch #147: Loss:1.5797, Accuracy:0.2620, Validation Loss:1.6150, Validation Accuracy:0.2200\n",
    "Epoch #148: Loss:1.5801, Accuracy:0.2678, Validation Loss:1.6178, Validation Accuracy:0.2167\n",
    "Epoch #149: Loss:1.5801, Accuracy:0.2669, Validation Loss:1.6204, Validation Accuracy:0.2085\n",
    "Epoch #150: Loss:1.5793, Accuracy:0.2686, Validation Loss:1.6202, Validation Accuracy:0.2233\n",
    "Epoch #151: Loss:1.5782, Accuracy:0.2702, Validation Loss:1.6176, Validation Accuracy:0.2167\n",
    "Epoch #152: Loss:1.5765, Accuracy:0.2710, Validation Loss:1.6192, Validation Accuracy:0.2233\n",
    "Epoch #153: Loss:1.5790, Accuracy:0.2632, Validation Loss:1.6185, Validation Accuracy:0.2217\n",
    "Epoch #154: Loss:1.5795, Accuracy:0.2698, Validation Loss:1.6159, Validation Accuracy:0.2200\n",
    "Epoch #155: Loss:1.5782, Accuracy:0.2702, Validation Loss:1.6160, Validation Accuracy:0.2332\n",
    "Epoch #156: Loss:1.5789, Accuracy:0.2706, Validation Loss:1.6158, Validation Accuracy:0.2250\n",
    "Epoch #157: Loss:1.5790, Accuracy:0.2645, Validation Loss:1.6197, Validation Accuracy:0.2282\n",
    "Epoch #158: Loss:1.5776, Accuracy:0.2678, Validation Loss:1.6194, Validation Accuracy:0.2250\n",
    "Epoch #159: Loss:1.5782, Accuracy:0.2657, Validation Loss:1.6173, Validation Accuracy:0.2266\n",
    "Epoch #160: Loss:1.5737, Accuracy:0.2743, Validation Loss:1.6248, Validation Accuracy:0.2003\n",
    "Epoch #161: Loss:1.5737, Accuracy:0.2665, Validation Loss:1.6206, Validation Accuracy:0.2167\n",
    "Epoch #162: Loss:1.5765, Accuracy:0.2702, Validation Loss:1.6211, Validation Accuracy:0.2102\n",
    "Epoch #163: Loss:1.5732, Accuracy:0.2698, Validation Loss:1.6216, Validation Accuracy:0.2200\n",
    "Epoch #164: Loss:1.5761, Accuracy:0.2620, Validation Loss:1.6166, Validation Accuracy:0.2233\n",
    "Epoch #165: Loss:1.5753, Accuracy:0.2706, Validation Loss:1.6203, Validation Accuracy:0.2118\n",
    "Epoch #166: Loss:1.5739, Accuracy:0.2706, Validation Loss:1.6207, Validation Accuracy:0.2184\n",
    "Epoch #167: Loss:1.5756, Accuracy:0.2694, Validation Loss:1.6300, Validation Accuracy:0.2184\n",
    "Epoch #168: Loss:1.5751, Accuracy:0.2706, Validation Loss:1.6339, Validation Accuracy:0.2102\n",
    "Epoch #169: Loss:1.5798, Accuracy:0.2657, Validation Loss:1.6274, Validation Accuracy:0.2135\n",
    "Epoch #170: Loss:1.6022, Accuracy:0.2402, Validation Loss:1.6207, Validation Accuracy:0.2250\n",
    "Epoch #171: Loss:1.6096, Accuracy:0.2390, Validation Loss:1.6300, Validation Accuracy:0.2102\n",
    "Epoch #172: Loss:1.5958, Accuracy:0.2390, Validation Loss:1.6202, Validation Accuracy:0.2102\n",
    "Epoch #173: Loss:1.5925, Accuracy:0.2485, Validation Loss:1.6123, Validation Accuracy:0.2036\n",
    "Epoch #174: Loss:1.5902, Accuracy:0.2591, Validation Loss:1.6422, Validation Accuracy:0.2315\n",
    "Epoch #175: Loss:1.6102, Accuracy:0.2448, Validation Loss:1.6453, Validation Accuracy:0.2200\n",
    "Epoch #176: Loss:1.6020, Accuracy:0.2390, Validation Loss:1.6115, Validation Accuracy:0.2200\n",
    "Epoch #177: Loss:1.6054, Accuracy:0.2431, Validation Loss:1.6185, Validation Accuracy:0.2151\n",
    "Epoch #178: Loss:1.6024, Accuracy:0.2542, Validation Loss:1.6106, Validation Accuracy:0.2250\n",
    "Epoch #179: Loss:1.5954, Accuracy:0.2517, Validation Loss:1.6101, Validation Accuracy:0.2233\n",
    "Epoch #180: Loss:1.5930, Accuracy:0.2567, Validation Loss:1.6108, Validation Accuracy:0.2151\n",
    "Epoch #181: Loss:1.5919, Accuracy:0.2600, Validation Loss:1.6116, Validation Accuracy:0.2135\n",
    "Epoch #182: Loss:1.5914, Accuracy:0.2591, Validation Loss:1.6112, Validation Accuracy:0.2102\n",
    "Epoch #183: Loss:1.5901, Accuracy:0.2591, Validation Loss:1.6113, Validation Accuracy:0.2069\n",
    "Epoch #184: Loss:1.5899, Accuracy:0.2522, Validation Loss:1.6108, Validation Accuracy:0.2135\n",
    "Epoch #185: Loss:1.5895, Accuracy:0.2517, Validation Loss:1.6108, Validation Accuracy:0.2217\n",
    "Epoch #186: Loss:1.5887, Accuracy:0.2637, Validation Loss:1.6128, Validation Accuracy:0.2217\n",
    "Epoch #187: Loss:1.5876, Accuracy:0.2653, Validation Loss:1.6143, Validation Accuracy:0.2217\n",
    "Epoch #188: Loss:1.5866, Accuracy:0.2649, Validation Loss:1.6140, Validation Accuracy:0.2250\n",
    "Epoch #189: Loss:1.5856, Accuracy:0.2661, Validation Loss:1.6152, Validation Accuracy:0.2118\n",
    "Epoch #190: Loss:1.5856, Accuracy:0.2706, Validation Loss:1.6183, Validation Accuracy:0.2069\n",
    "Epoch #191: Loss:1.5867, Accuracy:0.2678, Validation Loss:1.6168, Validation Accuracy:0.2020\n",
    "Epoch #192: Loss:1.5859, Accuracy:0.2694, Validation Loss:1.6188, Validation Accuracy:0.2102\n",
    "Epoch #193: Loss:1.5847, Accuracy:0.2686, Validation Loss:1.6194, Validation Accuracy:0.2069\n",
    "Epoch #194: Loss:1.5853, Accuracy:0.2694, Validation Loss:1.6200, Validation Accuracy:0.2102\n",
    "Epoch #195: Loss:1.5837, Accuracy:0.2706, Validation Loss:1.6190, Validation Accuracy:0.2102\n",
    "Epoch #196: Loss:1.5840, Accuracy:0.2698, Validation Loss:1.6209, Validation Accuracy:0.2069\n",
    "Epoch #197: Loss:1.5826, Accuracy:0.2682, Validation Loss:1.6196, Validation Accuracy:0.2118\n",
    "Epoch #198: Loss:1.5828, Accuracy:0.2690, Validation Loss:1.6164, Validation Accuracy:0.2118\n",
    "Epoch #199: Loss:1.5826, Accuracy:0.2735, Validation Loss:1.6180, Validation Accuracy:0.2118\n",
    "Epoch #200: Loss:1.5833, Accuracy:0.2674, Validation Loss:1.6176, Validation Accuracy:0.1921\n",
    "Epoch #201: Loss:1.5803, Accuracy:0.2678, Validation Loss:1.6194, Validation Accuracy:0.2053\n",
    "Epoch #202: Loss:1.5811, Accuracy:0.2739, Validation Loss:1.6231, Validation Accuracy:0.2118\n",
    "Epoch #203: Loss:1.5816, Accuracy:0.2678, Validation Loss:1.6247, Validation Accuracy:0.2085\n",
    "Epoch #204: Loss:1.5816, Accuracy:0.2678, Validation Loss:1.6210, Validation Accuracy:0.2053\n",
    "Epoch #205: Loss:1.5833, Accuracy:0.2669, Validation Loss:1.6211, Validation Accuracy:0.2085\n",
    "Epoch #206: Loss:1.5820, Accuracy:0.2698, Validation Loss:1.6186, Validation Accuracy:0.2135\n",
    "Epoch #207: Loss:1.5818, Accuracy:0.2694, Validation Loss:1.6207, Validation Accuracy:0.2102\n",
    "Epoch #208: Loss:1.5820, Accuracy:0.2694, Validation Loss:1.6160, Validation Accuracy:0.2102\n",
    "Epoch #209: Loss:1.5817, Accuracy:0.2682, Validation Loss:1.6206, Validation Accuracy:0.2102\n",
    "Epoch #210: Loss:1.5823, Accuracy:0.2710, Validation Loss:1.6251, Validation Accuracy:0.2069\n",
    "Epoch #211: Loss:1.5849, Accuracy:0.2674, Validation Loss:1.6206, Validation Accuracy:0.2053\n",
    "Epoch #212: Loss:1.5848, Accuracy:0.2702, Validation Loss:1.6173, Validation Accuracy:0.2085\n",
    "Epoch #213: Loss:1.5861, Accuracy:0.2686, Validation Loss:1.6197, Validation Accuracy:0.2069\n",
    "Epoch #214: Loss:1.5883, Accuracy:0.2649, Validation Loss:1.6157, Validation Accuracy:0.2085\n",
    "Epoch #215: Loss:1.5884, Accuracy:0.2637, Validation Loss:1.6157, Validation Accuracy:0.2135\n",
    "Epoch #216: Loss:1.5879, Accuracy:0.2628, Validation Loss:1.6125, Validation Accuracy:0.2118\n",
    "Epoch #217: Loss:1.5852, Accuracy:0.2686, Validation Loss:1.6161, Validation Accuracy:0.2102\n",
    "Epoch #218: Loss:1.5845, Accuracy:0.2678, Validation Loss:1.6164, Validation Accuracy:0.2085\n",
    "Epoch #219: Loss:1.5835, Accuracy:0.2678, Validation Loss:1.6199, Validation Accuracy:0.2167\n",
    "Epoch #220: Loss:1.5814, Accuracy:0.2669, Validation Loss:1.6209, Validation Accuracy:0.2069\n",
    "Epoch #221: Loss:1.5810, Accuracy:0.2674, Validation Loss:1.6221, Validation Accuracy:0.2102\n",
    "Epoch #222: Loss:1.5803, Accuracy:0.2690, Validation Loss:1.6248, Validation Accuracy:0.2085\n",
    "Epoch #223: Loss:1.5798, Accuracy:0.2735, Validation Loss:1.6247, Validation Accuracy:0.2118\n",
    "Epoch #224: Loss:1.5822, Accuracy:0.2641, Validation Loss:1.6254, Validation Accuracy:0.2184\n",
    "Epoch #225: Loss:1.5802, Accuracy:0.2583, Validation Loss:1.6281, Validation Accuracy:0.2069\n",
    "Epoch #226: Loss:1.5840, Accuracy:0.2657, Validation Loss:1.6226, Validation Accuracy:0.2250\n",
    "Epoch #227: Loss:1.5834, Accuracy:0.2567, Validation Loss:1.6291, Validation Accuracy:0.2085\n",
    "Epoch #228: Loss:1.5872, Accuracy:0.2526, Validation Loss:1.6230, Validation Accuracy:0.2250\n",
    "Epoch #229: Loss:1.5849, Accuracy:0.2645, Validation Loss:1.6246, Validation Accuracy:0.2118\n",
    "Epoch #230: Loss:1.5824, Accuracy:0.2641, Validation Loss:1.6277, Validation Accuracy:0.2151\n",
    "Epoch #231: Loss:1.5816, Accuracy:0.2624, Validation Loss:1.6255, Validation Accuracy:0.2184\n",
    "Epoch #232: Loss:1.5834, Accuracy:0.2595, Validation Loss:1.6243, Validation Accuracy:0.2167\n",
    "Epoch #233: Loss:1.5822, Accuracy:0.2686, Validation Loss:1.6273, Validation Accuracy:0.2151\n",
    "Epoch #234: Loss:1.5850, Accuracy:0.2612, Validation Loss:1.6206, Validation Accuracy:0.2102\n",
    "Epoch #235: Loss:1.5911, Accuracy:0.2604, Validation Loss:1.6311, Validation Accuracy:0.2167\n",
    "Epoch #236: Loss:1.5929, Accuracy:0.2579, Validation Loss:1.6259, Validation Accuracy:0.2167\n",
    "Epoch #237: Loss:1.5875, Accuracy:0.2575, Validation Loss:1.6196, Validation Accuracy:0.2167\n",
    "Epoch #238: Loss:1.5886, Accuracy:0.2579, Validation Loss:1.6164, Validation Accuracy:0.2217\n",
    "Epoch #239: Loss:1.5859, Accuracy:0.2649, Validation Loss:1.6191, Validation Accuracy:0.2217\n",
    "Epoch #240: Loss:1.5823, Accuracy:0.2694, Validation Loss:1.6200, Validation Accuracy:0.2167\n",
    "Epoch #241: Loss:1.5816, Accuracy:0.2715, Validation Loss:1.6209, Validation Accuracy:0.2233\n",
    "Epoch #242: Loss:1.5812, Accuracy:0.2665, Validation Loss:1.6226, Validation Accuracy:0.2200\n",
    "Epoch #243: Loss:1.5815, Accuracy:0.2624, Validation Loss:1.6211, Validation Accuracy:0.2200\n",
    "Epoch #244: Loss:1.5819, Accuracy:0.2690, Validation Loss:1.6215, Validation Accuracy:0.2315\n",
    "Epoch #245: Loss:1.5816, Accuracy:0.2620, Validation Loss:1.6238, Validation Accuracy:0.2135\n",
    "Epoch #246: Loss:1.5809, Accuracy:0.2641, Validation Loss:1.6243, Validation Accuracy:0.2085\n",
    "Epoch #247: Loss:1.5800, Accuracy:0.2694, Validation Loss:1.6273, Validation Accuracy:0.2036\n",
    "Epoch #248: Loss:1.5803, Accuracy:0.2694, Validation Loss:1.6312, Validation Accuracy:0.2069\n",
    "Epoch #249: Loss:1.5812, Accuracy:0.2669, Validation Loss:1.6283, Validation Accuracy:0.2069\n",
    "Epoch #250: Loss:1.5787, Accuracy:0.2690, Validation Loss:1.6263, Validation Accuracy:0.2184\n",
    "Epoch #251: Loss:1.5790, Accuracy:0.2698, Validation Loss:1.6256, Validation Accuracy:0.2102\n",
    "Epoch #252: Loss:1.5778, Accuracy:0.2653, Validation Loss:1.6237, Validation Accuracy:0.2118\n",
    "Epoch #253: Loss:1.5786, Accuracy:0.2649, Validation Loss:1.6228, Validation Accuracy:0.2118\n",
    "Epoch #254: Loss:1.5774, Accuracy:0.2690, Validation Loss:1.6251, Validation Accuracy:0.2167\n",
    "Epoch #255: Loss:1.5791, Accuracy:0.2731, Validation Loss:1.6288, Validation Accuracy:0.2167\n",
    "Epoch #256: Loss:1.5782, Accuracy:0.2731, Validation Loss:1.6257, Validation Accuracy:0.2135\n",
    "Epoch #257: Loss:1.5748, Accuracy:0.2715, Validation Loss:1.6255, Validation Accuracy:0.2233\n",
    "Epoch #258: Loss:1.5758, Accuracy:0.2735, Validation Loss:1.6269, Validation Accuracy:0.2184\n",
    "Epoch #259: Loss:1.5765, Accuracy:0.2702, Validation Loss:1.6233, Validation Accuracy:0.2118\n",
    "Epoch #260: Loss:1.5760, Accuracy:0.2715, Validation Loss:1.6243, Validation Accuracy:0.2200\n",
    "Epoch #261: Loss:1.5767, Accuracy:0.2674, Validation Loss:1.6292, Validation Accuracy:0.2102\n",
    "Epoch #262: Loss:1.5760, Accuracy:0.2690, Validation Loss:1.6269, Validation Accuracy:0.2085\n",
    "Epoch #263: Loss:1.5741, Accuracy:0.2694, Validation Loss:1.6305, Validation Accuracy:0.2167\n",
    "Epoch #264: Loss:1.5738, Accuracy:0.2723, Validation Loss:1.6295, Validation Accuracy:0.2102\n",
    "Epoch #265: Loss:1.5739, Accuracy:0.2706, Validation Loss:1.6335, Validation Accuracy:0.2085\n",
    "Epoch #266: Loss:1.5724, Accuracy:0.2743, Validation Loss:1.6322, Validation Accuracy:0.2167\n",
    "Epoch #267: Loss:1.5714, Accuracy:0.2682, Validation Loss:1.6311, Validation Accuracy:0.2118\n",
    "Epoch #268: Loss:1.5714, Accuracy:0.2723, Validation Loss:1.6315, Validation Accuracy:0.2217\n",
    "Epoch #269: Loss:1.5717, Accuracy:0.2702, Validation Loss:1.6320, Validation Accuracy:0.2200\n",
    "Epoch #270: Loss:1.5720, Accuracy:0.2657, Validation Loss:1.6308, Validation Accuracy:0.2184\n",
    "Epoch #271: Loss:1.5700, Accuracy:0.2682, Validation Loss:1.6342, Validation Accuracy:0.2102\n",
    "Epoch #272: Loss:1.5709, Accuracy:0.2715, Validation Loss:1.6356, Validation Accuracy:0.2102\n",
    "Epoch #273: Loss:1.5704, Accuracy:0.2706, Validation Loss:1.6354, Validation Accuracy:0.2217\n",
    "Epoch #274: Loss:1.5704, Accuracy:0.2690, Validation Loss:1.6374, Validation Accuracy:0.2250\n",
    "Epoch #275: Loss:1.5701, Accuracy:0.2698, Validation Loss:1.6347, Validation Accuracy:0.2184\n",
    "Epoch #276: Loss:1.5705, Accuracy:0.2645, Validation Loss:1.6359, Validation Accuracy:0.2135\n",
    "Epoch #277: Loss:1.5712, Accuracy:0.2727, Validation Loss:1.6376, Validation Accuracy:0.2151\n",
    "Epoch #278: Loss:1.5748, Accuracy:0.2719, Validation Loss:1.6388, Validation Accuracy:0.2332\n",
    "Epoch #279: Loss:1.5762, Accuracy:0.2702, Validation Loss:1.6377, Validation Accuracy:0.2102\n",
    "Epoch #280: Loss:1.5791, Accuracy:0.2628, Validation Loss:1.6286, Validation Accuracy:0.2167\n",
    "Epoch #281: Loss:1.5793, Accuracy:0.2645, Validation Loss:1.6362, Validation Accuracy:0.2151\n",
    "Epoch #282: Loss:1.5926, Accuracy:0.2563, Validation Loss:1.6283, Validation Accuracy:0.2118\n",
    "Epoch #283: Loss:1.5933, Accuracy:0.2456, Validation Loss:1.6302, Validation Accuracy:0.2118\n",
    "Epoch #284: Loss:1.5933, Accuracy:0.2476, Validation Loss:1.6282, Validation Accuracy:0.2102\n",
    "Epoch #285: Loss:1.6039, Accuracy:0.2407, Validation Loss:1.6145, Validation Accuracy:0.2135\n",
    "Epoch #286: Loss:1.5865, Accuracy:0.2583, Validation Loss:1.6128, Validation Accuracy:0.2167\n",
    "Epoch #287: Loss:1.5824, Accuracy:0.2657, Validation Loss:1.6139, Validation Accuracy:0.2135\n",
    "Epoch #288: Loss:1.5826, Accuracy:0.2674, Validation Loss:1.6153, Validation Accuracy:0.2069\n",
    "Epoch #289: Loss:1.5824, Accuracy:0.2637, Validation Loss:1.6168, Validation Accuracy:0.2003\n",
    "Epoch #290: Loss:1.5813, Accuracy:0.2620, Validation Loss:1.6166, Validation Accuracy:0.2085\n",
    "Epoch #291: Loss:1.5802, Accuracy:0.2616, Validation Loss:1.6189, Validation Accuracy:0.2036\n",
    "Epoch #292: Loss:1.5799, Accuracy:0.2595, Validation Loss:1.6212, Validation Accuracy:0.2036\n",
    "Epoch #293: Loss:1.5781, Accuracy:0.2591, Validation Loss:1.6226, Validation Accuracy:0.2184\n",
    "Epoch #294: Loss:1.5772, Accuracy:0.2583, Validation Loss:1.6240, Validation Accuracy:0.2250\n",
    "Epoch #295: Loss:1.5774, Accuracy:0.2600, Validation Loss:1.6248, Validation Accuracy:0.2135\n",
    "Epoch #296: Loss:1.5771, Accuracy:0.2678, Validation Loss:1.6264, Validation Accuracy:0.2250\n",
    "Epoch #297: Loss:1.5764, Accuracy:0.2632, Validation Loss:1.6274, Validation Accuracy:0.2036\n",
    "Epoch #298: Loss:1.5757, Accuracy:0.2690, Validation Loss:1.6252, Validation Accuracy:0.2282\n",
    "Epoch #299: Loss:1.5772, Accuracy:0.2612, Validation Loss:1.6208, Validation Accuracy:0.2381\n",
    "Epoch #300: Loss:1.5760, Accuracy:0.2669, Validation Loss:1.6212, Validation Accuracy:0.2151\n",
    "\n",
    "Test:\n",
    "Test Loss:1.62120521, Accuracy:0.2151\n",
    "Labels: ['05', '02', '03', '01', '04']\n",
    "Confusion Matrix:\n",
    "      05  02  03  01  04\n",
    "t:05  55   1   1  55  30\n",
    "t:02  37   4   3  43  27\n",
    "t:03  36   1   2  57  19\n",
    "t:01  50   2   2  46  26\n",
    "t:04  44   3   3  38  24\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.25      0.39      0.30       142\n",
    "          02       0.36      0.04      0.06       114\n",
    "          03       0.18      0.02      0.03       115\n",
    "          01       0.19      0.37      0.25       126\n",
    "          04       0.19      0.21      0.20       112\n",
    "\n",
    "    accuracy                           0.22       609\n",
    "   macro avg       0.24      0.20      0.17       609\n",
    "weighted avg       0.24      0.22      0.18       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 22:22:06 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 53 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6102361986398306, 1.606182902904567, 1.6056776395180739, 1.6055055655086374, 1.6050312000346694, 1.6053887652646144, 1.6057788859838726, 1.6060778546607357, 1.6059878015361593, 1.6053894000687623, 1.605246023591516, 1.6049202406543426, 1.6047564666650957, 1.604950545456609, 1.6046187208203846, 1.605086629026629, 1.6051888137028134, 1.6051597144999137, 1.605402896752694, 1.6053695212835553, 1.6052090041155886, 1.6053986437802243, 1.6055151363115985, 1.6058680574686461, 1.6059506653956397, 1.6064268757752793, 1.6066888760659104, 1.6071885910331714, 1.6068104022046419, 1.6068242191289641, 1.6072226410428878, 1.6073579662930593, 1.607240797459394, 1.6078273525770466, 1.607111740777841, 1.6070695876683703, 1.606887537270344, 1.6065432759145601, 1.6074210683308994, 1.6073236338219228, 1.6062102186660265, 1.605113284732712, 1.6079818317651358, 1.6086769748008114, 1.6082426454437582, 1.605316639533771, 1.6061417655208818, 1.6097327729164086, 1.608875401893077, 1.606361356079089, 1.6067814502027038, 1.6070348511775727, 1.6084757031282573, 1.6085806916695706, 1.6072425760071853, 1.6059536493470516, 1.6087789251690818, 1.607530536322758, 1.613099894695877, 1.6104801241400206, 1.6076781640107605, 1.6082418501279232, 1.6105816931951613, 1.6065598914188703, 1.605381917092209, 1.6067316902094875, 1.6101975742427783, 1.6062755533822848, 1.60495531970057, 1.6067478860344597, 1.6081878238515117, 1.6075418868479863, 1.607902447578355, 1.6086436535533035, 1.6076775233342337, 1.608561873827466, 1.605370439919345, 1.6070804942417614, 1.610571426319567, 1.608661019156132, 1.6089140065197873, 1.610206912303793, 1.6071120103200276, 1.609987653534988, 1.6116189281341478, 1.6105024174516425, 1.6135050704326537, 1.6139034620059536, 1.6132966094024859, 1.6170364152426007, 1.6118767306526698, 1.6154712295688822, 1.6121304681148436, 1.6167293168445331, 1.6151982651555479, 1.6160025737555743, 1.6181075394838706, 1.6169572906149627, 1.611839708044807, 1.6182235418673612, 1.618001123758764, 1.6149295460805908, 1.614797982089038, 1.6118793945594374, 1.613957439150129, 1.610629078398393, 1.6121758069898107, 1.6130769385884351, 1.6136821625855169, 1.613658260437851, 1.6132424216356576, 1.6138940894936498, 1.6185281214064173, 1.6186084894123922, 1.6160402413463748, 1.6233787411343679, 1.6210692449547779, 1.620662452552119, 1.6222404615436672, 1.6240799131456072, 1.6245150233332943, 1.6219559667145678, 1.6265639603040096, 1.6215138562598643, 1.6216417815297695, 1.6203162537028246, 1.6190876995988668, 1.6141946208105102, 1.6173696273261886, 1.6175018010664064, 1.6149510015995043, 1.6189251795582387, 1.6176304864178737, 1.6151534622330188, 1.6171464039187127, 1.6171390407386868, 1.61653650355065, 1.617598202428207, 1.6189304791843557, 1.615969445122091, 1.6149703651813452, 1.616124163511743, 1.6146782380215248, 1.6151581787319214, 1.6156727457281403, 1.618610626371036, 1.6149854309648912, 1.6178061488422462, 1.620420602937833, 1.6202212865716719, 1.617632177662967, 1.6192317022674385, 1.618486878711406, 1.6159103799531809, 1.61602585186512, 1.615817193523025, 1.6197315109970143, 1.6193702422535086, 1.6172647133640858, 1.6248280207316081, 1.620564828952545, 1.6211205315707353, 1.6215746825551751, 1.6165642634596926, 1.6202517031448815, 1.6207218457912576, 1.6300008717820367, 1.6339257323291698, 1.6273999084980029, 1.6207402958267036, 1.6300131971221448, 1.620227623847122, 1.6123499885959969, 1.6422379788115302, 1.6452790334306915, 1.611494676428671, 1.6184754078024126, 1.610643125324218, 1.6101208232306494, 1.6108295762871678, 1.6116274364280387, 1.6111516427915475, 1.6113208109522101, 1.6108382409820807, 1.6107528800839077, 1.6127717778796242, 1.6143440196079573, 1.6140333956294068, 1.6151776213951299, 1.61827414767887, 1.6168241483237356, 1.6187728796099208, 1.6194000439886584, 1.6199901816488682, 1.6189598330527495, 1.620934716586409, 1.6195798276484699, 1.6164057262621099, 1.6180004738821772, 1.617550760654393, 1.6193658777058417, 1.623058874618831, 1.6247348368461496, 1.6210028810062627, 1.621139333361671, 1.6186350503774307, 1.620744526484134, 1.6159793210930034, 1.6206207034623095, 1.6251442528319084, 1.620573054002032, 1.617318795623842, 1.619734242436138, 1.6156911859762884, 1.6157266890082649, 1.61247919208702, 1.6161439363983856, 1.6163959144958722, 1.6198520057502834, 1.6209180088857515, 1.6220982838146791, 1.6248359406131438, 1.6247043672257848, 1.625370662592119, 1.628066341278001, 1.6225710745124002, 1.629099969989169, 1.6229944655852169, 1.62457370934228, 1.6276504369009108, 1.6254581935300028, 1.624336550192684, 1.6272720903011377, 1.6205997864405315, 1.6310626720560009, 1.625925829257871, 1.6196287236190194, 1.6164211361670533, 1.6191490861191147, 1.6200459547622255, 1.620939041006154, 1.6226424873364578, 1.621114862376246, 1.6215378359229302, 1.6237508138803818, 1.624321703840359, 1.6272535106818664, 1.6312290387004857, 1.6283171697594654, 1.6262587457650597, 1.6255676548664988, 1.6236671183888352, 1.6227662573111272, 1.6251432194889863, 1.6288216123831487, 1.6257304702877802, 1.6255217489154858, 1.6269083346052122, 1.6233040503484666, 1.624300747865135, 1.629215190171804, 1.626916751289994, 1.6304540837731072, 1.6294518993014382, 1.6335163034241775, 1.6321654208187986, 1.6310672910734154, 1.6314761378299232, 1.6319519917561698, 1.6307621350625074, 1.6342497735187924, 1.6355745663196581, 1.635372222546482, 1.6373830473873219, 1.6346876039880838, 1.6358592232264126, 1.637609001255192, 1.6388300731655803, 1.6376654399048127, 1.628577280710092, 1.6362469562364526, 1.6283241175665644, 1.6301998974654475, 1.6281781131998072, 1.614513670086665, 1.6128174706632867, 1.6138657223806396, 1.6153454042616344, 1.6167928796683626, 1.6165552456390682, 1.6189189049214956, 1.621163507596221, 1.6226297789215063, 1.6240337020266429, 1.624842345616696, 1.6264223682469334, 1.627381570820738, 1.6251750563948808, 1.6207856748295926, 1.6212052474859704], 'val_acc': [0.19376026172258193, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23152709327797194, 0.23316912940276668, 0.23481116572330737, 0.23152709327797194, 0.23316912940276668, 0.23481116552756143, 0.23481116552756143, 0.23481116552756143, 0.23316912940276668, 0.23152709327797194, 0.23152709327797194, 0.23152709327797194, 0.23152709327797194, 0.23152709327797194, 0.23481116552756143, 0.23481116552756143, 0.23316912940276668, 0.23316912940276668, 0.23645320165235617, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23481116552756143, 0.23316912940276668, 0.24466338247207586, 0.23481116552756143, 0.23481116552756143, 0.24302134634728112, 0.23645320165235617, 0.23481116552756143, 0.22988505685955823, 0.23152709318009895, 0.23481116572330737, 0.23645320184810212, 0.23481116552756143, 0.23481116552756143, 0.23645320165235617, 0.2413793101246134, 0.24137931022248635, 0.23316912940276668, 0.2413793100267404, 0.23481116552756143, 0.2298850571531772, 0.2249589489745389, 0.23481116552756143, 0.2331691293048937, 0.23645320165235617, 0.23316912940276668, 0.2315270933758449, 0.23316912950063964, 0.23316912940276668, 0.23316912950063964, 0.23481116552756143, 0.23481116552756143, 0.23809523777715091, 0.2348111656254344, 0.2348111656254344, 0.23645320165235617, 0.23481116552756143, 0.2315270933758449, 0.23809523807076985, 0.2282430211262554, 0.2282430211262554, 0.2364532019459751, 0.23481116572330737, 0.22660098490358768, 0.22660098490358768, 0.22003284001291679, 0.22824302102838243, 0.22824302093050947, 0.22167487643133046, 0.2085385870414806, 0.2085385870414806, 0.19376026191832788, 0.20853858723722654, 0.20197044234655565, 0.21182265938894307, 0.20361247856922338, 0.2167487677633273, 0.2118226591931971, 0.20197044333751957, 0.21510673163853256, 0.20197044273804757, 0.2331691293048937, 0.21510673163853256, 0.22824302044114456, 0.20853858684573462, 0.21018062306840235, 0.21674876756758135, 0.21510673154065957, 0.2167487677633273, 0.22331691206676033, 0.21018062306840235, 0.2085385869436076, 0.21018062306840235, 0.22495894809368208, 0.2085385869436076, 0.21510673134491362, 0.20032840661325282, 0.21510673163853256, 0.19868637078207702, 0.20197044254230162, 0.2068965511124318, 0.20853858733509953, 0.2052545147918911, 0.20525451537912898, 0.20197044283592056, 0.2003284067111258, 0.2134646955137378, 0.20032840651537984, 0.2036124788628423, 0.20853858723722654, 0.2167487678612003, 0.19868637058633107, 0.21182265948681606, 0.22167487652920345, 0.2134646955137378, 0.20361247866709636, 0.20853858713935358, 0.22660098480571472, 0.22824302093050947, 0.21839080408386802, 0.23152709327797194, 0.22167487643133046, 0.22660098480571472, 0.22003284011078977, 0.23152709298435298, 0.22660098451209576, 0.21510673173640554, 0.22331691226250627, 0.21018062336202128, 0.22003284001291679, 0.2167487676654543, 0.20853858713935358, 0.22331691226250627, 0.2167487677633273, 0.22331691236037926, 0.22167487652920345, 0.22003284020866276, 0.23316912950063964, 0.2249589489745389, 0.2282430213220014, 0.22495894887666593, 0.22660098470784173, 0.20032840641750688, 0.21674876805694623, 0.21018062316627534, 0.22003284030653572, 0.22331691245825225, 0.211822659682562, 0.21839080408386802, 0.21839080408386802, 0.21018062355776726, 0.21346469570948376, 0.22495894828942806, 0.21018062394925918, 0.21018062336202128, 0.2036124788628423, 0.23152709327797194, 0.22003284011078977, 0.22003284011078977, 0.21510673163853256, 0.224958948485174, 0.22331691226250627, 0.21510673163853256, 0.2134646955137378, 0.21018062326414833, 0.2068965511124318, 0.21346469570948376, 0.22167487623558452, 0.22167487623558452, 0.22167487623558452, 0.22495894819155507, 0.21182265938894307, 0.2068965511124318, 0.20197044254230162, 0.21018062316627534, 0.20689655091668585, 0.21018062316627534, 0.21018062316627534, 0.20689655091668585, 0.21182265938894307, 0.21182265929107008, 0.21182265938894307, 0.1921182259892791, 0.2052545147918911, 0.21182265929107008, 0.2085385870414806, 0.2052545148897641, 0.2085385870414806, 0.21346469541586482, 0.21018062306840235, 0.21018062316627534, 0.21018062306840235, 0.20689655081881286, 0.2052545147918911, 0.2085385870414806, 0.20689655081881286, 0.2085385870414806, 0.21346469541586482, 0.21182265938894307, 0.2101806229705294, 0.2085385869436076, 0.2167487676654543, 0.20689655072093988, 0.21018062306840235, 0.2085385869436076, 0.2118226591931971, 0.2183908035945031, 0.20689655062306692, 0.22495894838730102, 0.20853858713935358, 0.224958948485174, 0.21182265948681606, 0.2151067319321515, 0.21839080418174098, 0.21674876795907325, 0.21510673163853256, 0.21018062316627534, 0.21674876805694623, 0.21674876805694623, 0.2167487678612003, 0.2216748768228224, 0.22167487643133046, 0.21674876795907325, 0.22331691236037926, 0.22003284011078977, 0.22003284011078977, 0.23152709298435298, 0.21346469541586482, 0.20853858713935358, 0.20361247866709636, 0.20689655091668585, 0.20689655101455884, 0.21839080388812204, 0.21018062316627534, 0.21182265929107008, 0.21182265929107008, 0.2167487677633273, 0.2167487677633273, 0.21346469541586482, 0.22331691245825225, 0.21839080398599503, 0.21182265929107008, 0.22003284030653572, 0.21018062326414833, 0.2085385870414806, 0.2167487678612003, 0.21018062326414833, 0.20853858713935358, 0.2167487677633273, 0.21182265929107008, 0.22167487623558452, 0.22003284001291679, 0.21839080398599503, 0.21018062306840235, 0.21018062306840235, 0.22167487643133046, 0.22495894877879294, 0.21839080379024906, 0.21346469541586482, 0.21510673144278658, 0.2331691292070207, 0.21018062316627534, 0.2167487676654543, 0.2151067318342785, 0.21182265948681606, 0.21182265997618094, 0.2101806237535132, 0.2134646956116108, 0.21674876746970836, 0.2134646951222459, 0.20689655091668585, 0.2003284063196339, 0.2085385870414806, 0.20361247866709636, 0.20361247837347743, 0.21839080349663012, 0.22495894898677302, 0.21346469522011888, 0.22495894898677302, 0.2036124794623143, 0.22824302113848954, 0.23809523699416707, 0.21510673134491362], 'loss': [1.613561258472701, 1.6077667543775491, 1.6059406295204555, 1.6047823214677814, 1.6046223335931924, 1.6046005755724113, 1.604781430506853, 1.6046224990175, 1.6045758819188425, 1.6043501445644934, 1.6039394857457532, 1.6038234136432592, 1.6036867762982723, 1.6035897768988012, 1.60308963591558, 1.6028269106847304, 1.6025546025446553, 1.6020191998941942, 1.6020281678597295, 1.6015447555381415, 1.6012127981048834, 1.6006603063743952, 1.6003604956965671, 1.600039696154898, 1.5996432674004557, 1.59936529390621, 1.5990154986019252, 1.5988512310404064, 1.5987438103501557, 1.5987692212177254, 1.598626378429499, 1.5982024065033367, 1.598487747914982, 1.5977571885933377, 1.5974987185221678, 1.5972613547616916, 1.5968622328564372, 1.5972529003017981, 1.5972807669786457, 1.59684633107156, 1.5964991593997337, 1.5967787020505086, 1.5978898387180462, 1.5968108976646125, 1.596980672058873, 1.5971078768403133, 1.6008478218035531, 1.5970008358083956, 1.5988312087264638, 1.5971160853912698, 1.5970729680031965, 1.596173711772817, 1.5957658222323816, 1.5961691236593885, 1.5963246392518342, 1.5969619790141831, 1.5971154385278847, 1.5983658185974527, 1.5969950576582483, 1.598122522816276, 1.5972716086699, 1.5971980559018113, 1.5961547094693664, 1.5952649837646642, 1.5987258317778976, 1.596741982407149, 1.5959873945071712, 1.5946600069010772, 1.594425866736034, 1.5942190558024256, 1.593507534767323, 1.593475154488973, 1.5930974427189915, 1.5927223825846364, 1.5927316712158173, 1.5941551079250704, 1.5933055786626296, 1.5931637435478352, 1.5930977819391834, 1.5924624202677358, 1.5914068992133013, 1.592593097882594, 1.5911809599130304, 1.5908825268735631, 1.59201735622829, 1.5923625018807162, 1.590844307787854, 1.5892252470678372, 1.5881554244724876, 1.588838488805955, 1.5896590604429617, 1.588508266785796, 1.5883193433162368, 1.5869758509022989, 1.5873968885175012, 1.5868999501273373, 1.5855207764882082, 1.5883910145358138, 1.5906224700214926, 1.5885303327435096, 1.5894554395695242, 1.5899082125089987, 1.590747624843762, 1.5938154710146926, 1.5923066348265817, 1.5921261713000538, 1.5906739293182655, 1.5902334159894156, 1.5899240639665044, 1.5901835100361943, 1.5889857393025861, 1.5874289083774575, 1.587516526519885, 1.588667957787641, 1.5861513270489733, 1.5842589419480466, 1.5844324254402145, 1.5839384528400962, 1.5828628290850035, 1.5816070145894858, 1.5818673130423138, 1.5820672634935478, 1.582425135606613, 1.5816360168633274, 1.5832803844181664, 1.5805464032739094, 1.580381917366012, 1.5819708951934408, 1.584216255475853, 1.5840262809083692, 1.5818444017022542, 1.5842491923905986, 1.582458903608381, 1.583076551317924, 1.5825369462340275, 1.5823342893647463, 1.583473756425924, 1.5818941566244042, 1.5816533754983233, 1.5818138462806874, 1.5824609447553661, 1.5822410096867618, 1.5788636691271647, 1.5802306933079902, 1.5778644166198355, 1.578619825717605, 1.5796842222585816, 1.5800861471242729, 1.5801308381239247, 1.5793375505314227, 1.578172601126058, 1.5764642871625614, 1.5790497180128, 1.5795408067761996, 1.5782098969884477, 1.578878427481994, 1.5790306728723358, 1.577600493176517, 1.5781573374902933, 1.5736660470218384, 1.5737269850482196, 1.5764573808568214, 1.5732087560258117, 1.5760883868352589, 1.575306674293424, 1.573924071343283, 1.5755615956484659, 1.5751294682402874, 1.579848675414522, 1.6021840892533257, 1.6095862129630494, 1.5958361228633466, 1.5924839438843776, 1.5902489881006354, 1.610180861357546, 1.601996347350996, 1.6053528967824071, 1.6023577646063583, 1.5954486108658494, 1.592974147953292, 1.5919400483431023, 1.59144707891241, 1.5901096885453994, 1.5898932864778585, 1.5894801721435798, 1.588681057003734, 1.5876045285798686, 1.5865750929168607, 1.585571029788415, 1.585616814967788, 1.5867342631185324, 1.5858796004152396, 1.5846829810426466, 1.5853276239528304, 1.58371757966537, 1.584002391170917, 1.5826006700860402, 1.5828098768571075, 1.5825851076682245, 1.5833208086554276, 1.5802513491691261, 1.5811191143196466, 1.5815612091910423, 1.5816362131792416, 1.583262096342365, 1.5819862979148693, 1.5818046586469459, 1.5820421872442507, 1.5816555190624888, 1.582289805794154, 1.5848838644595606, 1.5848379723100448, 1.5860672586997187, 1.588255000946703, 1.5884016901560634, 1.5878638966127587, 1.5852011140611872, 1.5844605024345602, 1.5834545003315261, 1.5814064684834568, 1.581027542443246, 1.5802793484204114, 1.5797564055640594, 1.5821865659964403, 1.580197008091811, 1.5839676363512232, 1.583396865944598, 1.5872225898981585, 1.5849420789330892, 1.5823977801344478, 1.5816255014779876, 1.5833711321104234, 1.5822292879621596, 1.5850043775609386, 1.5911369518088119, 1.5928864303310795, 1.5875072155645007, 1.5885689736881294, 1.5859217537502, 1.5823162143480116, 1.581599144720199, 1.5811798243062452, 1.5815063954868356, 1.5819411927424907, 1.5816212508712706, 1.5809060991667134, 1.580014781883365, 1.5802659345603332, 1.5812221501642183, 1.5787348914684947, 1.5789586540609903, 1.5777751335617942, 1.5785705360788584, 1.5773524646641537, 1.579084248219672, 1.5782078664650419, 1.5748438597704595, 1.5758472255123224, 1.5764791063214718, 1.5759639306724438, 1.576748482990069, 1.5760342361011543, 1.5741357161034304, 1.57380482234994, 1.5739131303783316, 1.572388895042623, 1.5713694654695798, 1.5713963497835508, 1.571683169586213, 1.5720478268374654, 1.5699666126063228, 1.5708863651972778, 1.5704434667524616, 1.5703861398129002, 1.570080670732737, 1.5704999497783747, 1.5711668995126806, 1.5747832512219093, 1.5762072719832465, 1.5791432953468338, 1.5792995579188853, 1.5925557957048044, 1.5933014643510508, 1.5932782613766021, 1.6038818819566918, 1.5865153106086307, 1.5823689915071524, 1.5825691430230895, 1.582425276454714, 1.5813281508197041, 1.580196326714032, 1.5798896747448117, 1.5781119835694957, 1.5771887777277576, 1.5773524921287991, 1.5771177476926017, 1.5763948567838884, 1.5756810407618966, 1.577219701939295, 1.57601041739971], 'acc': [0.1893223822300439, 0.21971252454134962, 0.23326488613347987, 0.23285420851541005, 0.23285421086532623, 0.23285420872959514, 0.23285420951290053, 0.23285421051039099, 0.2328542106694999, 0.2328542091028891, 0.2328542102778472, 0.23285420892542147, 0.23285420890706277, 0.23285420953125924, 0.23285421010037957, 0.23408624217128363, 0.23737166290165707, 0.2410677598609572, 0.2447638617893509, 0.24804928136312496, 0.2464065701075403, 0.24558521487140067, 0.24763860335340246, 0.24435318319214933, 0.24558521647472892, 0.24640657188833617, 0.2464065708908457, 0.2464065713008571, 0.24845995819788938, 0.24845995819788938, 0.24928131304237633, 0.2488706365992646, 0.2492813153922925, 0.24969199124792518, 0.24722792534367993, 0.24804928040235194, 0.2480492835172148, 0.2464065708908457, 0.2488706358159592, 0.2439425051824268, 0.2451745374491572, 0.24558521508558576, 0.24312114935880813, 0.24640657188833617, 0.24599589307694955, 0.24763860413670785, 0.23983572939338135, 0.2443531830146817, 0.24188911671878377, 0.24394250537825315, 0.2472279263411704, 0.25092402488544, 0.24763860335340246, 0.24640657108667205, 0.24640657149668346, 0.24640657304493555, 0.24353182856184746, 0.24435318319214933, 0.24435318321050806, 0.2414784414322714, 0.2509240260603981, 0.242710471544912, 0.24312114937716686, 0.24928131382568172, 0.23696098528358725, 0.2439425071406903, 0.24312115016047225, 0.24599589346860223, 0.24640657069501937, 0.24845995937284748, 0.24476385924360836, 0.24435318280049662, 0.24681724931057963, 0.2496919916579366, 0.2468172475297838, 0.24106776226594953, 0.24558521604635877, 0.24804928273390942, 0.24271047291569642, 0.24558521545887974, 0.24558521682966417, 0.2468172485089155, 0.25133469992104984, 0.24763860296174975, 0.24147843888652887, 0.24229979492433262, 0.25010267023677946, 0.2529774127799628, 0.25790554439262686, 0.257084189174846, 0.25420944680913027, 0.24969199245960072, 0.25954825384905694, 0.2587269004304306, 0.2501026706284321, 0.25544147668922706, 0.26036960951356675, 0.2521560573479968, 0.26078028634833117, 0.25338809157299064, 0.25626283393870636, 0.25954825525655884, 0.2525667349660666, 0.251745380317406, 0.25872689964712525, 0.2628336775719018, 0.25215605754382314, 0.2554414790942194, 0.2620123215524568, 0.26160164336526665, 0.26324435518997163, 0.2574948681453415, 0.2673511307831907, 0.2583162239689602, 0.2583162196240631, 0.2694045181085931, 0.26652977355207014, 0.27104722719172925, 0.27063655172774925, 0.273100615673731, 0.2739219689148897, 0.27227925863843677, 0.2652977424970153, 0.2821355252295304, 0.2694045180902344, 0.26447638409093666, 0.2648870633306934, 0.257494866187078, 0.2669404506193784, 0.26160164197612346, 0.26570841894012703, 0.26365503202473606, 0.25544147927168703, 0.2616016431510816, 0.25954825326157793, 0.2607802867399838, 0.26283367580946465, 0.2513347015243781, 0.2599589334621077, 0.2583162220290554, 0.26858316442070557, 0.2570841889606609, 0.27022587334473275, 0.2611909667079698, 0.2714579065722362, 0.26940451589942715, 0.262012320573325, 0.26776180761795515, 0.26694045218598916, 0.26858316306827984, 0.27022587332637404, 0.2710472285625137, 0.2632443544066662, 0.26981519351749694, 0.270225872738895, 0.2706365497327683, 0.2644763853026122, 0.26776180800960786, 0.26570841993761746, 0.27433265029037757, 0.2665297729645911, 0.2702258707439141, 0.26981519430080236, 0.2620123201816723, 0.27063655016113847, 0.2706365487536366, 0.2694045188735398, 0.27063655016113847, 0.26570841714097243, 0.24024640622814578, 0.23901437396141537, 0.23901437337393633, 0.24845995917702113, 0.25913757626770456, 0.24476386120187182, 0.23901437396141537, 0.2431211503746573, 0.2542094470049566, 0.2517453779307724, 0.25667351212589645, 0.25995893130801784, 0.2591375758760519, 0.2591375770693687, 0.25215605854131357, 0.251745380904885, 0.26365503124143064, 0.2652977417137099, 0.2648870633123347, 0.26611909734150224, 0.2706365525110546, 0.26776180464384247, 0.2694045171294614, 0.2685831642248792, 0.26940451828606077, 0.27063654894946293, 0.2698151927341915, 0.2681724854318513, 0.26899383847718367, 0.27351129544589065, 0.2673511270257727, 0.26776180802796656, 0.2739219726723078, 0.26776180761795515, 0.26776180464384247, 0.2669404531651209, 0.26981519551247785, 0.2694045185002458, 0.26940451946101884, 0.2681724848443723, 0.2710472283666873, 0.2673511315664961, 0.2702258746971585, 0.26858316246244207, 0.2648870627248556, 0.26365503222056236, 0.2628336769844227, 0.2685831606632875, 0.2677618089887396, 0.2677618066388234, 0.2669404535567736, 0.2673511299631679, 0.2689938402763382, 0.2735112938425624, 0.26406571062193757, 0.25831622220652306, 0.2657084167493197, 0.2566735109692971, 0.25256673692433007, 0.2644763849109595, 0.2640657078803687, 0.2624230001496583, 0.25954825525655884, 0.26858316203407195, 0.2611909661204908, 0.2603696093177404, 0.25790554635089036, 0.2574948641920971, 0.2579055418101669, 0.2648870619048328, 0.2694045157036008, 0.2714579069638889, 0.2665297745679193, 0.2624229976039158, 0.26899383749805195, 0.26201232096497773, 0.2640657102302849, 0.2694045157036008, 0.26940451711110264, 0.26694044940770284, 0.26899384105964363, 0.2698151945333461, 0.2652977406978607, 0.26488706546642454, 0.2689938408638173, 0.27310061704451544, 0.27310061367875005, 0.2714579039897762, 0.27351129345090974, 0.27022587489298483, 0.2714579059847571, 0.2673511294124063, 0.2689938400988706, 0.2694045168785589, 0.2722792608292441, 0.27063655231522826, 0.2743326491154195, 0.2681724862151567, 0.2722792629833339, 0.2702258707439141, 0.2657084207025642, 0.268172485823504, 0.27145790676806253, 0.2706365511402702, 0.26899384047216457, 0.2698151960999569, 0.2644763858900912, 0.2726899388389666, 0.2718685849736114, 0.2702258721146985, 0.262833676005291, 0.26447638804418105, 0.25626283372452124, 0.24558521585053242, 0.24763860354922881, 0.24065708325873655, 0.2583162212273913, 0.26570841753262514, 0.267351130391538, 0.263655031063963, 0.2620123201816723, 0.2616016421719498, 0.25954825525655884, 0.25913757804850046, 0.25831622083573863, 0.2599589326788023, 0.267761807189585, 0.2632443524484027, 0.26899384049052333, 0.2611909657471968, 0.26694045019100826]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
