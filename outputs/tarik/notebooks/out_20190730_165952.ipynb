{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf35.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 16:59:52 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': 'Split', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 5 Label(s): ['05', '03', '01', '04', '02'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000274AE00E198>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002749D926EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6080, Accuracy:0.2086, Validation Loss:1.6058, Validation Accuracy:0.2274\n",
    "Epoch #2: Loss:1.6055, Accuracy:0.2333, Validation Loss:1.6047, Validation Accuracy:0.2328\n",
    "Epoch #3: Loss:1.6049, Accuracy:0.2330, Validation Loss:1.6046, Validation Accuracy:0.2328\n",
    "Epoch #4: Loss:1.6048, Accuracy:0.2330, Validation Loss:1.6040, Validation Accuracy:0.2328\n",
    "Epoch #5: Loss:1.6040, Accuracy:0.2369, Validation Loss:1.6039, Validation Accuracy:0.2389\n",
    "Epoch #6: Loss:1.6036, Accuracy:0.2368, Validation Loss:1.6038, Validation Accuracy:0.2401\n",
    "Epoch #7: Loss:1.6037, Accuracy:0.2381, Validation Loss:1.6032, Validation Accuracy:0.2397\n",
    "Epoch #8: Loss:1.6034, Accuracy:0.2334, Validation Loss:1.6031, Validation Accuracy:0.2328\n",
    "Epoch #9: Loss:1.6033, Accuracy:0.2392, Validation Loss:1.6029, Validation Accuracy:0.2393\n",
    "Epoch #10: Loss:1.6040, Accuracy:0.2346, Validation Loss:1.6030, Validation Accuracy:0.2356\n",
    "Epoch #11: Loss:1.6033, Accuracy:0.2339, Validation Loss:1.6027, Validation Accuracy:0.2328\n",
    "Epoch #12: Loss:1.6028, Accuracy:0.2412, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #13: Loss:1.6019, Accuracy:0.2392, Validation Loss:1.6015, Validation Accuracy:0.2434\n",
    "Epoch #14: Loss:1.6015, Accuracy:0.2410, Validation Loss:1.6015, Validation Accuracy:0.2401\n",
    "Epoch #15: Loss:1.6027, Accuracy:0.2313, Validation Loss:1.6015, Validation Accuracy:0.2422\n",
    "Epoch #16: Loss:1.6014, Accuracy:0.2438, Validation Loss:1.6017, Validation Accuracy:0.2422\n",
    "Epoch #17: Loss:1.6007, Accuracy:0.2411, Validation Loss:1.6011, Validation Accuracy:0.2434\n",
    "Epoch #18: Loss:1.6009, Accuracy:0.2437, Validation Loss:1.6011, Validation Accuracy:0.2418\n",
    "Epoch #19: Loss:1.6013, Accuracy:0.2391, Validation Loss:1.6012, Validation Accuracy:0.2389\n",
    "Epoch #20: Loss:1.6011, Accuracy:0.2424, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #21: Loss:1.6003, Accuracy:0.2375, Validation Loss:1.6009, Validation Accuracy:0.2360\n",
    "Epoch #22: Loss:1.6002, Accuracy:0.2406, Validation Loss:1.6006, Validation Accuracy:0.2434\n",
    "Epoch #23: Loss:1.6002, Accuracy:0.2409, Validation Loss:1.6006, Validation Accuracy:0.2352\n",
    "Epoch #24: Loss:1.6001, Accuracy:0.2429, Validation Loss:1.6008, Validation Accuracy:0.2377\n",
    "Epoch #25: Loss:1.6007, Accuracy:0.2425, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #26: Loss:1.5995, Accuracy:0.2441, Validation Loss:1.6002, Validation Accuracy:0.2414\n",
    "Epoch #27: Loss:1.5997, Accuracy:0.2364, Validation Loss:1.6002, Validation Accuracy:0.2401\n",
    "Epoch #28: Loss:1.5988, Accuracy:0.2444, Validation Loss:1.6004, Validation Accuracy:0.2381\n",
    "Epoch #29: Loss:1.5992, Accuracy:0.2441, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #30: Loss:1.5990, Accuracy:0.2385, Validation Loss:1.6000, Validation Accuracy:0.2401\n",
    "Epoch #31: Loss:1.5987, Accuracy:0.2370, Validation Loss:1.6004, Validation Accuracy:0.2340\n",
    "Epoch #32: Loss:1.5989, Accuracy:0.2374, Validation Loss:1.6009, Validation Accuracy:0.2360\n",
    "Epoch #33: Loss:1.5986, Accuracy:0.2417, Validation Loss:1.6012, Validation Accuracy:0.2328\n",
    "Epoch #34: Loss:1.5995, Accuracy:0.2366, Validation Loss:1.6011, Validation Accuracy:0.2344\n",
    "Epoch #35: Loss:1.5996, Accuracy:0.2381, Validation Loss:1.6013, Validation Accuracy:0.2352\n",
    "Epoch #36: Loss:1.5985, Accuracy:0.2408, Validation Loss:1.6012, Validation Accuracy:0.2360\n",
    "Epoch #37: Loss:1.5988, Accuracy:0.2419, Validation Loss:1.6011, Validation Accuracy:0.2373\n",
    "Epoch #38: Loss:1.5984, Accuracy:0.2418, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #39: Loss:1.5986, Accuracy:0.2394, Validation Loss:1.6001, Validation Accuracy:0.2356\n",
    "Epoch #40: Loss:1.5986, Accuracy:0.2389, Validation Loss:1.5999, Validation Accuracy:0.2369\n",
    "Epoch #41: Loss:1.5979, Accuracy:0.2450, Validation Loss:1.5998, Validation Accuracy:0.2410\n",
    "Epoch #42: Loss:1.5975, Accuracy:0.2458, Validation Loss:1.6009, Validation Accuracy:0.2377\n",
    "Epoch #43: Loss:1.5980, Accuracy:0.2440, Validation Loss:1.6010, Validation Accuracy:0.2401\n",
    "Epoch #44: Loss:1.5983, Accuracy:0.2437, Validation Loss:1.6005, Validation Accuracy:0.2389\n",
    "Epoch #45: Loss:1.5980, Accuracy:0.2425, Validation Loss:1.6011, Validation Accuracy:0.2360\n",
    "Epoch #46: Loss:1.5984, Accuracy:0.2393, Validation Loss:1.6008, Validation Accuracy:0.2418\n",
    "Epoch #47: Loss:1.5978, Accuracy:0.2455, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #48: Loss:1.5976, Accuracy:0.2443, Validation Loss:1.6003, Validation Accuracy:0.2393\n",
    "Epoch #49: Loss:1.5978, Accuracy:0.2466, Validation Loss:1.5998, Validation Accuracy:0.2381\n",
    "Epoch #50: Loss:1.5985, Accuracy:0.2484, Validation Loss:1.5994, Validation Accuracy:0.2422\n",
    "Epoch #51: Loss:1.5977, Accuracy:0.2454, Validation Loss:1.5997, Validation Accuracy:0.2348\n",
    "Epoch #52: Loss:1.5971, Accuracy:0.2438, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #53: Loss:1.5987, Accuracy:0.2437, Validation Loss:1.6011, Validation Accuracy:0.2385\n",
    "Epoch #54: Loss:1.5983, Accuracy:0.2401, Validation Loss:1.5996, Validation Accuracy:0.2389\n",
    "Epoch #55: Loss:1.5970, Accuracy:0.2476, Validation Loss:1.5989, Validation Accuracy:0.2410\n",
    "Epoch #56: Loss:1.5980, Accuracy:0.2480, Validation Loss:1.6005, Validation Accuracy:0.2373\n",
    "Epoch #57: Loss:1.5996, Accuracy:0.2412, Validation Loss:1.6004, Validation Accuracy:0.2393\n",
    "Epoch #58: Loss:1.5978, Accuracy:0.2384, Validation Loss:1.6014, Validation Accuracy:0.2340\n",
    "Epoch #59: Loss:1.5976, Accuracy:0.2469, Validation Loss:1.6009, Validation Accuracy:0.2414\n",
    "Epoch #60: Loss:1.5980, Accuracy:0.2487, Validation Loss:1.6019, Validation Accuracy:0.2422\n",
    "Epoch #61: Loss:1.5968, Accuracy:0.2510, Validation Loss:1.6011, Validation Accuracy:0.2401\n",
    "Epoch #62: Loss:1.5969, Accuracy:0.2450, Validation Loss:1.6009, Validation Accuracy:0.2381\n",
    "Epoch #63: Loss:1.5966, Accuracy:0.2456, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #64: Loss:1.5966, Accuracy:0.2423, Validation Loss:1.6013, Validation Accuracy:0.2430\n",
    "Epoch #65: Loss:1.5976, Accuracy:0.2498, Validation Loss:1.6016, Validation Accuracy:0.2406\n",
    "Epoch #66: Loss:1.5975, Accuracy:0.2488, Validation Loss:1.6015, Validation Accuracy:0.2430\n",
    "Epoch #67: Loss:1.5970, Accuracy:0.2454, Validation Loss:1.6020, Validation Accuracy:0.2393\n",
    "Epoch #68: Loss:1.5970, Accuracy:0.2437, Validation Loss:1.6022, Validation Accuracy:0.2455\n",
    "Epoch #69: Loss:1.5969, Accuracy:0.2455, Validation Loss:1.6017, Validation Accuracy:0.2410\n",
    "Epoch #70: Loss:1.5963, Accuracy:0.2500, Validation Loss:1.6016, Validation Accuracy:0.2418\n",
    "Epoch #71: Loss:1.5963, Accuracy:0.2500, Validation Loss:1.6021, Validation Accuracy:0.2443\n",
    "Epoch #72: Loss:1.5959, Accuracy:0.2542, Validation Loss:1.6016, Validation Accuracy:0.2426\n",
    "Epoch #73: Loss:1.5972, Accuracy:0.2511, Validation Loss:1.6016, Validation Accuracy:0.2430\n",
    "Epoch #74: Loss:1.5962, Accuracy:0.2498, Validation Loss:1.6014, Validation Accuracy:0.2434\n",
    "Epoch #75: Loss:1.5979, Accuracy:0.2478, Validation Loss:1.6008, Validation Accuracy:0.2438\n",
    "Epoch #76: Loss:1.5964, Accuracy:0.2525, Validation Loss:1.6008, Validation Accuracy:0.2434\n",
    "Epoch #77: Loss:1.5960, Accuracy:0.2510, Validation Loss:1.6019, Validation Accuracy:0.2430\n",
    "Epoch #78: Loss:1.5959, Accuracy:0.2512, Validation Loss:1.6012, Validation Accuracy:0.2418\n",
    "Epoch #79: Loss:1.5966, Accuracy:0.2549, Validation Loss:1.6015, Validation Accuracy:0.2430\n",
    "Epoch #80: Loss:1.5960, Accuracy:0.2533, Validation Loss:1.6025, Validation Accuracy:0.2422\n",
    "Epoch #81: Loss:1.5965, Accuracy:0.2516, Validation Loss:1.6016, Validation Accuracy:0.2438\n",
    "Epoch #82: Loss:1.5972, Accuracy:0.2523, Validation Loss:1.6018, Validation Accuracy:0.2389\n",
    "Epoch #83: Loss:1.5965, Accuracy:0.2521, Validation Loss:1.6013, Validation Accuracy:0.2418\n",
    "Epoch #84: Loss:1.5969, Accuracy:0.2486, Validation Loss:1.6019, Validation Accuracy:0.2434\n",
    "Epoch #85: Loss:1.5952, Accuracy:0.2522, Validation Loss:1.6023, Validation Accuracy:0.2418\n",
    "Epoch #86: Loss:1.5961, Accuracy:0.2549, Validation Loss:1.6027, Validation Accuracy:0.2414\n",
    "Epoch #87: Loss:1.5957, Accuracy:0.2507, Validation Loss:1.6029, Validation Accuracy:0.2438\n",
    "Epoch #88: Loss:1.5951, Accuracy:0.2550, Validation Loss:1.6018, Validation Accuracy:0.2406\n",
    "Epoch #89: Loss:1.5962, Accuracy:0.2492, Validation Loss:1.6015, Validation Accuracy:0.2434\n",
    "Epoch #90: Loss:1.5950, Accuracy:0.2514, Validation Loss:1.6004, Validation Accuracy:0.2438\n",
    "Epoch #91: Loss:1.5951, Accuracy:0.2552, Validation Loss:1.6002, Validation Accuracy:0.2426\n",
    "Epoch #92: Loss:1.5950, Accuracy:0.2485, Validation Loss:1.6009, Validation Accuracy:0.2447\n",
    "Epoch #93: Loss:1.5943, Accuracy:0.2559, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #94: Loss:1.5951, Accuracy:0.2508, Validation Loss:1.6012, Validation Accuracy:0.2426\n",
    "Epoch #95: Loss:1.5950, Accuracy:0.2496, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #96: Loss:1.5947, Accuracy:0.2534, Validation Loss:1.5999, Validation Accuracy:0.2455\n",
    "Epoch #97: Loss:1.5947, Accuracy:0.2510, Validation Loss:1.6005, Validation Accuracy:0.2463\n",
    "Epoch #98: Loss:1.5946, Accuracy:0.2554, Validation Loss:1.5995, Validation Accuracy:0.2434\n",
    "Epoch #99: Loss:1.5941, Accuracy:0.2550, Validation Loss:1.5993, Validation Accuracy:0.2455\n",
    "Epoch #100: Loss:1.5938, Accuracy:0.2575, Validation Loss:1.5993, Validation Accuracy:0.2455\n",
    "Epoch #101: Loss:1.5941, Accuracy:0.2534, Validation Loss:1.5996, Validation Accuracy:0.2463\n",
    "Epoch #102: Loss:1.5949, Accuracy:0.2511, Validation Loss:1.6006, Validation Accuracy:0.2471\n",
    "Epoch #103: Loss:1.5947, Accuracy:0.2532, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #104: Loss:1.5955, Accuracy:0.2451, Validation Loss:1.6000, Validation Accuracy:0.2471\n",
    "Epoch #105: Loss:1.5945, Accuracy:0.2494, Validation Loss:1.6000, Validation Accuracy:0.2451\n",
    "Epoch #106: Loss:1.5950, Accuracy:0.2479, Validation Loss:1.5996, Validation Accuracy:0.2438\n",
    "Epoch #107: Loss:1.5950, Accuracy:0.2548, Validation Loss:1.5997, Validation Accuracy:0.2455\n",
    "Epoch #108: Loss:1.5953, Accuracy:0.2521, Validation Loss:1.6004, Validation Accuracy:0.2467\n",
    "Epoch #109: Loss:1.5936, Accuracy:0.2520, Validation Loss:1.6003, Validation Accuracy:0.2438\n",
    "Epoch #110: Loss:1.5940, Accuracy:0.2532, Validation Loss:1.6002, Validation Accuracy:0.2443\n",
    "Epoch #111: Loss:1.5942, Accuracy:0.2528, Validation Loss:1.5998, Validation Accuracy:0.2463\n",
    "Epoch #112: Loss:1.5934, Accuracy:0.2576, Validation Loss:1.6004, Validation Accuracy:0.2463\n",
    "Epoch #113: Loss:1.5931, Accuracy:0.2571, Validation Loss:1.6005, Validation Accuracy:0.2475\n",
    "Epoch #114: Loss:1.5940, Accuracy:0.2570, Validation Loss:1.5998, Validation Accuracy:0.2459\n",
    "Epoch #115: Loss:1.5928, Accuracy:0.2577, Validation Loss:1.6019, Validation Accuracy:0.2459\n",
    "Epoch #116: Loss:1.5962, Accuracy:0.2525, Validation Loss:1.6010, Validation Accuracy:0.2385\n",
    "Epoch #117: Loss:1.5944, Accuracy:0.2532, Validation Loss:1.6005, Validation Accuracy:0.2467\n",
    "Epoch #118: Loss:1.5950, Accuracy:0.2498, Validation Loss:1.5989, Validation Accuracy:0.2475\n",
    "Epoch #119: Loss:1.5932, Accuracy:0.2566, Validation Loss:1.5986, Validation Accuracy:0.2475\n",
    "Epoch #120: Loss:1.5942, Accuracy:0.2533, Validation Loss:1.5991, Validation Accuracy:0.2447\n",
    "Epoch #121: Loss:1.5929, Accuracy:0.2549, Validation Loss:1.5997, Validation Accuracy:0.2479\n",
    "Epoch #122: Loss:1.5929, Accuracy:0.2531, Validation Loss:1.5993, Validation Accuracy:0.2467\n",
    "Epoch #123: Loss:1.5923, Accuracy:0.2561, Validation Loss:1.5989, Validation Accuracy:0.2459\n",
    "Epoch #124: Loss:1.5924, Accuracy:0.2587, Validation Loss:1.5988, Validation Accuracy:0.2488\n",
    "Epoch #125: Loss:1.5920, Accuracy:0.2587, Validation Loss:1.5984, Validation Accuracy:0.2484\n",
    "Epoch #126: Loss:1.5917, Accuracy:0.2565, Validation Loss:1.5986, Validation Accuracy:0.2451\n",
    "Epoch #127: Loss:1.5918, Accuracy:0.2567, Validation Loss:1.5989, Validation Accuracy:0.2443\n",
    "Epoch #128: Loss:1.5918, Accuracy:0.2589, Validation Loss:1.5984, Validation Accuracy:0.2484\n",
    "Epoch #129: Loss:1.5923, Accuracy:0.2562, Validation Loss:1.5991, Validation Accuracy:0.2459\n",
    "Epoch #130: Loss:1.5917, Accuracy:0.2569, Validation Loss:1.5979, Validation Accuracy:0.2463\n",
    "Epoch #131: Loss:1.5924, Accuracy:0.2543, Validation Loss:1.5984, Validation Accuracy:0.2492\n",
    "Epoch #132: Loss:1.5930, Accuracy:0.2553, Validation Loss:1.5988, Validation Accuracy:0.2484\n",
    "Epoch #133: Loss:1.5923, Accuracy:0.2538, Validation Loss:1.5979, Validation Accuracy:0.2471\n",
    "Epoch #134: Loss:1.5923, Accuracy:0.2534, Validation Loss:1.5970, Validation Accuracy:0.2455\n",
    "Epoch #135: Loss:1.5917, Accuracy:0.2542, Validation Loss:1.5979, Validation Accuracy:0.2492\n",
    "Epoch #136: Loss:1.5921, Accuracy:0.2515, Validation Loss:1.5982, Validation Accuracy:0.2512\n",
    "Epoch #137: Loss:1.5920, Accuracy:0.2568, Validation Loss:1.5963, Validation Accuracy:0.2455\n",
    "Epoch #138: Loss:1.5918, Accuracy:0.2544, Validation Loss:1.5960, Validation Accuracy:0.2479\n",
    "Epoch #139: Loss:1.5917, Accuracy:0.2592, Validation Loss:1.5967, Validation Accuracy:0.2479\n",
    "Epoch #140: Loss:1.5922, Accuracy:0.2545, Validation Loss:1.5960, Validation Accuracy:0.2434\n",
    "Epoch #141: Loss:1.5912, Accuracy:0.2533, Validation Loss:1.5975, Validation Accuracy:0.2467\n",
    "Epoch #142: Loss:1.5909, Accuracy:0.2531, Validation Loss:1.5966, Validation Accuracy:0.2447\n",
    "Epoch #143: Loss:1.5912, Accuracy:0.2584, Validation Loss:1.5964, Validation Accuracy:0.2479\n",
    "Epoch #144: Loss:1.5913, Accuracy:0.2563, Validation Loss:1.5987, Validation Accuracy:0.2451\n",
    "Epoch #145: Loss:1.5927, Accuracy:0.2522, Validation Loss:1.5964, Validation Accuracy:0.2455\n",
    "Epoch #146: Loss:1.5906, Accuracy:0.2540, Validation Loss:1.5973, Validation Accuracy:0.2492\n",
    "Epoch #147: Loss:1.5904, Accuracy:0.2584, Validation Loss:1.5972, Validation Accuracy:0.2455\n",
    "Epoch #148: Loss:1.5913, Accuracy:0.2601, Validation Loss:1.5962, Validation Accuracy:0.2426\n",
    "Epoch #149: Loss:1.5915, Accuracy:0.2545, Validation Loss:1.5955, Validation Accuracy:0.2451\n",
    "Epoch #150: Loss:1.5897, Accuracy:0.2557, Validation Loss:1.5967, Validation Accuracy:0.2434\n",
    "Epoch #151: Loss:1.5896, Accuracy:0.2549, Validation Loss:1.5957, Validation Accuracy:0.2504\n",
    "Epoch #152: Loss:1.5900, Accuracy:0.2531, Validation Loss:1.5959, Validation Accuracy:0.2504\n",
    "Epoch #153: Loss:1.5890, Accuracy:0.2573, Validation Loss:1.5964, Validation Accuracy:0.2484\n",
    "Epoch #154: Loss:1.5893, Accuracy:0.2589, Validation Loss:1.5965, Validation Accuracy:0.2488\n",
    "Epoch #155: Loss:1.5895, Accuracy:0.2549, Validation Loss:1.5966, Validation Accuracy:0.2479\n",
    "Epoch #156: Loss:1.5886, Accuracy:0.2571, Validation Loss:1.5959, Validation Accuracy:0.2422\n",
    "Epoch #157: Loss:1.5903, Accuracy:0.2570, Validation Loss:1.5955, Validation Accuracy:0.2488\n",
    "Epoch #158: Loss:1.5897, Accuracy:0.2559, Validation Loss:1.5965, Validation Accuracy:0.2504\n",
    "Epoch #159: Loss:1.5903, Accuracy:0.2552, Validation Loss:1.5971, Validation Accuracy:0.2533\n",
    "Epoch #160: Loss:1.5892, Accuracy:0.2600, Validation Loss:1.5963, Validation Accuracy:0.2492\n",
    "Epoch #161: Loss:1.5906, Accuracy:0.2516, Validation Loss:1.5973, Validation Accuracy:0.2549\n",
    "Epoch #162: Loss:1.5887, Accuracy:0.2590, Validation Loss:1.5954, Validation Accuracy:0.2496\n",
    "Epoch #163: Loss:1.5881, Accuracy:0.2614, Validation Loss:1.5960, Validation Accuracy:0.2549\n",
    "Epoch #164: Loss:1.5895, Accuracy:0.2521, Validation Loss:1.5956, Validation Accuracy:0.2549\n",
    "Epoch #165: Loss:1.5888, Accuracy:0.2587, Validation Loss:1.5950, Validation Accuracy:0.2533\n",
    "Epoch #166: Loss:1.5890, Accuracy:0.2599, Validation Loss:1.5954, Validation Accuracy:0.2504\n",
    "Epoch #167: Loss:1.5896, Accuracy:0.2536, Validation Loss:1.5953, Validation Accuracy:0.2557\n",
    "Epoch #168: Loss:1.5881, Accuracy:0.2561, Validation Loss:1.5946, Validation Accuracy:0.2553\n",
    "Epoch #169: Loss:1.5882, Accuracy:0.2598, Validation Loss:1.5949, Validation Accuracy:0.2557\n",
    "Epoch #170: Loss:1.5882, Accuracy:0.2550, Validation Loss:1.5980, Validation Accuracy:0.2545\n",
    "Epoch #171: Loss:1.5893, Accuracy:0.2574, Validation Loss:1.5953, Validation Accuracy:0.2549\n",
    "Epoch #172: Loss:1.5883, Accuracy:0.2583, Validation Loss:1.5954, Validation Accuracy:0.2508\n",
    "Epoch #173: Loss:1.5880, Accuracy:0.2550, Validation Loss:1.5949, Validation Accuracy:0.2500\n",
    "Epoch #174: Loss:1.5891, Accuracy:0.2538, Validation Loss:1.5955, Validation Accuracy:0.2545\n",
    "Epoch #175: Loss:1.5887, Accuracy:0.2597, Validation Loss:1.5934, Validation Accuracy:0.2492\n",
    "Epoch #176: Loss:1.5886, Accuracy:0.2562, Validation Loss:1.5942, Validation Accuracy:0.2529\n",
    "Epoch #177: Loss:1.5898, Accuracy:0.2502, Validation Loss:1.5945, Validation Accuracy:0.2488\n",
    "Epoch #178: Loss:1.5885, Accuracy:0.2494, Validation Loss:1.5941, Validation Accuracy:0.2512\n",
    "Epoch #179: Loss:1.5875, Accuracy:0.2575, Validation Loss:1.5938, Validation Accuracy:0.2533\n",
    "Epoch #180: Loss:1.5873, Accuracy:0.2602, Validation Loss:1.5941, Validation Accuracy:0.2557\n",
    "Epoch #181: Loss:1.5871, Accuracy:0.2602, Validation Loss:1.5930, Validation Accuracy:0.2553\n",
    "Epoch #182: Loss:1.5871, Accuracy:0.2599, Validation Loss:1.5935, Validation Accuracy:0.2562\n",
    "Epoch #183: Loss:1.5867, Accuracy:0.2605, Validation Loss:1.5924, Validation Accuracy:0.2533\n",
    "Epoch #184: Loss:1.5871, Accuracy:0.2597, Validation Loss:1.5924, Validation Accuracy:0.2549\n",
    "Epoch #185: Loss:1.5865, Accuracy:0.2607, Validation Loss:1.5926, Validation Accuracy:0.2529\n",
    "Epoch #186: Loss:1.5868, Accuracy:0.2609, Validation Loss:1.5946, Validation Accuracy:0.2545\n",
    "Epoch #187: Loss:1.5896, Accuracy:0.2574, Validation Loss:1.5928, Validation Accuracy:0.2562\n",
    "Epoch #188: Loss:1.5873, Accuracy:0.2560, Validation Loss:1.5952, Validation Accuracy:0.2553\n",
    "Epoch #189: Loss:1.5868, Accuracy:0.2597, Validation Loss:1.5942, Validation Accuracy:0.2557\n",
    "Epoch #190: Loss:1.5869, Accuracy:0.2599, Validation Loss:1.5934, Validation Accuracy:0.2553\n",
    "Epoch #191: Loss:1.5861, Accuracy:0.2603, Validation Loss:1.5936, Validation Accuracy:0.2533\n",
    "Epoch #192: Loss:1.5866, Accuracy:0.2608, Validation Loss:1.5936, Validation Accuracy:0.2557\n",
    "Epoch #193: Loss:1.5863, Accuracy:0.2589, Validation Loss:1.5939, Validation Accuracy:0.2525\n",
    "Epoch #194: Loss:1.5863, Accuracy:0.2578, Validation Loss:1.5933, Validation Accuracy:0.2570\n",
    "Epoch #195: Loss:1.5861, Accuracy:0.2577, Validation Loss:1.5940, Validation Accuracy:0.2529\n",
    "Epoch #196: Loss:1.5867, Accuracy:0.2565, Validation Loss:1.5933, Validation Accuracy:0.2529\n",
    "Epoch #197: Loss:1.5867, Accuracy:0.2578, Validation Loss:1.5917, Validation Accuracy:0.2549\n",
    "Epoch #198: Loss:1.5859, Accuracy:0.2593, Validation Loss:1.5937, Validation Accuracy:0.2521\n",
    "Epoch #199: Loss:1.5881, Accuracy:0.2575, Validation Loss:1.5947, Validation Accuracy:0.2545\n",
    "Epoch #200: Loss:1.5866, Accuracy:0.2570, Validation Loss:1.5946, Validation Accuracy:0.2557\n",
    "Epoch #201: Loss:1.5859, Accuracy:0.2600, Validation Loss:1.5931, Validation Accuracy:0.2557\n",
    "Epoch #202: Loss:1.5856, Accuracy:0.2587, Validation Loss:1.5928, Validation Accuracy:0.2545\n",
    "Epoch #203: Loss:1.5855, Accuracy:0.2602, Validation Loss:1.5922, Validation Accuracy:0.2553\n",
    "Epoch #204: Loss:1.5869, Accuracy:0.2566, Validation Loss:1.5934, Validation Accuracy:0.2529\n",
    "Epoch #205: Loss:1.5865, Accuracy:0.2552, Validation Loss:1.5927, Validation Accuracy:0.2516\n",
    "Epoch #206: Loss:1.5855, Accuracy:0.2574, Validation Loss:1.5924, Validation Accuracy:0.2533\n",
    "Epoch #207: Loss:1.5857, Accuracy:0.2533, Validation Loss:1.5932, Validation Accuracy:0.2549\n",
    "Epoch #208: Loss:1.5851, Accuracy:0.2582, Validation Loss:1.5921, Validation Accuracy:0.2533\n",
    "Epoch #209: Loss:1.5848, Accuracy:0.2593, Validation Loss:1.5925, Validation Accuracy:0.2529\n",
    "Epoch #210: Loss:1.5854, Accuracy:0.2559, Validation Loss:1.5931, Validation Accuracy:0.2549\n",
    "Epoch #211: Loss:1.5855, Accuracy:0.2603, Validation Loss:1.5917, Validation Accuracy:0.2549\n",
    "Epoch #212: Loss:1.5848, Accuracy:0.2608, Validation Loss:1.5926, Validation Accuracy:0.2545\n",
    "Epoch #213: Loss:1.5844, Accuracy:0.2612, Validation Loss:1.5907, Validation Accuracy:0.2557\n",
    "Epoch #214: Loss:1.5848, Accuracy:0.2571, Validation Loss:1.5929, Validation Accuracy:0.2566\n",
    "Epoch #215: Loss:1.5858, Accuracy:0.2607, Validation Loss:1.5928, Validation Accuracy:0.2496\n",
    "Epoch #216: Loss:1.5855, Accuracy:0.2562, Validation Loss:1.5930, Validation Accuracy:0.2533\n",
    "Epoch #217: Loss:1.5846, Accuracy:0.2565, Validation Loss:1.5934, Validation Accuracy:0.2525\n",
    "Epoch #218: Loss:1.5841, Accuracy:0.2606, Validation Loss:1.5915, Validation Accuracy:0.2566\n",
    "Epoch #219: Loss:1.5847, Accuracy:0.2545, Validation Loss:1.5918, Validation Accuracy:0.2529\n",
    "Epoch #220: Loss:1.5842, Accuracy:0.2585, Validation Loss:1.5926, Validation Accuracy:0.2557\n",
    "Epoch #221: Loss:1.5857, Accuracy:0.2569, Validation Loss:1.5931, Validation Accuracy:0.2537\n",
    "Epoch #222: Loss:1.5857, Accuracy:0.2571, Validation Loss:1.5930, Validation Accuracy:0.2529\n",
    "Epoch #223: Loss:1.5844, Accuracy:0.2542, Validation Loss:1.5917, Validation Accuracy:0.2541\n",
    "Epoch #224: Loss:1.5839, Accuracy:0.2575, Validation Loss:1.5926, Validation Accuracy:0.2422\n",
    "Epoch #225: Loss:1.5835, Accuracy:0.2547, Validation Loss:1.5911, Validation Accuracy:0.2537\n",
    "Epoch #226: Loss:1.5833, Accuracy:0.2582, Validation Loss:1.5919, Validation Accuracy:0.2566\n",
    "Epoch #227: Loss:1.5843, Accuracy:0.2560, Validation Loss:1.5915, Validation Accuracy:0.2570\n",
    "Epoch #228: Loss:1.5842, Accuracy:0.2600, Validation Loss:1.5916, Validation Accuracy:0.2537\n",
    "Epoch #229: Loss:1.5864, Accuracy:0.2603, Validation Loss:1.5916, Validation Accuracy:0.2537\n",
    "Epoch #230: Loss:1.5840, Accuracy:0.2574, Validation Loss:1.5916, Validation Accuracy:0.2566\n",
    "Epoch #231: Loss:1.5833, Accuracy:0.2565, Validation Loss:1.5906, Validation Accuracy:0.2533\n",
    "Epoch #232: Loss:1.5836, Accuracy:0.2579, Validation Loss:1.5905, Validation Accuracy:0.2553\n",
    "Epoch #233: Loss:1.5841, Accuracy:0.2610, Validation Loss:1.5888, Validation Accuracy:0.2537\n",
    "Epoch #234: Loss:1.5835, Accuracy:0.2576, Validation Loss:1.5890, Validation Accuracy:0.2508\n",
    "Epoch #235: Loss:1.5838, Accuracy:0.2581, Validation Loss:1.5896, Validation Accuracy:0.2533\n",
    "Epoch #236: Loss:1.5835, Accuracy:0.2593, Validation Loss:1.5916, Validation Accuracy:0.2549\n",
    "Epoch #237: Loss:1.5827, Accuracy:0.2587, Validation Loss:1.5925, Validation Accuracy:0.2566\n",
    "Epoch #238: Loss:1.5835, Accuracy:0.2600, Validation Loss:1.5911, Validation Accuracy:0.2574\n",
    "Epoch #239: Loss:1.5830, Accuracy:0.2604, Validation Loss:1.5897, Validation Accuracy:0.2566\n",
    "Epoch #240: Loss:1.5828, Accuracy:0.2604, Validation Loss:1.5898, Validation Accuracy:0.2578\n",
    "Epoch #241: Loss:1.5824, Accuracy:0.2611, Validation Loss:1.5899, Validation Accuracy:0.2553\n",
    "Epoch #242: Loss:1.5827, Accuracy:0.2608, Validation Loss:1.5899, Validation Accuracy:0.2557\n",
    "Epoch #243: Loss:1.5825, Accuracy:0.2588, Validation Loss:1.5915, Validation Accuracy:0.2385\n",
    "Epoch #244: Loss:1.5841, Accuracy:0.2629, Validation Loss:1.5896, Validation Accuracy:0.2529\n",
    "Epoch #245: Loss:1.5828, Accuracy:0.2597, Validation Loss:1.5907, Validation Accuracy:0.2557\n",
    "Epoch #246: Loss:1.5816, Accuracy:0.2620, Validation Loss:1.5890, Validation Accuracy:0.2537\n",
    "Epoch #247: Loss:1.5821, Accuracy:0.2579, Validation Loss:1.5900, Validation Accuracy:0.2553\n",
    "Epoch #248: Loss:1.5820, Accuracy:0.2610, Validation Loss:1.5894, Validation Accuracy:0.2557\n",
    "Epoch #249: Loss:1.5830, Accuracy:0.2640, Validation Loss:1.5894, Validation Accuracy:0.2578\n",
    "Epoch #250: Loss:1.5821, Accuracy:0.2640, Validation Loss:1.5893, Validation Accuracy:0.2562\n",
    "Epoch #251: Loss:1.5818, Accuracy:0.2603, Validation Loss:1.5900, Validation Accuracy:0.2422\n",
    "Epoch #252: Loss:1.5828, Accuracy:0.2563, Validation Loss:1.5904, Validation Accuracy:0.2414\n",
    "Epoch #253: Loss:1.5815, Accuracy:0.2610, Validation Loss:1.5897, Validation Accuracy:0.2562\n",
    "Epoch #254: Loss:1.5834, Accuracy:0.2577, Validation Loss:1.5926, Validation Accuracy:0.2570\n",
    "Epoch #255: Loss:1.5812, Accuracy:0.2597, Validation Loss:1.5899, Validation Accuracy:0.2574\n",
    "Epoch #256: Loss:1.5813, Accuracy:0.2624, Validation Loss:1.5891, Validation Accuracy:0.2557\n",
    "Epoch #257: Loss:1.5815, Accuracy:0.2623, Validation Loss:1.5896, Validation Accuracy:0.2562\n",
    "Epoch #258: Loss:1.5806, Accuracy:0.2633, Validation Loss:1.5896, Validation Accuracy:0.2603\n",
    "Epoch #259: Loss:1.5816, Accuracy:0.2579, Validation Loss:1.5899, Validation Accuracy:0.2418\n",
    "Epoch #260: Loss:1.5812, Accuracy:0.2529, Validation Loss:1.5901, Validation Accuracy:0.2562\n",
    "Epoch #261: Loss:1.5816, Accuracy:0.2604, Validation Loss:1.5911, Validation Accuracy:0.2406\n",
    "Epoch #262: Loss:1.5829, Accuracy:0.2539, Validation Loss:1.5905, Validation Accuracy:0.2562\n",
    "Epoch #263: Loss:1.5819, Accuracy:0.2633, Validation Loss:1.5895, Validation Accuracy:0.2553\n",
    "Epoch #264: Loss:1.5813, Accuracy:0.2612, Validation Loss:1.5898, Validation Accuracy:0.2557\n",
    "Epoch #265: Loss:1.5820, Accuracy:0.2563, Validation Loss:1.5915, Validation Accuracy:0.2541\n",
    "Epoch #266: Loss:1.5815, Accuracy:0.2602, Validation Loss:1.5901, Validation Accuracy:0.2557\n",
    "Epoch #267: Loss:1.5804, Accuracy:0.2610, Validation Loss:1.5914, Validation Accuracy:0.2401\n",
    "Epoch #268: Loss:1.5802, Accuracy:0.2617, Validation Loss:1.5899, Validation Accuracy:0.2607\n",
    "Epoch #269: Loss:1.5809, Accuracy:0.2641, Validation Loss:1.5909, Validation Accuracy:0.2599\n",
    "Epoch #270: Loss:1.5805, Accuracy:0.2649, Validation Loss:1.5896, Validation Accuracy:0.2562\n",
    "Epoch #271: Loss:1.5790, Accuracy:0.2667, Validation Loss:1.5894, Validation Accuracy:0.2381\n",
    "Epoch #272: Loss:1.5803, Accuracy:0.2610, Validation Loss:1.5888, Validation Accuracy:0.2566\n",
    "Epoch #273: Loss:1.5799, Accuracy:0.2655, Validation Loss:1.5887, Validation Accuracy:0.2537\n",
    "Epoch #274: Loss:1.5791, Accuracy:0.2653, Validation Loss:1.5880, Validation Accuracy:0.2566\n",
    "Epoch #275: Loss:1.5790, Accuracy:0.2662, Validation Loss:1.5876, Validation Accuracy:0.2590\n",
    "Epoch #276: Loss:1.5796, Accuracy:0.2654, Validation Loss:1.5899, Validation Accuracy:0.2590\n",
    "Epoch #277: Loss:1.5793, Accuracy:0.2663, Validation Loss:1.5873, Validation Accuracy:0.2562\n",
    "Epoch #278: Loss:1.5799, Accuracy:0.2634, Validation Loss:1.5869, Validation Accuracy:0.2590\n",
    "Epoch #279: Loss:1.5797, Accuracy:0.2664, Validation Loss:1.5881, Validation Accuracy:0.2578\n",
    "Epoch #280: Loss:1.5791, Accuracy:0.2651, Validation Loss:1.5882, Validation Accuracy:0.2607\n",
    "Epoch #281: Loss:1.5798, Accuracy:0.2627, Validation Loss:1.5883, Validation Accuracy:0.2607\n",
    "Epoch #282: Loss:1.5788, Accuracy:0.2608, Validation Loss:1.5886, Validation Accuracy:0.2426\n",
    "Epoch #283: Loss:1.5806, Accuracy:0.2669, Validation Loss:1.5907, Validation Accuracy:0.2570\n",
    "Epoch #284: Loss:1.5806, Accuracy:0.2616, Validation Loss:1.5897, Validation Accuracy:0.2406\n",
    "Epoch #285: Loss:1.5790, Accuracy:0.2632, Validation Loss:1.5900, Validation Accuracy:0.2434\n",
    "Epoch #286: Loss:1.5787, Accuracy:0.2609, Validation Loss:1.5884, Validation Accuracy:0.2566\n",
    "Epoch #287: Loss:1.5786, Accuracy:0.2627, Validation Loss:1.5889, Validation Accuracy:0.2566\n",
    "Epoch #288: Loss:1.5793, Accuracy:0.2650, Validation Loss:1.5899, Validation Accuracy:0.2570\n",
    "Epoch #289: Loss:1.5791, Accuracy:0.2666, Validation Loss:1.5888, Validation Accuracy:0.2566\n",
    "Epoch #290: Loss:1.5786, Accuracy:0.2659, Validation Loss:1.5889, Validation Accuracy:0.2566\n",
    "Epoch #291: Loss:1.5783, Accuracy:0.2667, Validation Loss:1.5888, Validation Accuracy:0.2582\n",
    "Epoch #292: Loss:1.5789, Accuracy:0.2656, Validation Loss:1.5876, Validation Accuracy:0.2570\n",
    "Epoch #293: Loss:1.5786, Accuracy:0.2652, Validation Loss:1.5881, Validation Accuracy:0.2599\n",
    "Epoch #294: Loss:1.5779, Accuracy:0.2675, Validation Loss:1.5889, Validation Accuracy:0.2599\n",
    "Epoch #295: Loss:1.5791, Accuracy:0.2658, Validation Loss:1.5859, Validation Accuracy:0.2574\n",
    "Epoch #296: Loss:1.5784, Accuracy:0.2672, Validation Loss:1.5891, Validation Accuracy:0.2525\n",
    "Epoch #297: Loss:1.5777, Accuracy:0.2656, Validation Loss:1.5902, Validation Accuracy:0.2418\n",
    "Epoch #298: Loss:1.5774, Accuracy:0.2647, Validation Loss:1.5886, Validation Accuracy:0.2529\n",
    "Epoch #299: Loss:1.5779, Accuracy:0.2630, Validation Loss:1.5884, Validation Accuracy:0.2430\n",
    "Epoch #300: Loss:1.5802, Accuracy:0.2569, Validation Loss:1.5871, Validation Accuracy:0.2578\n",
    "\n",
    "Test:\n",
    "Test Loss:1.58711982, Accuracy:0.2578\n",
    "Labels: ['05', '03', '01', '04', '02']\n",
    "Confusion Matrix:\n",
    "       05  03  01   04  02\n",
    "t:05  446  18  15   83   5\n",
    "t:03  355  28  19   51   6\n",
    "t:01  393  16  19   71   4\n",
    "t:04  292  17  14  123   4\n",
    "t:02  324  14   8   99  12\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.25      0.79      0.38       567\n",
    "          03       0.30      0.06      0.10       459\n",
    "          01       0.25      0.04      0.07       503\n",
    "          04       0.29      0.27      0.28       450\n",
    "          02       0.39      0.03      0.05       457\n",
    "\n",
    "    accuracy                           0.26      2436\n",
    "   macro avg       0.30      0.24      0.17      2436\n",
    "weighted avg       0.29      0.26      0.18      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 18:02:11 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 18 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6057966477765238, 1.6047223197611291, 1.6046388082707848, 1.6039695068337452, 1.6038990502287014, 1.6038090769684765, 1.6032032350013996, 1.6030849433689087, 1.602899056154323, 1.6029994797041067, 1.6026845925742965, 1.6019066147420598, 1.601481902775506, 1.6014841192070095, 1.6014607788502484, 1.6017355991310283, 1.601077374174873, 1.6010731744453042, 1.6012188969378793, 1.6012382233279876, 1.600937209497336, 1.6005997593179713, 1.6005808995862312, 1.6008085754313102, 1.6007852078658606, 1.6002141953689124, 1.6002278179174965, 1.600426025774287, 1.6003009347101347, 1.6000125271150436, 1.600398599612106, 1.6009118932808561, 1.6012401089488189, 1.6010868447559026, 1.601299236756436, 1.6011656949477047, 1.6011014082553157, 1.600849551128832, 1.600062658829838, 1.5999229602234313, 1.5998338795647833, 1.6008761291237692, 1.6010134159442044, 1.6005015985914834, 1.601084292424332, 1.600793822449808, 1.5999332190734412, 1.6003331511674452, 1.599827327360269, 1.599437547630473, 1.5997024498549588, 1.6011099423876733, 1.601132020770231, 1.5996052676625243, 1.5989147117376719, 1.600534595096444, 1.600428259039943, 1.6014308025097024, 1.6008535707721179, 1.6019488210944315, 1.601083486929707, 1.6008678837167023, 1.6011338686120922, 1.601310534430255, 1.6015562126397695, 1.6015232069347487, 1.6019623066208437, 1.6021962443791782, 1.6016679863232892, 1.6016089098011135, 1.6020916992024639, 1.6015818992076054, 1.6015613486222642, 1.6013649825392098, 1.6007783814212568, 1.6008396495151989, 1.6018944839734357, 1.6012052246698214, 1.6014650590314066, 1.602468085797941, 1.6016344090400658, 1.6017848241505364, 1.6013202954982888, 1.601912041015813, 1.60226643751016, 1.602730817982716, 1.6029090828496246, 1.6018024626232328, 1.601510274977911, 1.6004430439280368, 1.6001674553443646, 1.6008950148897219, 1.6002884745010602, 1.6012391605596432, 1.5998851379933223, 1.5999014418700646, 1.6004669770035642, 1.5994664669428358, 1.5993299349188217, 1.5992502866707412, 1.5995574892056594, 1.6005541405262813, 1.5998515047267545, 1.6000372281019715, 1.5999529118999862, 1.5996036590222262, 1.5997214524812495, 1.6003960091100733, 1.6002668011168932, 1.6001683132989066, 1.5998233550875058, 1.6004001669500065, 1.600494862190021, 1.599775259522186, 1.601901104689036, 1.6009598492793067, 1.6004670298549732, 1.5989416951224917, 1.598647351726914, 1.599088280267512, 1.5996887253227297, 1.599322439218781, 1.5988781831926118, 1.598841696928679, 1.5984193094453985, 1.5985768226958652, 1.5989433593546425, 1.5983993464894286, 1.5990618163924695, 1.5979475945674728, 1.5984313873626133, 1.598809910525242, 1.5978859854840683, 1.5969753838916523, 1.5979322406458738, 1.598194156765742, 1.596274143369327, 1.595979957157755, 1.5967045262920836, 1.596007881884896, 1.5974717422071936, 1.596552033533995, 1.5964100339338305, 1.5986949949233207, 1.5964103753147845, 1.5973295192608887, 1.5971694667938308, 1.5961667563527675, 1.59547467497965, 1.5966886015752657, 1.5957165981944168, 1.59592749939372, 1.5964343692673055, 1.5965370012230082, 1.5966388396245896, 1.595932960118762, 1.5954810167572573, 1.5965444841995615, 1.5970704851087874, 1.5962683483101856, 1.5972502536961597, 1.5954345781814876, 1.59603889176411, 1.5956235008286725, 1.5950392388749397, 1.5953879834004419, 1.5952862336717804, 1.5946446373349143, 1.594902671029415, 1.5980171493708795, 1.5953312325360152, 1.5953733915178647, 1.5948538537487411, 1.595464488947137, 1.5933840443152316, 1.5941739702851119, 1.5944601481379743, 1.5941350099879925, 1.593819631340077, 1.594141584116054, 1.5929581801879582, 1.5934620580845473, 1.5924353131715496, 1.5923556343870993, 1.5926136839370226, 1.5945713743200443, 1.5928299047285308, 1.595240548913702, 1.5941667178972994, 1.5933759159642487, 1.5935968424886318, 1.5936197271487984, 1.5938587214167679, 1.5933483600224962, 1.5940149676036366, 1.5933100165208964, 1.5917160963190014, 1.5936534289264523, 1.5946893294652302, 1.5946090783195934, 1.5930732177396125, 1.5928039971635064, 1.5921835120284107, 1.593410089685412, 1.5927310021248553, 1.592355596608129, 1.5931673134097522, 1.5921275531521375, 1.5924781425833114, 1.5931008903459571, 1.5917420988208164, 1.5926030658931762, 1.5906962050592959, 1.5928649859279638, 1.5928248014356114, 1.5929679486943387, 1.5933737431840944, 1.5915117903883234, 1.5918051483987392, 1.5926341004363813, 1.5930990881129048, 1.5930285060347007, 1.5916600879190004, 1.5925547849564325, 1.5910991258026148, 1.5918618699012719, 1.59147987107338, 1.5916315655794442, 1.5916018750279994, 1.591562538702891, 1.5906300180651285, 1.5905331097212918, 1.5887623277595282, 1.5889650863183935, 1.5896056669294736, 1.591594855773625, 1.592481285088951, 1.5910905593721738, 1.5896742657096123, 1.5898301666006078, 1.5898939202767484, 1.5898891716755081, 1.5914588921958785, 1.589558560468489, 1.590680826082214, 1.589048982058057, 1.590041765830004, 1.5893858040886364, 1.589380300299483, 1.589259021192153, 1.5900499409642712, 1.590433071986795, 1.5897343556086223, 1.5926184270573758, 1.5899077238903452, 1.5891487569057297, 1.5895938926142426, 1.5895622050625153, 1.5899132999097576, 1.5901451692205344, 1.5910753835793983, 1.5905140186178273, 1.5895343840807334, 1.5897777037471776, 1.5915162118980646, 1.5901112916630085, 1.5913933609506767, 1.589929598109867, 1.5908649143914284, 1.5895949459232523, 1.5893790228613491, 1.5888128237575536, 1.5887129690455295, 1.5880392623456632, 1.587601436770021, 1.5899235315510791, 1.5872700785964189, 1.586899288965172, 1.5880637082756055, 1.5881861620544409, 1.5883197410549046, 1.58860259749032, 1.5906954813864822, 1.5897169367628927, 1.5899738682118933, 1.5883838476610106, 1.588882765354977, 1.589926760576433, 1.588819566227141, 1.588932513016198, 1.5888166179014935, 1.587629789593576, 1.5880809200221095, 1.588866261425864, 1.5858914168988933, 1.5890773354688497, 1.5901724618839708, 1.5885887625573696, 1.5884400660964264, 1.587119738065159], 'val_acc': [0.2274220030883263, 0.23275862068965517, 0.23275862068965517, 0.23275862068965517, 0.23891625613316722, 0.24014778325123154, 0.23973727422003285, 0.23275862068965517, 0.2393267651643659, 0.2356321838591095, 0.23275862068965517, 0.2397372741466281, 0.24343185545188453, 0.24014778090228, 0.2422003283827567, 0.24220032833382021, 0.24343185545188453, 0.24178981935155802, 0.23891625608423073, 0.23809523809523808, 0.2360426928903082, 0.2434318531518695, 0.23522167492578377, 0.23768472676402438, 0.23645320192150687, 0.24137931044270056, 0.24014778090228, 0.23809523579522307, 0.23809523579522307, 0.240147783300168, 0.233990147758783, 0.2360426927924352, 0.2327586205917822, 0.2344006568144499, 0.23522167477897432, 0.23604269286583993, 0.2372742176838892, 0.2364532018970386, 0.2356321838591095, 0.23686371090376906, 0.24096880121575592, 0.23768472896616644, 0.24014778090228, 0.23891625380868395, 0.2360426928903082, 0.2417898191313438, 0.23809523804630162, 0.2393267649930882, 0.23809523809523808, 0.24220032833382021, 0.23481116577224387, 0.23809523785055564, 0.23850574700409555, 0.238916256010826, 0.2409688011423512, 0.2372742199104995, 0.23932676513989767, 0.23399014773431473, 0.24137931019801812, 0.24220032833382021, 0.2401477809512165, 0.23809523804630162, 0.23809523802183336, 0.24302134624940813, 0.24055829201327952, 0.24302134624940813, 0.23932676509096118, 0.24548440046106848, 0.2409688010444782, 0.24178981910687558, 0.24425287348981364, 0.24261083731608243, 0.24302134624940813, 0.2434318553295433, 0.243842364360742, 0.24343185535401154, 0.24302134629834463, 0.2417898191802803, 0.24302134629834463, 0.24220032830935198, 0.243842364360742, 0.23891625586401652, 0.24178981925368503, 0.24343185535401154, 0.24178981925368503, 0.2413793100756769, 0.2438423644096785, 0.24055829203774776, 0.24343185540294804, 0.24384236433627374, 0.24261083721820945, 0.2446633824965441, 0.2380952378260874, 0.24261083734055067, 0.2413793100756769, 0.24548440053447323, 0.2463054185968706, 0.24343185535401154, 0.24548440055894147, 0.24548440055894147, 0.2463054186458071, 0.24712643670820447, 0.2413793100756769, 0.24712643670820447, 0.24507389145433806, 0.24384236438521023, 0.24548440055894147, 0.24671592765253753, 0.24384236443414672, 0.24425287339194068, 0.2463054185968706, 0.2463054185968706, 0.24753694569046666, 0.24589490956567192, 0.24589490951673543, 0.23850574693069082, 0.24671592765253753, 0.24753694569046666, 0.2475369457149349, 0.24466338247207586, 0.24794745477060184, 0.2467159276770058, 0.24589490959014015, 0.24876847285746745, 0.24835796382626876, 0.24507389150327455, 0.24425287344087718, 0.24835796377733227, 0.24589490956567192, 0.24630541862133884, 0.2491789818641979, 0.24835796387520526, 0.2471264367326727, 0.24548440055894147, 0.24917898183972964, 0.2512315270691278, 0.24548440053447323, 0.24794745479507008, 0.24794745479507008, 0.24343185530507505, 0.2467159276770058, 0.2446633823986711, 0.24794745474613358, 0.24507389157667928, 0.24548440065681446, 0.24917898193760263, 0.24548440060787796, 0.24261083726714594, 0.24507389150327455, 0.24343185535401154, 0.2504105090311987, 0.2504105066822472, 0.24835796380180053, 0.2487684728819357, 0.24794745477060184, 0.2422003282604155, 0.24876847285746745, 0.2504105090801352, 0.25328406990063795, 0.2491789819131344, 0.2549261060254327, 0.2495894908953966, 0.25492610842332075, 0.2549261060254327, 0.2532840722495895, 0.2504105066822472, 0.2557471265346546, 0.2553366175034559, 0.2557471265346546, 0.25451559939212204, 0.2549261084722572, 0.25082101571344584, 0.24999999992659527, 0.25451559939212204, 0.24917898183972964, 0.25287356091837576, 0.24876847283299922, 0.2512315269957231, 0.253284072298526, 0.2557471241367666, 0.2553366151055679, 0.25615763316796525, 0.253284072298526, 0.2549261060254327, 0.25287356319392257, 0.25451559699423404, 0.25615763321690177, 0.2553366175034559, 0.2557471241367666, 0.2553366151055679, 0.2532840722251212, 0.2557471241367666, 0.2524630518382406, 0.2569786512303626, 0.25287356319392257, 0.25287356321839083, 0.2549261060254327, 0.2520525452049299, 0.25451559699423404, 0.2557471241367666, 0.2557471241367666, 0.25451559699423404, 0.2553366151055679, 0.2528735608694393, 0.25164203605139, 0.2532840722251212, 0.2549261060743692, 0.2532840722251212, 0.25287356319392257, 0.2549261060743692, 0.2549261060743692, 0.2545155970431705, 0.2557471241367666, 0.25656814219916396, 0.24958949091986482, 0.25328406990063795, 0.25246305416272385, 0.25656814219916396, 0.2528735608694393, 0.2557471241367666, 0.25369457893183667, 0.2528735631694543, 0.2541050879630353, 0.24220032840722497, 0.25369458125631994, 0.25656814219916396, 0.2569786512303626, 0.25369457898077313, 0.25369458132972467, 0.25656814219916396, 0.25328406990063795, 0.2553366151055679, 0.25369457893183667, 0.2508210180379291, 0.25328406990063795, 0.2549261060743692, 0.25656814219916396, 0.25738916026156133, 0.25656814224810043, 0.25779967169064805, 0.2553366151055679, 0.25574712418570306, 0.2385057470775003, 0.2528735608694393, 0.2557471241367666, 0.25369457893183667, 0.2553366151055679, 0.2557471241367666, 0.2577996692438235, 0.25615763321690177, 0.24220032605827344, 0.2413793102958911, 0.2561576354679803, 0.25697865118142615, 0.25738916261051287, 0.2557471241367666, 0.25615763321690177, 0.26026272352888863, 0.24178981702707475, 0.2561576354679803, 0.24055829228243022, 0.25615763537010733, 0.2553366174545194, 0.25574712641231334, 0.2541050879630353, 0.25574712641231334, 0.24014778090228, 0.2606732325111508, 0.25985221444875345, 0.2561576354679803, 0.23809523807076985, 0.25656814219916396, 0.25369458132972467, 0.25656814459705196, 0.25903119643529254, 0.25903119633741956, 0.2561576355169168, 0.25903119633741956, 0.25779967159277506, 0.2606732325111508, 0.2606732325111508, 0.24261083748736015, 0.2569786536282507, 0.24055829228243022, 0.2434318531518695, 0.256568144499179, 0.256568144499179, 0.2569786535303777, 0.25656814219916396, 0.25656814224810043, 0.25821018067291024, 0.2569786512303626, 0.25985221684664145, 0.25985221444875345, 0.25738916026156133, 0.2524630518382406, 0.24178981942496278, 0.2528735631694543, 0.2430213441206708, 0.25779967159277506], 'loss': [1.6080384185916345, 1.6054512408234989, 1.6048859838587548, 1.6047782368728511, 1.603997814826652, 1.603562035257077, 1.603680326365837, 1.6033624245156008, 1.60328568457578, 1.603986202960631, 1.603332089153893, 1.6028313153088705, 1.6019386221006422, 1.601458572215368, 1.6027406810490257, 1.6014051244978542, 1.6007215769628724, 1.6009466072861909, 1.6013390945947634, 1.6010995371875332, 1.6003295385372467, 1.60024324653085, 1.6001675441279792, 1.600090012951798, 1.6007085392362528, 1.5994835154476597, 1.5997020083531217, 1.5987940551808728, 1.5991987469259963, 1.5989845534859253, 1.5987053454534228, 1.5989015974792857, 1.5985942509140076, 1.5995480578538084, 1.5995513620807404, 1.5984651193971262, 1.5987562171732377, 1.5984262321519167, 1.598634029805538, 1.5985727504048748, 1.597942331537329, 1.5975371051373177, 1.5979892127567739, 1.5983464023660585, 1.5980157155520618, 1.5984040236326214, 1.5978273363818378, 1.5976144916467843, 1.5978176622664904, 1.5984590291976928, 1.597674304797664, 1.597091613559997, 1.5987272647861583, 1.5982951162287342, 1.5969602734156458, 1.5979990187611668, 1.5996488086007215, 1.5977646806156856, 1.5975843656723994, 1.5980183509830577, 1.5968479090402747, 1.5969459352062467, 1.5966335709824455, 1.5965606097567988, 1.597565185020102, 1.5974977350822464, 1.5969614999739785, 1.5969743299288426, 1.5968781120723279, 1.596295537106555, 1.5963294003778414, 1.5958802832715075, 1.5972228261234824, 1.5962018678810073, 1.5978731327722695, 1.5963934223265128, 1.596034966014494, 1.5959261981858366, 1.596603461898083, 1.5960163917384842, 1.5965178795174162, 1.597206082432177, 1.596509095090126, 1.596899380037672, 1.595225089384545, 1.5960627609209848, 1.5957258698387067, 1.5951291254658473, 1.596172619796142, 1.5949786961690602, 1.595132133701254, 1.594972446713849, 1.5943159281105965, 1.5951354647563958, 1.595043682464584, 1.5947221971879995, 1.594729060607769, 1.5946437505236397, 1.5940707455425536, 1.593834036527473, 1.5941093057577616, 1.5948795777326736, 1.594658688500187, 1.5954678985862027, 1.5944651065176272, 1.5949657765746852, 1.5949663561717196, 1.5953498209771189, 1.5936310554676723, 1.5939773093014038, 1.5942454857013553, 1.5934370950996508, 1.5931077622290264, 1.5940185845020616, 1.5928077146013169, 1.596183666801061, 1.5944299815371785, 1.5950404632752435, 1.5932054799684998, 1.5942052559196582, 1.592925223432772, 1.5929024249866024, 1.592346317077809, 1.5923725962883637, 1.5919970424757846, 1.5917256941295992, 1.5917777959815775, 1.5918058207392447, 1.5923247182148927, 1.5916735137512552, 1.5924390954892982, 1.5929824013974387, 1.5922528784377863, 1.5923014070463866, 1.591699275196945, 1.5921294944242286, 1.5920172306056875, 1.5917554573356738, 1.5917235948221884, 1.5921702124499688, 1.591173039961155, 1.5908834456907894, 1.5911690008958508, 1.59129252345655, 1.5926837496199402, 1.5906244241236662, 1.5903964228698606, 1.5913275105752493, 1.5914558821390297, 1.589745285378834, 1.5895672251801227, 1.5899992635852258, 1.5889524506347625, 1.5893064177746157, 1.58945119136168, 1.588613398461861, 1.5903024552049578, 1.589736045753197, 1.5903315984248136, 1.5891813430453228, 1.590588557450923, 1.5886980682917444, 1.588077056040754, 1.5894929891249483, 1.5887635884588505, 1.589034123440298, 1.5895566477667868, 1.5881224996500192, 1.5881895868195646, 1.5881821480130267, 1.5893404747671172, 1.588303384447979, 1.5880211586824922, 1.5890800426628067, 1.5886963401242202, 1.5885537341390057, 1.5897641123687463, 1.588501624405017, 1.5874779738929483, 1.5873252244455858, 1.587139556197415, 1.5871404534737432, 1.5866815749135106, 1.5870527036380964, 1.5865287545770101, 1.586815622552954, 1.5895707559781398, 1.587277229463785, 1.5867512332830096, 1.5868933562135794, 1.5860646095608784, 1.586608982869487, 1.5862955121289044, 1.5862815386950357, 1.5861336692402739, 1.586748477957332, 1.5866638579652539, 1.5858974116049263, 1.5881355784512154, 1.5866340852616014, 1.5859093589214819, 1.585582986406722, 1.5854603357628385, 1.5868794500460615, 1.5865380473695008, 1.5854887481098057, 1.5857362306583098, 1.5850719333918921, 1.5847615359989768, 1.5853585544797675, 1.5854770856226739, 1.5848333507592673, 1.5844370718119816, 1.5848327066864076, 1.5858120050763203, 1.585504039406042, 1.584582918968044, 1.584144775138009, 1.5847267214032903, 1.5842348149180168, 1.5856912557594096, 1.5856708434572944, 1.58444496116599, 1.583926349694724, 1.5835364449440332, 1.5833425896368478, 1.5843023205684685, 1.5841739835190822, 1.586448473656202, 1.58403806505262, 1.5832938689715563, 1.5836061238752988, 1.5840882365463695, 1.5834604933031775, 1.5838392245451283, 1.5834991668528844, 1.5827325371011816, 1.5835080184486123, 1.5829763076143832, 1.5827804884626635, 1.5823954372190596, 1.582696434066036, 1.582497837264435, 1.5841060913074188, 1.5828004320054574, 1.5816396049405514, 1.582086800892495, 1.5820249185425055, 1.583019692550205, 1.5821200623404563, 1.5817871750257833, 1.5827531669663697, 1.5815201357894364, 1.5834051821510895, 1.5812066051259912, 1.5813283845874073, 1.5814873885325094, 1.5806061135180431, 1.581621435192821, 1.5812284786843176, 1.5816469078925601, 1.5829203840153907, 1.5818830375064326, 1.5812505830239956, 1.5820485364240298, 1.5815464743843313, 1.5804470446075503, 1.580200617171411, 1.5808919277524067, 1.5805340242092125, 1.5790399537683757, 1.580288972012561, 1.5798711426693801, 1.5790693486740457, 1.5790408661722892, 1.5795569223544925, 1.5793338693877266, 1.579866890300226, 1.5796541353515532, 1.5790738455813524, 1.579786657503743, 1.578828012037571, 1.5806049186835789, 1.5805891309185929, 1.5790419294604041, 1.5786671247325639, 1.5785525000805238, 1.579312435949118, 1.579140146899762, 1.578563172763378, 1.5782874211148805, 1.5789082182016705, 1.5786291813703532, 1.577887924789648, 1.5790544288603923, 1.5783959785770831, 1.5776870014731155, 1.5774497172670932, 1.5779217664710794, 1.5801896865362994], 'acc': [0.2086242299794661, 0.23326488706365503, 0.23295687886234182, 0.23295687885010266, 0.23685831622176592, 0.23675564682336803, 0.23809034907597537, 0.23336755647429205, 0.23921971253178692, 0.23459958932238192, 0.23388090349687932, 0.24117043119925982, 0.23921971252872715, 0.24096509240858363, 0.23131416836558427, 0.24383983573507234, 0.24106776180698153, 0.2437371663274951, 0.2391170431211499, 0.242402464071828, 0.23747433264887063, 0.2405544147966334, 0.2408624229979466, 0.2429158110913555, 0.24250513347328567, 0.2441478439486247, 0.2364476386036961, 0.24435318272706175, 0.2441478439547443, 0.23850102670016474, 0.23696098563852253, 0.23737166325047276, 0.24168377823408624, 0.23655030801127333, 0.2380903490882145, 0.24075975358119, 0.24188911704618093, 0.24178644764472326, 0.2394250513224631, 0.238911704312115, 0.24496919917864476, 0.24579055442090397, 0.24404517454410726, 0.2437371663305549, 0.24250513347634545, 0.23932238193018482, 0.24548254620123203, 0.24425051335008238, 0.24661190963868487, 0.24835728953078054, 0.24537987678447543, 0.24383983572895276, 0.24373716633667447, 0.24014373716938422, 0.2476386037083377, 0.24804928132028795, 0.24117043121455875, 0.2383983572956473, 0.2469199178644764, 0.2486652977412731, 0.2510266940574137, 0.24496919917864476, 0.24558521560574947, 0.2422997946734301, 0.24979466120320423, 0.2487679671580297, 0.24537987680283416, 0.24373716632443532, 0.2454825462073516, 0.25000000001223915, 0.25000000000611955, 0.2542094455913352, 0.25112936346193115, 0.24979466120320423, 0.24784394250513347, 0.25246406571453844, 0.2510266940482343, 0.2512320328664486, 0.2549281314168378, 0.2532854209506781, 0.25164271047227926, 0.25225872691162315, 0.25205338807810995, 0.2485626283245165, 0.2521560574948665, 0.25492813142295734, 0.2507186858438613, 0.25503080082135526, 0.24917864476997995, 0.25143737166324437, 0.2552361396365097, 0.24845995893223818, 0.25585215606361444, 0.25082135523613963, 0.24958932238804976, 0.25338809035519555, 0.2510266940574137, 0.2554414784455446, 0.25503080082135526, 0.2574948665358937, 0.253388090349076, 0.25112936345581155, 0.25318275154004105, 0.2450718685831622, 0.24938398357901484, 0.24794661191271072, 0.2548254620153801, 0.25205338807810995, 0.2519507186919512, 0.2531827515522802, 0.25277207392197126, 0.2575975359342916, 0.2570841889117043, 0.25698151950718684, 0.2577002053449286, 0.25246406571453844, 0.25318275154004105, 0.24979466120320423, 0.25657084188911705, 0.2532854209445585, 0.2549281314168378, 0.2530800821416432, 0.25605749487264934, 0.2587268993901031, 0.25872689939622273, 0.2564681724907192, 0.2566735112813954, 0.2589322381807793, 0.25616016425880805, 0.2568788501026694, 0.2543121149774939, 0.2553388090226685, 0.2537987679671458, 0.25338809035519555, 0.2542094455913352, 0.25154004108000094, 0.25677618070427155, 0.2544147843942505, 0.25924024640657084, 0.25451745380488755, 0.2532854209445585, 0.25308008213552363, 0.2584188911704312, 0.25626283368780384, 0.2521560574979263, 0.2540041067823003, 0.25841889118267036, 0.26006160164883, 0.2545174538110071, 0.25574948665909697, 0.2549281314168378, 0.25308008213552363, 0.2572895277268588, 0.25893223819301847, 0.25492813142295734, 0.25708418889946516, 0.2569815194949477, 0.25585215606055467, 0.25523613963039016, 0.2599589322412528, 0.2516427104845184, 0.2590349076036555, 0.2613963038891982, 0.25205338809034905, 0.2587268994084619, 0.2598562628336756, 0.25359342916117067, 0.2560574948665298, 0.25975359344139726, 0.25503080082135526, 0.25739219711301753, 0.25831622176897356, 0.2550308008274748, 0.2537987679671458, 0.25965092403076023, 0.25616016428328636, 0.25020533881209467, 0.2493839835728953, 0.2574948665358937, 0.2601642710502877, 0.2601642710349888, 0.2598562628581539, 0.26047227926689986, 0.2596509240368798, 0.26067761807593476, 0.26088295689108926, 0.25739219713137623, 0.25595482547425147, 0.25965092401240153, 0.2598562628214364, 0.2602669404639845, 0.26078028747433263, 0.2589322382052576, 0.2578028747463863, 0.2577002053265699, 0.25646817249683873, 0.2578028747463863, 0.25934291582332747, 0.2574948665358937, 0.25698151950718684, 0.25995893225043215, 0.25872689939622273, 0.26016427104722795, 0.2565708418952366, 0.2552361396426293, 0.25739219712831646, 0.2532854209506781, 0.25821355234915716, 0.25934291579884916, 0.25585215606361444, 0.2602669404272671, 0.26078028747433263, 0.261190965098522, 0.2570841889117043, 0.2606776180698152, 0.25616016425880805, 0.25646817248765935, 0.26057494867141734, 0.25451745379876795, 0.2585215605749487, 0.2568788501026694, 0.2570841889361826, 0.25420944559745473, 0.2574948665358937, 0.25472279261392244, 0.2582135523613963, 0.25595482547425147, 0.259958932238193, 0.26026694045786497, 0.2573921971252567, 0.2564681724845996, 0.2579055441509037, 0.2609856262956067, 0.2575975359404112, 0.25811088295687884, 0.25934291579884916, 0.25872689938704335, 0.2599589322443126, 0.26036960985626284, 0.2603696098623824, 0.26108829570012415, 0.2607802874865718, 0.25882956878850105, 0.26293634496919915, 0.2596509240368798, 0.26201232033466165, 0.2579055441356048, 0.2609856262833676, 0.2639630390266129, 0.26396303901437373, 0.2602669404395062, 0.2562628336755647, 0.2609856262956067, 0.2577002053449286, 0.25965092401240153, 0.26242299793437274, 0.26232032854515425, 0.26334702259950815, 0.2579055441509037, 0.2528747433142496, 0.26036960985626284, 0.2539014373839024, 0.263347022587269, 0.2611909651046416, 0.25626283368780384, 0.26016427104722795, 0.2609856262833676, 0.2617043121272289, 0.26406570842501076, 0.2648870636550308, 0.2667351129394048, 0.2609856262956067, 0.2655030800943747, 0.2652977412853398, 0.26622176592599683, 0.2654004106898572, 0.2663244353243947, 0.26344969200402557, 0.2664271047258524, 0.26509240247630483, 0.26273100616628386, 0.2607802874865718, 0.26694045174843967, 0.2616016427227114, 0.2632443531888711, 0.26088295689108926, 0.26273100616628386, 0.26498973306260803, 0.2666324435318275, 0.26591375771244447, 0.26673511293634494, 0.26560574949277255, 0.26519507186858315, 0.267453798755728, 0.2658110882956879, 0.2672484599650518, 0.26560574949889215, 0.26468172485823505, 0.26303901437371663, 0.2568788501026694]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
