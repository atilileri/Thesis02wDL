{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf54.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 00:23:29 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '2Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000020E808F5E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000020EADA26EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0762, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0751, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0726, Accuracy:0.3943, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0717, Accuracy:0.3943, Validation Loss:1.0714, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0705, Accuracy:0.4025, Validation Loss:1.0700, Validation Accuracy:0.4204\n",
    "Epoch #14: Loss:1.0684, Accuracy:0.4279, Validation Loss:1.0676, Validation Accuracy:0.4269\n",
    "Epoch #15: Loss:1.0651, Accuracy:0.4415, Validation Loss:1.0634, Validation Accuracy:0.4384\n",
    "Epoch #16: Loss:1.0593, Accuracy:0.4591, Validation Loss:1.0564, Validation Accuracy:0.4729\n",
    "Epoch #17: Loss:1.0493, Accuracy:0.4743, Validation Loss:1.0443, Validation Accuracy:0.4729\n",
    "Epoch #18: Loss:1.0326, Accuracy:0.4854, Validation Loss:1.0233, Validation Accuracy:0.4910\n",
    "Epoch #19: Loss:1.0055, Accuracy:0.4982, Validation Loss:0.9937, Validation Accuracy:0.4926\n",
    "Epoch #20: Loss:0.9731, Accuracy:0.4961, Validation Loss:0.9664, Validation Accuracy:0.4959\n",
    "Epoch #21: Loss:0.9443, Accuracy:0.5055, Validation Loss:0.9568, Validation Accuracy:0.5156\n",
    "Epoch #22: Loss:0.9345, Accuracy:0.5232, Validation Loss:0.9536, Validation Accuracy:0.5156\n",
    "Epoch #23: Loss:0.9292, Accuracy:0.5224, Validation Loss:0.9578, Validation Accuracy:0.4926\n",
    "Epoch #24: Loss:0.9264, Accuracy:0.5269, Validation Loss:0.9481, Validation Accuracy:0.5123\n",
    "Epoch #25: Loss:0.9192, Accuracy:0.5302, Validation Loss:0.9489, Validation Accuracy:0.4926\n",
    "Epoch #26: Loss:0.9190, Accuracy:0.5335, Validation Loss:0.9458, Validation Accuracy:0.5090\n",
    "Epoch #27: Loss:0.9188, Accuracy:0.5363, Validation Loss:0.9450, Validation Accuracy:0.5123\n",
    "Epoch #28: Loss:0.9158, Accuracy:0.5388, Validation Loss:0.9438, Validation Accuracy:0.5189\n",
    "Epoch #29: Loss:0.9148, Accuracy:0.5326, Validation Loss:0.9427, Validation Accuracy:0.5090\n",
    "Epoch #30: Loss:0.9133, Accuracy:0.5347, Validation Loss:0.9417, Validation Accuracy:0.5074\n",
    "Epoch #31: Loss:0.9113, Accuracy:0.5335, Validation Loss:0.9397, Validation Accuracy:0.5189\n",
    "Epoch #32: Loss:0.9099, Accuracy:0.5446, Validation Loss:0.9398, Validation Accuracy:0.5156\n",
    "Epoch #33: Loss:0.9087, Accuracy:0.5413, Validation Loss:0.9431, Validation Accuracy:0.4975\n",
    "Epoch #34: Loss:0.9105, Accuracy:0.5331, Validation Loss:0.9383, Validation Accuracy:0.5271\n",
    "Epoch #35: Loss:0.9070, Accuracy:0.5507, Validation Loss:0.9386, Validation Accuracy:0.5090\n",
    "Epoch #36: Loss:0.9051, Accuracy:0.5368, Validation Loss:0.9346, Validation Accuracy:0.5107\n",
    "Epoch #37: Loss:0.9036, Accuracy:0.5359, Validation Loss:0.9324, Validation Accuracy:0.5205\n",
    "Epoch #38: Loss:0.9027, Accuracy:0.5478, Validation Loss:0.9355, Validation Accuracy:0.5057\n",
    "Epoch #39: Loss:0.9024, Accuracy:0.5368, Validation Loss:0.9306, Validation Accuracy:0.5337\n",
    "Epoch #40: Loss:0.8989, Accuracy:0.5503, Validation Loss:0.9299, Validation Accuracy:0.5255\n",
    "Epoch #41: Loss:0.8972, Accuracy:0.5474, Validation Loss:0.9293, Validation Accuracy:0.5255\n",
    "Epoch #42: Loss:0.8962, Accuracy:0.5491, Validation Loss:0.9277, Validation Accuracy:0.5304\n",
    "Epoch #43: Loss:0.8952, Accuracy:0.5474, Validation Loss:0.9245, Validation Accuracy:0.5402\n",
    "Epoch #44: Loss:0.8938, Accuracy:0.5515, Validation Loss:0.9252, Validation Accuracy:0.5320\n",
    "Epoch #45: Loss:0.8940, Accuracy:0.5478, Validation Loss:0.9239, Validation Accuracy:0.5320\n",
    "Epoch #46: Loss:0.8924, Accuracy:0.5470, Validation Loss:0.9205, Validation Accuracy:0.5419\n",
    "Epoch #47: Loss:0.8906, Accuracy:0.5495, Validation Loss:0.9200, Validation Accuracy:0.5452\n",
    "Epoch #48: Loss:0.8899, Accuracy:0.5466, Validation Loss:0.9204, Validation Accuracy:0.5337\n",
    "Epoch #49: Loss:0.8888, Accuracy:0.5552, Validation Loss:0.9161, Validation Accuracy:0.5419\n",
    "Epoch #50: Loss:0.8875, Accuracy:0.5478, Validation Loss:0.9152, Validation Accuracy:0.5517\n",
    "Epoch #51: Loss:0.8879, Accuracy:0.5581, Validation Loss:0.9289, Validation Accuracy:0.5107\n",
    "Epoch #52: Loss:0.8953, Accuracy:0.5507, Validation Loss:0.9114, Validation Accuracy:0.5534\n",
    "Epoch #53: Loss:0.8897, Accuracy:0.5458, Validation Loss:0.9108, Validation Accuracy:0.5567\n",
    "Epoch #54: Loss:0.8860, Accuracy:0.5556, Validation Loss:0.9115, Validation Accuracy:0.5468\n",
    "Epoch #55: Loss:0.8837, Accuracy:0.5651, Validation Loss:0.9137, Validation Accuracy:0.5287\n",
    "Epoch #56: Loss:0.8848, Accuracy:0.5540, Validation Loss:0.9092, Validation Accuracy:0.5616\n",
    "Epoch #57: Loss:0.8855, Accuracy:0.5569, Validation Loss:0.9091, Validation Accuracy:0.5435\n",
    "Epoch #58: Loss:0.8810, Accuracy:0.5622, Validation Loss:0.9108, Validation Accuracy:0.5353\n",
    "Epoch #59: Loss:0.8819, Accuracy:0.5606, Validation Loss:0.9068, Validation Accuracy:0.5468\n",
    "Epoch #60: Loss:0.8811, Accuracy:0.5602, Validation Loss:0.9058, Validation Accuracy:0.5501\n",
    "Epoch #61: Loss:0.8804, Accuracy:0.5593, Validation Loss:0.9065, Validation Accuracy:0.5698\n",
    "Epoch #62: Loss:0.8800, Accuracy:0.5663, Validation Loss:0.9127, Validation Accuracy:0.5320\n",
    "Epoch #63: Loss:0.8791, Accuracy:0.5659, Validation Loss:0.9037, Validation Accuracy:0.5517\n",
    "Epoch #64: Loss:0.8778, Accuracy:0.5659, Validation Loss:0.9029, Validation Accuracy:0.5517\n",
    "Epoch #65: Loss:0.8782, Accuracy:0.5643, Validation Loss:0.9082, Validation Accuracy:0.5402\n",
    "Epoch #66: Loss:0.8766, Accuracy:0.5626, Validation Loss:0.9017, Validation Accuracy:0.5501\n",
    "Epoch #67: Loss:0.8751, Accuracy:0.5737, Validation Loss:0.9019, Validation Accuracy:0.5452\n",
    "Epoch #68: Loss:0.8748, Accuracy:0.5663, Validation Loss:0.9001, Validation Accuracy:0.5665\n",
    "Epoch #69: Loss:0.8742, Accuracy:0.5696, Validation Loss:0.9094, Validation Accuracy:0.5386\n",
    "Epoch #70: Loss:0.8800, Accuracy:0.5692, Validation Loss:0.9059, Validation Accuracy:0.5419\n",
    "Epoch #71: Loss:0.8748, Accuracy:0.5696, Validation Loss:0.9012, Validation Accuracy:0.5714\n",
    "Epoch #72: Loss:0.8734, Accuracy:0.5676, Validation Loss:0.9011, Validation Accuracy:0.5517\n",
    "Epoch #73: Loss:0.8731, Accuracy:0.5700, Validation Loss:0.8960, Validation Accuracy:0.5599\n",
    "Epoch #74: Loss:0.8723, Accuracy:0.5713, Validation Loss:0.9016, Validation Accuracy:0.5501\n",
    "Epoch #75: Loss:0.8727, Accuracy:0.5651, Validation Loss:0.8975, Validation Accuracy:0.5780\n",
    "Epoch #76: Loss:0.8737, Accuracy:0.5676, Validation Loss:0.8927, Validation Accuracy:0.5829\n",
    "Epoch #77: Loss:0.8782, Accuracy:0.5655, Validation Loss:0.8984, Validation Accuracy:0.5550\n",
    "Epoch #78: Loss:0.8756, Accuracy:0.5721, Validation Loss:0.9191, Validation Accuracy:0.5320\n",
    "Epoch #79: Loss:0.8832, Accuracy:0.5565, Validation Loss:0.8940, Validation Accuracy:0.5796\n",
    "Epoch #80: Loss:0.8739, Accuracy:0.5684, Validation Loss:0.8909, Validation Accuracy:0.5780\n",
    "Epoch #81: Loss:0.8719, Accuracy:0.5791, Validation Loss:0.8928, Validation Accuracy:0.5649\n",
    "Epoch #82: Loss:0.8692, Accuracy:0.5758, Validation Loss:0.8940, Validation Accuracy:0.5567\n",
    "Epoch #83: Loss:0.8691, Accuracy:0.5708, Validation Loss:0.8936, Validation Accuracy:0.5616\n",
    "Epoch #84: Loss:0.8677, Accuracy:0.5786, Validation Loss:0.8862, Validation Accuracy:0.5764\n",
    "Epoch #85: Loss:0.8681, Accuracy:0.5799, Validation Loss:0.8887, Validation Accuracy:0.5846\n",
    "Epoch #86: Loss:0.8683, Accuracy:0.5770, Validation Loss:0.8863, Validation Accuracy:0.5681\n",
    "Epoch #87: Loss:0.8689, Accuracy:0.5770, Validation Loss:0.8871, Validation Accuracy:0.5681\n",
    "Epoch #88: Loss:0.8666, Accuracy:0.5778, Validation Loss:0.8940, Validation Accuracy:0.5517\n",
    "Epoch #89: Loss:0.8663, Accuracy:0.5700, Validation Loss:0.8835, Validation Accuracy:0.5681\n",
    "Epoch #90: Loss:0.8660, Accuracy:0.5733, Validation Loss:0.8899, Validation Accuracy:0.5583\n",
    "Epoch #91: Loss:0.8607, Accuracy:0.5885, Validation Loss:0.8880, Validation Accuracy:0.5862\n",
    "Epoch #92: Loss:0.8736, Accuracy:0.5700, Validation Loss:0.8872, Validation Accuracy:0.5878\n",
    "Epoch #93: Loss:0.8728, Accuracy:0.5717, Validation Loss:0.8801, Validation Accuracy:0.5829\n",
    "Epoch #94: Loss:0.8629, Accuracy:0.5803, Validation Loss:0.8898, Validation Accuracy:0.5616\n",
    "Epoch #95: Loss:0.8632, Accuracy:0.5799, Validation Loss:0.8782, Validation Accuracy:0.5796\n",
    "Epoch #96: Loss:0.8623, Accuracy:0.5758, Validation Loss:0.8781, Validation Accuracy:0.5764\n",
    "Epoch #97: Loss:0.8696, Accuracy:0.5828, Validation Loss:0.8951, Validation Accuracy:0.5435\n",
    "Epoch #98: Loss:0.8633, Accuracy:0.5795, Validation Loss:0.8777, Validation Accuracy:0.5764\n",
    "Epoch #99: Loss:0.8601, Accuracy:0.5803, Validation Loss:0.8777, Validation Accuracy:0.5796\n",
    "Epoch #100: Loss:0.8617, Accuracy:0.5836, Validation Loss:0.8790, Validation Accuracy:0.5665\n",
    "Epoch #101: Loss:0.8593, Accuracy:0.5832, Validation Loss:0.8758, Validation Accuracy:0.5829\n",
    "Epoch #102: Loss:0.8611, Accuracy:0.5844, Validation Loss:0.8762, Validation Accuracy:0.5928\n",
    "Epoch #103: Loss:0.8618, Accuracy:0.5766, Validation Loss:0.8774, Validation Accuracy:0.5862\n",
    "Epoch #104: Loss:0.8616, Accuracy:0.5778, Validation Loss:0.8753, Validation Accuracy:0.5780\n",
    "Epoch #105: Loss:0.8652, Accuracy:0.5778, Validation Loss:0.8764, Validation Accuracy:0.5764\n",
    "Epoch #106: Loss:0.8646, Accuracy:0.5811, Validation Loss:0.9134, Validation Accuracy:0.5386\n",
    "Epoch #107: Loss:0.8747, Accuracy:0.5618, Validation Loss:0.8749, Validation Accuracy:0.5846\n",
    "Epoch #108: Loss:0.8719, Accuracy:0.5692, Validation Loss:0.8850, Validation Accuracy:0.5911\n",
    "Epoch #109: Loss:0.8683, Accuracy:0.5807, Validation Loss:0.8797, Validation Accuracy:0.5846\n",
    "Epoch #110: Loss:0.8660, Accuracy:0.5770, Validation Loss:0.8940, Validation Accuracy:0.5501\n",
    "Epoch #111: Loss:0.8649, Accuracy:0.5811, Validation Loss:0.8763, Validation Accuracy:0.5846\n",
    "Epoch #112: Loss:0.8607, Accuracy:0.5807, Validation Loss:0.8759, Validation Accuracy:0.5862\n",
    "Epoch #113: Loss:0.8590, Accuracy:0.5795, Validation Loss:0.8775, Validation Accuracy:0.5780\n",
    "Epoch #114: Loss:0.8576, Accuracy:0.5807, Validation Loss:0.8814, Validation Accuracy:0.5665\n",
    "Epoch #115: Loss:0.8597, Accuracy:0.5819, Validation Loss:0.8751, Validation Accuracy:0.5780\n",
    "Epoch #116: Loss:0.8567, Accuracy:0.5803, Validation Loss:0.8757, Validation Accuracy:0.5829\n",
    "Epoch #117: Loss:0.8581, Accuracy:0.5774, Validation Loss:0.8785, Validation Accuracy:0.5649\n",
    "Epoch #118: Loss:0.8571, Accuracy:0.5844, Validation Loss:0.8730, Validation Accuracy:0.5796\n",
    "Epoch #119: Loss:0.8558, Accuracy:0.5840, Validation Loss:0.8732, Validation Accuracy:0.5862\n",
    "Epoch #120: Loss:0.8555, Accuracy:0.5860, Validation Loss:0.8734, Validation Accuracy:0.5878\n",
    "Epoch #121: Loss:0.8565, Accuracy:0.5869, Validation Loss:0.8936, Validation Accuracy:0.5468\n",
    "Epoch #122: Loss:0.8652, Accuracy:0.5754, Validation Loss:0.8748, Validation Accuracy:0.5764\n",
    "Epoch #123: Loss:0.8561, Accuracy:0.5795, Validation Loss:0.8763, Validation Accuracy:0.5796\n",
    "Epoch #124: Loss:0.8640, Accuracy:0.5823, Validation Loss:0.8742, Validation Accuracy:0.5846\n",
    "Epoch #125: Loss:0.8585, Accuracy:0.5852, Validation Loss:0.8768, Validation Accuracy:0.5714\n",
    "Epoch #126: Loss:0.8552, Accuracy:0.5869, Validation Loss:0.8787, Validation Accuracy:0.5698\n",
    "Epoch #127: Loss:0.8572, Accuracy:0.5832, Validation Loss:0.8739, Validation Accuracy:0.5829\n",
    "Epoch #128: Loss:0.8538, Accuracy:0.5836, Validation Loss:0.8738, Validation Accuracy:0.5862\n",
    "Epoch #129: Loss:0.8544, Accuracy:0.5856, Validation Loss:0.8808, Validation Accuracy:0.5649\n",
    "Epoch #130: Loss:0.8544, Accuracy:0.5877, Validation Loss:0.8732, Validation Accuracy:0.5895\n",
    "Epoch #131: Loss:0.8535, Accuracy:0.5832, Validation Loss:0.8767, Validation Accuracy:0.5829\n",
    "Epoch #132: Loss:0.8601, Accuracy:0.5799, Validation Loss:0.8745, Validation Accuracy:0.5780\n",
    "Epoch #133: Loss:0.8555, Accuracy:0.5823, Validation Loss:0.8766, Validation Accuracy:0.5747\n",
    "Epoch #134: Loss:0.8560, Accuracy:0.5885, Validation Loss:0.8940, Validation Accuracy:0.5567\n",
    "Epoch #135: Loss:0.8630, Accuracy:0.5848, Validation Loss:0.8769, Validation Accuracy:0.5796\n",
    "Epoch #136: Loss:0.8545, Accuracy:0.5795, Validation Loss:0.8776, Validation Accuracy:0.5796\n",
    "Epoch #137: Loss:0.8546, Accuracy:0.5836, Validation Loss:0.8764, Validation Accuracy:0.5780\n",
    "Epoch #138: Loss:0.8532, Accuracy:0.5906, Validation Loss:0.8796, Validation Accuracy:0.5665\n",
    "Epoch #139: Loss:0.8526, Accuracy:0.5823, Validation Loss:0.8764, Validation Accuracy:0.5764\n",
    "Epoch #140: Loss:0.8528, Accuracy:0.5897, Validation Loss:0.8752, Validation Accuracy:0.5878\n",
    "Epoch #141: Loss:0.8526, Accuracy:0.5889, Validation Loss:0.8880, Validation Accuracy:0.5567\n",
    "Epoch #142: Loss:0.8558, Accuracy:0.5774, Validation Loss:0.8775, Validation Accuracy:0.5796\n",
    "Epoch #143: Loss:0.8535, Accuracy:0.5836, Validation Loss:0.8788, Validation Accuracy:0.5698\n",
    "Epoch #144: Loss:0.8523, Accuracy:0.5947, Validation Loss:0.8774, Validation Accuracy:0.5780\n",
    "Epoch #145: Loss:0.8532, Accuracy:0.5893, Validation Loss:0.8779, Validation Accuracy:0.5780\n",
    "Epoch #146: Loss:0.8540, Accuracy:0.5823, Validation Loss:0.8766, Validation Accuracy:0.5780\n",
    "Epoch #147: Loss:0.8514, Accuracy:0.5852, Validation Loss:0.8885, Validation Accuracy:0.5583\n",
    "Epoch #148: Loss:0.8576, Accuracy:0.5795, Validation Loss:0.8758, Validation Accuracy:0.5878\n",
    "Epoch #149: Loss:0.8549, Accuracy:0.5832, Validation Loss:0.8873, Validation Accuracy:0.5813\n",
    "Epoch #150: Loss:0.8575, Accuracy:0.5811, Validation Loss:0.8798, Validation Accuracy:0.5698\n",
    "Epoch #151: Loss:0.8512, Accuracy:0.5852, Validation Loss:0.8807, Validation Accuracy:0.5714\n",
    "Epoch #152: Loss:0.8494, Accuracy:0.5901, Validation Loss:0.8764, Validation Accuracy:0.5829\n",
    "Epoch #153: Loss:0.8502, Accuracy:0.5906, Validation Loss:0.8763, Validation Accuracy:0.5911\n",
    "Epoch #154: Loss:0.8494, Accuracy:0.5910, Validation Loss:0.8819, Validation Accuracy:0.5616\n",
    "Epoch #155: Loss:0.8497, Accuracy:0.5906, Validation Loss:0.8770, Validation Accuracy:0.5829\n",
    "Epoch #156: Loss:0.8473, Accuracy:0.5914, Validation Loss:0.8767, Validation Accuracy:0.5846\n",
    "Epoch #157: Loss:0.8484, Accuracy:0.5864, Validation Loss:0.8752, Validation Accuracy:0.5796\n",
    "Epoch #158: Loss:0.8470, Accuracy:0.5885, Validation Loss:0.8741, Validation Accuracy:0.5780\n",
    "Epoch #159: Loss:0.8477, Accuracy:0.5869, Validation Loss:0.8851, Validation Accuracy:0.5616\n",
    "Epoch #160: Loss:0.8574, Accuracy:0.5848, Validation Loss:0.9026, Validation Accuracy:0.5435\n",
    "Epoch #161: Loss:0.8577, Accuracy:0.5725, Validation Loss:0.8765, Validation Accuracy:0.5780\n",
    "Epoch #162: Loss:0.8483, Accuracy:0.5889, Validation Loss:0.8752, Validation Accuracy:0.5813\n",
    "Epoch #163: Loss:0.8470, Accuracy:0.5934, Validation Loss:0.8808, Validation Accuracy:0.5583\n",
    "Epoch #164: Loss:0.8517, Accuracy:0.5955, Validation Loss:0.8989, Validation Accuracy:0.5550\n",
    "Epoch #165: Loss:0.8520, Accuracy:0.5844, Validation Loss:0.8793, Validation Accuracy:0.5911\n",
    "Epoch #166: Loss:0.8502, Accuracy:0.5836, Validation Loss:0.8746, Validation Accuracy:0.5862\n",
    "Epoch #167: Loss:0.8518, Accuracy:0.5906, Validation Loss:0.8905, Validation Accuracy:0.5534\n",
    "Epoch #168: Loss:0.8516, Accuracy:0.5832, Validation Loss:0.8769, Validation Accuracy:0.5764\n",
    "Epoch #169: Loss:0.8489, Accuracy:0.5799, Validation Loss:0.8951, Validation Accuracy:0.5813\n",
    "Epoch #170: Loss:0.8664, Accuracy:0.5721, Validation Loss:0.8811, Validation Accuracy:0.5599\n",
    "Epoch #171: Loss:0.8519, Accuracy:0.5869, Validation Loss:0.8767, Validation Accuracy:0.5796\n",
    "Epoch #172: Loss:0.8467, Accuracy:0.5889, Validation Loss:0.8774, Validation Accuracy:0.5895\n",
    "Epoch #173: Loss:0.8503, Accuracy:0.5795, Validation Loss:0.8752, Validation Accuracy:0.5813\n",
    "Epoch #174: Loss:0.8438, Accuracy:0.5930, Validation Loss:0.8729, Validation Accuracy:0.5895\n",
    "Epoch #175: Loss:0.8447, Accuracy:0.5885, Validation Loss:0.8726, Validation Accuracy:0.5813\n",
    "Epoch #176: Loss:0.8448, Accuracy:0.5864, Validation Loss:0.8753, Validation Accuracy:0.5731\n",
    "Epoch #177: Loss:0.8433, Accuracy:0.5934, Validation Loss:0.8805, Validation Accuracy:0.5698\n",
    "Epoch #178: Loss:0.8487, Accuracy:0.5893, Validation Loss:0.8728, Validation Accuracy:0.5862\n",
    "Epoch #179: Loss:0.8439, Accuracy:0.5873, Validation Loss:0.8776, Validation Accuracy:0.5813\n",
    "Epoch #180: Loss:0.8508, Accuracy:0.5852, Validation Loss:0.8733, Validation Accuracy:0.5796\n",
    "Epoch #181: Loss:0.8469, Accuracy:0.5943, Validation Loss:0.8803, Validation Accuracy:0.5698\n",
    "Epoch #182: Loss:0.8424, Accuracy:0.5910, Validation Loss:0.8741, Validation Accuracy:0.5928\n",
    "Epoch #183: Loss:0.8448, Accuracy:0.5889, Validation Loss:0.8729, Validation Accuracy:0.5780\n",
    "Epoch #184: Loss:0.8414, Accuracy:0.5910, Validation Loss:0.8748, Validation Accuracy:0.5731\n",
    "Epoch #185: Loss:0.8424, Accuracy:0.5901, Validation Loss:0.8711, Validation Accuracy:0.5813\n",
    "Epoch #186: Loss:0.8418, Accuracy:0.5943, Validation Loss:0.8750, Validation Accuracy:0.5747\n",
    "Epoch #187: Loss:0.8437, Accuracy:0.5901, Validation Loss:0.8700, Validation Accuracy:0.5796\n",
    "Epoch #188: Loss:0.8434, Accuracy:0.5901, Validation Loss:0.8714, Validation Accuracy:0.5829\n",
    "Epoch #189: Loss:0.8436, Accuracy:0.5832, Validation Loss:0.8713, Validation Accuracy:0.5813\n",
    "Epoch #190: Loss:0.8440, Accuracy:0.5873, Validation Loss:0.8707, Validation Accuracy:0.5911\n",
    "Epoch #191: Loss:0.8491, Accuracy:0.5856, Validation Loss:0.8745, Validation Accuracy:0.5665\n",
    "Epoch #192: Loss:0.8412, Accuracy:0.5922, Validation Loss:0.8736, Validation Accuracy:0.5747\n",
    "Epoch #193: Loss:0.8420, Accuracy:0.5848, Validation Loss:0.8730, Validation Accuracy:0.5813\n",
    "Epoch #194: Loss:0.8437, Accuracy:0.5869, Validation Loss:0.8786, Validation Accuracy:0.5796\n",
    "Epoch #195: Loss:0.8497, Accuracy:0.5910, Validation Loss:0.8891, Validation Accuracy:0.5501\n",
    "Epoch #196: Loss:0.8482, Accuracy:0.5844, Validation Loss:0.8718, Validation Accuracy:0.5813\n",
    "Epoch #197: Loss:0.8384, Accuracy:0.5914, Validation Loss:0.8704, Validation Accuracy:0.5862\n",
    "Epoch #198: Loss:0.8402, Accuracy:0.5864, Validation Loss:0.8944, Validation Accuracy:0.5468\n",
    "Epoch #199: Loss:0.8485, Accuracy:0.5881, Validation Loss:0.8691, Validation Accuracy:0.5780\n",
    "Epoch #200: Loss:0.8516, Accuracy:0.5852, Validation Loss:0.8764, Validation Accuracy:0.5829\n",
    "Epoch #201: Loss:0.8423, Accuracy:0.5901, Validation Loss:0.8728, Validation Accuracy:0.5747\n",
    "Epoch #202: Loss:0.8362, Accuracy:0.5979, Validation Loss:0.8717, Validation Accuracy:0.5895\n",
    "Epoch #203: Loss:0.8450, Accuracy:0.5811, Validation Loss:0.8711, Validation Accuracy:0.5731\n",
    "Epoch #204: Loss:0.8422, Accuracy:0.5864, Validation Loss:0.8751, Validation Accuracy:0.5649\n",
    "Epoch #205: Loss:0.8366, Accuracy:0.5955, Validation Loss:0.8680, Validation Accuracy:0.5796\n",
    "Epoch #206: Loss:0.8381, Accuracy:0.5955, Validation Loss:0.8685, Validation Accuracy:0.5796\n",
    "Epoch #207: Loss:0.8357, Accuracy:0.5951, Validation Loss:0.8777, Validation Accuracy:0.5632\n",
    "Epoch #208: Loss:0.8407, Accuracy:0.5930, Validation Loss:0.8691, Validation Accuracy:0.5731\n",
    "Epoch #209: Loss:0.8353, Accuracy:0.5930, Validation Loss:0.8673, Validation Accuracy:0.5862\n",
    "Epoch #210: Loss:0.8388, Accuracy:0.5881, Validation Loss:0.8726, Validation Accuracy:0.5665\n",
    "Epoch #211: Loss:0.8349, Accuracy:0.5951, Validation Loss:0.8675, Validation Accuracy:0.5796\n",
    "Epoch #212: Loss:0.8354, Accuracy:0.5955, Validation Loss:0.8687, Validation Accuracy:0.5895\n",
    "Epoch #213: Loss:0.8358, Accuracy:0.5971, Validation Loss:0.8767, Validation Accuracy:0.5616\n",
    "Epoch #214: Loss:0.8383, Accuracy:0.5910, Validation Loss:0.8691, Validation Accuracy:0.5714\n",
    "Epoch #215: Loss:0.8340, Accuracy:0.5938, Validation Loss:0.8665, Validation Accuracy:0.5813\n",
    "Epoch #216: Loss:0.8351, Accuracy:0.5955, Validation Loss:0.8686, Validation Accuracy:0.5796\n",
    "Epoch #217: Loss:0.8333, Accuracy:0.5955, Validation Loss:0.8785, Validation Accuracy:0.5632\n",
    "Epoch #218: Loss:0.8371, Accuracy:0.5901, Validation Loss:0.8718, Validation Accuracy:0.5731\n",
    "Epoch #219: Loss:0.8385, Accuracy:0.5889, Validation Loss:0.8730, Validation Accuracy:0.5813\n",
    "Epoch #220: Loss:0.8357, Accuracy:0.5959, Validation Loss:0.8740, Validation Accuracy:0.5681\n",
    "Epoch #221: Loss:0.8355, Accuracy:0.5975, Validation Loss:0.8760, Validation Accuracy:0.5649\n",
    "Epoch #222: Loss:0.8353, Accuracy:0.5951, Validation Loss:0.8646, Validation Accuracy:0.5681\n",
    "Epoch #223: Loss:0.8317, Accuracy:0.5967, Validation Loss:0.8640, Validation Accuracy:0.5747\n",
    "Epoch #224: Loss:0.8308, Accuracy:0.5967, Validation Loss:0.8654, Validation Accuracy:0.5829\n",
    "Epoch #225: Loss:0.8363, Accuracy:0.5963, Validation Loss:0.8640, Validation Accuracy:0.5796\n",
    "Epoch #226: Loss:0.8389, Accuracy:0.5922, Validation Loss:0.8738, Validation Accuracy:0.5616\n",
    "Epoch #227: Loss:0.8364, Accuracy:0.5984, Validation Loss:0.8739, Validation Accuracy:0.5616\n",
    "Epoch #228: Loss:0.8322, Accuracy:0.5967, Validation Loss:0.8648, Validation Accuracy:0.5911\n",
    "Epoch #229: Loss:0.8364, Accuracy:0.5926, Validation Loss:0.8665, Validation Accuracy:0.5829\n",
    "Epoch #230: Loss:0.8350, Accuracy:0.5901, Validation Loss:0.8675, Validation Accuracy:0.5616\n",
    "Epoch #231: Loss:0.8300, Accuracy:0.6041, Validation Loss:0.8755, Validation Accuracy:0.5616\n",
    "Epoch #232: Loss:0.8311, Accuracy:0.5967, Validation Loss:0.8641, Validation Accuracy:0.5846\n",
    "Epoch #233: Loss:0.8327, Accuracy:0.6000, Validation Loss:0.8622, Validation Accuracy:0.5846\n",
    "Epoch #234: Loss:0.8277, Accuracy:0.5984, Validation Loss:0.8653, Validation Accuracy:0.5616\n",
    "Epoch #235: Loss:0.8281, Accuracy:0.6053, Validation Loss:0.8872, Validation Accuracy:0.5435\n",
    "Epoch #236: Loss:0.8377, Accuracy:0.5938, Validation Loss:0.8733, Validation Accuracy:0.5583\n",
    "Epoch #237: Loss:0.8304, Accuracy:0.5988, Validation Loss:0.8675, Validation Accuracy:0.5829\n",
    "Epoch #238: Loss:0.8347, Accuracy:0.5901, Validation Loss:0.8600, Validation Accuracy:0.5911\n",
    "Epoch #239: Loss:0.8259, Accuracy:0.6041, Validation Loss:0.8603, Validation Accuracy:0.5599\n",
    "Epoch #240: Loss:0.8251, Accuracy:0.5988, Validation Loss:0.8903, Validation Accuracy:0.5435\n",
    "Epoch #241: Loss:0.8339, Accuracy:0.5910, Validation Loss:0.8573, Validation Accuracy:0.5747\n",
    "Epoch #242: Loss:0.8238, Accuracy:0.5984, Validation Loss:0.8726, Validation Accuracy:0.5846\n",
    "Epoch #243: Loss:0.8425, Accuracy:0.5848, Validation Loss:0.8620, Validation Accuracy:0.5911\n",
    "Epoch #244: Loss:0.8291, Accuracy:0.6041, Validation Loss:0.8719, Validation Accuracy:0.5649\n",
    "Epoch #245: Loss:0.8215, Accuracy:0.5984, Validation Loss:0.8597, Validation Accuracy:0.5698\n",
    "Epoch #246: Loss:0.8200, Accuracy:0.6066, Validation Loss:0.8583, Validation Accuracy:0.5550\n",
    "Epoch #247: Loss:0.8234, Accuracy:0.5996, Validation Loss:0.8607, Validation Accuracy:0.5649\n",
    "Epoch #248: Loss:0.8185, Accuracy:0.6004, Validation Loss:0.8843, Validation Accuracy:0.5452\n",
    "Epoch #249: Loss:0.8294, Accuracy:0.5979, Validation Loss:0.8634, Validation Accuracy:0.5632\n",
    "Epoch #250: Loss:0.8314, Accuracy:0.5992, Validation Loss:0.8624, Validation Accuracy:0.5911\n",
    "Epoch #251: Loss:0.8221, Accuracy:0.6045, Validation Loss:0.8552, Validation Accuracy:0.5977\n",
    "Epoch #252: Loss:0.8209, Accuracy:0.6070, Validation Loss:0.8550, Validation Accuracy:0.5632\n",
    "Epoch #253: Loss:0.8213, Accuracy:0.6016, Validation Loss:0.8811, Validation Accuracy:0.5534\n",
    "Epoch #254: Loss:0.8345, Accuracy:0.5959, Validation Loss:0.8930, Validation Accuracy:0.5402\n",
    "Epoch #255: Loss:0.8476, Accuracy:0.5910, Validation Loss:0.8553, Validation Accuracy:0.5944\n",
    "Epoch #256: Loss:0.8219, Accuracy:0.5992, Validation Loss:0.8512, Validation Accuracy:0.5796\n",
    "Epoch #257: Loss:0.8151, Accuracy:0.6074, Validation Loss:0.8539, Validation Accuracy:0.5649\n",
    "Epoch #258: Loss:0.8146, Accuracy:0.6136, Validation Loss:0.8542, Validation Accuracy:0.5681\n",
    "Epoch #259: Loss:0.8137, Accuracy:0.6107, Validation Loss:0.8505, Validation Accuracy:0.5681\n",
    "Epoch #260: Loss:0.8102, Accuracy:0.6136, Validation Loss:0.8584, Validation Accuracy:0.5649\n",
    "Epoch #261: Loss:0.8115, Accuracy:0.6049, Validation Loss:0.8530, Validation Accuracy:0.5731\n",
    "Epoch #262: Loss:0.8099, Accuracy:0.6131, Validation Loss:0.8529, Validation Accuracy:0.5764\n",
    "Epoch #263: Loss:0.8124, Accuracy:0.6049, Validation Loss:0.8551, Validation Accuracy:0.5698\n",
    "Epoch #264: Loss:0.8112, Accuracy:0.6115, Validation Loss:0.8504, Validation Accuracy:0.5681\n",
    "Epoch #265: Loss:0.8083, Accuracy:0.6111, Validation Loss:0.8481, Validation Accuracy:0.5829\n",
    "Epoch #266: Loss:0.8077, Accuracy:0.6070, Validation Loss:0.8465, Validation Accuracy:0.5813\n",
    "Epoch #267: Loss:0.8096, Accuracy:0.6119, Validation Loss:0.8475, Validation Accuracy:0.5764\n",
    "Epoch #268: Loss:0.8104, Accuracy:0.6136, Validation Loss:0.8645, Validation Accuracy:0.5616\n",
    "Epoch #269: Loss:0.8131, Accuracy:0.6168, Validation Loss:0.8621, Validation Accuracy:0.5534\n",
    "Epoch #270: Loss:0.8092, Accuracy:0.6103, Validation Loss:0.8462, Validation Accuracy:0.5714\n",
    "Epoch #271: Loss:0.8057, Accuracy:0.6152, Validation Loss:0.8449, Validation Accuracy:0.5764\n",
    "Epoch #272: Loss:0.8124, Accuracy:0.6140, Validation Loss:0.8420, Validation Accuracy:0.5846\n",
    "Epoch #273: Loss:0.8080, Accuracy:0.6127, Validation Loss:0.8431, Validation Accuracy:0.5780\n",
    "Epoch #274: Loss:0.8196, Accuracy:0.6078, Validation Loss:0.8393, Validation Accuracy:0.5813\n",
    "Epoch #275: Loss:0.8167, Accuracy:0.6053, Validation Loss:0.8565, Validation Accuracy:0.5977\n",
    "Epoch #276: Loss:0.8132, Accuracy:0.6016, Validation Loss:0.8459, Validation Accuracy:0.6043\n",
    "Epoch #277: Loss:0.8103, Accuracy:0.6057, Validation Loss:0.8545, Validation Accuracy:0.5665\n",
    "Epoch #278: Loss:0.8044, Accuracy:0.6148, Validation Loss:0.8525, Validation Accuracy:0.5681\n",
    "Epoch #279: Loss:0.8080, Accuracy:0.6148, Validation Loss:0.8378, Validation Accuracy:0.5813\n",
    "Epoch #280: Loss:0.7992, Accuracy:0.6177, Validation Loss:0.8398, Validation Accuracy:0.5928\n",
    "Epoch #281: Loss:0.8000, Accuracy:0.6242, Validation Loss:0.8435, Validation Accuracy:0.5796\n",
    "Epoch #282: Loss:0.7960, Accuracy:0.6160, Validation Loss:0.8373, Validation Accuracy:0.5895\n",
    "Epoch #283: Loss:0.7993, Accuracy:0.6181, Validation Loss:0.8363, Validation Accuracy:0.5829\n",
    "Epoch #284: Loss:0.7984, Accuracy:0.6181, Validation Loss:0.8356, Validation Accuracy:0.5911\n",
    "Epoch #285: Loss:0.7978, Accuracy:0.6189, Validation Loss:0.8480, Validation Accuracy:0.5665\n",
    "Epoch #286: Loss:0.7987, Accuracy:0.6140, Validation Loss:0.8426, Validation Accuracy:0.5731\n",
    "Epoch #287: Loss:0.7943, Accuracy:0.6148, Validation Loss:0.8344, Validation Accuracy:0.5944\n",
    "Epoch #288: Loss:0.7987, Accuracy:0.6168, Validation Loss:0.8367, Validation Accuracy:0.5961\n",
    "Epoch #289: Loss:0.7959, Accuracy:0.6090, Validation Loss:0.8364, Validation Accuracy:0.5895\n",
    "Epoch #290: Loss:0.7948, Accuracy:0.6201, Validation Loss:0.8325, Validation Accuracy:0.5961\n",
    "Epoch #291: Loss:0.7917, Accuracy:0.6242, Validation Loss:0.8370, Validation Accuracy:0.5878\n",
    "Epoch #292: Loss:0.7932, Accuracy:0.6214, Validation Loss:0.8333, Validation Accuracy:0.5961\n",
    "Epoch #293: Loss:0.7934, Accuracy:0.6222, Validation Loss:0.8346, Validation Accuracy:0.5961\n",
    "Epoch #294: Loss:0.7995, Accuracy:0.6119, Validation Loss:0.8376, Validation Accuracy:0.5846\n",
    "Epoch #295: Loss:0.7934, Accuracy:0.6148, Validation Loss:0.8356, Validation Accuracy:0.5829\n",
    "Epoch #296: Loss:0.7921, Accuracy:0.6160, Validation Loss:0.8335, Validation Accuracy:0.5961\n",
    "Epoch #297: Loss:0.7891, Accuracy:0.6218, Validation Loss:0.8297, Validation Accuracy:0.5961\n",
    "Epoch #298: Loss:0.7933, Accuracy:0.6214, Validation Loss:0.8305, Validation Accuracy:0.6043\n",
    "Epoch #299: Loss:0.8056, Accuracy:0.6021, Validation Loss:0.8316, Validation Accuracy:0.5961\n",
    "Epoch #300: Loss:0.8154, Accuracy:0.6045, Validation Loss:0.8291, Validation Accuracy:0.5928\n",
    "\n",
    "Test:\n",
    "Test Loss:0.82907367, Accuracy:0.5928\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  153   55  32\n",
    "t:02   78  146   3\n",
    "t:03   67   13  62\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.51      0.64      0.57       240\n",
    "          02       0.68      0.64      0.66       227\n",
    "          03       0.64      0.44      0.52       142\n",
    "\n",
    "    accuracy                           0.59       609\n",
    "   macro avg       0.61      0.57      0.58       609\n",
    "weighted avg       0.61      0.59      0.59       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 01:03:56 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 26 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.07450792315754, 1.0752001327442613, 1.0747914273163368, 1.0742988226253216, 1.0743023681718924, 1.0742996058049068, 1.0740067577127166, 1.0737461016095917, 1.0734858612708857, 1.0730247718751529, 1.0724004816343435, 1.0714323792747285, 1.0699764447063451, 1.0675578540181878, 1.0634437713325513, 1.056350592909188, 1.0442916873249122, 1.0232937803800861, 0.9936615168950437, 0.9664219105967943, 0.9567764629479896, 0.9536349918258993, 0.9577582494182931, 0.9480617262655486, 0.9488970249744472, 0.9458483732588381, 0.9450397622604871, 0.9437878713231956, 0.9427487227717056, 0.9417139298614414, 0.9396537424895564, 0.9398447204693198, 0.9430584184250417, 0.9383393792291775, 0.9385569947106498, 0.9346368804158053, 0.9324482113661241, 0.9355177257840074, 0.9306203952955299, 0.9298529277489886, 0.9293479688453361, 0.9276892772840553, 0.9245118726846229, 0.9252182159126294, 0.9239361980082758, 0.9205217797963686, 0.9199925730576852, 0.9203610931124006, 0.9160516575247979, 0.9151889203217229, 0.9289341973162245, 0.9113809996833551, 0.9108281353815827, 0.9115302440372398, 0.9137063954264072, 0.9092098504843187, 0.9091351879837086, 0.9107783497456455, 0.906771475751999, 0.9057820375721247, 0.9065162237054609, 0.9126933405943496, 0.903682500564406, 0.9028640142802534, 0.9082445921177543, 0.901701569459317, 0.9018919633331361, 0.9000852191193742, 0.9093726558246832, 0.9059007107134914, 0.9011810404326528, 0.9011047087083701, 0.8960212611016773, 0.9016193673136982, 0.8974748658037733, 0.8927302479939704, 0.8983525649471628, 0.9190711151007165, 0.8939921441923808, 0.8908573110115352, 0.8928268781827979, 0.8940366648492359, 0.8936464696486399, 0.8861830747382002, 0.8886904117509062, 0.8863039732175116, 0.8870618087904794, 0.8939774496410476, 0.883454392891995, 0.8898555110828043, 0.8880189298017468, 0.887158916893068, 0.8800985656544101, 0.8897955660358047, 0.8781854941927153, 0.878075683156062, 0.8951444615088464, 0.8776740426891935, 0.877650909235912, 0.8790062246064247, 0.875810799829674, 0.8761682090500893, 0.8774454786468218, 0.8753453993249214, 0.8764233811344028, 0.9134036311375097, 0.8749117569383142, 0.8849833901292585, 0.8797089382149708, 0.8940326489251236, 0.8762820131085777, 0.8759344035181506, 0.87753357333307, 0.8813566249383886, 0.8751045190446287, 0.8756692092406926, 0.878520281933407, 0.8730416660042624, 0.8732013022175368, 0.8734031969494812, 0.8935818772010615, 0.874839108663631, 0.8763379397846404, 0.8742443236028424, 0.8768404533318894, 0.8786531494951796, 0.8739491215676118, 0.8737766564577475, 0.880768202600025, 0.873172294819492, 0.8766589006179659, 0.8745001057294398, 0.8765936931365816, 0.8939695850577456, 0.8768675818623385, 0.8776398547764482, 0.8764314860937434, 0.8796179657694937, 0.8763921344025773, 0.8752288520825516, 0.888040927928461, 0.8774554404523377, 0.8787666862625598, 0.87738342854777, 0.8779266212374118, 0.8765584725660253, 0.8885031719317381, 0.8758455096990213, 0.8872917527439951, 0.8797713448652884, 0.880741912547395, 0.8764494053640194, 0.876258388235064, 0.8818656639708282, 0.8769787835761635, 0.8767497373136198, 0.875182491120055, 0.874134473906362, 0.8851074925784407, 0.9025704397748061, 0.8764611759600772, 0.8752172836920702, 0.8808418782473785, 0.8988634799110087, 0.879349440189418, 0.8746029500695089, 0.8905357258464707, 0.8769294206536267, 0.8951167669789545, 0.881066937066847, 0.8766887789876591, 0.8774316174839126, 0.8752264421561669, 0.8728870212150912, 0.8726039221525583, 0.8752547347878392, 0.8804855051103289, 0.8727909485107572, 0.8775678633273333, 0.8732722280256462, 0.8803126137636369, 0.8740619834029224, 0.8728584731349412, 0.8748274557108949, 0.8710777510954633, 0.8750150317237491, 0.8699587128432513, 0.8713596098137215, 0.8713445310130691, 0.870728177683694, 0.8744729167134891, 0.8736090903798935, 0.8730004237007429, 0.8785599709927351, 0.8890731783923257, 0.8718246892951, 0.8703784990584713, 0.8943751281118159, 0.8691338931006947, 0.8764033729611164, 0.8728482762385276, 0.871682599358175, 0.8711013428879097, 0.8750554010981605, 0.8680223751146413, 0.8685120693568525, 0.8776691568700355, 0.8690955541012518, 0.867283860450895, 0.8725511619610152, 0.8675146884910383, 0.8686811296614911, 0.8766731590473006, 0.8690620748867542, 0.866487360548699, 0.8686020113955969, 0.8785191303599252, 0.8717570159039866, 0.8729617642651638, 0.8739558369263836, 0.8759750591710284, 0.8646036612576452, 0.8640272257167522, 0.8653564666488096, 0.863971793984349, 0.8738284140384843, 0.8739292663697931, 0.8647656614948768, 0.8665136989505812, 0.8675445522776574, 0.8754901237088472, 0.864092060008464, 0.8621784037557142, 0.8653384449055238, 0.8871557209487815, 0.8733384819649319, 0.8675151889155847, 0.860029974888111, 0.8602815541532044, 0.8902626736410733, 0.8573342538232287, 0.8726282452519109, 0.8619588481381609, 0.8719134498112308, 0.8596791010380574, 0.8583333094914755, 0.860681253505262, 0.8843254778772739, 0.863408430065036, 0.862440258802843, 0.8551880323828147, 0.8549513330404785, 0.8810671436767077, 0.8929640685004749, 0.8553054658650178, 0.8511757420005861, 0.8538705127188333, 0.8542478296948576, 0.8505107562725963, 0.8583911803164114, 0.8530255120571807, 0.8528522959679414, 0.855099954824338, 0.8503927867400822, 0.8481003205568723, 0.8465013686072063, 0.8474702726444, 0.8645308536457507, 0.8621499073524976, 0.846229301003987, 0.8448732913225546, 0.8419702926097049, 0.8430583278338114, 0.8393153180238258, 0.856511993654843, 0.8458640981581802, 0.8545286467509904, 0.8524561292432212, 0.8378247659781883, 0.8398117963316405, 0.843469139586137, 0.8372825294096873, 0.8362874369120167, 0.835576886418222, 0.8479620162489379, 0.8425984742801961, 0.8344246474001403, 0.8366615316159228, 0.8364010637225384, 0.832502534138941, 0.8369840047825342, 0.8333084416898405, 0.8345720041757343, 0.837590871186092, 0.8356047917665128, 0.8335400999864725, 0.8297378310233305, 0.8304712710709408, 0.8315588532410232, 0.8290736793688757], 'val_acc': [0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.42036124701766153, 0.4269293915168405, 0.4384236444882767, 0.47290640359833125, 0.47290640359833125, 0.4909687970072178, 0.49261083313201254, 0.495894905381602, 0.515599338683393, 0.515599338683393, 0.49261083738948713, 0.5123152664338035, 0.49261083738948713, 0.5090311939884681, 0.5123152664338035, 0.5188834110308554, 0.509031194184214, 0.5073891579615463, 0.5188834109329825, 0.515599338781266, 0.4975369413106508, 0.5270935916548292, 0.509031194184214, 0.5106732303090088, 0.5205254471556502, 0.5057471221303705, 0.5336617362518812, 0.5254515555300344, 0.5254515556279075, 0.5303776640022917, 0.5402298809468061, 0.5320197003228324, 0.5320197004207053, 0.5418719171694738, 0.5451559894190633, 0.5336617365455001, 0.5418719171694738, 0.5517241340161153, 0.5106732302111358, 0.553366170238783, 0.5566502425862455, 0.5467980256417311, 0.5287356281711159, 0.5615763511563757, 0.5435139531963956, 0.5353037727681679, 0.5467980256417311, 0.5500820978913206, 0.5697865317803494, 0.5320197002249594, 0.5517241340161153, 0.5517241340161153, 0.5402298810446791, 0.5500820976955746, 0.5451559894190633, 0.5665024594328869, 0.5385878448220114, 0.5418719172673468, 0.5714285680030171, 0.5517241340161153, 0.559934314444343, 0.5500820977934475, 0.5779967126000691, 0.5829228206808343, 0.5550082063635777, 0.5320197002249594, 0.5796387486269908, 0.5779967123064501, 0.5648604231123462, 0.5566502423904995, 0.5615763508627567, 0.5763546759859094, 0.5845648570013751, 0.5681444952640627, 0.5681444953619357, 0.5517241338203693, 0.5681444953619357, 0.5582922786131672, 0.5862068929304238, 0.5878489290552186, 0.5829228206808343, 0.5615763508627567, 0.579638748235499, 0.5763546760837824, 0.5435139532942685, 0.5763546759859094, 0.5796387483333719, 0.566502458943522, 0.5829228205829614, 0.5927750374296029, 0.5862068929304238, 0.5779967121107042, 0.5763546761816554, 0.5385878448220114, 0.5845648566098832, 0.5911330012069351, 0.5845648566098832, 0.5500820977934475, 0.5845648567077562, 0.586206892734678, 0.5779967120128312, 0.5665024590413951, 0.5779967123064501, 0.5829228204850885, 0.5648604230144733, 0.5796387484312449, 0.5862068929304238, 0.5878489290552186, 0.5467980254459851, 0.5763546761816554, 0.579638748039753, 0.5845648566098832, 0.5714285677093982, 0.5697865313888575, 0.5829228206808343, 0.5862068926368049, 0.5648604230144733, 0.5894909652778864, 0.5829228202893425, 0.5779967118170852, 0.5747126398611146, 0.5566502423904995, 0.5796387483333719, 0.57963874794188, 0.5779967121107042, 0.566502459139268, 0.5763546756922905, 0.5878489288594727, 0.5566502424883725, 0.5796387484312449, 0.5697865312909846, 0.5779967121107042, 0.5779967121107042, 0.5779967121107042, 0.5582922785152943, 0.5878489288594727, 0.5812807843602937, 0.5697865311931115, 0.5714285675136522, 0.5829228202893425, 0.5911330011090622, 0.5615763506670107, 0.5829228205829614, 0.5845648566098832, 0.579638748039753, 0.5779967119149583, 0.5615763507648838, 0.5435139532942685, 0.5779967119149583, 0.5812807841645478, 0.5582922785152943, 0.5550082062657048, 0.5911330012069351, 0.586206892538932, 0.553366170238783, 0.5763546757901635, 0.5812807845560396, 0.559934314640089, 0.579638748137626, 0.5894909650821404, 0.5812807842624207, 0.5894909648863944, 0.5812807842624207, 0.5730706033448281, 0.5697865312909846, 0.5862068926368049, 0.5812807844581667, 0.57963874794188, 0.5697865312909846, 0.5927750372338569, 0.5779967118170852, 0.573070603540574, 0.5812807841645478, 0.5747126395674957, 0.579638748137626, 0.5829228204850885, 0.5812807843602937, 0.5911330011090622, 0.566502458943522, 0.5747126393717498, 0.5812807843602937, 0.579638748137626, 0.5500820979891935, 0.5812807842624207, 0.586206892734678, 0.5467980256417311, 0.5779967119149583, 0.5829228203872154, 0.5747126395674957, 0.5894909650821404, 0.5730706033448281, 0.5648604229166003, 0.579638748137626, 0.579638748137626, 0.5632183867918055, 0.573070603540574, 0.586206892734678, 0.566502458943522, 0.579638748137626, 0.5894909650821404, 0.5615763505691378, 0.5714285671221603, 0.5812807843602937, 0.579638748137626, 0.5632183867918055, 0.573070603442701, 0.5812807843602937, 0.5681444950683168, 0.5648604228187273, 0.5681444950683168, 0.5747126396653688, 0.5829228204850885, 0.579638748235499, 0.5615763503733918, 0.5615763506670107, 0.5911330013048082, 0.5829228206808343, 0.5615763503733918, 0.5615763505691378, 0.5845648567077562, 0.5845648567077562, 0.5615763505691378, 0.5435139530985226, 0.5582922783195483, 0.5829228205829614, 0.5911330013048082, 0.5599343143464701, 0.5435139530006496, 0.5747126399589877, 0.5845648567077562, 0.5911330012069351, 0.5648604226229813, 0.5697865311931115, 0.5550082058742128, 0.5648604228187273, 0.5451559890275715, 0.5632183865960596, 0.5911330013048082, 0.5977011458039871, 0.5632183865960596, 0.5533661699451641, 0.5402298805553142, 0.5944170734565246, 0.579638748235499, 0.5648604227208543, 0.5681444951661898, 0.5681444948725708, 0.5648604228187273, 0.5730706033448281, 0.5763546756922905, 0.5697865310952386, 0.5681444948725708, 0.5829228202893425, 0.5812807841645478, 0.5763546758880365, 0.5615763502755189, 0.5533661697494181, 0.5714285671221603, 0.5763546758880365, 0.5845648564141372, 0.5779967117192123, 0.5812807840666747, 0.5977011456082412, 0.6042692902052931, 0.5665024584541571, 0.5681444949704438, 0.5812807841645478, 0.5927750374296029, 0.579638747844007, 0.5894909647885215, 0.5829228202893425, 0.5911330009133162, 0.566502458747776, 0.5730706033448281, 0.5944170729671597, 0.5960591093855734, 0.5894909647885215, 0.5960591091898274, 0.5878489284679808, 0.5960591093855734, 0.5960591092877005, 0.5845648562183913, 0.5829228201914695, 0.5960591093855734, 0.5960591092877005, 0.6042692900095471, 0.5960591094834464, 0.592775037038111], 'loss': [1.0762365752421854, 1.0749533211181297, 1.0751293961761914, 1.0743875729229906, 1.0744195985598242, 1.0743634454523514, 1.0741692384900008, 1.0737620443289284, 1.0733959206810233, 1.0731175772218489, 1.0725709366847358, 1.071665731641546, 1.0705302761320705, 1.068444407108628, 1.0650782467648234, 1.0592666423296293, 1.0493311232365132, 1.0325770542606925, 1.0055105551801913, 0.973100767194368, 0.944274589046071, 0.9344683441293313, 0.929176875159481, 0.9264320160574002, 0.9191952157559091, 0.9190162391388441, 0.9188271770487086, 0.9158394349429152, 0.9147636534496989, 0.9133066074314549, 0.9113480928986959, 0.909924245272329, 0.9086692955949224, 0.9105473612123446, 0.9069537714521498, 0.9051139204409088, 0.9036337643922966, 0.9026770333734626, 0.9023726001412472, 0.8989398157327327, 0.8971509838985466, 0.8961896858665732, 0.8952183780484131, 0.8938316397843175, 0.8940170719148687, 0.8924466026392316, 0.8906259541394039, 0.889900971756334, 0.8887748659024248, 0.8875334795495567, 0.8879283506277895, 0.8952722199888444, 0.8897352188764411, 0.8860268849856555, 0.8837110748036442, 0.8848136415471776, 0.8854527940740331, 0.8810308992495527, 0.8818948463737597, 0.8811449286384504, 0.8803892835209747, 0.8799646674729961, 0.8790906855457862, 0.87777727697909, 0.8782264825010202, 0.8766265558755862, 0.8751287353846572, 0.8748359795223761, 0.8741537669111327, 0.8799523860032553, 0.8748367629501609, 0.8733778141852032, 0.8731230488792827, 0.8723256063167564, 0.872701151072367, 0.8736935954808699, 0.8782311629710501, 0.8756025527047425, 0.8832061184015607, 0.8738665476227199, 0.8718657157993904, 0.8692276891496882, 0.869059061979611, 0.8677439862452982, 0.8680545033125907, 0.8683361244887052, 0.8688533844154718, 0.8666275706868887, 0.8663111910193363, 0.8659725143924142, 0.8606892885123925, 0.8736105574229904, 0.8728389214441272, 0.8629114339239053, 0.8632092717002304, 0.8622919626794067, 0.8696133065272651, 0.8633180795753761, 0.8600798620580403, 0.8617018140316989, 0.8592621332321324, 0.8611312579325338, 0.8618423748799663, 0.8616293494951064, 0.8651605090321457, 0.8645943172168927, 0.8746596415184851, 0.8718938883814723, 0.868325896600923, 0.8659737049921338, 0.8649468845655296, 0.8606759616726478, 0.859036910950036, 0.8576387584576617, 0.8597108476215809, 0.8567338706286781, 0.8581092796041735, 0.8570956327097616, 0.8558234067668171, 0.8555143934990102, 0.8564911675649013, 0.865177125803499, 0.8561341079353552, 0.8640106043286881, 0.8585142885390248, 0.855154320420181, 0.8571677100731852, 0.8537594854464521, 0.8543882567290163, 0.8543984209977136, 0.8534856945827022, 0.8600969962026059, 0.8554555554409536, 0.8559531939837477, 0.862970460024213, 0.8544511663105943, 0.8545607557286962, 0.8532157424783805, 0.8526222558726521, 0.8528493604376086, 0.8526207534684292, 0.8558363684883352, 0.8535454446774978, 0.8523465100010318, 0.853161456061095, 0.8540346446957676, 0.8514373705861994, 0.8576490342249861, 0.8548894035742758, 0.8574775830431396, 0.8511908510627199, 0.8494188755444677, 0.8501863300922715, 0.8493670292703522, 0.849664869347637, 0.8473356628809622, 0.8484283222065324, 0.8469875693321228, 0.8477084163767601, 0.8574260644599397, 0.8576987044522405, 0.84832977548517, 0.8469953506389438, 0.8516864631455048, 0.8520003013542301, 0.8502223594722317, 0.8518238443613542, 0.8515775794366057, 0.8489461346573408, 0.8664197626789493, 0.8518759939704832, 0.8467042673539822, 0.8503460375924864, 0.8437779614568002, 0.8447423837023349, 0.8447719326009496, 0.8432724710607431, 0.8486721424351482, 0.8439084070908706, 0.8507680877523011, 0.8469142757157281, 0.8424329936137189, 0.8448241173853864, 0.8414170185888082, 0.8423914517954879, 0.8418153523664454, 0.8437492309164952, 0.8434080751769597, 0.8435808778053926, 0.8440494099436844, 0.8490790730873907, 0.8411580227483714, 0.8420060003073063, 0.8436954021943424, 0.8496805137432576, 0.8481856429846135, 0.8384020280544273, 0.8401702083601355, 0.8485331848906296, 0.8515655051511416, 0.8422610520092614, 0.8361978050374886, 0.8449673671497212, 0.8422409323451455, 0.8365888882466656, 0.8381439129429921, 0.8356702781311051, 0.8406682532426023, 0.8353142783382346, 0.8387911369668385, 0.8348961451215177, 0.8353852716804285, 0.8358430177524104, 0.8382584439410811, 0.8339841408896006, 0.8351081256748959, 0.8333236125460396, 0.8371384008464382, 0.8385481084396706, 0.8357062801688114, 0.8354824518520974, 0.8352890766866398, 0.8317474024251746, 0.8308374399522002, 0.8363170200794384, 0.8389234314953766, 0.8363529231269257, 0.8322326539478263, 0.836442276486626, 0.8350108965955966, 0.8300471825521339, 0.8310891374915043, 0.8326612619159158, 0.8276586028829492, 0.8281298147089917, 0.8377302025867439, 0.8303561547453643, 0.8347263691606462, 0.8259201725405589, 0.8250882572217154, 0.8339341072331219, 0.823753642815584, 0.8425174544968889, 0.8290895396679089, 0.8215378153250692, 0.8200003978652876, 0.8234021163819017, 0.8185395112762216, 0.8294129516554564, 0.8313931529526838, 0.8221399172865145, 0.8209199561720266, 0.8213016842669775, 0.8344742318932771, 0.8475851281957215, 0.8218855062304581, 0.8150993376052356, 0.8145629613796054, 0.8136803385413403, 0.8102016425965013, 0.8115083815625561, 0.8098603529607001, 0.8123809672234239, 0.811158378339646, 0.8083021106171657, 0.8077234539407969, 0.8095636158998007, 0.8104424522642727, 0.8130754102426877, 0.809180342025091, 0.8056871777687229, 0.8123978739401643, 0.8080109555373691, 0.8195899924947986, 0.8167435327349747, 0.8131936279165671, 0.8103309586307597, 0.8043542343488219, 0.8079759321173603, 0.7991976402868236, 0.7999891081629837, 0.7960109918024506, 0.7992919009079433, 0.7984406063444071, 0.7978202260250429, 0.7986554461093409, 0.7942912861551837, 0.7986776255239452, 0.7959147785478549, 0.7948249723632232, 0.7916965532841379, 0.7931746370248971, 0.7933526224179435, 0.7994810022857399, 0.7933789104162056, 0.792075382121045, 0.7891285460342862, 0.7932677484880483, 0.8055747861979679, 0.8153664385268821], 'acc': [0.3942505115233897, 0.39425051195175986, 0.39425051449750237, 0.3942505113275634, 0.39425051273506523, 0.39425051171921605, 0.3942505133225443, 0.3942505152808078, 0.39425051351837065, 0.39425051547663414, 0.394250513714197, 0.39425051312671794, 0.40246406368895965, 0.427926080158359, 0.44147843916802926, 0.459137577136684, 0.4743326492010935, 0.4854209464555893, 0.4981519488583355, 0.49609856155129184, 0.5055441449310256, 0.5232032863021632, 0.5223819338075923, 0.5268993858439232, 0.5301848091383979, 0.5334702275372138, 0.5363449687096128, 0.5388090359846425, 0.5326488742593377, 0.5347022605872497, 0.5334702267539085, 0.5445585245958834, 0.5412731048262829, 0.5330595499191441, 0.5507186815479208, 0.5367556496567305, 0.5359342926581537, 0.5478439390047375, 0.5367556425335471, 0.550308010857208, 0.5474332607991886, 0.5490759774155196, 0.5474332619741468, 0.5515400429281121, 0.5478439378297794, 0.5470225918709131, 0.5494866569918528, 0.5466119130778851, 0.5552361430573513, 0.5478439395922166, 0.55811088638384, 0.5507186900418887, 0.5457905564709611, 0.555646813160585, 0.5650923987678433, 0.5540041038632638, 0.5568788527463251, 0.5622176558330074, 0.56057494418577, 0.560164266959353, 0.559342912114866, 0.5663244331886637, 0.5659137621062982, 0.565913753416504, 0.5642710512423662, 0.5626283399867815, 0.5737166293347885, 0.5663244360036673, 0.5696098519791323, 0.5691991751443679, 0.5696098527624377, 0.5675564723827511, 0.57002053292625, 0.5712525703579003, 0.5650924058910268, 0.5675564719910984, 0.5655030844882284, 0.5720739261815191, 0.556468175519908, 0.5683778280105434, 0.5790554381738698, 0.5757700164460059, 0.5708418850291681, 0.5786447603599736, 0.5798767939974885, 0.5770020575983569, 0.5770020504751734, 0.5778234082570556, 0.5700205297930284, 0.573305956489986, 0.5885010257393918, 0.5700205307721602, 0.5716632422235712, 0.5802874712239056, 0.5798767939974885, 0.5757700235691893, 0.582751544642987, 0.5794661236984284, 0.5802874743571271, 0.5835728933434222, 0.5831622131796098, 0.5843942513211308, 0.5765913724654509, 0.5778234130303228, 0.5778234104845803, 0.5811088252850871, 0.5618069778232848, 0.5691991771026313, 0.5806981486461491, 0.5770020487127363, 0.5811088270475243, 0.5806981484503227, 0.5794661233067757, 0.5806981484503227, 0.5819301811087058, 0.5802874781512627, 0.5774127338456422, 0.5843942481879091, 0.5839835690032286, 0.5860369563102722, 0.5868583204320325, 0.5753593390237625, 0.5794661157919396, 0.5823408606850391, 0.5852156020532643, 0.5868583142879807, 0.5831622180752686, 0.5835728907976797, 0.5856262814337713, 0.5876796679575096, 0.5831622137670889, 0.5798767961515783, 0.5823408662416117, 0.5885010226061701, 0.5848049260018053, 0.5794661150086342, 0.5835728945183803, 0.5905544101090402, 0.5823408577476439, 0.5897330570269904, 0.5889117089140342, 0.5774127280932433, 0.5835728945183803, 0.5946611878563491, 0.5893223784297889, 0.582340858335123, 0.5852156076098369, 0.5794661157919396, 0.5831622163128314, 0.5811088327999232, 0.5852156034240488, 0.5901437336659284, 0.5905544105006929, 0.5909650896853735, 0.5905544193863135, 0.5913757665201379, 0.5864476422264835, 0.5885010276976552, 0.5868583152671125, 0.5848049242393681, 0.5724845958931001, 0.5889117031616352, 0.5934291542188342, 0.5954825440716205, 0.5843942487753881, 0.5835728996833003, 0.5905544103048666, 0.583162221869404, 0.5798767936058358, 0.5720739186666829, 0.586858320823685, 0.5889117079349024, 0.5794661173585504, 0.593018478559028, 0.5885010231936492, 0.5864476368657373, 0.5934291625169758, 0.589322380779705, 0.5872689936684877, 0.585215601269959, 0.5942505102382794, 0.5909650894895472, 0.5889116998325873, 0.5909650910561579, 0.5901437371908027, 0.5942505127840219, 0.5901437334701021, 0.5901437346450602, 0.5831622147462207, 0.5872689905352662, 0.5856262879694757, 0.5921971229312356, 0.5848049238477154, 0.586858313308849, 0.5909650941893795, 0.5843942547236135, 0.5913757665201379, 0.5864476432056153, 0.5880903534820682, 0.5852156044031805, 0.5901437415724172, 0.5979466103675184, 0.5811088254809135, 0.5864476358866055, 0.5954825477923211, 0.595482542113357, 0.5950718676285087, 0.5930184779715489, 0.5930184771882435, 0.588090344792274, 0.5950718666493771, 0.5954825440716205, 0.5971252545438998, 0.5909650896853735, 0.5938398399392193, 0.5954825448549259, 0.5954825442674468, 0.5901437328826231, 0.5889117012033717, 0.5958932195356005, 0.5975359333369277, 0.5950718652785926, 0.5967145788840934, 0.5967145777091354, 0.5963038967620176, 0.5921971223437565, 0.5983572885730672, 0.5967145771216563, 0.5926078015284372, 0.5901437358200183, 0.6041067738552602, 0.5967145755550455, 0.5999999976745621, 0.5983572877897619, 0.6053388065136434, 0.5938398322285567, 0.5987679691285324, 0.5901437358200183, 0.6041067754218711, 0.5987679640370472, 0.5909650904686788, 0.5983572897480254, 0.5848049261976317, 0.6041067777717872, 0.5983572899438517, 0.6065708391720265, 0.5995893179024024, 0.6004106780342008, 0.59794661350074, 0.5991786440050333, 0.6045174526482882, 0.6069815189441861, 0.6016427095176258, 0.5958932211022112, 0.59096508772711, 0.5991786400885063, 0.6073921963664296, 0.6135523573084288, 0.6106776177026408, 0.613552363574872, 0.6049281283080945, 0.6131416812569699, 0.604928130853837, 0.6114989723513014, 0.6110882959081897, 0.6069815175734017, 0.6119096499693711, 0.6135523600499978, 0.6168377821695143, 0.6102669422386608, 0.6151950695431453, 0.6139630392346783, 0.6127310067721216, 0.6078028706554515, 0.6053388084719068, 0.6016427104967575, 0.6057494841317131, 0.6147843976040396, 0.6147843964290814, 0.6176591354473905, 0.624229978707292, 0.6160164257584166, 0.6180698148278975, 0.6180698158070292, 0.6188911694765581, 0.6139630372764149, 0.6147843954499498, 0.6168377821695143, 0.6090349060554034, 0.620123204289031, 0.6242299785114657, 0.6213552353808033, 0.6221765900294639, 0.6119096507526766, 0.6147843915334228, 0.6160164249751112, 0.6217659143696576, 0.621355235772456, 0.6020533915173102, 0.6045174522566354]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
