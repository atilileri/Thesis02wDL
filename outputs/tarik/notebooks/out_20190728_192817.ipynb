{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf68.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 19:28:17 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'Split', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 5 Label(s): ['05', '03', '04', '01', '02'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F10226E198>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F126E96EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6073, Accuracy:0.2244, Validation Loss:1.6058, Validation Accuracy:0.2328\n",
    "Epoch #2: Loss:1.6056, Accuracy:0.2330, Validation Loss:1.6054, Validation Accuracy:0.2328\n",
    "Epoch #3: Loss:1.6054, Accuracy:0.2330, Validation Loss:1.6055, Validation Accuracy:0.2328\n",
    "Epoch #4: Loss:1.6051, Accuracy:0.2330, Validation Loss:1.6048, Validation Accuracy:0.2328\n",
    "Epoch #5: Loss:1.6046, Accuracy:0.2330, Validation Loss:1.6043, Validation Accuracy:0.2328\n",
    "Epoch #6: Loss:1.6046, Accuracy:0.2330, Validation Loss:1.6037, Validation Accuracy:0.2328\n",
    "Epoch #7: Loss:1.6042, Accuracy:0.2398, Validation Loss:1.6038, Validation Accuracy:0.2401\n",
    "Epoch #8: Loss:1.6034, Accuracy:0.2344, Validation Loss:1.6036, Validation Accuracy:0.2328\n",
    "Epoch #9: Loss:1.6039, Accuracy:0.2330, Validation Loss:1.6034, Validation Accuracy:0.2381\n",
    "Epoch #10: Loss:1.6036, Accuracy:0.2381, Validation Loss:1.6031, Validation Accuracy:0.2381\n",
    "Epoch #11: Loss:1.6033, Accuracy:0.2340, Validation Loss:1.6029, Validation Accuracy:0.2393\n",
    "Epoch #12: Loss:1.6022, Accuracy:0.2406, Validation Loss:1.6026, Validation Accuracy:0.2484\n",
    "Epoch #13: Loss:1.6021, Accuracy:0.2389, Validation Loss:1.6025, Validation Accuracy:0.2488\n",
    "Epoch #14: Loss:1.6020, Accuracy:0.2420, Validation Loss:1.6017, Validation Accuracy:0.2463\n",
    "Epoch #15: Loss:1.6016, Accuracy:0.2405, Validation Loss:1.6009, Validation Accuracy:0.2463\n",
    "Epoch #16: Loss:1.6016, Accuracy:0.2415, Validation Loss:1.6018, Validation Accuracy:0.2467\n",
    "Epoch #17: Loss:1.6022, Accuracy:0.2422, Validation Loss:1.6009, Validation Accuracy:0.2434\n",
    "Epoch #18: Loss:1.6030, Accuracy:0.2337, Validation Loss:1.6011, Validation Accuracy:0.2315\n",
    "Epoch #19: Loss:1.6018, Accuracy:0.2345, Validation Loss:1.6004, Validation Accuracy:0.2492\n",
    "Epoch #20: Loss:1.6009, Accuracy:0.2375, Validation Loss:1.6008, Validation Accuracy:0.2438\n",
    "Epoch #21: Loss:1.6009, Accuracy:0.2385, Validation Loss:1.6004, Validation Accuracy:0.2492\n",
    "Epoch #22: Loss:1.6004, Accuracy:0.2417, Validation Loss:1.6002, Validation Accuracy:0.2479\n",
    "Epoch #23: Loss:1.6011, Accuracy:0.2396, Validation Loss:1.6000, Validation Accuracy:0.2414\n",
    "Epoch #24: Loss:1.6002, Accuracy:0.2401, Validation Loss:1.5997, Validation Accuracy:0.2479\n",
    "Epoch #25: Loss:1.5996, Accuracy:0.2377, Validation Loss:1.6001, Validation Accuracy:0.2422\n",
    "Epoch #26: Loss:1.5997, Accuracy:0.2350, Validation Loss:1.5994, Validation Accuracy:0.2422\n",
    "Epoch #27: Loss:1.5990, Accuracy:0.2376, Validation Loss:1.5993, Validation Accuracy:0.2418\n",
    "Epoch #28: Loss:1.5992, Accuracy:0.2388, Validation Loss:1.5995, Validation Accuracy:0.2484\n",
    "Epoch #29: Loss:1.6009, Accuracy:0.2396, Validation Loss:1.5993, Validation Accuracy:0.2451\n",
    "Epoch #30: Loss:1.5994, Accuracy:0.2386, Validation Loss:1.5995, Validation Accuracy:0.2434\n",
    "Epoch #31: Loss:1.5994, Accuracy:0.2406, Validation Loss:1.5992, Validation Accuracy:0.2484\n",
    "Epoch #32: Loss:1.5994, Accuracy:0.2355, Validation Loss:1.5996, Validation Accuracy:0.2344\n",
    "Epoch #33: Loss:1.5997, Accuracy:0.2381, Validation Loss:1.5989, Validation Accuracy:0.2484\n",
    "Epoch #34: Loss:1.5999, Accuracy:0.2364, Validation Loss:1.5995, Validation Accuracy:0.2356\n",
    "Epoch #35: Loss:1.5990, Accuracy:0.2358, Validation Loss:1.5988, Validation Accuracy:0.2401\n",
    "Epoch #36: Loss:1.5994, Accuracy:0.2381, Validation Loss:1.5991, Validation Accuracy:0.2430\n",
    "Epoch #37: Loss:1.6026, Accuracy:0.2316, Validation Loss:1.6008, Validation Accuracy:0.2381\n",
    "Epoch #38: Loss:1.6010, Accuracy:0.2363, Validation Loss:1.5999, Validation Accuracy:0.2438\n",
    "Epoch #39: Loss:1.5994, Accuracy:0.2408, Validation Loss:1.5993, Validation Accuracy:0.2516\n",
    "Epoch #40: Loss:1.5984, Accuracy:0.2423, Validation Loss:1.5981, Validation Accuracy:0.2471\n",
    "Epoch #41: Loss:1.5986, Accuracy:0.2403, Validation Loss:1.5987, Validation Accuracy:0.2467\n",
    "Epoch #42: Loss:1.5985, Accuracy:0.2421, Validation Loss:1.5978, Validation Accuracy:0.2479\n",
    "Epoch #43: Loss:1.5984, Accuracy:0.2418, Validation Loss:1.5982, Validation Accuracy:0.2455\n",
    "Epoch #44: Loss:1.5982, Accuracy:0.2377, Validation Loss:1.5974, Validation Accuracy:0.2459\n",
    "Epoch #45: Loss:1.5987, Accuracy:0.2390, Validation Loss:1.5982, Validation Accuracy:0.2438\n",
    "Epoch #46: Loss:1.5995, Accuracy:0.2341, Validation Loss:1.5993, Validation Accuracy:0.2381\n",
    "Epoch #47: Loss:1.5985, Accuracy:0.2395, Validation Loss:1.5984, Validation Accuracy:0.2418\n",
    "Epoch #48: Loss:1.5997, Accuracy:0.2373, Validation Loss:1.5994, Validation Accuracy:0.2410\n",
    "Epoch #49: Loss:1.5989, Accuracy:0.2375, Validation Loss:1.5989, Validation Accuracy:0.2389\n",
    "Epoch #50: Loss:1.5991, Accuracy:0.2440, Validation Loss:1.5986, Validation Accuracy:0.2385\n",
    "Epoch #51: Loss:1.5989, Accuracy:0.2433, Validation Loss:1.5985, Validation Accuracy:0.2356\n",
    "Epoch #52: Loss:1.5988, Accuracy:0.2363, Validation Loss:1.5979, Validation Accuracy:0.2426\n",
    "Epoch #53: Loss:1.5992, Accuracy:0.2417, Validation Loss:1.5978, Validation Accuracy:0.2430\n",
    "Epoch #54: Loss:1.5980, Accuracy:0.2437, Validation Loss:1.5984, Validation Accuracy:0.2525\n",
    "Epoch #55: Loss:1.5982, Accuracy:0.2386, Validation Loss:1.5982, Validation Accuracy:0.2422\n",
    "Epoch #56: Loss:1.5980, Accuracy:0.2389, Validation Loss:1.5979, Validation Accuracy:0.2385\n",
    "Epoch #57: Loss:1.5987, Accuracy:0.2378, Validation Loss:1.5979, Validation Accuracy:0.2434\n",
    "Epoch #58: Loss:1.5991, Accuracy:0.2381, Validation Loss:1.5980, Validation Accuracy:0.2414\n",
    "Epoch #59: Loss:1.5979, Accuracy:0.2407, Validation Loss:1.5978, Validation Accuracy:0.2410\n",
    "Epoch #60: Loss:1.5975, Accuracy:0.2449, Validation Loss:1.5971, Validation Accuracy:0.2516\n",
    "Epoch #61: Loss:1.5970, Accuracy:0.2473, Validation Loss:1.5969, Validation Accuracy:0.2492\n",
    "Epoch #62: Loss:1.5984, Accuracy:0.2407, Validation Loss:1.5978, Validation Accuracy:0.2557\n",
    "Epoch #63: Loss:1.5976, Accuracy:0.2454, Validation Loss:1.5976, Validation Accuracy:0.2615\n",
    "Epoch #64: Loss:1.5971, Accuracy:0.2414, Validation Loss:1.5968, Validation Accuracy:0.2504\n",
    "Epoch #65: Loss:1.5973, Accuracy:0.2422, Validation Loss:1.5965, Validation Accuracy:0.2451\n",
    "Epoch #66: Loss:1.5974, Accuracy:0.2357, Validation Loss:1.5971, Validation Accuracy:0.2373\n",
    "Epoch #67: Loss:1.5978, Accuracy:0.2347, Validation Loss:1.5970, Validation Accuracy:0.2586\n",
    "Epoch #68: Loss:1.5966, Accuracy:0.2403, Validation Loss:1.5970, Validation Accuracy:0.2451\n",
    "Epoch #69: Loss:1.5988, Accuracy:0.2397, Validation Loss:1.5973, Validation Accuracy:0.2484\n",
    "Epoch #70: Loss:1.5974, Accuracy:0.2386, Validation Loss:1.5975, Validation Accuracy:0.2545\n",
    "Epoch #71: Loss:1.5974, Accuracy:0.2438, Validation Loss:1.5977, Validation Accuracy:0.2537\n",
    "Epoch #72: Loss:1.5977, Accuracy:0.2440, Validation Loss:1.5970, Validation Accuracy:0.2570\n",
    "Epoch #73: Loss:1.5981, Accuracy:0.2370, Validation Loss:1.5978, Validation Accuracy:0.2414\n",
    "Epoch #74: Loss:1.5989, Accuracy:0.2371, Validation Loss:1.5971, Validation Accuracy:0.2434\n",
    "Epoch #75: Loss:1.5977, Accuracy:0.2381, Validation Loss:1.5965, Validation Accuracy:0.2537\n",
    "Epoch #76: Loss:1.5978, Accuracy:0.2392, Validation Loss:1.5967, Validation Accuracy:0.2393\n",
    "Epoch #77: Loss:1.5976, Accuracy:0.2345, Validation Loss:1.5958, Validation Accuracy:0.2443\n",
    "Epoch #78: Loss:1.5973, Accuracy:0.2387, Validation Loss:1.5964, Validation Accuracy:0.2492\n",
    "Epoch #79: Loss:1.5971, Accuracy:0.2493, Validation Loss:1.5965, Validation Accuracy:0.2603\n",
    "Epoch #80: Loss:1.5982, Accuracy:0.2471, Validation Loss:1.5971, Validation Accuracy:0.2586\n",
    "Epoch #81: Loss:1.5967, Accuracy:0.2421, Validation Loss:1.5962, Validation Accuracy:0.2525\n",
    "Epoch #82: Loss:1.5966, Accuracy:0.2411, Validation Loss:1.5958, Validation Accuracy:0.2479\n",
    "Epoch #83: Loss:1.5981, Accuracy:0.2424, Validation Loss:1.5958, Validation Accuracy:0.2434\n",
    "Epoch #84: Loss:1.5960, Accuracy:0.2406, Validation Loss:1.5965, Validation Accuracy:0.2566\n",
    "Epoch #85: Loss:1.5990, Accuracy:0.2416, Validation Loss:1.5955, Validation Accuracy:0.2599\n",
    "Epoch #86: Loss:1.5967, Accuracy:0.2427, Validation Loss:1.5957, Validation Accuracy:0.2484\n",
    "Epoch #87: Loss:1.5971, Accuracy:0.2472, Validation Loss:1.5950, Validation Accuracy:0.2619\n",
    "Epoch #88: Loss:1.5962, Accuracy:0.2417, Validation Loss:1.5947, Validation Accuracy:0.2508\n",
    "Epoch #89: Loss:1.5955, Accuracy:0.2464, Validation Loss:1.5942, Validation Accuracy:0.2479\n",
    "Epoch #90: Loss:1.5964, Accuracy:0.2469, Validation Loss:1.5945, Validation Accuracy:0.2586\n",
    "Epoch #91: Loss:1.5961, Accuracy:0.2515, Validation Loss:1.5951, Validation Accuracy:0.2586\n",
    "Epoch #92: Loss:1.5964, Accuracy:0.2506, Validation Loss:1.5949, Validation Accuracy:0.2619\n",
    "Epoch #93: Loss:1.5960, Accuracy:0.2546, Validation Loss:1.5951, Validation Accuracy:0.2656\n",
    "Epoch #94: Loss:1.5956, Accuracy:0.2503, Validation Loss:1.5943, Validation Accuracy:0.2578\n",
    "Epoch #95: Loss:1.5960, Accuracy:0.2505, Validation Loss:1.5941, Validation Accuracy:0.2578\n",
    "Epoch #96: Loss:1.5959, Accuracy:0.2478, Validation Loss:1.5941, Validation Accuracy:0.2623\n",
    "Epoch #97: Loss:1.5963, Accuracy:0.2512, Validation Loss:1.5944, Validation Accuracy:0.2541\n",
    "Epoch #98: Loss:1.5960, Accuracy:0.2437, Validation Loss:1.5940, Validation Accuracy:0.2594\n",
    "Epoch #99: Loss:1.5955, Accuracy:0.2501, Validation Loss:1.5946, Validation Accuracy:0.2566\n",
    "Epoch #100: Loss:1.5957, Accuracy:0.2501, Validation Loss:1.5939, Validation Accuracy:0.2594\n",
    "Epoch #101: Loss:1.5949, Accuracy:0.2473, Validation Loss:1.5936, Validation Accuracy:0.2562\n",
    "Epoch #102: Loss:1.5953, Accuracy:0.2474, Validation Loss:1.5935, Validation Accuracy:0.2570\n",
    "Epoch #103: Loss:1.5957, Accuracy:0.2505, Validation Loss:1.5950, Validation Accuracy:0.2582\n",
    "Epoch #104: Loss:1.5951, Accuracy:0.2497, Validation Loss:1.5939, Validation Accuracy:0.2574\n",
    "Epoch #105: Loss:1.5955, Accuracy:0.2498, Validation Loss:1.5938, Validation Accuracy:0.2562\n",
    "Epoch #106: Loss:1.5951, Accuracy:0.2464, Validation Loss:1.5936, Validation Accuracy:0.2553\n",
    "Epoch #107: Loss:1.5958, Accuracy:0.2475, Validation Loss:1.5936, Validation Accuracy:0.2574\n",
    "Epoch #108: Loss:1.5958, Accuracy:0.2510, Validation Loss:1.5935, Validation Accuracy:0.2627\n",
    "Epoch #109: Loss:1.5945, Accuracy:0.2516, Validation Loss:1.5932, Validation Accuracy:0.2553\n",
    "Epoch #110: Loss:1.5949, Accuracy:0.2476, Validation Loss:1.5931, Validation Accuracy:0.2533\n",
    "Epoch #111: Loss:1.5945, Accuracy:0.2465, Validation Loss:1.5927, Validation Accuracy:0.2582\n",
    "Epoch #112: Loss:1.5954, Accuracy:0.2463, Validation Loss:1.5930, Validation Accuracy:0.2578\n",
    "Epoch #113: Loss:1.5951, Accuracy:0.2485, Validation Loss:1.5925, Validation Accuracy:0.2603\n",
    "Epoch #114: Loss:1.5943, Accuracy:0.2462, Validation Loss:1.5926, Validation Accuracy:0.2562\n",
    "Epoch #115: Loss:1.5939, Accuracy:0.2557, Validation Loss:1.5927, Validation Accuracy:0.2586\n",
    "Epoch #116: Loss:1.5941, Accuracy:0.2488, Validation Loss:1.5927, Validation Accuracy:0.2574\n",
    "Epoch #117: Loss:1.5952, Accuracy:0.2500, Validation Loss:1.5920, Validation Accuracy:0.2578\n",
    "Epoch #118: Loss:1.5935, Accuracy:0.2555, Validation Loss:1.5927, Validation Accuracy:0.2557\n",
    "Epoch #119: Loss:1.5936, Accuracy:0.2509, Validation Loss:1.5920, Validation Accuracy:0.2582\n",
    "Epoch #120: Loss:1.5938, Accuracy:0.2516, Validation Loss:1.5922, Validation Accuracy:0.2566\n",
    "Epoch #121: Loss:1.5939, Accuracy:0.2562, Validation Loss:1.5918, Validation Accuracy:0.2582\n",
    "Epoch #122: Loss:1.5947, Accuracy:0.2470, Validation Loss:1.5927, Validation Accuracy:0.2521\n",
    "Epoch #123: Loss:1.5948, Accuracy:0.2518, Validation Loss:1.5917, Validation Accuracy:0.2599\n",
    "Epoch #124: Loss:1.5926, Accuracy:0.2512, Validation Loss:1.5911, Validation Accuracy:0.2672\n",
    "Epoch #125: Loss:1.5938, Accuracy:0.2540, Validation Loss:1.5909, Validation Accuracy:0.2586\n",
    "Epoch #126: Loss:1.5937, Accuracy:0.2518, Validation Loss:1.5930, Validation Accuracy:0.2623\n",
    "Epoch #127: Loss:1.5937, Accuracy:0.2503, Validation Loss:1.5923, Validation Accuracy:0.2545\n",
    "Epoch #128: Loss:1.5992, Accuracy:0.2325, Validation Loss:1.6008, Validation Accuracy:0.2151\n",
    "Epoch #129: Loss:1.5982, Accuracy:0.2456, Validation Loss:1.5966, Validation Accuracy:0.2512\n",
    "Epoch #130: Loss:1.5977, Accuracy:0.2450, Validation Loss:1.5987, Validation Accuracy:0.2570\n",
    "Epoch #131: Loss:1.5973, Accuracy:0.2475, Validation Loss:1.5955, Validation Accuracy:0.2541\n",
    "Epoch #132: Loss:1.5939, Accuracy:0.2483, Validation Loss:1.5951, Validation Accuracy:0.2553\n",
    "Epoch #133: Loss:1.5946, Accuracy:0.2468, Validation Loss:1.5939, Validation Accuracy:0.2525\n",
    "Epoch #134: Loss:1.5942, Accuracy:0.2552, Validation Loss:1.5934, Validation Accuracy:0.2541\n",
    "Epoch #135: Loss:1.5949, Accuracy:0.2494, Validation Loss:1.5940, Validation Accuracy:0.2537\n",
    "Epoch #136: Loss:1.5936, Accuracy:0.2493, Validation Loss:1.5926, Validation Accuracy:0.2537\n",
    "Epoch #137: Loss:1.5951, Accuracy:0.2461, Validation Loss:1.5931, Validation Accuracy:0.2611\n",
    "Epoch #138: Loss:1.5957, Accuracy:0.2472, Validation Loss:1.5940, Validation Accuracy:0.2512\n",
    "Epoch #139: Loss:1.5948, Accuracy:0.2493, Validation Loss:1.5948, Validation Accuracy:0.2701\n",
    "Epoch #140: Loss:1.5948, Accuracy:0.2512, Validation Loss:1.5934, Validation Accuracy:0.2590\n",
    "Epoch #141: Loss:1.5936, Accuracy:0.2576, Validation Loss:1.5943, Validation Accuracy:0.2475\n",
    "Epoch #142: Loss:1.5967, Accuracy:0.2417, Validation Loss:1.5942, Validation Accuracy:0.2566\n",
    "Epoch #143: Loss:1.5939, Accuracy:0.2524, Validation Loss:1.5951, Validation Accuracy:0.2459\n",
    "Epoch #144: Loss:1.5945, Accuracy:0.2462, Validation Loss:1.5943, Validation Accuracy:0.2545\n",
    "Epoch #145: Loss:1.5936, Accuracy:0.2490, Validation Loss:1.5936, Validation Accuracy:0.2484\n",
    "Epoch #146: Loss:1.5941, Accuracy:0.2535, Validation Loss:1.5936, Validation Accuracy:0.2582\n",
    "Epoch #147: Loss:1.5941, Accuracy:0.2507, Validation Loss:1.5938, Validation Accuracy:0.2541\n",
    "Epoch #148: Loss:1.5934, Accuracy:0.2507, Validation Loss:1.5936, Validation Accuracy:0.2603\n",
    "Epoch #149: Loss:1.5935, Accuracy:0.2474, Validation Loss:1.5933, Validation Accuracy:0.2533\n",
    "Epoch #150: Loss:1.5936, Accuracy:0.2553, Validation Loss:1.5935, Validation Accuracy:0.2623\n",
    "Epoch #151: Loss:1.5935, Accuracy:0.2530, Validation Loss:1.5927, Validation Accuracy:0.2541\n",
    "Epoch #152: Loss:1.5931, Accuracy:0.2574, Validation Loss:1.5926, Validation Accuracy:0.2635\n",
    "Epoch #153: Loss:1.5938, Accuracy:0.2531, Validation Loss:1.5928, Validation Accuracy:0.2566\n",
    "Epoch #154: Loss:1.5928, Accuracy:0.2521, Validation Loss:1.5925, Validation Accuracy:0.2660\n",
    "Epoch #155: Loss:1.5933, Accuracy:0.2565, Validation Loss:1.5936, Validation Accuracy:0.2557\n",
    "Epoch #156: Loss:1.5924, Accuracy:0.2548, Validation Loss:1.5918, Validation Accuracy:0.2640\n",
    "Epoch #157: Loss:1.5921, Accuracy:0.2560, Validation Loss:1.5915, Validation Accuracy:0.2541\n",
    "Epoch #158: Loss:1.5914, Accuracy:0.2565, Validation Loss:1.5920, Validation Accuracy:0.2545\n",
    "Epoch #159: Loss:1.5918, Accuracy:0.2597, Validation Loss:1.5911, Validation Accuracy:0.2656\n",
    "Epoch #160: Loss:1.5927, Accuracy:0.2591, Validation Loss:1.5923, Validation Accuracy:0.2615\n",
    "Epoch #161: Loss:1.5909, Accuracy:0.2578, Validation Loss:1.5915, Validation Accuracy:0.2619\n",
    "Epoch #162: Loss:1.5911, Accuracy:0.2569, Validation Loss:1.5911, Validation Accuracy:0.2635\n",
    "Epoch #163: Loss:1.5908, Accuracy:0.2562, Validation Loss:1.5903, Validation Accuracy:0.2623\n",
    "Epoch #164: Loss:1.5905, Accuracy:0.2516, Validation Loss:1.5908, Validation Accuracy:0.2574\n",
    "Epoch #165: Loss:1.5909, Accuracy:0.2533, Validation Loss:1.5909, Validation Accuracy:0.2570\n",
    "Epoch #166: Loss:1.5901, Accuracy:0.2551, Validation Loss:1.5911, Validation Accuracy:0.2512\n",
    "Epoch #167: Loss:1.5906, Accuracy:0.2539, Validation Loss:1.5907, Validation Accuracy:0.2553\n",
    "Epoch #168: Loss:1.5903, Accuracy:0.2555, Validation Loss:1.5911, Validation Accuracy:0.2488\n",
    "Epoch #169: Loss:1.5928, Accuracy:0.2493, Validation Loss:1.5908, Validation Accuracy:0.2455\n",
    "Epoch #170: Loss:1.5909, Accuracy:0.2496, Validation Loss:1.5894, Validation Accuracy:0.2574\n",
    "Epoch #171: Loss:1.5896, Accuracy:0.2621, Validation Loss:1.5898, Validation Accuracy:0.2570\n",
    "Epoch #172: Loss:1.5897, Accuracy:0.2540, Validation Loss:1.5897, Validation Accuracy:0.2533\n",
    "Epoch #173: Loss:1.5894, Accuracy:0.2575, Validation Loss:1.5896, Validation Accuracy:0.2619\n",
    "Epoch #174: Loss:1.5901, Accuracy:0.2585, Validation Loss:1.5894, Validation Accuracy:0.2586\n",
    "Epoch #175: Loss:1.5898, Accuracy:0.2532, Validation Loss:1.5890, Validation Accuracy:0.2594\n",
    "Epoch #176: Loss:1.5902, Accuracy:0.2530, Validation Loss:1.5889, Validation Accuracy:0.2623\n",
    "Epoch #177: Loss:1.5888, Accuracy:0.2607, Validation Loss:1.5890, Validation Accuracy:0.2627\n",
    "Epoch #178: Loss:1.5903, Accuracy:0.2586, Validation Loss:1.5875, Validation Accuracy:0.2623\n",
    "Epoch #179: Loss:1.5894, Accuracy:0.2548, Validation Loss:1.5906, Validation Accuracy:0.2549\n",
    "Epoch #180: Loss:1.5920, Accuracy:0.2555, Validation Loss:1.5903, Validation Accuracy:0.2603\n",
    "Epoch #181: Loss:1.5928, Accuracy:0.2511, Validation Loss:1.5929, Validation Accuracy:0.2529\n",
    "Epoch #182: Loss:1.5933, Accuracy:0.2490, Validation Loss:1.5910, Validation Accuracy:0.2557\n",
    "Epoch #183: Loss:1.5925, Accuracy:0.2482, Validation Loss:1.5929, Validation Accuracy:0.2525\n",
    "Epoch #184: Loss:1.5921, Accuracy:0.2541, Validation Loss:1.5915, Validation Accuracy:0.2582\n",
    "Epoch #185: Loss:1.5924, Accuracy:0.2522, Validation Loss:1.5911, Validation Accuracy:0.2619\n",
    "Epoch #186: Loss:1.5917, Accuracy:0.2563, Validation Loss:1.5913, Validation Accuracy:0.2635\n",
    "Epoch #187: Loss:1.5913, Accuracy:0.2584, Validation Loss:1.5908, Validation Accuracy:0.2578\n",
    "Epoch #188: Loss:1.5922, Accuracy:0.2499, Validation Loss:1.5904, Validation Accuracy:0.2652\n",
    "Epoch #189: Loss:1.5907, Accuracy:0.2524, Validation Loss:1.5890, Validation Accuracy:0.2635\n",
    "Epoch #190: Loss:1.5911, Accuracy:0.2559, Validation Loss:1.5896, Validation Accuracy:0.2709\n",
    "Epoch #191: Loss:1.5911, Accuracy:0.2494, Validation Loss:1.5897, Validation Accuracy:0.2672\n",
    "Epoch #192: Loss:1.5905, Accuracy:0.2517, Validation Loss:1.5889, Validation Accuracy:0.2689\n",
    "Epoch #193: Loss:1.5916, Accuracy:0.2508, Validation Loss:1.5923, Validation Accuracy:0.2471\n",
    "Epoch #194: Loss:1.5919, Accuracy:0.2483, Validation Loss:1.5898, Validation Accuracy:0.2656\n",
    "Epoch #195: Loss:1.5922, Accuracy:0.2536, Validation Loss:1.5913, Validation Accuracy:0.2504\n",
    "Epoch #196: Loss:1.5923, Accuracy:0.2512, Validation Loss:1.5917, Validation Accuracy:0.2640\n",
    "Epoch #197: Loss:1.5919, Accuracy:0.2520, Validation Loss:1.5914, Validation Accuracy:0.2685\n",
    "Epoch #198: Loss:1.5917, Accuracy:0.2531, Validation Loss:1.5921, Validation Accuracy:0.2599\n",
    "Epoch #199: Loss:1.5913, Accuracy:0.2527, Validation Loss:1.5910, Validation Accuracy:0.2648\n",
    "Epoch #200: Loss:1.5913, Accuracy:0.2548, Validation Loss:1.5897, Validation Accuracy:0.2685\n",
    "Epoch #201: Loss:1.5905, Accuracy:0.2513, Validation Loss:1.5899, Validation Accuracy:0.2681\n",
    "Epoch #202: Loss:1.5904, Accuracy:0.2501, Validation Loss:1.5903, Validation Accuracy:0.2512\n",
    "Epoch #203: Loss:1.5902, Accuracy:0.2522, Validation Loss:1.5912, Validation Accuracy:0.2615\n",
    "Epoch #204: Loss:1.5908, Accuracy:0.2498, Validation Loss:1.5904, Validation Accuracy:0.2590\n",
    "Epoch #205: Loss:1.5903, Accuracy:0.2520, Validation Loss:1.5905, Validation Accuracy:0.2713\n",
    "Epoch #206: Loss:1.5951, Accuracy:0.2504, Validation Loss:1.5950, Validation Accuracy:0.2627\n",
    "Epoch #207: Loss:1.5921, Accuracy:0.2487, Validation Loss:1.5913, Validation Accuracy:0.2566\n",
    "Epoch #208: Loss:1.5900, Accuracy:0.2507, Validation Loss:1.5897, Validation Accuracy:0.2590\n",
    "Epoch #209: Loss:1.5902, Accuracy:0.2505, Validation Loss:1.5895, Validation Accuracy:0.2644\n",
    "Epoch #210: Loss:1.5894, Accuracy:0.2520, Validation Loss:1.5895, Validation Accuracy:0.2631\n",
    "Epoch #211: Loss:1.5893, Accuracy:0.2545, Validation Loss:1.5902, Validation Accuracy:0.2648\n",
    "Epoch #212: Loss:1.5893, Accuracy:0.2565, Validation Loss:1.5895, Validation Accuracy:0.2648\n",
    "Epoch #213: Loss:1.5895, Accuracy:0.2506, Validation Loss:1.5893, Validation Accuracy:0.2545\n",
    "Epoch #214: Loss:1.5894, Accuracy:0.2521, Validation Loss:1.5890, Validation Accuracy:0.2615\n",
    "Epoch #215: Loss:1.5893, Accuracy:0.2541, Validation Loss:1.5885, Validation Accuracy:0.2677\n",
    "Epoch #216: Loss:1.5888, Accuracy:0.2545, Validation Loss:1.5900, Validation Accuracy:0.2607\n",
    "Epoch #217: Loss:1.5889, Accuracy:0.2526, Validation Loss:1.5906, Validation Accuracy:0.2557\n",
    "Epoch #218: Loss:1.5893, Accuracy:0.2531, Validation Loss:1.5904, Validation Accuracy:0.2681\n",
    "Epoch #219: Loss:1.5890, Accuracy:0.2545, Validation Loss:1.5898, Validation Accuracy:0.2681\n",
    "Epoch #220: Loss:1.5892, Accuracy:0.2528, Validation Loss:1.5888, Validation Accuracy:0.2685\n",
    "Epoch #221: Loss:1.5888, Accuracy:0.2520, Validation Loss:1.5893, Validation Accuracy:0.2664\n",
    "Epoch #222: Loss:1.5884, Accuracy:0.2557, Validation Loss:1.5889, Validation Accuracy:0.2648\n",
    "Epoch #223: Loss:1.5882, Accuracy:0.2570, Validation Loss:1.5890, Validation Accuracy:0.2681\n",
    "Epoch #224: Loss:1.5882, Accuracy:0.2549, Validation Loss:1.5885, Validation Accuracy:0.2705\n",
    "Epoch #225: Loss:1.5881, Accuracy:0.2539, Validation Loss:1.5899, Validation Accuracy:0.2557\n",
    "Epoch #226: Loss:1.5888, Accuracy:0.2543, Validation Loss:1.5903, Validation Accuracy:0.2685\n",
    "Epoch #227: Loss:1.5877, Accuracy:0.2565, Validation Loss:1.5894, Validation Accuracy:0.2578\n",
    "Epoch #228: Loss:1.5871, Accuracy:0.2561, Validation Loss:1.5885, Validation Accuracy:0.2656\n",
    "Epoch #229: Loss:1.5882, Accuracy:0.2550, Validation Loss:1.5891, Validation Accuracy:0.2594\n",
    "Epoch #230: Loss:1.5887, Accuracy:0.2556, Validation Loss:1.5908, Validation Accuracy:0.2611\n",
    "Epoch #231: Loss:1.5876, Accuracy:0.2555, Validation Loss:1.5881, Validation Accuracy:0.2664\n",
    "Epoch #232: Loss:1.5877, Accuracy:0.2563, Validation Loss:1.5874, Validation Accuracy:0.2668\n",
    "Epoch #233: Loss:1.5866, Accuracy:0.2561, Validation Loss:1.5880, Validation Accuracy:0.2640\n",
    "Epoch #234: Loss:1.5880, Accuracy:0.2512, Validation Loss:1.5893, Validation Accuracy:0.2410\n",
    "Epoch #235: Loss:1.5869, Accuracy:0.2578, Validation Loss:1.5870, Validation Accuracy:0.2656\n",
    "Epoch #236: Loss:1.5864, Accuracy:0.2570, Validation Loss:1.5872, Validation Accuracy:0.2648\n",
    "Epoch #237: Loss:1.5865, Accuracy:0.2550, Validation Loss:1.5880, Validation Accuracy:0.2640\n",
    "Epoch #238: Loss:1.5878, Accuracy:0.2525, Validation Loss:1.5879, Validation Accuracy:0.2677\n",
    "Epoch #239: Loss:1.5862, Accuracy:0.2566, Validation Loss:1.5872, Validation Accuracy:0.2627\n",
    "Epoch #240: Loss:1.5857, Accuracy:0.2565, Validation Loss:1.5869, Validation Accuracy:0.2656\n",
    "Epoch #241: Loss:1.5860, Accuracy:0.2537, Validation Loss:1.5874, Validation Accuracy:0.2648\n",
    "Epoch #242: Loss:1.5856, Accuracy:0.2583, Validation Loss:1.5870, Validation Accuracy:0.2660\n",
    "Epoch #243: Loss:1.5862, Accuracy:0.2537, Validation Loss:1.5870, Validation Accuracy:0.2672\n",
    "Epoch #244: Loss:1.5848, Accuracy:0.2573, Validation Loss:1.5884, Validation Accuracy:0.2599\n",
    "Epoch #245: Loss:1.5861, Accuracy:0.2556, Validation Loss:1.5890, Validation Accuracy:0.2656\n",
    "Epoch #246: Loss:1.5866, Accuracy:0.2538, Validation Loss:1.5872, Validation Accuracy:0.2537\n",
    "Epoch #247: Loss:1.5852, Accuracy:0.2561, Validation Loss:1.5871, Validation Accuracy:0.2664\n",
    "Epoch #248: Loss:1.5848, Accuracy:0.2598, Validation Loss:1.5873, Validation Accuracy:0.2668\n",
    "Epoch #249: Loss:1.5844, Accuracy:0.2573, Validation Loss:1.5876, Validation Accuracy:0.2660\n",
    "Epoch #250: Loss:1.5888, Accuracy:0.2545, Validation Loss:1.5891, Validation Accuracy:0.2430\n",
    "Epoch #251: Loss:1.5844, Accuracy:0.2590, Validation Loss:1.5873, Validation Accuracy:0.2615\n",
    "Epoch #252: Loss:1.5847, Accuracy:0.2577, Validation Loss:1.5873, Validation Accuracy:0.2656\n",
    "Epoch #253: Loss:1.5844, Accuracy:0.2569, Validation Loss:1.5881, Validation Accuracy:0.2590\n",
    "Epoch #254: Loss:1.5857, Accuracy:0.2554, Validation Loss:1.5867, Validation Accuracy:0.2648\n",
    "Epoch #255: Loss:1.5852, Accuracy:0.2514, Validation Loss:1.5872, Validation Accuracy:0.2545\n",
    "Epoch #256: Loss:1.5851, Accuracy:0.2561, Validation Loss:1.5870, Validation Accuracy:0.2685\n",
    "Epoch #257: Loss:1.5849, Accuracy:0.2544, Validation Loss:1.5869, Validation Accuracy:0.2660\n",
    "Epoch #258: Loss:1.5847, Accuracy:0.2568, Validation Loss:1.5879, Validation Accuracy:0.2644\n",
    "Epoch #259: Loss:1.5845, Accuracy:0.2578, Validation Loss:1.5871, Validation Accuracy:0.2656\n",
    "Epoch #260: Loss:1.5841, Accuracy:0.2551, Validation Loss:1.5874, Validation Accuracy:0.2681\n",
    "Epoch #261: Loss:1.5842, Accuracy:0.2561, Validation Loss:1.5891, Validation Accuracy:0.2664\n",
    "Epoch #262: Loss:1.5858, Accuracy:0.2551, Validation Loss:1.5882, Validation Accuracy:0.2660\n",
    "Epoch #263: Loss:1.5838, Accuracy:0.2571, Validation Loss:1.5870, Validation Accuracy:0.2718\n",
    "Epoch #264: Loss:1.5838, Accuracy:0.2512, Validation Loss:1.5870, Validation Accuracy:0.2693\n",
    "Epoch #265: Loss:1.5840, Accuracy:0.2525, Validation Loss:1.5865, Validation Accuracy:0.2644\n",
    "Epoch #266: Loss:1.5844, Accuracy:0.2491, Validation Loss:1.5883, Validation Accuracy:0.2668\n",
    "Epoch #267: Loss:1.5850, Accuracy:0.2545, Validation Loss:1.5875, Validation Accuracy:0.2434\n",
    "Epoch #268: Loss:1.5836, Accuracy:0.2583, Validation Loss:1.5877, Validation Accuracy:0.2549\n",
    "Epoch #269: Loss:1.5853, Accuracy:0.2574, Validation Loss:1.5892, Validation Accuracy:0.2590\n",
    "Epoch #270: Loss:1.5859, Accuracy:0.2471, Validation Loss:1.5879, Validation Accuracy:0.2586\n",
    "Epoch #271: Loss:1.5833, Accuracy:0.2615, Validation Loss:1.5861, Validation Accuracy:0.2672\n",
    "Epoch #272: Loss:1.5830, Accuracy:0.2597, Validation Loss:1.5864, Validation Accuracy:0.2619\n",
    "Epoch #273: Loss:1.5829, Accuracy:0.2570, Validation Loss:1.5866, Validation Accuracy:0.2631\n",
    "Epoch #274: Loss:1.5841, Accuracy:0.2587, Validation Loss:1.5868, Validation Accuracy:0.2640\n",
    "Epoch #275: Loss:1.5831, Accuracy:0.2573, Validation Loss:1.5858, Validation Accuracy:0.2562\n",
    "Epoch #276: Loss:1.5826, Accuracy:0.2532, Validation Loss:1.5861, Validation Accuracy:0.2549\n",
    "Epoch #277: Loss:1.5828, Accuracy:0.2569, Validation Loss:1.5866, Validation Accuracy:0.2701\n",
    "Epoch #278: Loss:1.5834, Accuracy:0.2564, Validation Loss:1.5872, Validation Accuracy:0.2697\n",
    "Epoch #279: Loss:1.5833, Accuracy:0.2589, Validation Loss:1.5860, Validation Accuracy:0.2656\n",
    "Epoch #280: Loss:1.5831, Accuracy:0.2573, Validation Loss:1.5867, Validation Accuracy:0.2619\n",
    "Epoch #281: Loss:1.5824, Accuracy:0.2552, Validation Loss:1.5868, Validation Accuracy:0.2607\n",
    "Epoch #282: Loss:1.5821, Accuracy:0.2573, Validation Loss:1.5871, Validation Accuracy:0.2603\n",
    "Epoch #283: Loss:1.5831, Accuracy:0.2566, Validation Loss:1.5876, Validation Accuracy:0.2689\n",
    "Epoch #284: Loss:1.5828, Accuracy:0.2531, Validation Loss:1.5878, Validation Accuracy:0.2553\n",
    "Epoch #285: Loss:1.5826, Accuracy:0.2581, Validation Loss:1.5877, Validation Accuracy:0.2631\n",
    "Epoch #286: Loss:1.5832, Accuracy:0.2571, Validation Loss:1.5871, Validation Accuracy:0.2599\n",
    "Epoch #287: Loss:1.5829, Accuracy:0.2577, Validation Loss:1.5853, Validation Accuracy:0.2549\n",
    "Epoch #288: Loss:1.5831, Accuracy:0.2536, Validation Loss:1.5894, Validation Accuracy:0.2660\n",
    "Epoch #289: Loss:1.5831, Accuracy:0.2542, Validation Loss:1.5868, Validation Accuracy:0.2672\n",
    "Epoch #290: Loss:1.5820, Accuracy:0.2561, Validation Loss:1.5862, Validation Accuracy:0.2627\n",
    "Epoch #291: Loss:1.5829, Accuracy:0.2571, Validation Loss:1.5848, Validation Accuracy:0.2668\n",
    "Epoch #292: Loss:1.5825, Accuracy:0.2560, Validation Loss:1.5847, Validation Accuracy:0.2656\n",
    "Epoch #293: Loss:1.5825, Accuracy:0.2592, Validation Loss:1.5858, Validation Accuracy:0.2689\n",
    "Epoch #294: Loss:1.5822, Accuracy:0.2551, Validation Loss:1.5861, Validation Accuracy:0.2660\n",
    "Epoch #295: Loss:1.5827, Accuracy:0.2580, Validation Loss:1.5857, Validation Accuracy:0.2681\n",
    "Epoch #296: Loss:1.5821, Accuracy:0.2576, Validation Loss:1.5872, Validation Accuracy:0.2533\n",
    "Epoch #297: Loss:1.5835, Accuracy:0.2538, Validation Loss:1.5867, Validation Accuracy:0.2586\n",
    "Epoch #298: Loss:1.5818, Accuracy:0.2609, Validation Loss:1.5861, Validation Accuracy:0.2689\n",
    "Epoch #299: Loss:1.5813, Accuracy:0.2599, Validation Loss:1.5856, Validation Accuracy:0.2627\n",
    "Epoch #300: Loss:1.5816, Accuracy:0.2585, Validation Loss:1.5864, Validation Accuracy:0.2672\n",
    "\n",
    "Test:\n",
    "Test Loss:1.58644402, Accuracy:0.2672\n",
    "Labels: ['05', '03', '04', '01', '02']\n",
    "Confusion Matrix:\n",
    "       05  03   04   01  02\n",
    "t:05  297  19   57  178  16\n",
    "t:03  206  20   50  172  11\n",
    "t:04  172  12  108  149   9\n",
    "t:01  192   9   72  205  25\n",
    "t:02  186   9   87  154  21\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.28      0.52      0.37       567\n",
    "          03       0.29      0.04      0.08       459\n",
    "          04       0.29      0.24      0.26       450\n",
    "          01       0.24      0.41      0.30       503\n",
    "          02       0.26      0.05      0.08       457\n",
    "\n",
    "    accuracy                           0.27      2436\n",
    "   macro avg       0.27      0.25      0.22      2436\n",
    "weighted avg       0.27      0.27      0.22      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 20:33:02 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 4 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6057528212348424, 1.6054372525176, 1.605454322935521, 1.6048345372007398, 1.6042710303868761, 1.6036821208368186, 1.6037678422990496, 1.6035787150973366, 1.6033948994622442, 1.6030990216140872, 1.602924658942888, 1.602559959947182, 1.6024774686847805, 1.6017035292874415, 1.6008986620284458, 1.6017587795633401, 1.6009339832124256, 1.6010635106630122, 1.6004051786338167, 1.6008329172244018, 1.6003962669075025, 1.6001729262481965, 1.6000133116648507, 1.5997111439117657, 1.6001303327103162, 1.5994201863340556, 1.5993274736091225, 1.5994835271819667, 1.599303355749409, 1.5994568779355003, 1.5992485513828072, 1.5995532303608109, 1.598945073110521, 1.5995151487673054, 1.5988234400944952, 1.5990750133893368, 1.6008365115117165, 1.5998928531245842, 1.599291790882355, 1.5981120411398375, 1.598679167100753, 1.5977678443802206, 1.5981506477240075, 1.5973891117694148, 1.5982259357308324, 1.599325087465872, 1.5983702615759838, 1.5993776117835334, 1.598904447798267, 1.5985623789929795, 1.5985384813474708, 1.5978818662060892, 1.5978192314138553, 1.5983733199108605, 1.598236376820331, 1.59790076175934, 1.5979077557624854, 1.5980027003828527, 1.5978456250160982, 1.5970934351481045, 1.5968555217893252, 1.5977540501624297, 1.597588349249954, 1.5968010533227905, 1.5964975680036497, 1.597149860291254, 1.5970423082608503, 1.5969942252232718, 1.5973105428645569, 1.597541512722648, 1.5976793227720338, 1.5970011499323478, 1.5978394531459839, 1.5971276272693877, 1.5964790970233862, 1.5967227124619758, 1.5958037677852588, 1.5964337943614215, 1.5964890953355235, 1.5970989432436689, 1.5962157067406941, 1.59584283554691, 1.5958070494662757, 1.5965179561198444, 1.5954784272339544, 1.5956675133290157, 1.5950420114207151, 1.594689331814182, 1.594206944279287, 1.5945141176480573, 1.5951126876527257, 1.5949414798191615, 1.5951386472861755, 1.5942805958498876, 1.594089784841428, 1.594069491662024, 1.5943964369387071, 1.593960366225595, 1.594586632521869, 1.5939015195091761, 1.5935849064872378, 1.5935493574549608, 1.5950167006851221, 1.593931057183026, 1.5938166505206004, 1.593627870767966, 1.5936160768781389, 1.5935176693159958, 1.5932037932140681, 1.5931487981909014, 1.5927045171092493, 1.593042713472213, 1.5924884973488418, 1.5925652726334696, 1.5927495052074563, 1.5927202640887357, 1.591994710743721, 1.5926731744619034, 1.5920073899925244, 1.5922093802484973, 1.591816392438165, 1.592716585239166, 1.5916653910685448, 1.5911251097085637, 1.5909154428832832, 1.5929520096880658, 1.5922799656543825, 1.6008305238385505, 1.5966108397310004, 1.5986799215056822, 1.5955214398639348, 1.5951319923150324, 1.5938767392451345, 1.5934372333861728, 1.593961765809208, 1.5925938219859683, 1.5931490011794618, 1.594046438660332, 1.5948473369742457, 1.5933790576869045, 1.5942997495920592, 1.5941775557638584, 1.5951242605453642, 1.594287785011755, 1.5935933787638723, 1.593635628571847, 1.5938409457261535, 1.5935574071160679, 1.5932734871928524, 1.593464835328226, 1.5927042088093624, 1.592640274459701, 1.5927557496993217, 1.5925295051487012, 1.5936461516789027, 1.5918065665782184, 1.591458402439487, 1.5919511153780181, 1.5911318281013977, 1.592323173443085, 1.5915006913966538, 1.5911106139372526, 1.5903290514092532, 1.590788722821253, 1.5908733389060485, 1.5911189470385096, 1.5906909510419873, 1.5910559031372196, 1.590773524517692, 1.5893988867698632, 1.589818498771179, 1.5897119906539792, 1.5895839831707708, 1.5893724820101007, 1.5889821408808917, 1.5888863999659597, 1.5889900439282747, 1.58745698153679, 1.5906032351241715, 1.5903351684704985, 1.5928718141342815, 1.5910435081115497, 1.5928614529091345, 1.5914868684042067, 1.5910527181547067, 1.5913113445679739, 1.5908493349704835, 1.5904221836177783, 1.5890108877410638, 1.5896027824170091, 1.589672888049547, 1.5888776830068754, 1.5922566875448367, 1.5898343804238857, 1.5912812609586418, 1.5916818478228816, 1.5913625712856674, 1.5920799855136716, 1.5909552926500443, 1.5896899328247471, 1.589934327528003, 1.590318608949533, 1.5912444438840367, 1.5904398365756758, 1.5905397130155015, 1.5949669900198875, 1.5912983037763824, 1.5897485356417, 1.589500041039315, 1.5895269003211963, 1.5902164704693948, 1.5894570434817736, 1.5893132719891803, 1.5890411504579491, 1.588506218443559, 1.5899749647807606, 1.5905947614773153, 1.590435626863063, 1.5898133917590864, 1.5887953957117642, 1.589349295901156, 1.5889409304839637, 1.588980359005419, 1.588511493992923, 1.5898862773757458, 1.5902813369613171, 1.5893879199067165, 1.5885068579456099, 1.5890503454286673, 1.5907982901007867, 1.5881297112685706, 1.5874190592804958, 1.5879501607421975, 1.5892772138216618, 1.5869738736567631, 1.5872116206314764, 1.588022598100609, 1.5878514100373868, 1.5871518319854987, 1.5869335327633887, 1.5874318585215728, 1.587025230349774, 1.5869968771347271, 1.5884044370040518, 1.5890274963942654, 1.5871936078925046, 1.5871304126795878, 1.587319590970996, 1.5875763589721203, 1.5890513499969332, 1.5872882906047778, 1.5872604302780577, 1.5880651360466367, 1.5866907140108557, 1.5872475556747863, 1.5869957345655594, 1.586882413510227, 1.5878659155959958, 1.58713001140037, 1.5874080740172287, 1.5891291596032129, 1.5881655595964204, 1.586953122823305, 1.58696575548457, 1.5864700538967238, 1.588261263711112, 1.5874739061239709, 1.5877194154047223, 1.5891821762219633, 1.587925818949106, 1.5861345320108098, 1.5864081149813773, 1.586567437707497, 1.586806092747718, 1.585768997767093, 1.5861200867419565, 1.5865907036807934, 1.5871597227008862, 1.5860171028350178, 1.5867160452997744, 1.5868280053334478, 1.5871455986511531, 1.5876167241379937, 1.5878213377813204, 1.5876654419797198, 1.5870537969279173, 1.585275577794155, 1.5894085826544926, 1.5867900826856616, 1.5862334827679914, 1.584756627654403, 1.5847128657088883, 1.5857585114602777, 1.586088815933378, 1.5856914328432632, 1.5871852205696169, 1.5866598574007282, 1.586146188095481, 1.5856437794680667, 1.5864439186791481], 'val_acc': [0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.24014778305548556, 0.23275862044497272, 0.23809523789949213, 0.23809523789949213, 0.2393267649930882, 0.24835796380180053, 0.24876847285746745, 0.2463054186458071, 0.2463054185968706, 0.2467159276280693, 0.24343185535401154, 0.2315270933758449, 0.24917897958865112, 0.24384236438521023, 0.24917898188866613, 0.2479474548195383, 0.24137931019801812, 0.2479474548195383, 0.24220032833382021, 0.24220032833382021, 0.24178981930262153, 0.248357963850737, 0.2450738914788063, 0.2434318553784798, 0.248357963850737, 0.23440065661870396, 0.248357963850737, 0.23563218368783176, 0.24014778307995382, 0.24302134639621759, 0.23809523792396037, 0.24384236438521023, 0.2516420361737312, 0.2471264366837362, 0.24671592765253753, 0.24794745479507008, 0.24548440053447323, 0.24589490956567192, 0.24384236438521023, 0.23809523789949213, 0.24178981925368503, 0.24096880119128766, 0.23891625615763548, 0.23850574695515905, 0.23563218373676825, 0.24261083726714594, 0.24302134646962234, 0.2524630542850651, 0.24220032830935198, 0.23850574695515905, 0.24343185550082103, 0.24137931022248635, 0.24096880124022416, 0.25164203612479474, 0.2491789818641979, 0.2557471241367666, 0.2614942530693092, 0.2504105090311987, 0.24507389155221102, 0.23727421988603126, 0.25862068755090334, 0.24507389155221102, 0.2483579617219997, 0.254515597092107, 0.25369458142759765, 0.25697865137717213, 0.24137930809374905, 0.24343185545188453, 0.2536945812807882, 0.2393267650420247, 0.2442528734653454, 0.24917898198653912, 0.26026272563315767, 0.25862068735515736, 0.2524630542850651, 0.24794745474613358, 0.24343185542741627, 0.256568144401306, 0.2598522143998169, 0.24835796377733227, 0.2619047618313572, 0.2508210178666514, 0.24794745474613358, 0.25862068955729944, 0.25862068955729944, 0.2619047619047619, 0.26559934328342305, 0.2577996692438235, 0.25779967144596555, 0.26231526863594556, 0.2541050879630353, 0.25944170536861827, 0.256568144401306, 0.25944170786437926, 0.25615763537010733, 0.2569786534325047, 0.25821018059950546, 0.2573891624881716, 0.25615763534563907, 0.2553366172832417, 0.25738916246370336, 0.2627257776671442, 0.2553366151545044, 0.25328407244533546, 0.2582101784218317, 0.25779967188639397, 0.2602627257799672, 0.25615763321690177, 0.2586206874040939, 0.2573891604573073, 0.2577996694395695, 0.25574712418570306, 0.2582101785197047, 0.25656814224810043, 0.25821018067291024, 0.25205254500918395, 0.25985221672430026, 0.2672413773528852, 0.25862068745303035, 0.262315271376389, 0.2545155970431705, 0.21510673163853256, 0.25123152704465956, 0.2569786514261086, 0.2541050903609234, 0.2553366151545044, 0.2524630543829381, 0.2541050880609083, 0.2536945811095105, 0.2536945811829152, 0.26108374418492, 0.25123152697125484, 0.27011494052233953, 0.25903119893105353, 0.24753694581280788, 0.256568144499179, 0.24589490961460841, 0.25451559917190786, 0.24835796375286404, 0.25821018057503725, 0.25410509031198686, 0.2602627257799672, 0.25328406990063795, 0.26231527122957954, 0.2541050879630353, 0.26354679842104856, 0.25656814442577425, 0.2660098501614162, 0.2557471263633769, 0.2639573071586283, 0.2541050902630504, 0.2545155993431856, 0.26559934132596347, 0.2614942505246117, 0.2619047621983808, 0.26354679592528757, 0.26231527122957954, 0.2573891628062588, 0.25697865157291805, 0.2512315249403905, 0.25533661530131385, 0.2487684706553254, 0.2454843984546724, 0.2573891629530683, 0.2569786538239966, 0.25328407254320845, 0.26190476210050784, 0.2586206876977128, 0.25944170546649126, 0.26231526912531045, 0.2627257779118267, 0.262315268978501, 0.2549261060254327, 0.26026272592677663, 0.25287356314498605, 0.2557471242346396, 0.2524630542850651, 0.2582101783239587, 0.2619047618558254, 0.2635467958274146, 0.25779966948850597, 0.26518883430116086, 0.2635467983721121, 0.2709359584868639, 0.2672413773528852, 0.2688834134287435, 0.2471264368794822, 0.265599341179154, 0.25041050678012017, 0.26395730710969184, 0.2684729068933058, 0.2598522171402604, 0.2647783254657081, 0.26847290664862333, 0.26806239766636114, 0.25123152518507297, 0.261494252849095, 0.25903119648422906, 0.2713464676159356, 0.26272578021184173, 0.2565681426395924, 0.25903119643529254, 0.264367813791939, 0.2631362869430254, 0.26477832541677165, 0.26477832536783513, 0.25451559733678947, 0.2614942528735632, 0.2676518886351624, 0.26067323530053077, 0.2557471269261465, 0.2680623978131706, 0.2680623978621071, 0.26847290664862333, 0.2664203614436934, 0.2647783230678201, 0.26806239536634613, 0.2705254519024897, 0.2557471265346546, 0.2684729067464963, 0.2577996716417115, 0.2655993411302175, 0.25944170536861827, 0.2610837415423495, 0.2664203616394394, 0.26683087067063804, 0.26395730490754976, 0.24096879896467738, 0.2655993411302175, 0.26477832541677165, 0.26395730740331075, 0.2676518887330354, 0.2627257780096997, 0.265599341179154, 0.26477832311675664, 0.2660098525103677, 0.2672413797507732, 0.2598522170913239, 0.265599341179154, 0.2536945817701531, 0.2664203617373124, 0.26683086837062303, 0.26600985021035267, 0.2430213441696073, 0.2614942530693092, 0.2655993436259785, 0.25903119883318054, 0.26477832311675664, 0.254515599489995, 0.2684729068933058, 0.26600985021035267, 0.26436781403662146, 0.2655993412280905, 0.2680623978621071, 0.26642036168837585, 0.2660098502592892, 0.2717569767939438, 0.26929392485783016, 0.26436781408555793, 0.26683087067063804, 0.24343185564763048, 0.2549261084722572, 0.25903119658210205, 0.25862068745303035, 0.26724137979970974, 0.2619047598983658, 0.26313628943878636, 0.26395730750118374, 0.2561576335594572, 0.25492610891268563, 0.2701149430181005, 0.2697044314911409, 0.2655993412280905, 0.26190476214944436, 0.2606732350069118, 0.2602627261225226, 0.2688834159245045, 0.2553366151055679, 0.2631362869430254, 0.2598522144976899, 0.25492610651479763, 0.2660098501614162, 0.26724137740182174, 0.2627257776671442, 0.26683087057276506, 0.265599341179154, 0.26888341587556797, 0.26600985021035267, 0.2680623978131706, 0.25328407239639894, 0.2586206875019669, 0.2688834159245045, 0.2627257802607782, 0.26724137965290023], 'loss': [1.6073392824960195, 1.605570268679938, 1.6054435438688777, 1.6050736390100122, 1.604638981084804, 1.6046489443867114, 1.6041967366510348, 1.6034172988280624, 1.6039248163450426, 1.6036028570707819, 1.6033197771597203, 1.6021704399610202, 1.60205109496381, 1.6019742186309376, 1.6016277759716495, 1.6016462321643712, 1.6021542261268569, 1.6029582497030803, 1.6017624559833283, 1.6009115503064415, 1.6008553019784069, 1.6004027455249605, 1.6011326626830522, 1.6001503290826535, 1.5995787620054869, 1.5997059081858922, 1.5990013764869015, 1.5992216956688883, 1.6008553494662965, 1.599429459591421, 1.5993574793089098, 1.5993849231477146, 1.599725586628767, 1.599851797787316, 1.5989526231675666, 1.5994430936092714, 1.602598891023248, 1.6009638001052262, 1.599403093287098, 1.598440006185606, 1.598558988757202, 1.5984963339212248, 1.5984144935862485, 1.5982097711406449, 1.5986612622008431, 1.5995370199548145, 1.5985071611600246, 1.5997391440785151, 1.5988904510435382, 1.5990931936358035, 1.5989042854406996, 1.598841032257315, 1.5991654012727052, 1.5980216846328985, 1.598246221963385, 1.5980249204674786, 1.5986723353975363, 1.5990767285564351, 1.597851201347257, 1.5974893724159538, 1.5970466187357657, 1.5983810158480853, 1.5975930556134765, 1.5971457473061657, 1.5972687303652753, 1.5974035149482242, 1.5977786506715497, 1.5965522993271846, 1.5988257156015666, 1.5974450032569054, 1.5973504483577408, 1.5977213556516832, 1.5981381554867942, 1.5988925956847486, 1.5976588470490316, 1.5978469436907916, 1.5975740265797296, 1.597307730847071, 1.5970893492199312, 1.5981550951513177, 1.5966910336786226, 1.596556215413542, 1.598115947897674, 1.595978583153758, 1.5989839910726527, 1.5967364180504173, 1.5970642967635356, 1.5961561746910613, 1.5955369627696043, 1.5964291627402178, 1.5961006982615351, 1.5964219952510856, 1.5960175463795907, 1.595618278192054, 1.5960273245766423, 1.595890181118458, 1.5963244165972763, 1.5959805299124434, 1.5954938528229323, 1.5957435064981607, 1.5948948409767856, 1.5953215945672694, 1.595709334849332, 1.595116587928678, 1.5954966608258978, 1.5950685039438017, 1.595821300375388, 1.5958488418825842, 1.594538591333973, 1.5949431404196017, 1.5944557494940943, 1.595390036659319, 1.5950606422992213, 1.5942956613564148, 1.5938972501049786, 1.594087782190076, 1.5951650211698465, 1.5935459230959539, 1.5936073350710547, 1.5938345593838232, 1.5938552510322241, 1.5946833857275866, 1.594760602259783, 1.5926148900750725, 1.5938472931879502, 1.5936934046187194, 1.5937303509310776, 1.5992274160992193, 1.5982295554766175, 1.597728849926034, 1.59732067663322, 1.5939436714262443, 1.5946212510063908, 1.594190399602698, 1.5949479189251972, 1.5935686828174631, 1.5950618597026722, 1.5957470217769396, 1.5947705351596495, 1.5948117496541394, 1.59362826391412, 1.5967376025060853, 1.5938576715438029, 1.5944903022699533, 1.5936464959346293, 1.5941470782125264, 1.594074695800609, 1.593430888775193, 1.5934677433918634, 1.5936264934481048, 1.5935414926716924, 1.5930737711320913, 1.5937529827779813, 1.5927713120497717, 1.5933137581823298, 1.5924366800692047, 1.592100713629987, 1.5913789653680164, 1.5917835504367366, 1.5927449949468186, 1.5908970171421215, 1.5911256559086042, 1.590848571808676, 1.5905369787979908, 1.5909317138503465, 1.5900538486132143, 1.5906376467103587, 1.5903200259688455, 1.5927658175051336, 1.59085110183614, 1.5896217508727275, 1.5896987141035421, 1.5894466784455692, 1.590115520596749, 1.5897936756361193, 1.5901637785733358, 1.5888091663560338, 1.5902524233353945, 1.5893562716380283, 1.5919956666977744, 1.5928362523750603, 1.593332130120765, 1.5924509377939746, 1.592093464921877, 1.5924322542958191, 1.5917212260577223, 1.5913109976652957, 1.5922163072797553, 1.5907244018950257, 1.5910572388333706, 1.5911036384179118, 1.5905251882893838, 1.591610089562512, 1.5918856418597869, 1.5922238387611123, 1.592303149166538, 1.5919366879629648, 1.591745519099539, 1.5913027710983152, 1.591268461049215, 1.5905440066629366, 1.5903507591517798, 1.5902411420242497, 1.5907649318785149, 1.5902537896158269, 1.5951265394320477, 1.5921058117731397, 1.5900211499212213, 1.590224415222967, 1.5893710592688965, 1.5893362053610707, 1.5893269020429137, 1.589483923344152, 1.5893762111174252, 1.5892835818277002, 1.5888164992205172, 1.5889247095315608, 1.589255003615816, 1.5890003152941288, 1.5891874573803535, 1.588750140720814, 1.58835003420068, 1.5881655596609723, 1.588163217479933, 1.5880601177959717, 1.5888463739496972, 1.5877290972938773, 1.5871487851015595, 1.5881643886194092, 1.5887459110675162, 1.5875807322026279, 1.5876617200075969, 1.586634117328166, 1.5880212621767174, 1.586940349984218, 1.5863601798149594, 1.5864810176453796, 1.587821851227073, 1.5861903251808527, 1.5857133700372747, 1.5860362074947945, 1.585623595846752, 1.5862401892272353, 1.5848425549403353, 1.5860682957471028, 1.5866264466142752, 1.5851583678130008, 1.5848382812750659, 1.5844130934631067, 1.588771511984557, 1.5843719116226604, 1.5846806293150728, 1.584382467593011, 1.585739983523406, 1.5851719825419557, 1.585055558784297, 1.5849446138562118, 1.5846546748335602, 1.5844845286140208, 1.5840509587489604, 1.5841993502767668, 1.5858100096547874, 1.5838087857871086, 1.5838214682847322, 1.584024294152152, 1.5844226335353186, 1.584955786043124, 1.5835885621194232, 1.5853016235989956, 1.5858567639298018, 1.5832917212950375, 1.5829921955445463, 1.582914176026409, 1.584100317955017, 1.5830588897395672, 1.5826227311480952, 1.5827925883279443, 1.5833818754865894, 1.5833266118713474, 1.583125854615558, 1.582385868066635, 1.5821304369756084, 1.5831027131305828, 1.582803619275103, 1.5826135677968207, 1.583246963616514, 1.5829076870266172, 1.5831158239983436, 1.5831075645325365, 1.5820296927399213, 1.5829342747615838, 1.5824732911170631, 1.5824791279172017, 1.582196367643697, 1.5827385293874408, 1.5820590030975654, 1.5835440269975447, 1.5817789566835094, 1.5813074669554004, 1.5815722969278418], 'acc': [0.22443531828127358, 0.23295687885622224, 0.23295687885010266, 0.23295687885622224, 0.23295687885010266, 0.23295687885316246, 0.2398357289650112, 0.23439425052558616, 0.23295687883786353, 0.23809034906373622, 0.23398357289527721, 0.24055441478439424, 0.238911704312115, 0.2419917864476386, 0.2404517453553985, 0.24147843942505134, 0.24219712526279308, 0.23367556468478462, 0.23449691992092425, 0.23747433264887063, 0.23850102670016474, 0.24168377824020582, 0.23963039014985674, 0.240143737172444, 0.23767967146096533, 0.23501026694657132, 0.23757700205950766, 0.23880903490759753, 0.23963039014373716, 0.23860369609856263, 0.24055441478439424, 0.23552361397527816, 0.23809034908209492, 0.2364476386036961, 0.23583162217965117, 0.2380903490882145, 0.23162217660361492, 0.2363449692114178, 0.24075975359342916, 0.24229979466119098, 0.24034907597535934, 0.2420944558399169, 0.24178644765084284, 0.2376796714640251, 0.23901437371663245, 0.23408624230285444, 0.2395277207422795, 0.23726899384289551, 0.23747433264887063, 0.24404517455022684, 0.24332648870942528, 0.2363449692052982, 0.24168377823408624, 0.24373716633667447, 0.2386036961046822, 0.23891170432435413, 0.23778234087466216, 0.23809034907903515, 0.2406570841950313, 0.24486652976188816, 0.247330595485606, 0.2406570841889117, 0.24537987680283416, 0.24137577002053387, 0.24219712526891266, 0.2357289527751337, 0.23470225872689937, 0.2403490759875985, 0.23973305955437418, 0.23860369609856263, 0.24383983573201257, 0.24404517455022684, 0.23696098562628337, 0.2370636550369204, 0.23809034908209492, 0.23921971251342822, 0.23449691991786448, 0.23870636550613986, 0.2492813141744974, 0.24712525667963087, 0.24209445585521586, 0.24106776179474237, 0.242402464071828, 0.2405544147721551, 0.2415811088295688, 0.24271047228538034, 0.24722792607802874, 0.24168377823714604, 0.2464065708449489, 0.2469199178644764, 0.25154004105552263, 0.25061601642710474, 0.2546201232032854, 0.25030800821661214, 0.25051334702258726, 0.24784394250513347, 0.2512320328664486, 0.2437371663305549, 0.250102669410637, 0.25010266940757725, 0.24733059548866576, 0.24743326489318324, 0.25051334702564704, 0.24969199179868679, 0.2497946611909651, 0.2464065708418891, 0.24753593430382026, 0.2510266940574137, 0.25164271047227926, 0.24763860369609855, 0.24650924023416743, 0.24630390143737166, 0.24845995891999906, 0.24620123204509337, 0.25574948665909697, 0.24876796714579055, 0.2500000000030598, 0.2555441478439425, 0.2509240246528962, 0.2516427104845184, 0.25616016425880805, 0.24702258726899384, 0.251848049269075, 0.2512320328664486, 0.25400410678841984, 0.25184804928743376, 0.25030800821355237, 0.232546201244272, 0.24558521561186905, 0.24496919918476434, 0.24753593430382026, 0.2482546201232033, 0.24681724845995892, 0.2552361396426293, 0.24938398356065614, 0.24928131416837782, 0.24609856262833676, 0.24722792608108854, 0.2492813141744974, 0.25123203285726925, 0.25759753594653073, 0.2416837782463254, 0.25236139629166227, 0.2462012320328542, 0.24897330595788525, 0.2534907597566532, 0.25071868583774176, 0.25071868583774176, 0.24743326489318324, 0.25533880904714673, 0.25297741273100616, 0.2573921971252567, 0.25308008213552363, 0.25205338807810995, 0.25646817249683873, 0.2548254620123203, 0.25595482544977316, 0.2564681724845996, 0.25965092402770046, 0.2591375770051132, 0.25780287474332647, 0.25687885010878897, 0.25616016425880805, 0.25164271047227926, 0.2532854209200802, 0.25513347022893246, 0.2539014373839024, 0.2555441478500621, 0.2492813141744974, 0.24958932239416934, 0.2621149897330595, 0.25400410678841984, 0.2574948665297741, 0.2585215605749487, 0.25318275156451936, 0.25297741273100616, 0.26067761805757605, 0.2586242299794661, 0.2548254620367986, 0.2555441478439425, 0.25112936345581155, 0.24897330595482547, 0.24815195073092497, 0.25410677618375793, 0.25215605750098613, 0.2562628336755647, 0.2584188911704312, 0.2498973305832434, 0.2523613963039014, 0.2558521560330166, 0.24938398358513442, 0.25174537987679674, 0.2508213552422592, 0.24825462012932287, 0.25359342916423044, 0.2512320328664486, 0.2519507186919512, 0.2530800821232845, 0.2526694045235734, 0.2548254620153801, 0.2513347022648465, 0.2501026694167566, 0.2521560574948665, 0.24979466119708466, 0.25195071868583163, 0.2504106776241894, 0.24866529775351226, 0.250718685834682, 0.25051334702870687, 0.25195071868583163, 0.25451745379876795, 0.2564681724907192, 0.25061601642710474, 0.25205338809034905, 0.2541067761868177, 0.2545174538018278, 0.25256673512517547, 0.25308008213552363, 0.25451745378652885, 0.25277207392197126, 0.2519507186735925, 0.2557494866652166, 0.256981519519426, 0.25492813142907694, 0.2539014373594241, 0.25431211498973305, 0.25646817249683873, 0.25605749487876894, 0.25503080083359436, 0.2556468172606991, 0.2555441478470023, 0.25626283368168423, 0.25605749486958956, 0.2512320328419703, 0.2578028747555656, 0.256981519519426, 0.2550308008091161, 0.2524640657084189, 0.2565708418921768, 0.2564681724907192, 0.2536960985503892, 0.25831622176591373, 0.2536960985626283, 0.2572895277268588, 0.25564681724845995, 0.25379876797326534, 0.2560574948665298, 0.2597535934322179, 0.2572895277268588, 0.25451745379876795, 0.25903490759753595, 0.2577002053510482, 0.2568788501026694, 0.2554414784516642, 0.25143737166324437, 0.25605749487264934, 0.2544147844003701, 0.25677618070427155, 0.2578028747494461, 0.25513347021363353, 0.2560574948665298, 0.2551334702258727, 0.2570841889117043, 0.251232032860329, 0.25246406571453844, 0.2490759753624027, 0.2545174538110071, 0.25831622177203334, 0.25739219712831646, 0.2471252566735113, 0.26149897330595484, 0.2596509240246407, 0.25698151951024667, 0.2587268993901031, 0.257289527723799, 0.25318275154616066, 0.2568788501149086, 0.25636550308008216, 0.2589322381991381, 0.2572895277268588, 0.2552361396365097, 0.25728952770850005, 0.2565708418952366, 0.2530800821232845, 0.25811088295687884, 0.25708418891782386, 0.2577002053449286, 0.2535934291581109, 0.25420944558521563, 0.25605749487264934, 0.25708418892394347, 0.25595482547425147, 0.2592402464096306, 0.25513347023811184, 0.2580082135523614, 0.2575975359404112, 0.2537987679671458, 0.26088295689108926, 0.2598562628336756, 0.2585215605749487]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
